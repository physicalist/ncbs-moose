7 Jan 2009

I have been playing around with a new messaging system, one that uses an
intermediate buffer. The big advantage of this sytem is that it should
simplify messaging in multithread and multinode systems. 

Sync messages begin to look a bit like the kinetics optimizations in 
GENESIS. 
Differing: The msg source uses 'send' to place the data in a safe buffer.
Similar: The msg dest scans list of ptrs to places in this buffer.
Similar: Will need an 'ACTION'-like mechanism for calling dest funcs from
clocks.
Possibly this will work a bit faster than existing MOOSE messaging.

Async messages are harder. Need to assemble all outgoing data on any given
thread into an expanding buffer for that thread. Data packets include src id.
These packets are transferred (issues here about selecting for targets).
On target thread, src id used to look up whatever part of the ConnTainer info
specifies the dest elements. Then the packet is delivered. Best if done through
a scan of dests, since that retains similarity with Sync messages. 

Need to introduce a mechanism for calling 'ACTIONS' directly.

=============================================================================
26 Jan 2009

send -> buffer
process -> lookup buffer
trigger -> call func. Could restrict to fixed set, or provide a func lookup 
	index. If fixed set could use virtual funcs.

Or, eliminate trigger and provide only process, proc2, and reinit.
	Would like to be able to call arb funcs.

SendBack, SendTo become harder.


Process op: well defined, clock ticked.
Proc2: ditto
Reinit: ditto.

Then: generic arrival op: Scan for op request. The memory location has
both the operation identifier and the arguments.

Or: build up scan list through messages... sounds like GENESIS.

Or: Alerting mechanism. 'SendTo' or 'SendBack' puts target id on queue

High traffic messages are scheduled.
	Synaptic input is scheduled even though it is sporadic. Total
	traffic is expected to be big.
Low traffic messages are polled by queue. When called, these are added
	to a queue for the target object.
Buffer info includes only data.
	For regular input, like conc and Vm, data only is the conc/Vm.
	For synaptic input the data includes source object info.
	For low traffic messages the data includes complete conn info, plus args
	For sendback messages like channelGates: It can be scheduled, so
		the data includes return info and the op must use this to place
		the response in the right location. Would be nice to do
		efficiently in array form.
	For field assignment: Regular low traffic
	For field readout: Data includes Id for field access object.
		This is a temporary from the command line
		It is a regular object for plots etc.


Design requirements:
- Thread safe
- Buffered data delivery for threading and for multinode operations.
- Very fast for scheduled operations, whether threaded or not.
- Connections remain fully traversible, 
- Connections remain usable bidirectionally by multiple messages.

Design desiderata
- Completely deterministic for single-thread case (consider real-time ops)

=============================================================================
30 Jan 2009
There is a problem to be sorted for any queued buffer messaging: Ensuring that
things go into the buffer without stepping on each others' toes. For
example, sending spikes. If we allocate a separate buffer per thread for
each target object, things get costly and messy. But that may be better 
than mutexes for writing into the buffer.

We expect single 'synapse' objects to manage many input axons, each connecting
with a distinct weight. Suppose 100 of these, so the convergence is up to 300.
Assume a 16-way system, we don't want to manage 16 buffers for each synapse.

Per-clock buffers: An extra step to unsort them. More info to put in to 
identify dest.

Thread-safe queue for writing: There is an extra overhead in mutex juggling
for every 'send', though the subsequent reads can be clean as they are done
on the object owning the queue. If we have per-object queues should be 
manageable.

=============================================================================
31 Jan 2009
A problem with the messaging concept: Can't put the message buf on the
target element, because the data may go to multiple targets. Instead
need the target element to manage ptr to the msg src(s) and read them.
Either that, or have src element push data into multiple target bufs.
Latter makes more sense, given that we may need to push data also into
an MPI buffer or so.
But by the same token data comes into the MPI Recv buffer and needs to
be dispersed. 
So, one way or another, the postmaster must be an active participant
in getting data in or out of the buffer.

Cost:
Pushing data:
Src needs addr for each buffer, N addrs.      Iterate N times.
Each Dest needs buffer, N * datasize.         N dests each 1 lookup.
Multhreading: No change, no need for any locks.
MPI: Push right into MPI buffer. At remote node need to do further push.
Sporadic Msgs: Push into queues of every target. No redundancy.
			: Push into a single queue, later push into target?
				No particular advantage.
Variant on Moose 1.1 approach: Target object guarantees thread-safe
	handling of incoming data... tricky. Need to put each incoming 
	arg into separate location indexed by msg src itself.

Pulling data:
Src needs single buffer, no iteration. 1 addr. No iteration.
Each dest needs addr of buffer: N* ptrsize    N dests each 1 lookup.
Multhreading: No change, no need for any locks.
MPI: Postmasters on src need to pull in data. At remote node usual pull.
Sporadic msgs: Push into a single queue, managed by msg. This
	subsequently needs to do multiple pushes anyway, unless there
	is a very high chance that each entry will be of interest to each
	receiving synapse.
	: Push into multiple queues, basically into the target object.

Seems like pulling data works better for scheduled messages.
Something like pushing needed for sporadic msgs. 
Like old GENESIS.

=============================================================================
1 Feb 2009
Now the location of the buffers.
sched data buffer
- On source object:
	+ No extra storage or management
	. Need to redirect pointers if objects are deleted or moved
	. Need to redirect pointers for zombies. But redirection needed anyway
	- Mixes message transfer with object representation.
- In a separate buffer managed by the Msg:
	+ Management relatively straightforward, set up at msg creation time.
	+ Deleting and moving objects managed along with messages.
	. Zombies could do a hack and reuse the same msg space.
	+ Separates message transfer from object representation.

async data buffers: Synaptic input.
- On dest objects:
	+ Clean synapse management.
	- Mixes message transfer with object representation.
	. Sender must scan through all dests.
	- Extra outgoing buffers.
- In multiple separate buffers managed by each of the Msgs:
	+ Again, management straightforward, set up at msg creation time.
	+ Separates message transfer from object representation.
	- Issue of additional data: weight, release prob, history, etc.
	. Many-to-many msgs have a possible economy of assignment.
	- All targets must scan through all potential sources
- Input Q on Msg, synapse-local Q on objects
	- Initial op: Get data from axon to Msg.
	- Option 1: push onto Msg Q, accumulate on Msg.
		- Thread locking needed.
		- Msg subsequently called on tick to clean input Q.
		- This could be a rare call if syn delay is large.
		- Now it shuffles data into synapse-specific Q.
			- Depending on update rate and convergence onto syn,
				this could be a single entry Q.
			- Multi-thread op here too, but very local.
		- This synapse-specific Q could also be the sorted event Q
		- Object scans base of event Q each timestep.
		- Note that we cannot do event queueing on the incoming
			APs, because of different delays to target.
	- Option 2: Immediately sort onto target synapse Qs.
		- Thread locking needed.
		- This op has to immediately do the shuffling into 
			synapse-specific Q, since otherwise full scan needed.
		- This synapse-specific Q could also be the sorted event Q
		- Object scans base of event Q each timestep.
	- When object pops event Q, it has to locate syn wt, prob, history, etc.
		- Option 1: Msg carries this info.
			+ These are features of projection pattern
			- Need to template whole Syn Msg structure
			+ Projection could compute on fly.
		- Option 2: Target obj carries this info
			+ Local calculations easier. Postsyn compt history too.
			+ No templating needed.
			- Need index in msg for target obj to look up info.

async data buffers: Sporadic input.
- On Dest objects:
	+ Clean data management
	- Need to send out to all targets (But # likely small )
	- Mixes message transfer with object representation
- On Src objects
	+ Single point of assignment
	- All targets must scan through all potential sources
	- Possible economy of assignment.
- On buffers managed by each Msg:
	- All targets must scan through all potential sources
	- Possible economy of assignment.

Seems like the best bet is to have Msgs manage the sync buffers.
The clinching point is that about separation of message data transfer
from objects.
(Added 15 Feb: Another issue overrides this, for async messages: data flow
should be unidirectional, that is, messages cannot be changed by their targets.
This would happen if the spikeQ was on the message rather than the target.)

Implemented a first pass test simulation using reacs and mols. Hard coded
in buffer info. Works. Helps set up requirements for messaging.

In this variant, the Element manages a vector< double* > that points to
the data buffers, and this in turn is referenced by a vector< unsigned int >
which converts the slots into the correct buffer location. The Element
provides some helper functions for doing individual msg data (double) lookup,
and others for taking sums and products of sequences: such ops are common.


Next:
	* Get svn working for this.
	+ Implement sporadic messages for field access
	+ Implement something that uses synaptic messages
	- Implement message setup framework
	- Implement field set/get
	- Array elements and messages
	- Scheduling
	- Multithreading
	- MPI
	- Implement distribution of elements (entries) on threads and nodes.
	- Benchmarking

=============================================================================
2 Feb 2009
Svn now working, the path is
https://moose.svn.sourceforge.net/svnroot/moose/moose/branches/Msg

For sporadic messages:
- Use indexing equivalent to Finfo definitions.
- Use a template for an adaptor function from char* args to the class-specific
	func. The adaptor func can also do the typecasting for the class itself.

Need also a queue for synaptic input. Scan on sched, but variable # of entries.
=============================================================================
8 Feb 2009
For async messages, no point in defining a specific class for the 
data packets. There will always be a FuncId but after that no telling 
what args to take.

Implemented a first simple pass at async messages for field access.
Checked in as revision 1009.

Trying to template it out. No luck.
=============================================================================
10 Feb 2009
After some more template contortions, it works. I'm not sure if this beast
will compile on other systems, though.
Checked in as revision 1010.

Siji tested it on Windows. Somewhat to my surprise, it works there too. Mac
is OK too.

=============================================================================
12 Feb 2009

First pass design for synaptic messaging, based on above description 
dated Feb 1 (though it has been updated since).

Upon tick at src:
src -> Msg -> scan through list of targets -> (threadlock) target-specific Q
Msg contains all the synaptic info, including weight, delay etc.
Msg also manages a Q for each target synapse object.

Upon tick at target synapse: Query Msg Q. Collect all data if event arrived.

=============================================================================
15 Feb 2009

Question: Should the Msg be const?
	- Gets messy with bidirectional data flow in plastic synapses.
		If we keep Msg const, then there are separate synaptic state
		variables needed on the target.
		If we allow it to vary, then the target has to write to Msg.
	- The synapse Q itself involves bidirectional data flow. Even if the
		Msg manages the pushing internally, the object has to tell it
		to pop. Not good.
	- These are issues with bidirectional data flow. However, it does
		not mean that Msgs have to be constant objects. For example,
		we could still have a projection as an object with regular
		Msg and other inputs, which could change during the
		simulation. But it would also adhere to the rule that it
		gets input from msgs, but does not affect the msgs.
Summary: 
	- Q cannot be on Msg.
	- A Msg is const from viewpoint of target: Unidirectional data flow.
	- A Msg can however be a normal variable element with a clock tick,
		and other incoming msgs.

Accessing Msg info:
	- A Msg is an edge between individual src and dest entries in arrays.
		Either src or dest entry can access through independent indices.
	- A Msg is also an edge between the array containers. Likewise.
	- A Msg may (should?) be an accessible object with fields. Name
		could be msg field name.
		setfield /foo/axon/projection[] delay 1e-3
		setfield /bar/GluR/incoming[23] weight 5
		showfield /bar/GluR/incoming[]/src
		showfield /foo/axon/projection[][dest=/bar/#/incoming] weight



Data structures for synapses:

* Fix up tests so they use assertions, not printouts
* Check in. Currently at revision 1012.
- Start setting up synapses.
	I have a skeleton in place now, in the uncompiled files
	Send.cpp, Msg.h. There are still-to-be-fixed changes to
	Element.h and Data.h to let us access Data::pushQ and
	to have indices into multiple data entries within Element.
	The threading stuff has to be dealt with at the Element.cpp
	level to lock out attempts to push onto the same Data.


A preliminary design for Elements and messages:
- All elements are array elements. 
	- The Data* d_ becomes a vector.
	- procBuf remains as is, but its range is looked up using
		indexing for element index.
- Conns are all many2many or equivalent all-to-all variants.
	- They all connect one element (the whole array) to another.
	- Typical sparse matrix implementation
	- Usually traversed only for async (spike) messages, so 
		bidrectional traversal is rare.
	- Element manages vector of vector of Conn*. 
		- First index for slot
		- Second index for various other targets.

=============================================================================
16 Feb 2009
Implemented a first pass at the framework for a lot of this stuff.
Compiles. Not in the test. 
Checked in. Currently at revision 1018.

=============================================================================
17 Feb 2009
First test for IntFire: directly insert synaptic events into its queue.
Confirmed that it responds correctly. Generates another time-series output,
need to set up to handle assertions.
Checked in as revision 1019.

Set up above test to use assertions rather than printout.
Checked in as revision 1020.

Implemented a recurrent spiking loop using messaging. Seems to work fine.
Checked in as revision 1021.

Next:
Attach sync messages also to the Msg data structure. Key issue is who
owns and manages the buffer. This has been discussed above on 1 Feb.
- Msg: Favoured because it separates data transfer from object,
	However it mixes data transfer with connectivity specification.
- (originating) Element: This also separates data transfer, easier to manage.
- Object: This has been ruled out.

Managing connection info in messages:
	- Can extract ptr info from the finfo + message, use to fill ptr bufs
		for sync messages
	- Message slots predefined only for small set invoked by Process or
		initPhase. Rest go via a map to look up entry in MsgVec.

Field assignment
	- Set inserts a request into the queue.
	- Get either needs formation of a temporary message...
		or: Inserts a request with additional info to tell where to
		send data back to?
=============================================================================
18 Feb 2009
How do we set up the locations to use in the data buffer?
The object knows ahead of time exactly what it has to place during
process and other calls. This is built into the Finfo for the msg
source. So at element creation time we can build the send buffer.
The only time this will change is if the element is resized or deleted.

=============================================================================
19 Feb 2009

Reconfigured Element to use vector of Data ptrs.
Some patching later, it compiles and runs again.

Next: Begin to set up the benchmarks for sync and async messaging.
	Look at memory and speed scaling as we do so.
	- run reac system with 1 to 1e8 Data entries
		- Ordered messages
		- Sparse matrix messages
		- Many individual elements.
	- run spiking system ditto.

Gear up with this for testing with multi-threading.

=============================================================================
20 Feb 2009

Checked in as revision 1023

Setting up a main.cpp::testSyncArray( unsigned int size ) function to
build a reaction model with a big test array for a reaction scheme

=============================================================================
21 Feb 2009
Compiled testSyncArray: OK. Ran it: Failed.

=============================================================================
22 Feb 2009.
Checked in as revision 1032.

Got the testSyncArray stuff to work. Did a little profiling. Spends
about 25% of its time in Element::sumBuf. 
Here are the timings of a single A <===> B reaction on this 1.4Ghz machine
(Lenovo X301), using O3:
syncArray10     ops=100000      time=0.057695
syncArray100    ops=1e+06       time=0.313191
syncArray1000   ops=1e+07       time=3.13012
syncArray10000  ops=1e+08       time=31.7042

Takes about 4 times longer with debugging and no optimization:
syncArray10     ops=100000      time=0.170935
syncArray100    ops=1e+06       time=1.18491
syncArray1000   ops=1e+07       time=11.4804
syncArray10000  ops=1e+08       time=115.368

For reference, genesis does:
completed 1000001 steps in 39.670479 cpu seconds 
(for spine_v59.g, which is a model with 104 mols, 55 reacs and 76 enz).
This is about 2e8 ops, so genesis is almost 2x faster. Amazing. This new 
messaging was supposed to go much faster.

Checked in as revision 1033.
Decide whether further optimization comes first, or the threading.

Did some minor tweaking to use const iterators. This gives about 
3% improvement, useful, but not too exciting.
Checked in as revision 1034.

Implemented skeleton code for threads: creates and joins threads only.
Compiles, runs.
Checked in as revision 1040.

For threading:
- simplest approach, probably not practical:
	Launch a thread for each processor on each clock tick.
	Thread computes process for subset of entries.
	Join threads after clock tick.
	Problem is that thread launches and joins are not cheap.
- Condition approach, trickier but maybe faster:
	Launch threads for each processor when calculations starts.
	Thread runs independent scheduling.
	At end of each tick:
	mutex to increment count # of threads completed.
		If all threads done
			signals to master thread/all other threads
		else
			cond_wait for count
	close mutex
	go on to next tick.
- pthread_barriers
	This is probably the cleanest. 
	Launch threads for each processor when calculations starts.
	Thread runs independent scheduling.
	At end of each tick, put a barrier.
	Problem is that pthread barriers are reported to be very slow.
	Let's see.
- busy-loop barriers
	Same as above, only don't use pthreads barriers. Instead do a
	mutex-protected increment of # of threads completed,
	close mutex
	and do a busy-loop barrier on the # of threads completed.
	
Working on pthread_barrier based implementation.

Well, it looks like it works. The thread advantage isn't huge on my
2-core laptop:
syncArray10     ops=100000      time=0.056823
syncArray100    ops=1000000     time=0.311645
syncArray1000   ops=10000000    time=3.16343
syncArray10000  ops=100000000   time=31.2464
Main: creating 2 threads
syncArray10     ops=100000      time=0.25066
syncArray100    ops=1000000     time=0.524708
syncArray1000   ops=10000000    time=2.19136
syncArray10000  ops=100000000   time=19.7902
Main: creating 4 threads
syncArray10     ops=100000      time=0.755361
syncArray100    ops=1000000     time=0.970983
syncArray1000   ops=10000000    time=2.61468
syncArray10000  ops=100000000   time=22.7247

However, I need to do a lot more testing, including confirming that it
gives the right answers. Do the current calculations go across threads?

Also I need to check if the 4-core opteron nodes do better.
Checked in as revision 1048.

Did check that calculations give the right answers.
Checked in as revision 1049.

=============================================================================
24 Feb 2009
Ran on 4-CPU opteron (2 chips x 2 cores each). 
Original( 1 thread ):
syncArray10     ops=100000      time=0.02312
syncArray100    ops=1000000     time=0.308771
syncArray1000   ops=10000000    time=3.13899
syncArray10000  ops=100000000   time=32.1735

Main: creating 2 threads
syncArray10     ops=100000      time=0.111654
syncArray100    ops=1000000     time=0.26688
syncArray1000   ops=10000000    time=1.77432
syncArray10000  ops=100000000   time=17.8221

Main: creating 4 threads
syncArray10     ops=100000      time=0.363325
syncArray100    ops=1000000     time=0.522495
syncArray1000   ops=10000000    time=2.02002
syncArray10000  ops=100000000   time=9.49299

Well, that is interesting. it goes 1.8x faster on 2 cores, and 3.4x faster on 
4 cores. Not linear scaling, but not bad either. But this is only effective
for large arrays. The barrier overhead looks pretty bad.
Successive runs on the same node cause marked improvements in single-node
performance, but not as steep for threading. For example, three runs later
we have:

1 thread:
syncArray10     ops=100000      time=0.023302
syncArray100    ops=1000000     time=0.274423
syncArray1000   ops=10000000    time=2.51467
syncArray10000  ops=100000000   time=27.6373

Main: creating 2 threads
syncArray10     ops=100000      time=0.141666
syncArray100    ops=1000000     time=0.263635
syncArray1000   ops=10000000    time=1.71346
syncArray10000  ops=100000000   time=17.0478

Main: creating 4 threads
syncArray10     ops=100000      time=0.315527
syncArray100    ops=1000000     time=0.420413
syncArray1000   ops=10000000    time=1.25926
syncArray10000  ops=100000000   time=9.42896


Trying now my own implementation of barriers. I would have liked to try it
on gj, but time to go and still debugging. 
Here is the status:
c0 thread=0, cycle = 0counter = 1
c1 Main: waiting for threads
thread=1, cycle = 1counter = 0
thread=1, cycle = 1counter = 1

Implies that we've gone through the barrier withough letting thread 0 do so.

Here is more info:
c0 thread=0, cycle = 0, counter = 1, tc[0] = 0, tc[1] = 0
c1 thread=1, cycle = 1, counter = 0, tc[0] = 0, tc[1] = 0
thread=1, cycle = 1, counter = 1, tc[0] = 0, tc[1] = 1

I fixed this by making the 'cycle' variable a volatile. But, at least on my
laptop, there is not much improvement:

1 thread:
syncArray10     ops=100000     		time=0.055594	0.0575	0.056	0.057
syncArray100    ops=1000000    		time=0.311743	0.3118	0.303	0.322
syncArray1000   ops=10000000   		time=3.07243	3.147	3.09	3.11
syncArray10000  ops=100000000  		time=31.0668	31.4	30.8	30.9
Main: creating 2 threads, pthreads barrier
syncArray10 	ops=100000      	time=0.241881	0.8	0.82	0.505
syncArray100        ops=1000000     	time=0.606399	0.57	0.78	0.66
syncArray1000       ops=10000000    	time=2.12019	2.03	2.51	2.06
syncArray10000      ops=100000000   	time=18.5136	18.2	19.5	18.1

Main: creating 2 threads, my barrier:
syncArray10 	ops=100000     		time=0.053082	0.032	0.07	0.08
syncArray100        ops=1000000     	time=0.294042	0.18	0.21	0.23
syncArray1000       ops=10000000    	time=1.85085	1.77	2.0	1.76
syncArray10000      ops=100000000   	time=18.137	17.6	19.5	17.5

more seriously, it fails if # threads > # processors. Why?
Tried making counter also volatile. Doesn't help.

One good thing is that MyBarrier seems to have much less overhead: its
speedup is respectable and consistent even for 100 entries in the array.
Based on the single-thread timings for 10 entries, I estimate it costs
around 0.05 sec / 10K ~ 5 usec per invocation on my laptop. The 
pthreads barrier is around 0.8 sec / 10K = 80 usec.

Let's see how it scales on the cluster nodes.

Well, that was entertaining. Two things to try:
- multithreading on the main MOOSE kinetic solver
	Looked at it. Should work reasonably well for bigger models with
	>100 molecules. But I'll have to write my own RK5 solver for multi
	threading because the GSL one has a monolithic driver for the 
	calculations that could be split between threads.
- contine with implementation for the synaptic input queue code.
	Looked at it. A major issue turned up: the 'process' call
	manipulates both target and source spike queues. The 'sendSpike'
	function after much indirection pushes the spike info onto all 
	target queues. The 'process' function examines and pops the local
	queue. I need to modify this so that the push and test/pop are on
	different clock ticks. This may well happen in more realistic
	neuron models. Here I can separate the harvesting of the spikes
	onto a different tick than the testing and sending spike msgs.
	With this fixed, I need to protect the spike msg handling.
	Mutexes are a blunt instrument here, because they protect code
	rather than individual data structures. Ideally I want only to
	protect the buffer(s) I am working on.
	I've suggested an approach to this in the Element::addSpike 
	function:
		// mutex lock
		// Check if index is busy: bool vector
		// Flag index as busy
		// release mutex
		// do stuff
		// ?unflag index
		// Carry merrily on.
	but this has many loose ends.


=============================================================================
25 Feb 2009
Checking it in so that I can run the tests on gj.

=============================================================================
