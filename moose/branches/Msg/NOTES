7 Jan 2009

I have been playing around with a new messaging system, one that uses an
intermediate buffer. The big advantage of this sytem is that it should
simplify messaging in multithread and multinode systems. 

Sync messages begin to look a bit like the kinetics optimizations in 
GENESIS. 
Differing: The msg source uses 'send' to place the data in a safe buffer.
Similar: The msg dest scans list of ptrs to places in this buffer.
Similar: Will need an 'ACTION'-like mechanism for calling dest funcs from
clocks.
Possibly this will work a bit faster than existing MOOSE messaging.

Async messages are harder. Need to assemble all outgoing data on any given
thread into an expanding buffer for that thread. Data packets include src id.
These packets are transferred (issues here about selecting for targets).
On target thread, src id used to look up whatever part of the ConnTainer info
specifies the dest elements. Then the packet is delivered. Best if done through
a scan of dests, since that retains similarity with Sync messages. 

Need to introduce a mechanism for calling 'ACTIONS' directly.

=============================================================================
26 Jan 2009

send -> buffer
process -> lookup buffer
trigger -> call func. Could restrict to fixed set, or provide a func lookup 
	index. If fixed set could use virtual funcs.

Or, eliminate trigger and provide only process, proc2, and reinit.
	Would like to be able to call arb funcs.

SendBack, SendTo become harder.


Process op: well defined, clock ticked.
Proc2: ditto
Reinit: ditto.

Then: generic arrival op: Scan for op request. The memory location has
both the operation identifier and the arguments.

Or: build up scan list through messages... sounds like GENESIS.

Or: Alerting mechanism. 'SendTo' or 'SendBack' puts target id on queue

High traffic messages are scheduled.
	Synaptic input is scheduled even though it is sporadic. Total
	traffic is expected to be big.
Low traffic messages are polled by queue. When called, these are added
	to a queue for the target object.
Buffer info includes only data.
	For regular input, like conc and Vm, data only is the conc/Vm.
	For synaptic input the data includes source object info.
	For low traffic messages the data includes complete conn info, plus args
	For sendback messages like channelGates: It can be scheduled, so
		the data includes return info and the op must use this to place
		the response in the right location. Would be nice to do
		efficiently in array form.
	For field assignment: Regular low traffic
	For field readout: Data includes Id for field access object.
		This is a temporary from the command line
		It is a regular object for plots etc.


Design requirements:
- Thread safe
- Buffered data delivery for threading and for multinode operations.
- Very fast for scheduled operations, whether threaded or not.
- Connections remain fully traversible, 
- Connections remain usable bidirectionally by multiple messages.

Design desiderata
- Completely deterministic for single-thread case (consider real-time ops)

=============================================================================
30 Jan 2009
There is a problem to be sorted for any queued buffer messaging: Ensuring that
things go into the buffer without stepping on each others' toes. For
example, sending spikes. If we allocate a separate buffer per thread for
each target object, things get costly and messy. But that may be better 
than mutexes for writing into the buffer.

We expect single 'synapse' objects to manage many input axons, each connecting
with a distinct weight. Suppose 100 of these, so the convergence is up to 300.
Assume a 16-way system, we don't want to manage 16 buffers for each synapse.

Per-clock buffers: An extra step to unsort them. More info to put in to 
identify dest.

Thread-safe queue for writing: There is an extra overhead in mutex juggling
for every 'send', though the subsequent reads can be clean as they are done
on the object owning the queue. If we have per-object queues should be 
manageable.

=============================================================================
31 Jan 2009
A problem with the messaging concept: Can't put the message buf on the
target element, because the data may go to multiple targets. Instead
need the target element to manage ptr to the msg src(s) and read them.
Either that, or have src element push data into multiple target bufs.
Latter makes more sense, given that we may need to push data also into
an MPI buffer or so.
But by the same token data comes into the MPI Recv buffer and needs to
be dispersed. 
So, one way or another, the postmaster must be an active participant
in getting data in or out of the buffer.

Cost:
Pushing data:
Src needs addr for each buffer, N addrs.      Iterate N times.
Each Dest needs buffer, N * datasize.         N dests each 1 lookup.
Multhreading: No change, no need for any locks.
MPI: Push right into MPI buffer. At remote node need to do further push.
Sporadic Msgs: Push into queues of every target. No redundancy.
			: Push into a single queue, later push into target?
				No particular advantage.
Variant on Moose 1.1 approach: Target object guarantees thread-safe
	handling of incoming data... tricky. Need to put each incoming 
	arg into separate location indexed by msg src itself.

Pulling data:
Src needs single buffer, no iteration. 1 addr. No iteration.
Each dest needs addr of buffer: N* ptrsize    N dests each 1 lookup.
Multhreading: No change, no need for any locks.
MPI: Postmasters on src need to pull in data. At remote node usual pull.
Sporadic msgs: Push into a single queue, managed by msg. This
	subsequently needs to do multiple pushes anyway, unless there
	is a very high chance that each entry will be of interest to each
	receiving synapse.
	: Push into multiple queues, basically into the target object.

Seems like pulling data works better for scheduled messages.
Something like pushing needed for sporadic msgs. 
Like old GENESIS.

=============================================================================
1 Feb 2009
Now the location of the buffers.
sched data buffer
- On source object:
	+ No extra storage or management
	. Need to redirect pointers if objects are deleted or moved
	. Need to redirect pointers for zombies. But redirection needed anyway
	- Mixes message transfer with object representation.
- In a separate buffer managed by the Msg:
	+ Management relatively straightforward, set up at msg creation time.
	+ Deleting and moving objects managed along with messages.
	. Zombies could do a hack and reuse the same msg space.
	+ Separates message transfer from object representation.

async data buffers: Synaptic input.
- On dest objects:
	+ Clean synapse management.
	- Mixes message transfer with object representation.
	. Sender must scan through all dests.
	- Extra outgoing buffers.
- In multiple separate buffers managed by each of the Msgs:
	+ Again, management straightforward, set up at msg creation time.
	+ Separates message transfer from object representation.
	- Issue of additional data: weight, release prob, history, etc.
	. Many-to-many msgs have a possible economy of assignment.
	- All targets must scan through all potential sources
- Input Q on Msg, synapse-local Q on objects
	- Initial op: Get data from axon to Msg.
	- Option 1: push onto Msg Q, accumulate on Msg.
		- Thread locking needed.
		- Msg subsequently called on tick to clean input Q.
		- This could be a rare call if syn delay is large.
		- Now it shuffles data into synapse-specific Q.
			- Depending on update rate and convergence onto syn,
				this could be a single entry Q.
			- Multi-thread op here too, but very local.
		- This synapse-specific Q could also be the sorted event Q
		- Object scans base of event Q each timestep.
		- Note that we cannot do event queueing on the incoming
			APs, because of different delays to target.
	- Option 2: Immediately sort onto target synapse Qs.
		- Thread locking needed.
		- This op has to immediately do the shuffling into 
			synapse-specific Q, since otherwise full scan needed.
		- This synapse-specific Q could also be the sorted event Q
		- Object scans base of event Q each timestep.
	- When object pops event Q, it has to locate syn wt, prob, history, etc.
		- Option 1: Msg carries this info.
			+ These are features of projection pattern
			- Need to template whole Syn Msg structure
			+ Projection could compute on fly.
		- Option 2: Target obj carries this info
			+ Local calculations easier. Postsyn compt history too.
			+ No templating needed.
			- Need index in msg for target obj to look up info.

async data buffers: Sporadic input.
- On Dest objects:
	+ Clean data management
	- Need to send out to all targets (But # likely small )
	- Mixes message transfer with object representation
- On Src objects
	+ Single point of assignment
	- All targets must scan through all potential sources
	- Possible economy of assignment.
- On buffers managed by each Msg:
	- All targets must scan through all potential sources
	- Possible economy of assignment.

Seems like the best bet is to have Msgs manage the sync buffers.
The clinching point is that about separation of message data transfer
from objects.
(Added 15 Feb: Another issue overrides this, for async messages: data flow
should be unidirectional, that is, messages cannot be changed by their targets.
This would happen if the spikeQ was on the message rather than the target.)

Implemented a first pass test simulation using reacs and mols. Hard coded
in buffer info. Works. Helps set up requirements for messaging.

In this variant, the Element manages a vector< double* > that points to
the data buffers, and this in turn is referenced by a vector< unsigned int >
which converts the slots into the correct buffer location. The Element
provides some helper functions for doing individual msg data (double) lookup,
and others for taking sums and products of sequences: such ops are common.


Next:
	* Get svn working for this.
	+ Implement sporadic messages for field access
	+ Implement something that uses synaptic messages
	- Implement message setup framework
	- Implement field set/get
	- Array elements and messages
	- Scheduling
	- Multithreading
	- MPI
	- Implement distribution of elements (entries) on threads and nodes.
	- Benchmarking

=============================================================================
2 Feb 2009
Svn now working, the path is
https://moose.svn.sourceforge.net/svnroot/moose/moose/branches/Msg

For sporadic messages:
- Use indexing equivalent to Finfo definitions.
- Use a template for an adaptor function from char* args to the class-specific
	func. The adaptor func can also do the typecasting for the class itself.

Need also a queue for synaptic input. Scan on sched, but variable # of entries.
=============================================================================
8 Feb 2009
For async messages, no point in defining a specific class for the 
data packets. There will always be a FuncId but after that no telling 
what args to take.

Implemented a first simple pass at async messages for field access.
Checked in as revision 1009.

Trying to template it out. No luck.
=============================================================================
10 Feb 2009
After some more template contortions, it works. I'm not sure if this beast
will compile on other systems, though.
Checked in as revision 1010.

Siji tested it on Windows. Somewhat to my surprise, it works there too. Mac
is OK too.

=============================================================================
12 Feb 2009

First pass design for synaptic messaging, based on above description 
dated Feb 1 (though it has been updated since).

Upon tick at src:
src -> Msg -> scan through list of targets -> (threadlock) target-specific Q
Msg contains all the synaptic info, including weight, delay etc.
Msg also manages a Q for each target synapse object.

Upon tick at target synapse: Query Msg Q. Collect all data if event arrived.

=============================================================================
15 Feb 2009

Question: Should the Msg be const?
	- Gets messy with bidirectional data flow in plastic synapses.
		If we keep Msg const, then there are separate synaptic state
		variables needed on the target.
		If we allow it to vary, then the target has to write to Msg.
	- The synapse Q itself involves bidirectional data flow. Even if the
		Msg manages the pushing internally, the object has to tell it
		to pop. Not good.
	- These are issues with bidirectional data flow. However, it does
		not mean that Msgs have to be constant objects. For example,
		we could still have a projection as an object with regular
		Msg and other inputs, which could change during the
		simulation. But it would also adhere to the rule that it
		gets input from msgs, but does not affect the msgs.
Summary: 
	- Q cannot be on Msg.
	- A Msg is const from viewpoint of target: Unidirectional data flow.
	- A Msg can however be a normal variable element with a clock tick,
		and other incoming msgs.

Accessing Msg info:
	- A Msg is an edge between individual src and dest entries in arrays.
		Either src or dest entry can access through independent indices.
	- A Msg is also an edge between the array containers. Likewise.
	- A Msg may (should?) be an accessible object with fields. Name
		could be msg field name.
		setfield /foo/axon/projection[] delay 1e-3
		setfield /bar/GluR/incoming[23] weight 5
		showfield /bar/GluR/incoming[]/src
		showfield /foo/axon/projection[][dest=/bar/#/incoming] weight



Data structures for synapses:

* Fix up tests so they use assertions, not printouts
* Check in. Currently at revision 1012.
- Start setting up synapses.
	I have a skeleton in place now, in the uncompiled files
	Send.cpp, Msg.h. There are still-to-be-fixed changes to
	Element.h and Data.h to let us access Data::pushQ and
	to have indices into multiple data entries within Element.
	The threading stuff has to be dealt with at the Element.cpp
	level to lock out attempts to push onto the same Data.


A preliminary design for Elements and messages:
- All elements are array elements. 
	- The Data* d_ becomes a vector.
	- procBuf remains as is, but its range is looked up using
		indexing for element index.
- Conns are all many2many or equivalent all-to-all variants.
	- They all connect one element (the whole array) to another.
	- Typical sparse matrix implementation
	- Usually traversed only for async (spike) messages, so 
		bidrectional traversal is rare.
	- Element manages vector of vector of Conn*. 
		- First index for slot
		- Second index for various other targets.

=============================================================================
16 Feb 2009
Implemented a first pass at the framework for a lot of this stuff.
Compiles. Not in the test. 
Checked in. Currently at revision 1018.

=============================================================================
17 Feb 2009
First test for IntFire: directly insert synaptic events into its queue.
Confirmed that it responds correctly. Generates another time-series output,
need to set up to handle assertions.
Checked in as revision 1019.

Set up above test to use assertions rather than printout.
Checked in as revision 1020.

Implemented a recurrent spiking loop using messaging. Seems to work fine.
Checked in as revision 1021.

Next:
Attach sync messages also to the Msg data structure. Key issue is who
owns and manages the buffer. This has been discussed above on 1 Feb.
- Msg: Favoured because it separates data transfer from object,
	However it mixes data transfer with connectivity specification.
- (originating) Element: This also separates data transfer, easier to manage.
- Object: This has been ruled out.

Managing connection info in messages:
	- Can extract ptr info from the finfo + message, use to fill ptr bufs
		for sync messages
	- Message slots predefined only for small set invoked by Process or
		initPhase. Rest go via a map to look up entry in MsgVec.

Field assignment
	- Set inserts a request into the queue.
	- Get either needs formation of a temporary message...
		or: Inserts a request with additional info to tell where to
		send data back to?
=============================================================================
18 Feb 2009
How do we set up the locations to use in the data buffer?
The object knows ahead of time exactly what it has to place during
process and other calls. This is built into the Finfo for the msg
source. So at element creation time we can build the send buffer.
The only time this will change is if the element is resized or deleted.

=============================================================================
19 Feb 2009

Reconfigured Element to use vector of Data ptrs.
Some patching later, it compiles and runs again.

Next: Begin to set up the benchmarks for sync and async messaging.
	Look at memory and speed scaling as we do so.
	- run reac system with 1 to 1e8 Data entries
		- Ordered messages
		- Sparse matrix messages
		- Many individual elements.
	- run spiking system ditto.

Gear up with this for testing with multi-threading.

=============================================================================
20 Feb 2009

Checked in as revision 1023

Setting up a main.cpp::testSyncArray( unsigned int size ) function to
build a reaction model with a big test array for a reaction scheme

=============================================================================
21 Feb 2009
Compiled testSyncArray: OK. Ran it: Failed.

=============================================================================
22 Feb 2009.
Checked in as revision 1032.

Got the testSyncArray stuff to work. Did a little profiling. Spends
about 25% of its time in Element::sumBuf. 
Here are the timings of a single A <===> B reaction on this 1.4Ghz machine
(Lenovo X301), using O3:
syncArray10     ops=100000      time=0.057695
syncArray100    ops=1e+06       time=0.313191
syncArray1000   ops=1e+07       time=3.13012
syncArray10000  ops=1e+08       time=31.7042

Takes about 4 times longer with debugging and no optimization:
syncArray10     ops=100000      time=0.170935
syncArray100    ops=1e+06       time=1.18491
syncArray1000   ops=1e+07       time=11.4804
syncArray10000  ops=1e+08       time=115.368

For reference, genesis does:
completed 1000001 steps in 39.670479 cpu seconds 
(for spine_v59.g, which is a model with 104 mols, 55 reacs and 76 enz).
This is about 2e8 ops, so genesis is almost 2x faster. Amazing. This new 
messaging was supposed to go much faster.

Checked in as revision 1033.
Decide whether further optimization comes first, or the threading.

Did some minor tweaking to use const iterators. This gives about 
3% improvement, useful, but not too exciting.
Checked in as revision 1034.

Implemented skeleton code for threads: creates and joins threads only.
Compiles, runs.
Checked in as revision 1040.

For threading:
- simplest approach, probably not practical:
	Launch a thread for each processor on each clock tick.
	Thread computes process for subset of entries.
	Join threads after clock tick.
	Problem is that thread launches and joins are not cheap.
- Condition approach, trickier but maybe faster:
	Launch threads for each processor when calculations starts.
	Thread runs independent scheduling.
	At end of each tick:
	mutex to increment count # of threads completed.
		If all threads done
			signals to master thread/all other threads
		else
			cond_wait for count
	close mutex
	go on to next tick.
- pthread_barriers
	This is probably the cleanest. 
	Launch threads for each processor when calculations starts.
	Thread runs independent scheduling.
	At end of each tick, put a barrier.
	Problem is that pthread barriers are reported to be very slow.
	Let's see.
- busy-loop barriers
	Same as above, only don't use pthreads barriers. Instead do a
	mutex-protected increment of # of threads completed,
	close mutex
	and do a busy-loop barrier on the # of threads completed.
	
Working on pthread_barrier based implementation.

Well, it looks like it works. The thread advantage isn't huge on my
2-core laptop:
syncArray10     ops=100000      time=0.056823
syncArray100    ops=1000000     time=0.311645
syncArray1000   ops=10000000    time=3.16343
syncArray10000  ops=100000000   time=31.2464
Main: creating 2 threads
syncArray10     ops=100000      time=0.25066
syncArray100    ops=1000000     time=0.524708
syncArray1000   ops=10000000    time=2.19136
syncArray10000  ops=100000000   time=19.7902
Main: creating 4 threads
syncArray10     ops=100000      time=0.755361
syncArray100    ops=1000000     time=0.970983
syncArray1000   ops=10000000    time=2.61468
syncArray10000  ops=100000000   time=22.7247

However, I need to do a lot more testing, including confirming that it
gives the right answers. Do the current calculations go across threads?

Also I need to check if the 4-core opteron nodes do better.
Checked in as revision 1048.

Did check that calculations give the right answers.
Checked in as revision 1049.

=============================================================================
24 Feb 2009
Ran on 4-CPU opteron (2 chips x 2 cores each). 
Original( 1 thread ):
syncArray10     ops=100000      time=0.02312
syncArray100    ops=1000000     time=0.308771
syncArray1000   ops=10000000    time=3.13899
syncArray10000  ops=100000000   time=32.1735

Main: creating 2 threads
syncArray10     ops=100000      time=0.111654
syncArray100    ops=1000000     time=0.26688
syncArray1000   ops=10000000    time=1.77432
syncArray10000  ops=100000000   time=17.8221

Main: creating 4 threads
syncArray10     ops=100000      time=0.363325
syncArray100    ops=1000000     time=0.522495
syncArray1000   ops=10000000    time=2.02002
syncArray10000  ops=100000000   time=9.49299

Well, that is interesting. it goes 1.8x faster on 2 cores, and 3.4x faster on 
4 cores. Not linear scaling, but not bad either. But this is only effective
for large arrays. The barrier overhead looks pretty bad.
Successive runs on the same node cause marked improvements in single-node
performance, but not as steep for threading. For example, three runs later
we have:

1 thread:
syncArray10     ops=100000      time=0.023302
syncArray100    ops=1000000     time=0.274423
syncArray1000   ops=10000000    time=2.51467
syncArray10000  ops=100000000   time=27.6373

Main: creating 2 threads
syncArray10     ops=100000      time=0.141666
syncArray100    ops=1000000     time=0.263635
syncArray1000   ops=10000000    time=1.71346
syncArray10000  ops=100000000   time=17.0478

Main: creating 4 threads
syncArray10     ops=100000      time=0.315527
syncArray100    ops=1000000     time=0.420413
syncArray1000   ops=10000000    time=1.25926
syncArray10000  ops=100000000   time=9.42896


Trying now my own implementation of barriers. I would have liked to try it
on gj, but time to go and still debugging. 
Here is the status:
c0 thread=0, cycle = 0counter = 1
c1 Main: waiting for threads
thread=1, cycle = 1counter = 0
thread=1, cycle = 1counter = 1

Implies that we've gone through the barrier withough letting thread 0 do so.

Here is more info:
c0 thread=0, cycle = 0, counter = 1, tc[0] = 0, tc[1] = 0
c1 thread=1, cycle = 1, counter = 0, tc[0] = 0, tc[1] = 0
thread=1, cycle = 1, counter = 1, tc[0] = 0, tc[1] = 1

I fixed this by making the 'cycle' variable a volatile. But, at least on my
laptop, there is not much improvement:

1 thread:
syncArray10     ops=100000     		time=0.055594	0.0575	0.056	0.057
syncArray100    ops=1000000    		time=0.311743	0.3118	0.303	0.322
syncArray1000   ops=10000000   		time=3.07243	3.147	3.09	3.11
syncArray10000  ops=100000000  		time=31.0668	31.4	30.8	30.9
Main: creating 2 threads, pthreads barrier
syncArray10 	ops=100000      	time=0.241881	0.8	0.82	0.505
syncArray100        ops=1000000     	time=0.606399	0.57	0.78	0.66
syncArray1000       ops=10000000    	time=2.12019	2.03	2.51	2.06
syncArray10000      ops=100000000   	time=18.5136	18.2	19.5	18.1

Main: creating 2 threads, my barrier:
syncArray10 	ops=100000     		time=0.053082	0.032	0.07	0.08
syncArray100        ops=1000000     	time=0.294042	0.18	0.21	0.23
syncArray1000       ops=10000000    	time=1.85085	1.77	2.0	1.76
syncArray10000      ops=100000000   	time=18.137	17.6	19.5	17.5

more seriously, it fails if # threads > # processors. Why?
Tried making counter also volatile. Doesn't help.

One good thing is that MyBarrier seems to have much less overhead: its
speedup is respectable and consistent even for 100 entries in the array.
Based on the single-thread timings for 10 entries, I estimate it costs
around 0.05 sec / 10K ~ 5 usec per invocation on my laptop. The 
pthreads barrier is around 0.8 sec / 10K = 80 usec.

Let's see how it scales on the cluster nodes.

Well, that was entertaining. Two things to try:
- multithreading on the main MOOSE kinetic solver
	Looked at it. Should work reasonably well for bigger models with
	>100 molecules. But I'll have to write my own RK5 solver for multi
	threading because the GSL one has a monolithic driver for the 
	calculations that could be split between threads.
- contine with implementation for the synaptic input queue code.
	Looked at it. A major issue turned up: the 'process' call
	manipulates both target and source spike queues. The 'sendSpike'
	function after much indirection pushes the spike info onto all 
	target queues. The 'process' function examines and pops the local
	queue. I need to modify this so that the push and test/pop are on
	different clock ticks. This may well happen in more realistic
	neuron models. Here I can separate the harvesting of the spikes
	onto a different tick than the testing and sending spike msgs.
	With this fixed, I need to protect the spike msg handling.
	Mutexes are a blunt instrument here, because they protect code
	rather than individual data structures. Ideally I want only to
	protect the buffer(s) I am working on.
	I've suggested an approach to this in the Element::addSpike 
	function:
		// mutex lock
		// Check if index is busy: bool vector
		// Flag index as busy
		// release mutex
		// do stuff
		// ?unflag index
		// Carry merrily on.
	but this has many loose ends.


=============================================================================
25 Feb 2009
Checking it in so that I can run the tests on gj.
Checked in as revision 1055.
Oops, forgot to add MyBarrier.h
Checked in as revision 1056.
Now to run on gj.

One thread
syncArray10     ops=100000      time=0.023068	0.023	0.023	0.023
syncArray100    ops=1000000     time=0.235919	0.236	0.236	0.301
syncArray1000   ops=10000000    time=2.75227	2.46	2.61	2.52
syncArray10000  ops=100000000   time=32.2201	30.52	32.6	31.7
syncArray10000  ops=1e9		  				278.7

Main: creating 2 threads, pthreads barrier
syncArray10 ops=100000      time=0.123591	0.115	0.107	0.134
syncArray100    ops=1000000     time=0.330623	0.285	0.304	0.272
syncArray1000   ops=10000000    time=1.55576	1.74	1.727	1.76
syncArray10000  ops=100000000   time=15.2927	16.74	15.6	15.8
syncArray10000  ops=1e9	 					163.4

Main: creating 4 threads, pthreads barrier
syncArray10     ops=100000      time=0.309986	0.357	0.336	0.34
syncArray100    ops=1000000     time=0.521644	0.508	0.486	0.48
syncArray1000   ops=10000000    time=0.985141	1.842	0.989	1.38
syncArray10000  ops=100000000   time=8.70518	10.48	11.6	7.64
syncArray10000  ops=1e9						117

Main: creating 2 threads: MyBarrier
syncArray10 ops=100000      time=0.040132	0.043	0.040	0.042
syncArray100    ops=1000000     time=0.178236	0.179	0.178	0.194
syncArray1000   ops=10000000    time=1.63492	1.611	1.623	1.68
syncArray10000  ops=100000000   time=15.7017	16.7	16.6	16.4
syncArray10000  ops=1e9 					179

Main: creating 4 threads: MyBarrier
syncArray10     ops=100000      time=0.089063	0.249	0.076	0.132
syncArray100    ops=1000000     time=0.125938	0.130	0.139	0.161
syncArray1000   ops=10000000    time=0.835574	0.86	0.848	0.900
syncArray10000  ops=100000000   time=7.48444	8.17	8.33	8.31
syncArray10000  ops=1e9						114

Summary:
- MyBarrier works for 4 threads works on gj.
- The speedup is reasonable except for 10 entries in the array. 
- We're over 3.9 fold speedup on average, with MyBarrier on 4 nodes for
	10K entries. But for 1K entries and less the speedup is much smaller,
	possibly here we have cache considerations. This is confirmed by the
	last run with 100K entries. It goes very slowly here, less than 
	3x speedup, possibly because of cache contention? Perhaps it would
	work better to interleave the calculations of different threads,
	rather than to do them in separate blocks.

=============================================================================
19-20 Sep 2009
Dimensions of problem:
	Class management
		Initialization
	Element management
		Elements handle data struct management.
		Erefs do the ops that need index info.
		Elements in arrays: Already by default. Distribute over nodes
			Nested arrays: multiple child elements vs. clever lookup
		Field management: Extended fields? Child data as fields?
	Message management
		Definition at static init: sync for buf, async if hard-coded
		Creation, Deletion: Op on Msg, but sync needs extra work.
		Shared messages: Incorporated into piggybacking: below.
		Traversal
			Field access via messages
			Piggybacking onto messages
				Msg and func are just arguments to send().
			Using messages to scan for targets
			Open-ended messages? Floating messages: not on elms?
			Wildcards as message lists? On floating messages?
				+++Concept merging for connection maps.
				Must store original wildcard path in Msg.
			Iterating through messages and their targets
				Piggyback with returns instead of explicit iter
				Do we have to give return func an id? Nodes?
	Message use
		Process: Centrally called for computation. Also does ClearQ.
		ClearQ: Centrally called all the time.
		Sync: process->src->Data->buffer; process->dest->scan buffers.
		Async: send( Msg, func, args)->buffer; ClearQ->Elm->scan buffer.
		Do we separate spikes from other async (queued) msgs? No.
			Currently Data manages spike Q. Element will clear in
			Process/ClearQ.
		Passing arrays and other masses of data: Ptrs transform to mass
		Return messages?: Temp Msg made from Id of src.
		Functions and their ids: Sync across nodes. Expand dynamic? Bad.
		Would like to access Msg fields, specially dest list, like a
			wildcard. So make msg accessible like an object?
	Parallel stuff
		Threads: Element or index splits? Very sim dependent.
		MPI: Again, need sim manager to decide how to split.
		Object Id management: Master node hands out.
		Moving objects between nodes: Field-based expansion.
		Splitting Elements between nodes: Field-based expansion
	Simulation management
		Scheduling: Msgs from ticks, Special traverse func to call Proc.
		Solvers and objects: Take over Process. Element remaps FuncIds
			during clearQ. Replace Data with forwarding zombie obj.
		Solvers to each other: Messaging.
		Relation of objects to shell: As now.
		Relation of shell to parser: As now.
	Project management
		Unit tests: cxxunit or boost? Develop basecode using UTs
		Expectations for assertions:
		Expectations for documentation: Doxygen.

.............................................................................

I think I have a picture now of most of this framework. Now to design an
implmentation and testing process. Options:
1. Replace current Msg implementation with updated version.
	+ Will get rapidly to test key parts
	+ Smaller, more testable system.
	- Hands tied on heavier testing
2. Go into main MOOSE and begin replacement.
	- Horrible mess. Need to replace basecode part anyway.
	- Too tied into older forms.
	+ Get started on production use
3. Rebuild entire basecode with this design, plan to bolt old MOOSE computation
	modules on top
	+ Good idea for eventual upgrade.
	- Too much stuff to set up before serious testing on parallel stuff
	can begin.
	
Will go with option 1. the current revision is 1056.
Stages
	- Set up standalone Send for async.
		- Fix up Msg data structs to include original wildcard info.

=============================================================================

21 Sep
After some muddling, seems like the place to start is field access
(set/get) using messages. For context, assume that the operation is being
done between nodes. This forces the operation to be done in a general way.

set: Doesn't care about source, so current arrangement is fine. Here all the
	buffer needs to store is the field identity (given by funcId) and value.
get: Needs to specify source. Rather than use Id to do so, let's identify
	it by a message, since there may already be one used for repeat access.
	So we pass in the request to use the message in the access itself.
	This implies that even a transient message needs to register with the
	target Element. Should not be too hard now that we don't require
	argument checking. Messages will need to carry an Id identifying them
	on the src/dest. Otherwise we would have to put the entire Message info
	into the buffer.

	get(Id)->create temp object->locate target node->create temp message
	through to target Id->send data on temp message->target object gets
	message-> puts data onto message->back through postmaster->to 
	originating object->used in script->destroy temp object->destroy msg.

	Almost identical for single node

	This would work with little change for wildcards onto multiple nodes.
	Alternate approach is to ask the Shells to do this locally, and then
	transmit their data back to the originating shell.
	If I did not have to do the messaging, this would take somewhat less
	effort. However, if the messaging is standardized it would take less
	coding.
	
Things to do to get this to work:
Phase 1: Single node messaging
	- Code up MsgSet
	- Implement add and drop for msg.
	- Implement Cinfo that knows # and role of slots : predefined Msgs.
	- Implement scheduling and clearQ
	- Implement Finfo stuff to handle set/get functions.
	- Implement some set/get functions
	- Test above, and valgrind
	- Implement with wildcards: multiple targets.
	- Implement for tables, with predefined Msgs.
	- Implement delete of one of the msg ends.

Phase 2: Multinode messaging
	- Implement cross-node data transfer with hardcoded messages.
	- Implement cross-node message setup. Add, drop, valgrind.
	- Test single field access
	- Test massive tangled data flow.
	

Working from the middle out. Trying to implement a set/get function using
this supposedly clean approach. Current issue: Suppose we have a many->one
projection, e.g., to a channel lookup table. The get function needs to go
back to the originating object.
=============================================================================
22 Sep 2009
Finally some implementation. Goal is to figure out how to specify a single
object for a return message. Need to do the regular messaging first.
The current idea is to have the target Element itself do the final iteration
among target indices. This means that each buffer entry has the function,
its arguments, and a range of indices to which these are applied.
Efficient because the function and arguments are generated only once, saving
both space and time. The key thing is that the range of target object indices
is supplied for now by the Msg.

As far as value return is concerned, we want to tell the system to ignore
what the Msg says, and return the value only to the originating Element index.

=============================================================================
23 Sep 2009
Slowly taking shape. I need to factor in the presence of proxy elements. These
will be pretty real in the case of elements whose objects are distributed.
They may be virtual for elements that only represent their off-node actual
data. I don't know about even having proxy elements for ones that have neither
data nor any messages to the local node.

Looking into using streams to put values into the buffer, rather than the
ugly typecasts currently in use. Two things to do here:
- See how to attach streams to existing char buffers.
- Benchmark it to see if it is as fast as custom typecasting

Tried to do the stream stuff. Total pain.  Forget it for now.

Looking at messages coming to MultiNodeElement and MultiThreadElement
(the combination of the two will be worse).

Unless we precalculate the remapping, things will be hideous. We need to
work out how the range of the message splits up onto different threads or
nodes. So we need to go through all message entries (one range) and 
partition according to the local and remote ranges. At the message level,
this precalculation should result in formation of distinct sets of Ranges,
one per target node or thread.

=============================================================================
24 Sep 2009

Analysis continues. Hamstrung by lack of experience with prior implementations.
Here we have a design decision about when to expand the Range:
- During the final iteration on the target Element. The queue just stores an
	identifier for this message, and the index of the originating object.
- When we put the message on the queue. This means that the queue doesn't
	know about the message: a problem with returns. It will also put a
	lot of stuff repeatedly on the queue.

Seems clear we expand the Range only during final iteration. This means we
need to refer to the Msg on the target element by some Id.

So, another decision: Organizing Msgs. We have a 2-level organization: one is
for individual Msgs, which are maps between entries on single Elements. The
other is for conceptual connections, which are groups of Msgs and also store
the original constructor/wildcard strings for higher-level analysis. Options:
- Make a Connection class for the concepts, which manage Msgs internally.
	- Problem with identifying Msgs in the queue. We would like to do a
	single index lookup to find the Msg, rather than have to look up
	connection then Msg.
	- Problem is that the Conn index may change during setup, if there
	ever are any message deletes.

- Have a vector of Msgs, and a separate vector of Connections. Each Connection
	points to a group of Msgs.
	- Problem is that the Msg index may change during setup, if there
	ever are any message deletes.

After some pen/paper scribbling, it looks like I actually should set up a
3-layer structure:

Msg: Lowest, most specific. From single src Element to single dest Element,
	manages arbitrary indices. Implement as virtual base class with many
	subclasses for different cases of connection patterns. Stored as
	pointers. Shared between src and dest. 

Connection: Next level up. All targets from a single src Element, including
	a vector of Msgs. Equivalently, all srcs into a single Dest.
	Called by Send operation. Includes Func information.
	Also used in Wildcards. Can store original
	Wildcard info. Overlapping but not equivalent on src and dest.

Map: Highest, most general. All the conceptually related interconnections
	between groups of elements. What the GENESIS createmap command would
	build. Stores arrays of src and dest connections. Also stores the
	creation command and parameters in high-level form. Is itself an
	Element, and in due course will have
	capabilities to manipulate the Connections that are its fields.

=============================================================================
25 Sep 2009
Worked through control flow for different aspects of messaging by writing
function sequences. Mostly there, but need to figure out two related things:
- How to deal with specific info lookup for synapses
	This is simple enough. Use the target indices. Have a separate
	target index for each synapse, so that there is only one input coming
	into each. Local info solved.
- How to return data to caller, i.e., SendTo a specific target.
	The s->sendTo function does it by going straight to Element::addToQ.

=============================================================================
27 Sep 2009.
Starting re-implementation with Element. Did svn update to revision 1345.
=============================================================================
28 Sep 2009.
Compiled first pass skeleton version. Many things to do:
- Sort out FuncId stuff and how Slots are to set them up when messages are 
	added.
* Form of OpFunc: I think the size of args should be passed in, not worked
	out in OpFunc. Needed, for example, for off-node stuff.
	Fixed with creation of Qinfo.

- Handle creation of Elements and associated data
- Set up unit tests.

=============================================================================
29 Sep 2009.
FuncId stuff.
Option 1: Unique FuncId for every func. 
	- Simple and consistent.
Option 2: FuncId is semi-unique. Derived classes share FuncIds with their
	parent classes, for overridden functions.
	- We will have to maintain two sets of records: One for each unique
		OpFunc, and another for OpFuncs grouped by class FuncIds.
	- This helps with many inherited functions, such as reinit
	- This helps with zombie functions, which have to transparently replace
		the originals.
	- Helps with Slot/Conn design, since we lessen need to do separate
		Slots for multiple targets.
	Further options:
		- Use function name to determine overlap. Don't bother about
		inheritance, but do carry out RTTI checks during setup to
		ensure that the arguments match. Do not permit overlap without
		this.
			- This effectively sets up global functions.
		- Use function name plus inheritance to determine overlap.
		Do RTTI checks.
			- No globals, but will need to be careful about
			ensuring inheritance where we need it. E.g., reinit.


Trying to compile. Relevant file is Cinfo.h
=============================================================================
30 Sep 2009
Working on Ftype2.h. Also need to fix up OpFuncs and Ftype.h
Created a Dinfo class, to handle information and utility functions for the
Data type.
Struggling with Cinfo::create function. look in Neutral.h

Got it together, compiled.  Checked in as revision 1352

Next: 
	create a neutral *
	do some 'send' tests.
		Began with explict call to stuff data into buffer.

=============================================================================
2 Oct 2009
Working on most basic tests.
I'm now at the point where func requests have gone into the buffer, and the
element is doing clearQ. The execFunc fails because at this point I have no
Msgs on the Element, which is needed to farm out the function calls.
Examining how to handle sendTo calls here. Needed for table lookup like calls.
We need a way of specifying one target Data index in the Element.
Options:
	- Encode index also in Qinfo. Could make FuncId and MsgId shorts,
	and have a full uint for Data index.
		- Wasteful, as use of sendTo is very limited.
			- Table lookups
			- Solver controlling slaves.
		- Will need special messages too, since we need to encode
			the special request into the Qinfo.
	- Encode it as a flag on one bit of the msgid, using the rest of the 
		msgid as the data index. Use Msg # 0 as a
		special one which looks up this one index for its exec call.
		- Really ugly.
	- Ignore for now.
	- Encode SendBacks as an Element-level consolidation either way
		into vectors.
	- Encode SendBacks as individual Msgs.
	- Define special SendBack Msgs that read extra info in args.
	- Use sync msgs for this sort of thing.
The conclusion of all this is that if I need a SendBack capable message, I
have to set up a special msg.

There is a good bit of cleanup needed for the buffers, both for safety
and efficiency.
Safety:
	- Should do as a vector, with expansion as needed
Efficiency:
	- Align on ints or doubles?
	- Use sensible sizes for fields in Qinfo.
	- Map data directly to lookup structures like Qinfo, rather than
		doing a copy.
	
Got it together, compiled.  Checked in as revision 1353

One clarification: We have to pass in the SrcIndex for every call: many of the 
Msgs use it. For example, the OneToOne and the SparseMsg both use it to look
up targets. So Qinfo needs another field.

Also it is desirable that we should be able to use SendTo with regular
messages. Perhaps the added field for the SrcIndex in Qinfo can be used for
this?
Instead I think the data buffer should be used for the return index. Let's
have it so that if needed one can always write an OpFunc that can see the
src MsgId and src index (which together let us figure out the source Element).
These are there in the args, it is just a case of using the additional 
fields in Element::execFunc, or passing in a reference to Qinfo.
What remains is a way to tell the Msgs to use the extra index in the
args to determine the target of SendTo. Best to use the regular OpFuncs,
so this means that the Msgs also juggle the args.

Implemented it, tested the second part of it by stuffing the queue directly
and then calling clearQ. That part works.
Checked in as revision 1356

Next: 
- Get the 'send' part to work
- Do a valgrind cleanup. 
	- Figure out how to delete the data part cleanly.
	- Msg deletion cleanup.
	- Serious tests for memory leaks, lots of deletes and creates.
- Sort out message setup including type safety 
- Utility function (using messages) to 'get' field values.
- Heavy traffic tests
- Sync messages
- Start to play with nodes and threads.
- Optimization: buffer alignment, clean up management, use in-place rather than
	copy.

=============================================================================
3 Oct 2009
Before going into the above, doing a cleanup of memory allocation. 
Valgrind helped track things down.
Checked in as revision 1360: Major stuff cleaned.

Another round of cleaning up, this time mostly stuff from class initialization.
Now I have it so Valgrind is completely happy.
Checked in as revision 1361.

Implemented the 'send' operation using a hand-crafted Slot object. 
Ran through it with valgrind, good first pass.
Checked in as revision 1362.

=============================================================================
4 Oct 2009.
Look at message setup.
I've had a big simplification by separating the functions from the Msg.
For the existing MOOSE messaging concept, though, the idea is that in Process
or in response to a function call, an object should send data on a Message
specifically set up for the purpose. Using this message, the object calls
specific target functions whose type is predefined, and whose identity 
is set up at the same time as the messages themselves.

There is no room for slop: precisely the right # and type of target functions
must be known.

In the present design, the Slot does the job of tying Msg to function.
At compile time we know what the Slot type is for all slots we can use.
At Element creation time we can create predefined Slots for fast function calls.
	Has to be then, since we want to hardcode Slot identity into
	e.g. process functions.
At Message setup time we can attach a ConnId and a FuncId to these slots.

At compile time we know the ConnId for all precompiled Msgs.

Instead of a separate Slot class, let's use SrcFinfos for the slots. 
All SrcFinfos sharing a common Conn (Shared messages) just store the
index to this Conn.
The SrcFinfos/Slots do NOT store the FuncId, since they are static and the 
FuncId is defined at setup time or even runtime. Instead they index a vector of
FuncIds somewhere. This will be a fixed (statically defined) index, so we
know the starting size of the vector. Options:
	- On the Conn:
		- Should keep Conn just for the connection info
		- Clean association of Funcs with appropriate Conn
	- As a separate vector on the Element
		- A bit more economical, as it is a single vector for all the funcs.

Note that run-time defined Slots could either directly hold the FuncId, or
do this index stuff.
Note that in this design we do not need to have anything special for the
destinations in shared messages. The correct types have to exist, is all.

In many cases the shared messages had MsgSrc on both sides. Often these 
were for send-back situations. Also for bidirectional data such as 
reaction-Molecule data transfer. Options:
- Ignore this. Go back to GENESIS style where two messages had to be set up.
- Implement as a higher level op, that results in two distinct messages
	being formed, one each way, 
	- Can we put both on the same Msg?
		- This has some restrictions on there being matching conns.
		- Really only a minor matter for the msg creation func to handle
		- As at present, it could be symmetric or asymmetric.
- Try to do as a single conceptual Msg.
		- Don't see how it would work.

=============================================================================
8 Oct 2009
Target elements should decide which func to use for a given message?

Issue is having multiple possible target funcs from same SrcFinfo
e.g., mol conc going to plot, to MM enzyme and to channel.
Possible implementations:
Category 1: SrcFinfo has static-init defined conn Id and FuncIndex.
	Option 1:
		- connId looks up entry on Element::vector<Conn>
		- FuncIndex looks up entry on Element::vector<FuncId>
		- Conn marches through Msgs, using same FuncId for all.
		- Target finds OpFunc from FuncId using Cinfo::funcs_[FuncId]
		Issues:
		- Can't handle multiple kinds of target funcs, only derived.
	Option 2:
		- connId looks up entry on Element::vector<Conn>
		- FuncIndex looks up entry on Conn::vector<FuncId>
		- Conn marches through Msgs, using same FuncId for all.
		- Conn is a linked list, and next conn has next FuncId.
		- Target finds OpFunc from FuncId using Cinfo::funcs_[FuncId]
		Issues:
		- Ugly. Need to manage linked list, but only occasionally.
		- Puts Func info on Conn.
		- Costly. Each Conn manages a vector of FuncIds.
	Option 3:
		- connId looks up entry on Element::vector<Conn>
		- FuncIndex looks up entry on Element::vector<FuncLookup>
		- Conn marches through Msgs, using same FuncId for all.
		- Target finds OpFunc from FuncId using Target::funcs_[FuncId]
		Issues:
		- Need to set up vector of funcs_ on each target Element
		- Passing around and setting up a dynamic FuncLookup. Too cute.

Categorey 2: SrcFinfo has static-init defined conn Id and SrcFinfoId
	Option 4:
		- connId looks up entry on Element::vector< Conn >
		- SrcFinfoId plus specific, per msg index, to specify tgt.
		- Conn marches through Msgs, using same FuncId for all.
		- Target Cinfo has relatively small list of possible targets
			for a given SrcFinfoId, based on type matches.
			specific index pins it down.
		Issues:
		- Still problem with multiple kinds of target funcs on Conn.
Category 3: SrcFinfo has static-init defined MsgId, using Msg link lists.
	Option 5:
		- MsgId looks up entry on Element::vector< Msg* >
		- March through linked list of Msg* (all on vector)
		- Each Msg has a matching entry in vector funcs_< FuncId >.
		- Target Cinfo has relatively small list of possible targets
			for a given SrcFinfoId, based on type matches.
			specific index pins it down.
		Issues:
		- Still problem with multiple kinds of target funcs on Conn.
Category 4: Exception handling. Assume that a single FuncId will normally
	work, and treat other cases as rarely-used exceptions.
	Option 6:
		- connId looks up entry on Element::vector<Conn>
		- FuncIndex looks up entry on Element::vector<FuncId>
		Normal case: FuncId in range:
			- Conn marches through Msgs, using same FuncId for all.
		Exception: The FuncIndex is out of range
			- FuncIndex identifies vector of FuncIds
			- Conn marches through Msgs, using different FuncId
				for each, from vector of FuncIds.
		- Target finds OpFunc from FuncId using Cinfo::funcs_[FuncId]
=============================================================================
10 Oct 2009
Compiled implementation. runs up to part way, then croaks on uninitialized
variables.
=============================================================================
11 Oct 2009
Fixed croaking problem: I had redefined some private fields in SrcFinfo. 

Remaining issue with setting up the FuncIndex. This is something to be done
by Cinfo::init().

Fixed testSendMsg issue with setting up FuncIndex. 

Converted both to doing almost silent unit tests.

Implemented and cleared testCreateMsg, cleared valgrind. Need to make it silent.

Also it is unable to find the 'set' field for the field assignment part,
which is fine as I haven't yet defined it.

For doing 'set':
- Create Element explicitly, not using Create function
	Pass in a ptr to the field to be set/get? not needed.
- Create Msg using regular msg func.
- Call asend directly



Need to revisit all opfuncs to pass in Qinfo and Eref.
Done.

Need to revisit Conv<A> to return a reference or something similar, to avoid
alloc etc.

=============================================================================
12 Oct 2009.
Put the Qinfo into the OpFunc. Runs, clears.

I have two variants of the OpFunc, perhaps premature optimization. One of them
ignores all the additional info available, such as Qinfo and target eref.
The other is also derived from OpFunc, but its func takes the reference to
the Eref and the ptr to the Qinfo as additional args.

Still to set up.

Checked in as revision 1368.
=============================================================================
13 Oct 2009.
Implemented EpFunc, which is a variant of OpFunc that passes in the
Eref and Qinfo along with other args. This is needed whenever we have a 
function that manipulates the Element, or needs additional data about the
incoming message.

Checked in as revision 1370.

=============================================================================
14 Oct 2009.
Implemented testSet unit test. Seems OK, but need to clean up memory leaks.
Checked in as revision 1372.

Working on memory leak. Unexpected major problem with whole approach: the
allocated message m is needed during the clearQ, after the set command
has returned. So we have a dangling message.
Related problem: We will have clearQ after Set has returned. So script
command sequencing cannot be guaranteed, unless we tell the script execution
to hold off till clearQ. This gets worse with 'get'.

Working on 'get'. I have the skeleton in place.
=============================================================================
15 Oct 2009
More implementation on 'get'. 
- Msg leak issue could be handled if the msg is permanently stationed on the
	SetGet object (due to be the Shell). Its far end can dangle, and be
	connected to targets as needed.
- Would want to refine this to deal with wildcards, so want a Conn, not just a
	single Msg.
- Would want to do cleanup and continuation of script function on the
	SetGet::handleGet. This function is triggered only when the 'get'
	call returns. A bit fragile, will want a timeout.
- If we have multiple targets for 'get', we will need an index to go out to 
	each target, and come back with the data, so that it can be organized
	into a vector. The recipient function will then have to keep track of
	how many have returned.
- Do we need multiple 'get' buffers and funcIds? If the effective utilization
	is serial, should be OK to have just one.

After a day of implementation and debugging, seems to work.
Checked in as revision 1375.

This is leaking memory copiously. Next step is to organize set and get
through the SetGet object/shell, of which there should be just one instance.
In the current test run there were 100. This should allow us to reuse the
Msg from the SetGet object, and avoid the memory leaks.

Next Steps:
- Clean up Set/Get
- Heavy traffic tests
- Sync messages
- Start to play with nodes and threads.
- Optimization: buffer alignment, clean up management, use in-place rather than
	copy.
- Incorporate unit tests into cinfo
- Provide message tree diagnostics.

=============================================================================
17-18 Oct 2009.
Replaced SetGet with Shell. Set up automatic creation of Shell as /root,
during runtime init. Still leaks memory.
Checked in as revision 1384.

Working on handling Msgs from Shell. Need still to clear out old msgs during
Set.

Although it works now, valgrind picks up a nasty situation. When a dest
Element is deleted, Msgs on it need to be deleted too. Msgs know how to 
remove themselves from the src element, but not from Conns, which also
have pointers to them. Options:
1. Do a grungy search for Msg ptrs on all Conns. Deletes are rare so should
	be OK.
2. Store an extra index or two in each Msg for identifying parent Conn(s)
3. Conns do not store Msg pointers, but lookup indices for them on the Element.

Let's do #1. 
Done. This completely fixes the memory leaks that afflicted the 'set' function.
At some point I'll have to benchmark to figure out how much
of an impact the message deleting has on the overall performance.

=============================================================================
20 Oct 2009.
Also moved the 'get' function to use the Shell element.
Next I need to generalize both set and get to handle arbitrary types.

Trying to find a suitable place to do this. In the process I found that
Send.h and Async.h are no longer used. Removed them.
Checked in as revision 1388.
=============================================================================
26 Oct 2009.
Implemented Set and Get operations in a new templated SetGet class. Better
typechecking. Compiles but it doesn't yet work.
Checked in as revision 1404.

=============================================================================
31 Oct 2009.
Finally got to do some debugging. Fixed problem, now works, clears
valgrind. 
Checked in as revision 1422

* Need to test with doubles and other types. Done. Did a partial 
	re-implementation of IntFire, and did set/get on one of its 
	fields of type double.

Checked in as revision 1424

There are several issues with a full implementation of IntFire, most notably
that the design now requires there to be a distinct target Synapse object for
each incoming message. In the earlier version we had some extra info 
figured out by the system to identify the target SynInfo. here I just want
to use the index of the traget Eref. This is good, but now the destination
IntFire needs to juggle some subset of the target Synapses, which are
independent objects presumably on a child element.
For efficiency, the IntFire would like to have target Synapses as internal
fields.
For indexing synapses, we want each to be an individual array entry in a
big array of Synapses.

Assume we'll handle allocation etc within the parent IntFire.

How to set up indexing? 
	- Give each IntSyn the same #, which is the biggest # of syns on 
	any one. 
		- This needs us to be able to allocate an array of Data with
		holes in it. Easy to do if we have an indirection step on
		the Element, but as built the Element won't do it.

	- Set up precisely as many IntSyns as are needed.
		- Indexing and relating IntSyns to parent are both hard.

	- Explicitly make it look like a 2-D array, with variable length
		sub-indices.
	
	- Make it look like an array of Elements each with an array of
		Synapses.
=============================================================================
1 Nov 2009
Synaptic info options.
1. Separate SynInfo or Synapse objects. Each receives input from only
	one axon. The whole lot are in the same Element.
	Spike operations:
		- Spike arrives on a Synapse. 
		- Synapse sends Msg to parent Element, with delay etc info.
			It needs efficient orgn of the messaging to parent.
			Even with optimal organisation, this is costly, 
			going through entire msging again.
		- Parent Element updates its pendingEvents queue
	Process operations:
		- Check if queue is due. If so, handle event and pop queue
		- Check if Vm exceeds thresh. If so, send spike, and reset Vm.
			Otherwise do exp decay calculations on Vm. 

	This has an unpleasant extra messaging step from Synapse to parent.
	However, there may be efficiencies in the first msg from axon to
	Synapse as we guarantee a single input.

2. Messages are directed to Synapses but are processed directly by
	IntFire.
	Can't do this without some juggling of target index. See next.

3. Messages are directed to parents of Synapses. Munge the indexing of the
	target Element so that part is used for indexing it, 
	and part to index the correct Synapse.
	This could be a special case of an ability to index 2D arrays.
	But where does one stop?
	Or, the extra info could just be something that messages can generally
	do.
	This arrangement deals with the threading.
	It also eliminates the issue of passing info down to parent.

4. Give the Synapse a pointer to its parent Element or IntFire.
	Issues with threading.
5. Special OpFunc to munge index.
	- Create Element that deals with individual synapses, but points
		to the parent IntFire (or ReceptorChannel) Element.
	- OpFunc munges destIndex in some efficient manner. Bitmap may be
		best, using top n bytes for specific synapse index.
		For IntFire, we may need 2 bytes for synapse, leaving only
		2 for target IntFire. Insufficient.
	- OpFunc is class-local, so we can set up some reasonable subdivision.
	- Pass in synapse index as additional arg to the func encapsulated
		by the OpFunc.
	- Can generalize to other fields of Synapse
	- Can generalize to arbitrary arrays using templated 
		ArrayOpFunc with dimensions?


To test scalability, let's do this:
- Make a speed reference using a sparse matrix and customized code to replace
	messaging, as a recurrent network with N neurons and P probability
	of connections. Goal is to do N >= 1e5, P ~0.1.
- Make the equivalent network in MOOSE
- Profile, look at bottlenecks.
- Repeat with multithreading
- Repeat with multinodes


Let comp load of IntFire processing be I.
Let comp load of Synapse processing be S.
Let spiking occur every T timesteps.
N and P are defined above.
Let # of synapses per neuron = #

If we ignore communcations cost,
total load per timestep = I * N + N * # * S / T
Some numbers
N	#	I	S	T	Load	Notes
1e5	1e4	1	2	100	2e7	zero messaging cost
1e9	1e4	1	2	100	2e11	zero messaging cost
1e9	1e4	1	10	100	1e12	medium messaging cost
1e9	1e4	1	100	100	1e13	high messaging cost

Now we go down 10x smaller timesteps.

1e9	1e4	1000	2	1000	1e12	Realistic neuronal models,
						zero messaging cost
1e9	1e4	1000	10	1000	1.1e12	Some messaging cost.
1e9	1e4	1000	100	1000	2e12	High messaging cost.
					
Main points:
- in IntFire networks synaptic comput and messaging are overwhelming
- in realistic neuronal model networks, the costs are comparable.

Overall, efficiency does matter for spike messages.

we may need 13 digits x3.3 = 43 bits. Too much for even an int.

=============================================================================
2 Nov 2009
Working out implementations for accessing array fields, such as synapases 
on an IntFire.
See ArrayOpFunc.h::ArrayOpFunc0

The array of synapses should act just like any other array in terms of 
field access, adding messages, and so on.
	This means that the Eref::data() function has to behave the same way
	as for any other Element, and look up the correct Synapse without
	further effort.
When there is a need to do operations through the parent Data, then we use 
	Eref::data1() to return the parent data.
	Eref::index() has all information needed to look up synapse from data1.
To handle these cases, we have a separate set of OpFuncs that operate on
	data1 and pass in the index. This is the UpFuncs.
To do these seamlessly we need to make DataVec a virtual base class, and
	have 1D, 2D, 1D+field and similar options. The DataVec handles 
	deleting of data too, so if we need to have more than one Element
	refer to the same data, then suitable DataVecs have to be defined for
	each element, and only one may delete the data.
	Alternatively, we should have the Element itself be a virtual base 
	class.

Stages:
* Put in DataId for all Eref indices.
* Replace Data with char* in Element::data
* Make Element a virtual base class.

Checked in as revision 1425

Next:
* Derive off Element for Synapses on IntFunc. Checkin 1427.
* Do Synapse and SynElement implementation. Checkin 1428.
* Come up with special element creation options to set up this element. 1429
* Check that field access works with it	Checkin 1431.
+ Send spike messages to it
* Sort out how to handle its clearQ.
* Fix up sub-fields within DataId.
* Clean out const A& argument in OpFuncs. Should just pass argument, as most
	of them will be doubles. Will need separate classes for strings and
	things. Checked in as 1429, except for string stuff.

Sending spike Msg seems OK, need more tests.
=============================================================================
4-5 Nov 2009
working on testSendSpike. Runs but doesn't seem to make sense. After some
debugging got it to work, sends the spike info. Much of the problem is due
to the ugly handling of DataIds. Checkin 1433.

Need to fix that next. Fixed. Checkin 1434.
Also valgrind is very unhappy with the allocations. Need to fix that too.
	Fixed, it was a simple matter of deleting the IntFire element. 1435.

Next: 
- Implement a sparse matrix Msg type and use it to build a serious
	feedback network of IntFires.
- Clean up scheduling a bit so that we can see the data flow over multiple
	cycles.


Working on sparse matrix. In order to fill the synapse indices, I need
to fill the transpose of the matrix to start, and then transpose it.
Transposition of the sparse matrix:

Start with 

[	1	0	0	0	2	]
[	3	4	5	0	0	]
[	0	0	0	6	7	]

N_ =  1234567
Col = 0401234
rowStart = 0257

Transpose is:
[	1	3	0	]
[	0	4	0	]
[	0	5	0	]
[	0	0	6	]
[	2	0	7	]

N_ =  1345627
Col = 0111202
rowStart = 023457

To transpose.
Step 1. Sort N_ by col to get N_T
Step 2. Convert rowStart to row# for each entry, so, 
	0257 becomes
	0011122
	Sort this by col = 0401234 to get new set of cols:
	0111202
	Note that this sort needs to retain original order of otherwise equal
		numbers. So the first 4 comes before the last one.
Step 3. Sort the col itself to get the new sequence for row#s:
	0401234 becomes 0012344
	Then put row starts on each, whenever the value increments:
	02345
	and wrap it up with a 7.

Implemented. Compiles, not quite there with the unit tests.
=============================================================================
6 Nov: Fixed up, now does correct transposition.
Checked in as Revision 1436.

Next step is to do tests with messaging.
Working on it. An issue comes up with randomConnect: In the function I
set up the messaging to synapses, but the target object has not yet allocated
the synapses. Good, we can do this correctly after setting up. 
Bad, because we don't have a general way to tell objects that a specific
field needs to be assigned. It is numSynapses in IntFire, but could be
one or more different other fields.

=============================================================================
7 Nov.
Approach taken to allocate synapse memory, which is generalizable to other
kinds of array fields:

- We will usually have to access these other fields as part of the setup
	command. For example, setting synaptic weights.
- The messaging command itself passes input to a named field. More to the
	point, the SynElement is on a specific array. Should be able to
	provide info to it generally to define values in this array.
	- The UpFuncs serve this task in the Cinfo. However, we need this
	feature in the SynElement type classes quite generally. So it has
	to be something that the compiler enforces.
		- UpFuncs in the SynElement constructor?

- Remember that messages were to be the equivalent of wildcards. We should
	use the created message itself to assign fields, including setting
	up the weights and the allocation of synapses.
	- Setting weights: Implement a
		setrandom<double>( const Msg* m, const string& field, 
			double lo, double hi, bool isLog );
		function.


OK, hacked it in for now as hard-coded access functions within the SynElement.
Compiled stuff and cleaned up old printf testing, now uses assertions as
part of unit tests. Checkin 1438.

Successfully created a 10000 X 10000 sparse matrix with 10% connectivity.
So about 1e7 entries. Expect memory use to be about 1e7 * 8 bytes. The
transposition would have used about 1e7 * 12 bytes more.
Oddly it used over 1.5 G, perhaps would be less if I
reserved the space rather than fill it with push_back calls. 
For unit tests I'll use 1000X1000 as it is much faster.

Valgrind is not amused: an error somewhere. 
The size of rowStart() is 1 smaller thn it should be.
This was quite nasty. I put in assertions that should have caught it but
did not. I checked the web for odd interactions between assertions and
templates. Finally I realized that SparseMsg.o did not depend on SparseMatrix.h
in the makefile. So the SparseMsg was not seeing any of the updated code.
Fixed, compiled, reran, clears valgrind. Checking 1440.

=============================================================================
8 Nov 2009
Would like to implement a vector 'set' operation.
For now stay focussed on the sparse messaging.

I had a difficult bug in field assignment that only materialized after a very
large number of assignments. After a lot of meandering, turned out that the 
problem was that I was not clearing old messages out. In the absence of this
garbage collection, the system was correctly assigning new msgids as it went
along. In due course the system overflowed the 'short' field range.
Solution, of course, is to fix up the garbage collection of old messages,
or rather, the slots allocated to them.
Implemented it. Works. Valgrind takes several minutes to chew on it, but
eventually it too passes.  Checked in as revision 1441.

This is as thorough a test of set/get as any i
I've done so far. The vector 'set' operation would help.

Now working on synaptic delivery. The system is taking up over a gig of
RAM to store the pending synaptic events for just one timestep.

# of synapses = 1e3 * 100 = 1e5.
Should not happen even if every single synapse fires.

I wonder if the buffer keeps getting extended as the process call is done..
No, should have a cleanly separate eventq.

Tracked it down, the SparseMsg dispatcher was sending out stuff to all
targets regardless of the originating Eref. Fixed. Lots of tinkering later,
we have a reasonable IntFire network. It goes into saturation rather
quickly above a certain threshold of connectivity, otherwise decays.
Also it scales pretty well in terms of speed and memory. 
Need to do benchmarking.
Checked in as revision 1442.

Tried to do a profile. Failes outright in the optimization compile mode,
with or without profiling. Clears valgrind in debug mode. So I am confused.
Managed to track it down to very first unit test, was an innocuous array 
allocation. Fixed and now works.

Profiling shows that the most time is spent doing the field allocation.
Silly. Let's set up vector assignment.
Done, checked in as 1443.

Now did the profiling with a long (1000 timestep) run of the IntFire
network. The results are gratifying: By far the largest time is spent in the
heap operations for the synapse. All told, under 10% of the time is spent in
messaging.

  %   cumulative   self              self     total           
 time   seconds   seconds    calls   s/call   s/call  name    
 56.83     18.49    18.49 102317402     0.00     0.00  void std::__adjust_heap<__gnu_cxx::__normal_iterator<Synapse*, std::vector<Synapse, std::allocator<Synapse> > >, long, Synapse, std::less<Synapse> >(__gnu_cxx::__normal_iterator<Synapse*, std::vector<Synapse, std::allocator<Synapse> > >, long, long, Synapse, std::less<Synapse>)
 17.09     24.05     5.56 103414264     0.00     0.00  IntFire::addSpike(DataId, double const&)
  7.84     26.60     2.55 103414264     0.00     0.00  Synapse::Synapse(Synapse const&, double)
  4.95     28.21     1.61  1012617     0.00     0.00  SparseMsg::exec(Element*, char const*) const
  3.78     29.44     1.23  1024002     0.00     0.00  IntFire::process(ProcInfo const*, Eref const&)
  2.06     30.11     0.67 103414264     0.00     0.00  UpFunc1<IntFire, double>::op(Eref, char const*) const
  1.20     30.50     0.39 103414264     0.00     0.00  Eref::data1()
  0.86     30.78     0.28 102317402     0.00     0.00  Synapse::getWeight() const
  0.71     31.01     0.23 103340378     0.00     0.00  Synapse::getDelay() const
  0.65     31.22     0.21 104896327     0.00     0.00  Eref::Eref(Element*, DataId)
  0.61     31.42     0.20                             GetOpFunc<Synapse, double>::op(Eref, char const*) const


Checked in as 1445.
Now to change it so it is more like a unit test.

Calculations: 1024 * 102 synapses ~1e5
Towards the end, it was saturated: always firing. So rate = refractory
period = 2 timesteps.
# of timesteps = 1000.
So 5e7 synaptic events were transmitted, in about 30 sec. ~1.3 million/sec.

Back on 1 Nov, these were the planned steps:
To test scalability, let's do this:
- Make a speed reference using a sparse matrix and customized code to replace
	messaging, as a recurrent network with N neurons and P probability
	of connections. Goal is to do N >= 1e5, P ~0.1.
* Make the equivalent network in MOOSE
* Profile, look at bottlenecks.
---> Implement scheduling
- Repeat with multithreading
- Repeat with multinodes

I've done a couple of these. The next step is to do the standalone version
without messaging, to get a tighter estimate of the messaging overhead.
Then I need to insert a stage where I implement scheduling, before going
on to the multithread stuff.

=============================================================================
9 Nov 2009.
Made another branch, based on moose/Msg. In:

/home/bhalla/moose/IntFireNetworkNoMsg

Munged the IntFire and related code
in it to directly call functions instead of message passing.

=============================================================================
10 Nov 2009
After some debugging, managed to get the code to work. There is very little
difference with the profiling: 30.2 sec for the message-less version, as
compared to 32.54 sec for the messaging version.

Ran using 'time' a few times with optimization but no profiling.
messaging	non-messaging
37.3		33.7
37.1		33.5
37.1		33.7

So the difference is about 3.5 sec, or a bit over 10%. This is outstandingly
good.
Checked in the IntFireNetworkNoMsg stuff as the end of its branch. This is
revision 1448.

Then deleted the branch from my local machine. Still sits on SourceForge.

Now on to scheduling and threading.

The scheduler has to send messages for process and clearQ.
During runtime:
	process and clearQ alternate strictly. Many threads to coordinate.
	Shell has to be blocked, but with an option to halt runtime thread.
		Ideally this could be done reversibly.
	Graphics has to continue on yet another thread.
During setup:
	clearQ must run in coordination with the thread of the Shell.
	- Cannot be on the same thread, since we may need to hold up the shell
	while completing queued calls.
Graphics and the other threads
	- Graphics sits on a separate thread.
	- I need a separate channel for data to go from process to graphics.
		This is both for the OpenGL graphics, and for Qt.
		Looks like a thread-safe queue here. 
		Will need graphics first and computation first policy options.
	- Qt and OpenGl events will probably be handled by Python.

I'll use the recently implemented stuff for array fields to do the clock ticks.
Also I'll use priority queue to manage it, rather than the customized version.

=============================================================================

Need to call Erefs by value in EpFunc.h
Issue with having ticks as ArrayFields: they need to be sorted. If the
sort order is changed, then the messaging will have to be remapped accordingly.
This is do-able but involves a possibly messy extension of messaging functions.
The alternative is to have them as separate Elements, which is messy in other
ways.

=============================================================================
13 Nov
Working on Clock.cpp.
=============================================================================
14 Nov.
Put in skeleton of Clock and Tick, compiles.
=============================================================================
15 Nov.
Setting up unit tests. Need to define calling framework.
- Creation of ticks
	- Could create a certain # explicitly, like I do with Synapses.
	- Could have an 'addTick' function on the Clock. Would need dropTick.
		- the addClock function works better with this.
		- Messiness if I drop a tick in between the defined set.
	- Could create say 10 clocks by default, but manange only the
		set in the TickPtr vector.
	- Could get rid of 'stage' field by considering index on the Tick 
		vector. But there is no rule about ordering clocks by dt
		(though it is implicit somehwat in GENESIS for clock 0)

Anyway, now that I am back online, checked in a large backlog of changes
as revision 1455.

Went through the unit tests, converted the massive printouts into assertions.
Did a little cleaning using valgrind. Now OK. Checkin 1456

Starting up with a template for the TickElement. yet to compile.
=============================================================================
16 Nov
Now trying to compile.
=============================================================================
22 Nov.
Resuming work after IMMMI. Compilation works for the FieldElement template
to handle arrays of fields. Checked in as 1457.

Now worked through replacing SynElement with the FieldElement template.
Works, clears unit tests. Checked in as 1458.

Setting up clocks and ticks. Issue: How will ticks be added? Seems like
the safe thing to do is that any change at all in any of the tick
parameters (dt, stage, or # of ticks) should invoke a rebuild.

Implemented much of the Tick scheduling stuff. Checked in as 1459

Will need an UpValueFinfo: assignment of fields get diverted to parent.
Will need to sort out calling of Process and clearQ. Consider Reinit too.

=============================================================================
23 Nov.
Ways to approach the Tick field assignment stuff:
Pedantic: 
	The tick field access functions themselves ensure updates of the Clock.
	- Can do as a special case by making an UpValueFunc which calls
	the parent clock to do the field assignment, and handle updates.
	This is clean enough, a little tedious and ugly for the field funcs.
	- Can do as a general case by making all array field assignment calls
	into calls to the parent. Ugh.
	- Can do as a general case by providing extra args so that the function
	can work out who the parent is. Can ignore this stuff if not needed.
	Also somewhat ugh.

Pragmatic:
	Tick field access just updates fields locally. We need another call
	to the Clock to rebuild the schedule.
	The wrapping Shell functions for handling clock ticks does this.
	- This would allow calls to change ticks without having an effect
	on scheduling. Could be surprising.
	- There may yet be other cases which need to do similar things.

Hacks:
	- Provide ValueFinfo with an auxiliary SetFunc
	- Provide ValueFinfo with an auxiliary trigger func for whatever
		other operation is needed on parent when a field changes.


I'll use the UpValueFunc, which is what I had originally planned.
Checked in as 1460.
Implemented, tested for one field. Works OK. Valgrind also happy. 1461.
Implemented for second field as well. 1462.

Set up unit tests for setupTicks(). Looks good. 1463.
Called 'start'. Hangs, looks like infinite loop.

Implement getVec

=============================================================================
24 Nov.
Working on scheduling.
Algorithm:
Current time = 0
While (currrent time < end time)
	Sort all tickPtrs in order of nextt and stage.
	Execute the first in sequence.
		Current time becomes the nextt of the just executed tick.
		nextt is incremented.

Minor fix to this, since we want each tick to be called just as the system time
advances. So the first call on tick0 (with dt = 1) is at t = 1.
Checked in as 1464.

Next: handle process and clearQ alternately. This is a job for the Ticks.
To call all process then all ticks:
	No, the idea of the different stages is that a complete calculation
	can be sent on during the same overall timestep, comprising several
	ticks with the same dt but different stages.
So, assume we call process and clearQ alternately.

What to call first:
	Process:
	- ClearQ will have been called ahead of time by the system. For example,
	reinit will already have been called, and we have values ready to use.
	- If there were earler stages within this overall timestep, then 
		we will not have a way to access data passed in.
	ClearQ: 
	- This will allow a given tick to handle incoming data and deal with
	it, and pass it on in Process.
So, call ClearQ first.

Do we call ClearQ strictly alternating, or should stuff be cleared more often?
	- More often clearing adds compute cost
	- More often clearing might lessen queue length.
	- Never need stuff till Process.

So: Better to strictly alternate with Process.

Do we have multiple Process calls?
	- Several GENESIS type calculations need a separate 'init' stage then
	a 'process' stage. For example, the compartment uses
		- init: previous_state = Vm
		- process: traverse messages, do integration.
		If two compartments A and B exchange Vm, then they need to 
		exchange previous_state in order to avoid asymmetry.
	- In MOOSE, with the clearQ arrangement, this would not be an issue.
	The data exchanged will always be previous-state, due to the sequencing
	of clearQ and Process.
	Seems like I never use it in other contexts.
So, don't need multiple Process calls.

Almost there with the implementation, stuck because the 'advance' call
needs the correct Tick Eref as an argument.

=============================================================================
25 Nov.
One possible hack is to dual-derive Ticks from Msg. I don't like it.
Another is to store the Element for the Ticks in the Clock. Might be OK
if it is a child. Better if it is found from a Msg.
Separate from this, is how to rapidly access the list of Msgs from the Ticks.
	- have a distinct Conn for each Tick. They could have the same index. 
=============================================================================
26 Nov. Avoided hacks, got the TickElement from a message.
Set up a distinct Conn for each Tick, using the same index as the Tick's
own index.

Many changes needed to get all the pieces to work together. Compiles, 
yet to get it to work.

OK, now works. Cleaned out the scheduling test so it isn't verbose anymore.
Also valgrind is happy. Checked in as 1466.

Back on 1 and then 8 Nov, these were the planned steps:
To test scalability, let's do this:
- Make a speed reference using a sparse matrix and customized code to replace
	messaging, as a recurrent network with N neurons and P probability
	of connections. Goal is to do N >= 1e5, P ~0.1.
* Make the equivalent network in MOOSE
* Profile, look at bottlenecks.
* Implement scheduling
- Repeat with multithreading
- Repeat with multinodes
Now we add:
- Redo messaging with sync messages
- Make a good test case, say the signaling network again?
- Profile, look at bottlenecks
- Do multithreading
- Do multinodes.

=============================================================================
28 Nov.

Multithreading.
- Need to guarantee that clearQ is element local, and all outgoing
	messages emerge only at Process.
	- Otherwise we might add stuff to queues at the same time
	as we read from them.
	- Alternatively, have to maintain a separate queue for clearQ input
	vs output.
	- Or we could do a mutex to grab control of the target queue for the
	time we need to put data into it.
- Use a separate queue on each Element for each thread.
	- Alternatively use thread-safe queues. Less memory, more mutexes.
- Barrier after clearQ and after Process.
- Process is easy to multithread as it is data object-local.
	- separate sub-parts of data among threads.
- clearQ is hard to multithread as each msg in the queue could refer to many
	data objects, and there can be many msgs. 
- How to multithread the clearQ

Scaling up to huge numbers of threads: This should use the same decomposition
as scaling up to a similar huge number of nodes.

Suppose I have 1024 threads in my simple network example.
1. Single or multiple schedulers?
	- Do I want them to run out of sync? I'll need barrier synchs
		in either case.
2. Explicit Element Data splitting or implicit (and even dynamic) as per
	thread indexing?
	- Explicit would simplify the queueing model: It would still be the
	same as single-threaded, as each thread would manage one queue.
	- How to access stuff with explicit data splitting? Reference Element
		will have to keep track and if needed, redirect. But this
		can't be done for all messages: they will need to reshuffle.
	- Dynamic would do good things for load balancing.
3. Message structuring.
	- Explicit Element data splitting will put a separate queue request
	in for each Element proxy on each thread. So the clearQ can become
	single threaded, cleanly, but filling the queue requires thread 
	juggling.
	Note though that the queue is typically much fewer calls than the # of 
	calls needed when clearing it, since the target Msg distributes queue
	calls to many Data destinations.
	- At send time put messages through a crossbar that knows which
	threads need to receive each msg. So originating thread A has
	holding queues for threads A-Z. After barrier, we now access the queues
	by destination thread.
	- Simple implementation would be that each originating message just
	goes to each target thread holding queue. Clearly need to structure up
	this to only deliver a given message to the queues that really need it.
	Note that this is exactly what I would have had to do for multinode 
	stuff. 
		- This looks like a SparseMsg.
		- The target thread gets one big fat queue, rather than one
		per Element. Not hard, just need to add dest Id to Qinfo.
		- Alternative to one fat queue is for the message delivery
		crossbar to direct messages already into the correct Element.
	- another approach: give each Message a unique id for the entire
	simulation. When sending info, we don't now need src or dest Element,
	as this is all in the Msg. So a single fat queue is possible for
	each thread.
		- Qinfo already holds a MsgId, but it will have to become
			an unsigned int rather than short.
		- Will need a boolean to indicate msg direction.
		- Will need to expand srcIndex to be a full DataId.
		- funcId, and size remain.
		- Would like a way to cleanly handle tgtIndex for SendTo.
=============================================================================
29 Nov.
Current design looks like this:
- Single queue per thread, rather than one per Element. To be more precise,
  each thread has one incoming queue for each thread (including itself) and
  one outgoing queue for each thread (including itself).
  	- We could in principle replace this with a single pair of in and out
	queues per node, if they were individually thread-safe. Tradeoff in
	memory and speed. Probably dreadful for many threads.
		- Suppose a given thread wants to put data out at a fraction
		F of total time, and there are T threads. Then the fraction of
		time any thread is blocked by this thread is F/T.
		So the total fraction of time that any given thread is blocked
		is F again.
		- Suppose we provide a further number Q of queues per thread,
		to subdivide the total set T. Now the fraction of time
		any other thread is blocked is F/( Q * T ), so the total
		blockage fraction is F/Q. This may help, but gets messy.
		- Can we do the same for internode calls? Yes, there is an
		mpi_any_source option in recv. One can narrow things down
		by using tags for groups of messages.
	- Some object operations (like 'get calls' ) put stuff right back
	onto the queue. This will cause problems especially when we have a
	single queue per thread. Dual queues?
	- Also consider sched interleaving, where we do one set of objects 
	first so as to get their data ready for transfer. Again, dual queues
	needed. These could be the input and output queues for MPI.
	- In some cases would want a broadcast queue. If the # of target
	threads is more than half the total, for example, may as well send
	data once to all. Huge savings in memory too.
- Messages now have a unique system-wide Id. Qinfo refers to this, so queue
	knows (through Msg) which is target Element.
- Msg::addToQ is the function which puts packets onto the Queues. This
	is now expanded to decide which threads should receive each packet.
	For example, a large SparseMsg might use one SparseMatrix for the queue
	loading, and another on each thread, for clearing. The setup of this
	large SparseMsg will be the key step in thread/node decomposition.
- What happens to different clocks?
	- Some kinds of data transfer do not have to occur each dt: spike msgs.
	Want to accumulate them.
	- Sometimes we simply have two very different clocks going. Should we
	simply send data with the first available?
	- Sometimes we have small (fast) dt clocks within a thread, and can
	get by with slower dt clocks between threads. How to do?
		- Just check queue size for inter-thread data?

I think the next step is an implementation. Too many unknowns here.
- Rebuild current stuff as just a single queue. Benchmark.
- Set up simple implementation: one queue on each thread, for each target
	thread. So a 4-thread system would have 16 queues. Then there are the
	outgoing queues too.
	- Benchmark
- Set up more economical implementation: Each thread manages a thread-safe
	input queue, and is its only consumer. 
	Need 4x2 queues for a 4 thread system.
	- Benchmark.
- convert from threads to MPI.
=============================================================================
30 Nov.
Just for reference, the starting version is 1466.
We need to put a queue on each thread. Where should it reside?
- Clock. This definitely knows about the queues.
	But it requires that the scheduling be set up before even simple
	messages can be handled.
- Shell. This will be replicated on each node, but not necessarily for each
	thread.
	I already have a Shell dependency for set/get ops using msgs.
- ProcInfo.
- Static field of Element

call stack for addToQ:
Eref::asend: Creates Qinfo. calls Conn::asend.
Conn::asend: iterates through Msgs. Each calls addToQ. Qinfo is passed in.
Msg::addToQ: Checks which is src and which dest Element. Calls addToQ on 
	non-calling Element. Qinfo is passed in.
Element::addToQ: calls Qinfo::addToQ on the passed in qinfo. Passes in queue.
	This actualy does the queue juggling.

So perhaps the Queue should be on Qinfo as a static. if so, the call sequence
would be:
Eref::asend: Creates Qinfo. calls Conn::asend.
Conn::asend: iterates through Msgs. Each calls addToQ. Qinfo is passed in.
Msg::addToQ: Checks which is src and which dest Element. Fills in direction
	flag on Qinfo. Figures out the target Queue. Calls Qinfo::addToQ with
	the chosen queue index.
Qinfo::addToQ: does the queue work.

So it looks like want to put the queues on Qinfo. This seems like a sensible
place.

=============================================================================
5 Dec.
Need to work out relationship between global Msg vector, and managing msgs on
individual Elements.

Currently Element manages a vector of Msg ptrs. Element also manages
a vector of Conns. The Conns too manage a vector of Msg ptrs. To top it all,
each Msg keeps track of its index in the Element::Msg vector.

We need the fast lookup only for Conns, and there too it is a very tiny part
of the comp load. May as well always use MsgIds.
This may let us separate them... use MsgIds only for incoming, Conn only
for outgoing. Problem with bidirectional msgs.

Now we have to separate the MsgId as the index of the Msg on the Element,
from the MsgId as the universal Id for the Msg.
Note that we never seem to use either Msg::mid1() or Msg::mid2(). 
Likewise, the only time we ever use Element::getMsg is in clearQ.

Working on Qinfo::addToQ

Old call stack for clearQ:
Tick::advance: calls its conn::clearQ
Conn::clearQ: goes through all Msgs, calls 
Msg::clearQ: calls e2->clearQ. Why e2? because e1 is the Tick.
Element::clearQ: goes through its own queue, calls execFunc for each entry.
Element::ExecFunc: This figures out if it is a sendTo or regular msg.
	sendTo: figures out target, Qinfo works out op func, calls op.
	regular: calls Msg::exec on buf
	Msg::exec: figures out direction, traverses all targets calling op.

New version:
Tick::advance: calls Qinfo::clearQ with the appropriate Qid.
Qinfo::clearQ: goes through its own queue. Looks up for each entry, calls
Msg::exec( figures out direction, traverses targets calling op.

Working on Msgs.
Need to modify Elements so that they maintain a vector of Mids rather
than of pointers to Msgs. Likewise Conns.
Need to fix Element::dropMsg


=============================================================================

8 Dec
Still trying to compile, but enough done that a checkin is needed. 1476.
Compiled. Crashes. Checkin as 1477.

Currently stuck in testSendSpike around line 385. The message has been
added successfully, but in the IntFire::process when we try to send data,
there are no Msgs in the conn.

OK, turns out that the msg is deleted on the 'set' call.

=============================================================================

9 Dec. 
Got unit tests working up to testSendSpike. Checkin as 1479.

Ran into an issue where it seems like every time I do a set/get call,
it clears out all pre-existing messages. Happens at testAsync.cpp:567.

Fixed. Now it runs through the unit tests, but it does not like the old
values for Q size in testSparseMsg. I'm not sure if this is an issue with
ordering of the random number usage, or if it is fundamental. The output
looks reasonable.

Valgrind is happy with it first pass.

OK, checked out the old 1466 and tested for unit tests. Works fine, so
nothing has changed with the random number generator.

OK, put in a cout to check order of values. Order is correct and
matches with the old version.

Compared the printout of activation: turns out they were identical all along.
With this sorted it is easy to see that the difference in Q size was just 
because Qinfo is now 32 rather than 24 bytes. With this fixed, the unit
tests all clear. Valgrind is happy too. Checkin as 1481.

A bit of benchmarking: Ran using 'time' after enabling the unit tests in
main.cpp, and setting #runsteps to 1000 from 5.
36.22, 36.19, 36.17 sec are the times. Marginally faster than earlier even
though I have a bigger Qinfo. Good. Checked in as 1482 for reference.

Now I can go on to serious stuff with threads and the like.

Putting data onto Queues:
- Data goes to queue for local thread: 
	- Easy to send data
	- Emptying queue is messy.
- Data goes to separate queue for each dest thread
	- Need thread safety for filling queues.
	- Can pre-balance the incoming stuff on each thread.
	- Msg needs initial stage for dispersal of data.
	- Works well with multinode systems.
		- Want to have outgoing queue per node, not per thread on
		node. But this is just a matter of subdividing at target.
Emptying Queues:
- Each thread has its own incoming queue (option 2 above)
	- Msg pre-separates the sparse matrix per thread basis. Could be costly,
		requiring substantial duplication of msg per thread.
	- Need some care to handle msgs put back on queue as it is being
		cleared.
- Shared queue between threads
	- Need to ensure that any given object is handled by only one
		thread at a time.
	- Could do dynamic load balancing: just hand out targets as threads
		request them.
	- Could also partition based on object DataId. OK only if there
		are many target objects.
	- Something like this needed for async msgs arriving on a node.

Just implement a PsparseMsg and see what happens.
=============================================================================
10 Dec 2009.
Parallel sparse Msg and other Msgs.

Potential output queues: 
	- global: to go to all threads and nodes.
	- local: to go to specific threads and nodes
	- group: to go to groups of threads and nodes.

For a typical neural network with 10K randomly assigned targets per synapse:
	- If # nodes serving network is < 1K, we will usually want to
	send spike events to all the nodes/threads handling the network,
	as there will usually be one or more target neurons on each node.
	- Here we want each of the process threads to dump msgs into its own
	output queue, and later merge all the queues for processing.
	- Each process thread will have its own sparse matrix to handle the
	correct subset of synapses.

For a more 'clumpy' network where data goes to a small subset of threads,
	typically from within the small subset:
	- We want comms to be global within the subset. See above.
	- All other comms should be node specific. See below.

For a particularly sparse network, where any given Msg goes to a small random
	set of other threads:
	- Have thread-safe queues for each dest thread
	- Msgs know which target thread to go for.
	- Each 'send' call is directed by the Msg to the appropriate queue
	- When done, each local thread scans its input queue.
	- Msgs are set up with suitable subset of targets for the thread.

SUMMARY:
If we merge the 'global' with the 'group' cases, we need to maintain only
as many queues as there are threads + groups.
	- Threads within a group:
		- each thread has its own 'output' queue for data to go 
			within the group.
			- No special threading stuff for adding to queue.
		- Each group as a whole maintains an 'input' queue for
			stuff coming into the group from other groups.
			- This queue has to be thread-safe.
	- Threads outside any group:
		- Maintain only the thread-safe 'input' queue as above.

Any given Msg is between two Elements. We will assume that this pair is 
always in only one of the categories above: within a group or outside a group.

Data transfer of 'group' queues, from perspective of each thread.
	- During process, put off-group stuff into off-group queues.
		- on-node other threads; and off-node data each have queues.
	- During process, put in-group data into own 'output' queue.
	- When Process is done, consolidate all in-group 'output' queues.
	- Send consolidated in-group queue to all nodes in group
	- off-group, on-node queues are handled by their owner threads.
	- Send off-group, off-node queues to target nodes.
	- Receive consolidated queues from on-group nodes.
		[further consolidate?]
	- Receive mythread input queue from off-group, on-node threads
	- Recieve anythread input queues from off-group off-node
		[Consolidate input queues ?]
	- Iterate through consolidated queue for in-group, on-node.
	- Iterate through consolidated queue for in-group, off-node.
	- Iterate through input queue for off-group, on-node
	- Iterate through input queue for off-group, off-node.
		- Each thread will have to pick subset of entries to handle.

Data transfer of 'non-group' queues, from perspective of each thread:
	- During process, put off-group stuff into off-group queues.
		- on-node other threads; and off-node data each have queues.
	- During process, put own stuff into own input queue.
	- off-group, on-node queues are handled by their owner threads.
	- Send off-group, off-node queues to target nodes.
	- Receive mythread input queue from off-group, on-node threads
	- Recieve anythread input queues from off-group off-node
		[Consolidate input queues ?]
	- Iterate through input queue for off-group, on-node
	- Iterate through input queue for off-group, off-node.
		- Each thread will have to pick subset of entries to handle.

This is surprisingly messy. NEURON assumes everthing is in same group,
and broadcasts everything. Single group is probably sensible also for any
simulation run on a multicore single node system. However:
	- graphics threads will typically be off-group
	- Parser, systems setup and control threads will be off-group.
	- Could treat each solo thread as a one-thread group, thus reducing
		the problem.
	- On a single node, the overhead with consolidating everything into
		a single queue and scanning through that on all threads is
		not too bad. Main issue is skipping uninteresting msgs.

From the Msg viewpoint within a group:
	- addToQueue: Dump all outgoing stuff into the local thread queue.
	- Then by magic all queues are consolidated
	- Qinfo::clearQ: goes to Msg,
		- Msg decides what it should do on current thread.


- If Msg has < 100 targets, just do it on any given thread (depends on weight
	of each target func)
- If Msg has lots of targets, split by # of threads.

=============================================================================
12 Dec 2009.
Thread management. How does each msg know which thread it is on?

Clearing Q:
Clock, therefore Tick will know threadId.
Qinfo::clearQ could take an argument for the threadId
Msg::exec currently only takes buf as an argument, could also take threadId.
	Do we want to insert thread id into Qinfo? No, the buffer is shared.
	Options:
		- single Msg for all the targets, and it picks targets by thread
		- multiple Msgs, one per thread
	We need to be able to rebalance the targets. Also need a single point
	of reference for properties of msg. So use single Msg, pass in threadId.

Sending msgs:
Tick::advance( ProcInfo* )
Conn::process( ProcInfo* )
Msg::process( ProcInfo* )
Element::process( ProcInfo* )
Data::process( ProcInfo*, Eref )
SrcFinfo::send
Eref::asend, Eref::tsend
Conn::asend
Msg::addToQ
	While we have ProcInfo, we can put threadid into it.
	At this point the send functions do not handle thread info in any way.

Do we want to pass in threadId or qId?
	- threadId combines with internal Msg info to work out queue.
	- qId would be more explicit but would need some logic that depends
	on internal Msg info anyway.

Do we keep track of threadId within the node or for the entire system?
	- Related: is there a global we can access for the node Id?
	Well, we can pass ProcInfo around everywhere and it can keep track of
	thread as well as node id. Assume all is known.

Where do we assign and create threads?
	- Clock::start creates threads that last till end of run. 
		- Calling thread (from Shell) goes to sleep till run done.
		- each Tick advances through the sim schedule taking the 
			threadId.
		- We need to manage barriers or conditions to ensure thread 
			sync, within each Tick::advance.

OK, went through and implemented passing in of ProcInfo ptr into all 'send'
functions. Now we have thread info everywhere. If ProcInfo also holds node
info that too is present. Lots of changes. Would like to check in here but
am off net.

Valgrind: is happy.

Next step: start off simulation with optional # of cores. There really should
be a system call to find # of CPUs. We would typically use 1 thread on 
Shell, assorted threads for GUI, and then as many threads as there are 
cores for the simulations.

Minor addition to command line so moose can start up on specified # of cores.
moose -cores <n>
Default, of course, is 1. Would like to autodetect.

Implementation.

Shell should have a 'start' function which sets off the clocks.
This should be on thread 0, and it should block. 
Issue 1: We currently do not allow multiple shells. If we do, we will 
be in trouble because of assumptions about the thread with the shell,
and the use of static globals on it.  Defer for now.
Issue 2: Pthread_create( thread, attr, start_routine, arg)
	This is a bit awkward with C++. I will want to write a static function
	that goes to the appropriate Clock (there may be more than one) and
	identifies the appropriate thread. So my arg will have to identify
	both the clock and the thread. 
	

Need 2 barriers per tick: end of clearQ and end of Process.
Is there an efficient way to do this?
Condition variables: Each worker increments barrier count, this goes
on until it reaches # of threads available. Then send a signal to all to
proceed. This may well be what barrier does.

For now, just plug in the barriers. Later we can try for efficiency.

Do we make one clock+ticks set for each thread?
	- Barriers means that any calculations on them are redundant.
	- With one set, how would we call Tick::advanceThread?
	- We could have just thread0 manage the Clock juggling of threads
	- All the other threads do the 'advanceThread' call, but with
		a distinct procInfo.

=============================================================================
17 Dec 2009
Various options for handling threads and scheduling.
- Use current format, have the FIRSTWORKER thread advance and sort the 
	ticks. The other worker threads go through the same loops but don't
	alter any ticks. 
	The call sequence remains the same. 
	- Starting simulations: This is a bit tricky. 
		- clearQ calls will block, leaving entries dangling.
		- Other threads may want to addToQ or clearQ.
		- Note that this issue also needs handling for single-thread
		 	processing. Will also come up when rebuilding messaging.
			Need to shunt all such calls off to a local queue,
			but how do we clear it? Probably in Process.

- Make separate clocks and ticks for each thread.
	Easier call sequence, but tricky managing all the objects.

Let thread 0 be the parser thread. 
	- In single thread mode, parser has a non-preemptive event loop.
	This calls the GUI, the TTY and then the clearQ and Process
	for the shell, directly. Has to call clearQ because we may have info
	going to different programs, and for symmetry with multithread mode. 
		- The shell calls start on the clock, and does so directly:
		Not through the clearQ mechanism.
		- The shell knows the disposition of threads, and decides if it
		should call the regular start or just set off new threads.
	- In multithread mode, we run GUI, TTY and parser on separate threads.
	The parser is thread 0 and does a similar event loop with clearQ and
	process, except that it does not have to handle the GUI and TTY.

Likely threads: Parser, Python, TTY, GUI, then workers etc.

=============================================================================
21 Dec 2009.
Checked in 1491 with initial arguments to main() to select threading options. 

Starting sims: Where to create threads?
	- On Shell:
		- It knows all about the threading/node structure
		- Needs to call a Clock function directly: unpleasant
		- It has a clean way to separate 'start' calls away from clearQ
	- On Clock:
		- It has to ask Shell about threading.
		- It can deal with the clock functions internally.
		- No clean way to separate 'start' from clearQ.
		- Conceptually, the separate of clock calls is a scheduling
			not a shell function.

Looks like the Shell is the way to go.

Checked in as 1492.


Need to Fix up Ids to only handle Elements. Let Erefs deal with indices.
=============================================================================
22 Dec 2009.
Set up Shell::start which calls single thread or multithread versions of the
scheduling.

Setup seems to work, I create threads and harvest them. Now to actually get
them to do some work. Checked in as 1494.

Implemented threads through to TickPtrs.
Working on compilation.

Compiled, OK. Working on testing threaded clock tick sequencing. A little
tangent to implement setClock in Shell. Stuck in checkGet/checkSet,
which for some reason wants to prepend set_ onto the field name. This should
only be done in ValueFinfo, not in any of the others.
I see what happens: SetGet is quite independent of Finfos. 
I should separate out a SetField and GetField templated function from
the basic SetGet1< type >.

Done. Now we have Field<type>::set and Field<type>::get for doing fields,
which puts in the set_ and get_ prefixes. The others all use the regular
names of the field.
After some more work this now goes some way into the barriers before crashing.


=============================================================================
27 Dec 2009

Incremental progress on getting threaded scheduling to work. The usual
pthread pains.
At this point I have the system sometimes giving the correct sequence with
2 or 3 threads. 
checked in as 1498.
Rather reliable hang at 4 threads.
Seems like thread 0 has gotten one step ahead of the rest, and is hence
clearing the barrier instead of thread 2.

=============================================================================
28 Dec 2009
Turns out that this same problem (thread 0 getting one step ahead) can also
happen even with 2 or 3 threads, only less often than with 4 threads.

Sprinkled lots of barriers around in Clock::tStart. This fixed it. Works
for as many as 32 threads. Now to clean up. Checked in as 1501.

Trying to set up using mutexes. But how to set up the flag for the mutex?
=============================================================================
29 Dec 2009
Wrote a much tighter version of the Clock::tStart function that uses a
mutex and a counter variable to manage sorting and looping for all threads.
Compiled. Clears with 32 threads, but I'm a bit dubious about the 
stepping. Look at the sequence of output here:
Advance at 7 on thread 0
Advance at 7 on thread 0
Advance at 8 on thread 1
Advance at 8 on thread 0
Advance at 9 on thread 1
Advance at TickPtr::advance: post barrier2 at time = 9 on thread 1
Advance at 9 on thread 0
Advance at 9 on thread 1

=============================================================================
30 Dec 2009.
Analyzing sequence
1. Not reaching final tick. Stops at t=9.
2. The tick sequence is OK except that the barrier needs to be in the
	ticks, not the tickPtr.
3. The tStart routine goes through its while loop sometimes without
	movement on the TickPtr::advance. This seems to be due to the 
	nextTime_ field not really advancing, and hence a dummy attempt to
	get TickPtr to advance. Need to check. Confirmed. Turns out it happens
	even with the single-thread scheduling. Inefficient but the end 
	sequence is still OK. Let's fix. After analysis, looks OK.
4. Does too many steps as reported in "Advance at <time> on thread <t>"
5. Can we eliminate the second barrier on TickPtr::advance?
	- It is safe to assign nextTime_ when the first thread emerges, since
		all threads will have crossed the only reading of nextTime_.
	- When the first thread emerges, it should be safe to sort the 
		TickPtrs. Probably should use separate mutex.

	
There is a nastier problem with the threading case, in TickPtr::advance.
The issue is that I use thread 0 to advance nextTime. If thread 1
happens to emerge first nextTime is not incremented, so the thread is
able to go around the loop till the barrier again. Need to protect
nextTime using a barrier, or to have one independently for each thread.

Brute force implementation using a barrier works. But there is still oddness
in the sequencing of 'Advance'. Checked in as 1506.

Some cleanup by way of eliminating barriers in TickPtr::advance. More to go.
Checked in as 1507.

Figured out why the "Advance at <time> on thread <thread>"
sometimes does an extra round for one thread or another. The cout is
called after the function emerges from TickPtr::advance. If the second
thread to emerge does so after the first thread has done the sortTickPtrs,
then nextTime_ will already have been incremented. So the second
emerger reports a different time. No functional impact.
Item 4 above has the same issue, and can also be ignored.


Next steps:
- Implement item 5.
* Move barrier into Tick.
* Implement thread sched test as unit test.
- Set up real calculation, check values
- Benchmark
- Implement condition_wait instead of barrier.
- Benchmark with condition_wait instead of barrier.

=============================================================================
31 Dec 2009.
Moved barrier into Tick. Compiles, seems to work. Need to convert the printf
debugging of the threaded scheduling into a unit test.
=============================================================================
1 Jan 2010
Some cleanup of printf debugging. Checked in as 1515. 
I want to try to eliminate the remaining 'barrier' call in TickPtr::advance,
and replace it with a simple mutex. This did not work. I need to come back to 
it.

Implemented thread sched test as a unit test.
Done. Works. Checked in as 1516. Valgrind isn't happy though. Perhaps I need
to free some more barriers and things. Did so. Still isn't happy.
Checked in as 1517.
Turns out this is a known bug/feature of pthreads on Linux. See 
https://bugzilla.redhat.com/show_bug.cgi?id=483821
The discussion indicates that pthread_exit doesn't quite clean up, and
that it isn't likely to be fixed. So I'll leave it at this. It is only 56 bytes.

Onward now to setting up a real calculation. First phase is to totally
mess up the queue handling system. For now I've gone for a double-buffer
arrangment, but perhaps this is a false economy. For multinode stuff I'll need
to memcpy data onto the MPI outgoing buffer, anyway.

=============================================================================
3 Jan 2010
Is memcpy thread-safe? That is, if I have two adjacent segments of memory
into which I copy data, can I do this on different threads and be sure that
they will not step on each others' memory at the boundary?

Need a bit more general handling of mergeQ/readQ/clearQ/addToQ.

mergeQ is supposed to take writable per-thread Qs, combine them into another
	Q, and clear the per-thread Qs.
readQ is supposed to march through a Q performing operations as it specifies,
	perhaps by many threads at once. The Msg::exec function does the
	operation. For example, Tick::advance calls readQ on many threads.
	Each Msg::exec selects some range of Msg targets to scan through with
	the operation, set by the combination of threadId and Msg target list.
clearQ is supposed to zero out the Q contents.
addToQ puts data into the specified queue. We need to separate this function
	into the thread-unsafe version, which is called by Msgs operating
	within a group, and the thread-safe version, which is called when
	Msgs send data outside their own group.

Should I separate out the input and output queues, that readQ and addToQ 
	work on, respectively?
	- It will make it OK to allow immediate operations that affect Qs,
		during readQ. For example, the 'get' function needs to put
		data into a queue.
	- Still does not eliminate issue of thread safety in cases where
		many threads must write to same Q.

Group: has one inQ, and as many outQs as there are threads. These outQs
	do not need to be thread-safe, they are protected by the scheduler so
	they only get input from their own thread.
	The inQ is thread-safe. Data typically comes into it only during
	'process'. During 'read' and 'clear' it is blocked for write access.
	During 'process' it uses mutexes to protect data coming in. The
	Tick also does a mergeQ which takes data from a set of outQs and
	combines them into the inQ. This step looks like it has to be serial,
	which is unfortunate.

	- When doing MPI: Each group wants to 'send' all local info 
	to all other nodes working for the group. So it will want to do a 
	'send' on the inQ. This can happen on one thread while the worker
	threads clear the inQ on their own. However the MPI man page suggests
	that one should not access the contents of the send buffer till the
	send completes. See below for how to put these together.

Structural options with threads vs MPI:
	1. Always use MPI, never use threads. Suggested by a couple of people,
		also compatible with BlueGene machines.
	2. Always use threads, and use MPI only in virtual shared-memory mode.
		Never heard this option, sounds dubious.
	3. Use threads on local node whenever multicore, use MPI between nodes
		whenever multinode.
	4. Use threads on local node whenever multicore but no MPI, otherwise
		use MPI both for local cores and for other nodes.

For now, and based on the MPI analysis below, it looks like #1 would
be terribly slow. Worth benchmarking though.

For pure MPI, there are a couple of options:
	1. blocking vs non-blocking calls. Blocking is slower and prone to
		locks. Non-blocking requires extra buffers
	2. Async vs sync. Much better comm/comp balance if async. If we provide
		a guarantee that the async does not slip too far, we're OK.

For now, assume non-blocking and async.

For pure MPI, this is how data would go:
	1. Local node has a single outQ and one inQ. It has separate extQs,
		one for each group in the simulation.
	2. During Process, it adds data into outQ, and into each of the 
		extQs as per message targeting.
	3. At end of Process, each node sends a special message with currTime
		to node 0. Node 0 also sends info about the most laggard
		currTime.
	 	On all nodes it then copies outQs onto inQ. InQ goes into
		a 'send' buffer. Each mpiQ goes into its respective buffer.
		All these buffers go out using iSend calls.
	4. System always keeps an irecV posted for 'world'. 
		Whenever a 'test' clears and data has arrived, it grabs 
		data and pushes it into inQ, and posts another irecV.
		Could use memcpy or double buffering.
	5. At regular intervals throughout clearQ, it checks for arrival of
		data, and does 4 if so. Since the inQ is circular, it could
		clear up data as it goes along.
	6. On node 0, when the special Msg with the currTime arrives, it
		analyzes that and decides about load balancing. At the least
		it would send out a message to the way-ahead nodes to throttle
		back. If throttling, any given node, 
		it continues in the clearQ loop from 4 till node 0 sends to say
		to resume.  This won't be needed if we have a
		step lock for sync data.
	7. At end of clearQ, it zeros out the queue.
	8. At regular intervals throughout process, it checks for arrival of
		data, and does 4. if so. Note it does not do clearQ, just 
		accumulates data.

From the viewpoint of Queues, what we take from this is that if each node
is multithreaded, the queues needed are:
	One thread-safe inQ for each group on the node
	One non-safe outQ for each thread (i.e. all within-group threads)
	One thread-safe extQ for each non-self group in the simulation, 
		including on and off-node.
Buffers needed:
	One for catenation of all outQs
	one for each extQ

So the Queue has to also have info about groups. 
	Each group is known by: 
		# of local threads
		Group start index to index outQs
		list of external nodes or
		COM of external nodes?
	
So the indexing of queues is:
InQ: group #
outQ: groupStart + outQ #

So the inQ and the extQs are in the same set of indices, and the groupStart
begins at #groups for group 0 and goes up accordingly.

Actually this doesn't work, since it would involve renumbering queues when new
groups are added. So the current structure is:
InQ: first available #, in other words, groupStart.
outQs: Next set of numThreads #s.

When we do a mergeQ, we identify the group# and the rest happens internally
based on the Queue grouping.

Implemented, compiled, doesn't run. But many things have changed so I'll 
check it in. 1522.

=============================================================================
5 Jan 2010
Added a line in the unit test for Ticks : Qinfo::addSimGroup. This fixes up
the system, it clears the unit tests for many threads. But now there is a
problem that the unit tests will mess up subsequent scheduling. So I need
to put in a function to clear out old sim groups too. This will be tricky,
as all messages have to be recomputed. Should be responsibility of the 
Shell.

Implemented a stub function for Shell::loadBalance. It is called at init, and
can be called later. This replaces the deprecated Qinfo::setNumQs
Runs, clears unit tests with up to 16 threads. Valgrind is also OK with it
with the same caveat about 56 bytes.
Checked in as 1524.

=============================================================================
22 Jan 2010
Finally back to work after a bad spell of meetings and other duties.

Minor cleanup of headers. 
Did explicit seeding for the random number generator for the matrix unit tests.
Checked in as 1541.

Working on framework for a proper test. Given the current structure, the
remaining step is to assign workloads to the appropriate thread. 
Tick::advance( e, ProcInfo* info)
	if ( info->isFirstThread() )
		Qinfo::mergeQ( info->groupId );
	Qinfo::readQ( ProcInfo info ); // threadId also identifies outQ index.
		extract the Qinfo from the outQ buffer using info->threadId,
			which also identifies outQ index.
		sometimes: do hackForSendTo. This will require care about outQ
		Normally: extract msg from Qinfo
			m->exec( buf, info )
			Msg::exec needs two kinds of info from the ProcInfo.
				- a way to pass threadId to the op,
					in case it does output
				- A way to decide which subset of events to 
					operate on, based on the thread#
					within the group. Note that this cannot
					be the threadId, as the threadId may
					be reassigned.

This is OK for within group messaging. Need to work out how to pass
messages to other groups: they will have their own InQs which will typically
be busy during clearQ. 
	Options:
		- Do not allow messages to go out during clearQ
			- But we already have a return message in get()
				which is messy as it uses the Shell ProcInfo.
		- Have a thread-safe inQ buffer for each group, for messages
			coming in from outside.
	Do this later, after testing the within-group messaging.


Did the first phase from Tick::advance till Msg::exec. Lots of cleaning up
of code, went well. Compiles but doesn't clear unit tests.

=============================================================================
23 Jan 2010
Fixed, clears unit tests. Checkin 1543.

Setting up the ProcInfo to pass the right values in. Croaks. Turns out
that we have added two SimGroups: one for the shell, and one for everything
else. However, this fails because the # of threads is just numCores, whereas
the shell is assumed to be on another thread.

Went through and fixed up the non-threaded calculations. moose -s.
Now clears unit tests with this. Checkin 1544.
Now got it to work with different numbers of threads. Checkin 1545.
Next step is to build matrix using threading to decide which messages to
send where.

Some work to do in defining what Elements/Messages belong in what group.
	This determines which threads they are on, as well as numerous other
	things. 
For now I need to get the system to work just enough to do the IntFire network
	test.

=============================================================================
24 Jan 2010
Working through getting the IntFire network to be scheduled through threads.
Current problem: the Qinfo:mergeQ( groupId) function in Tick::advance.
How do we know which groupId to use? The ProcInfo holds the thread# in the
group, but perhaps it should also hold the group#. Done.

Difficult patch. Converting the original single queue array into inQ and outQ.
Much cleanup. Compiles but crashes.
=============================================================================
25 Jan 2010
Struggling with bugs. Cleared first one in mergeQ. Checkin as 1547.

Next one is stuck with the 'get' function in testGet():207.
=============================================================================
26 Jan 2010.
A lot of struggling later, turns out that the issue was that now I have
separated inQ and outQ, and the data was still waiting in the outQ. So it
just needed another clearQ call.

Having cleared that test, now on to the next one: testSetGet().
Same issue here: I need a double clearQ. With that in place, it clears all
the unit tests except the last one, using the -s option.
Checkin as 1548.

Fixed a bug in setting up number of threads in each queue. With that done, the
system clears all but the last unit test for various # of threads.
Checkin as 1549.

Narrowed it down to the SingleMsg between the clock tick and the 
IntFire element.

Replaced the ad-hoc message creation with SingleMsg::add. This took a bit
of work on setting up the message stub in IntFire, but it worked for single-
thread mode as well as up to 4 threads. Doesn't yet do partitioning of the
workload, but it clears unit tests.
Checkin as 1550.

=============================================================================
27 Jan 2010.
Further progress, got it to call the IntFire::process, which seems to be
happening in the correct sequence over all threads. Still crashes at the
end of it, and not clear yet if it is doing the right calculations.
Also I can now see a problem with the scheduling in moose -s (single-thread)
mode. 
Checkin as 1551.

The crash at the end of the run is due to left-over data in outQ_[0].
However, outQ_[0] should be reserved for the SimGroup0, which is used by the
Shell, and not touched by the runtime calculations.

Bypassing for now, working on PsparseMsg and its compilation.
Checkin as 1553.

=============================================================================
28 Jan 2010
Put in a unit test for the load balancing function in PsparseMsg. Works.
Tried out PsparseMsg in the testThreadIntFireNetwork. Still doesn't do what
it should.
Checkin as 1558.

Fixed up thread partitioning in Element::process. Currently hard-coded
in, doesn't look pretty, but clears tests till the final queueing segv.
Still need to sort out the threaded message inputs.
Checkin as 1559.

Minor hack to patch the final queueing segv by clearing the queue manually.
Still to properly track down, but at least it now clears unit tests.
Fix to the Element::process subdividing target Element indices.
Checkin as 1560.

The potential in IntFire does not cross threshold as we init it, so the 
network does nothing.
Looks like the initialization of IntFire isn't right.
=============================================================================
29 Jn 2010
Fixed a problem with the Qinfo::addToQ, which was putting everything into
outQ_[0]. Now there are some events coming through to the IntFire in the
thread test, but not as many as there should be.

With more debugging it gets messier. Most (usually all) of the first round
of spike messages go into just one thread. No subsequent messages emerge.

Lowered threshold so that all neurons will always fire.
Now all outQs hold the identical # of requests, all steps.

Raised threshold a bit to 0.1. Earlier was 0.2. Now we have
variable firing, but firing happens on all outQs. Except first step,
	where again almost all output is on a single queue.

Crashes occasionally, more often with more threads.

Checked Vm. That was it. It is only being set on one of the threads, the
first one.

Closer check. Turns out Vm is set correctly on all objects, right up to the
line before Shell::start in testThreadIntFireNetwork(). Something else
happens to zero it out on all but one thread.

Is there an issue with field access by indexing? Something odd about the
addresses of the IntFires.

Tracked it down to a silly pointer assignment error in Element::process.
With that fixed, it looks like the whole things works, including the 
numerical values of the Vm on different IntFires.
Also I'm not seeing crashes.

Checkin as 1562.

Some benchmarks. Increased runsteps to 1000 in testThreadIntFireNetwork(),
compiled with O3.

Previous	./moose		./moose -c 2	-c 4		-c 8
36.22		35.8		42.6/21.9	41.2/21.9	40.2/21.2
36.19		35.7		42.8/22.1	41.3/21.9	40.1/21.2
36.17		35.8		42.7/22.0	41.3/21.8	40.1/21.3

Surprising scaling with more threads. I really need to do profiling on this
to see where the loss in efficiency comes. 
I suspect in memcpy, as more threads don't seem to matter.

Made profile files:
profile.1thread
profile.2thread

I don't really see any difference. Not sure how to handle gprof with multiple
threads.
=============================================================================

2 Feb
Trying to set up unit tests for IntFire in scheduling, independent of #
of threads. Uses Vm as the calculated quantity. Currently doesn't even
come out.

TickPtr is a mess. Should just have the pointer to a Tick, as the TickPtr
gets sorted and hence shuffled all around. Instead it has a vector of 
pointers, and other nasty stuff.

Also the sort function should get rid of Ticks that lack targets.

Also the selected Conn should be specific for the tick, not a generic one.

What has happened in this bug is that there are two Ticks with small enough
dts to be called: tick0 (set to dt = 0.2) and tick1 (legacy set to 1.0)

The message to the targets was hard-coded to be on slot 10.

The Tick::advance doesn't care which tick it is, it just calls the Conn on
slot 10. 

Clearly need to do some designing here.

Step 1: Setting up messages from Tick to target Elements.
Alternatives:
	- Original idea had been to have a separate conn for each tick,
	indexed by the Tick index. Traversal efficient.
	. Issue is that the regular Msg::add function assumes the ConnId from
	the SrcFinfo, which is hard-coded. 
	. I can fudge this using a special Msg::add variant, but then the 
	SrcFinfo information does not agree with what is on the Msg. This
	may confuse traversal functions.
	. I can fudge this too using a series of 'process' SrcFinfos, one
	for each tick. This is easy but a hack.
	- Alternative: use regular messaging. 
	. Issue is that we will need to serially traverse the msgs in the Conn
	till we find the one(s) that have the target Elements for this Tick.
	All the targets will sit on the same Conn.
	- Alternative: Separate Elements for each Tick.
	. Issue is the use of pointers to other Elements within the Clock.
	# This really comes back to the bigger pending issue of multilevel
	object indexing.
Decision: Use the simple option of having a series of 'process' SrcFinfos

Step 2: Ticks and TickPtrs. 
	- Original idea: Ticks are separate array entries in the Clock.
	TickPtrs were supposed to have managed these in a lightweight manner so
	that sorting of TickPtrs is cheap. Furthermore, we should not have to
	manage unused Ticks at all.
	. Issue: TickPtrs are now heavy, with vectors in them. Bad to sort.
	. Issue: Unused Ticks are currently still managed. Can solve this
	using info about which ticks are connected to targets.
Decision: I've removed a couple of fields from TickPtr. I think the rest,
	including the vector< Tick* >, are OK for now.

Step 3: SetClock, UseClock, Resched, rebuild: High-level control functions.
	- Original idea: Genesis BC options, would like not to have to do this
	at all.
	. Issue: need automatic scheduling.
	. Issue: We need to use SetClock anyway as display needs it.


Step 4: Automatic scheduling of elements.
Alternatives:
	- Predefine certain ticks for each kind of object. For example,
	t0, t1 should be for all neuronal modeling objects.
	. Issue: Solvers. Do we have a separate tick for each solver type?
		Worse, if we have two identical nonuniform dt solvers, do we
		need two ticks for them? Or do the ticks only define external
		update times?
	. Issue: Different solution methods. Suppose we do some mols using
		Gillespie, and others using RK5.
	- Assume we will always have a SimManager of some kind. This does
		load balancing, scheduling, and so on. The default one
		knows about solvers for neuronal and signaling models.
		This doesn't deal with any of the higher-level funcs, and
		instead sets up suitable messages directly.
	. Issue: Where do we go to set plot dts? Separate, unlocked thread?


Checkin as 1566 prior to starting work on these.

Implemented step 1. This already fixes up the problem with 
testThreadIntFireNetwork() in testScheduling.cpp.
Checkin as 1567.
Step 2 has also already been done.

Cleanup of unit tests. Clears up to 17 threads (more gets too big for the
SparseMatrix limit).
Checkin as 1568.
Valgrind is happy, up to the last 56 bytes.

Fixed the bug with the simulation duration in -s mode. May need to revisit.
Checkin as 1569.

Ran the benchmarks again on a 4-node machine (gj: AMD Opteron 2.4 Ghz)

./moose		./moose -c 2	-c 4		-c 8
41.1		36.7/19.1	39.8/11.2	43.8/12.4
41.5		43.8/24.1	40.8/11.1	44.0/12.4
41.3		39.2/20.5	42.1/11.5	43.4/12.4

So it looks like the scaling on these machines is far better than on the
laptop. Almost linear speedup with # of threads. Unclear why it is so much
better scaling than the laptop was.
Laptop: Intel(R) Core(TM)2 Duo CPU     U9400
GJ node: Dual Core AMD Opteron(tm) Processor 280
Anyway, it is reassuring because I had been wondering how to do benchmarking
to find out why the laptop wasn't scaling well.

Next steps:
- Test on 8-core machine.

- Set up MPI-based parallel calculations
	- Shell::create and 'add'
	- MPI picks up and sends out appropriate inQs.
	- Polling iRecv for incoming stuff
	- Figure out which queue to put the incoming stuff.
	- One thread broadcasting Shell commands to all nodes.
	- Hard-code setup of IntFire test.
	- Parallel Element creation
	- Parallel Message creation
	- Deferred object instantiation at time of Shell::loadBalance
		- may not work, because typical scripts do field assignment
		right away.

=============================================================================
2 Feb continued.

Ran the benchmarks again on an 8-node machine
(ghevar:  Intel(R) Xeon(R) CPU           E5520  @ 2.27GHz)

./moose		./moose -c 2	-c 4		-c 8		-c 16
25.0		22.9/11.9	21.6/5.9	22.2/3.2	31.4/3.2
28.0		22.9/12.5	22.0/6.0	23.5/3.8	30.1/2.9
28.1		22.8/12.4	21.6/6.1	23.6/3.8	31.5/3.1

Also moose -s seems to go quite a bit faster: ~23.5 sec 
Unfortunately the -c segvs with some of the higher core numbers: 15, 17.
Overall, looks pretty linear. The hyperthreading is occasionally helpful,
in some other runs I've gotten it as low as 2.4 sec on 16 'cores', which means
hyperthreading on all 8 cores. There is a lot of variability.

=============================================================================
5 Feb 2010
Put in hooks for Shell::create and Shell::add functions. Involved making
4-argument OpFunc, SetGet and SrcFinfo.
Checkin as 1583

=============================================================================
14 Feb 2010
Working on Shell::create. Compiles, crashes on test.
Turns out to be a fairly fundamental problem with argument conversion in
messaging, specifically for strings.
I need to extend the Conv<T> class to return a pointer to the desired 
object, which will normally be a direct cast. For strings, I need to be
clever to handle the conversion. Perhaps the way to go is to create an
instance of the Conv<T> object, which typically is just the desired ptr.
But for strings it does a local allocation etc.
Will need to do benchmarking after all this to ensure we don't slow things
down massively.


Some design thoughts regarding interaction between parser and shell.
3 options:

Option 1:

	Node 0				Node 1
	Parser
	  ^ |
Regular	  | |
funcs	  | V
	Shell::interface funcs
	(Handles sync)
	  ^
Msg	  |-------------------------------|
funcs	  |                               |
	  V                               V
	Shell::msg funcs		Shell::msg funcs

- Needs ptr to shell in parser, or static interface funcs.
- Shell::interface funcs are blocking so that the msg funcs return
- Needs 2 layers of funcs in Shell (interface vs msg)
	OR
  Needs separate interface class.


Option 2:

	Node 0				Node 1
	Parser
	  ^
Msg	  |-------------------------------|
funcs	  |                               |
	  V                               V
	Shell::msg funcs <------------>	Shell::msg funcs
			  Sync and merge

- Needs parser to talk to the msgs, as it goes out to all nodes.
- Sync and merge looks messy.



Option 3:

	Node 0				Node 1
	Parser
	  ^ 
Msg	  |
funcs	  V
	Shell::interface funcs
	(Handles sync)
	  ^
msg	  |-------------------------------|
funcs	  |                               |
	  V                               V
	Shell::msg funcs		Shell::msg funcs

- Needs either ptr or msg interface on parser. But both of them are
	awkward, since parser funcs would like to call a func and get a
	return value. The Msgs don't do that.
- Needs 2 layers of funcs, only both are msg type.
- Which level is blocking?  Presumably the first level of Msg funcs.

.......................................................................

Option 2 is silly. Option 3 is awkward and will munge the current parser 
framework. Option 1 it is then.

Some use cases:
create
addmsg
start
init


=============================================================================
15 Feb 2010
Working on cleanup of conversions. This has cascaded down to OpFunc (done)
and SetGet. SetGet is a bit confusing and I suspect it may be ripe for
cleanup. There is a regular 'set' command which does appropriate type
checking and type conversion. Then there is an iSet command, used only in
the iStrSet. iStrSet itself is never used. The iSet does a lot of things
redundantly with the simple 'set' command.

Commented all the body of the iSet and iStrSet out. 
Got the whole thing to compile.
Surprisingly, works all through and clears unit tests. The new test for
Shell::create bails out but doesn't crash. Need to tighten.
Checkin as 1602.

=============================================================================
20 Feb 2010
Not sure why previous compile worked, perhaps I didn't do a make clean.
Anyway, I had to fix up a EpFunc and OpFunc to also use the new Conv syntax.
And it still fails in Shell::create.

Much struggling later, realized that the apparent bad passing of arguments
through OpFunc into Shell::create was just a gdb bug/limitation. Valgrind
was OK with it. The real problem was that new Cinfos were not being put
into the cinfoMap.  With that fixed, it clears unit tests and valgrind.
Checkin as 1609.
Oops, there is a small new leak.

That is probably because I don't delete the test Element.
Working on delete. Now I run into the framework issues I had deferred
- I need to have a parserCreate function that is blocking and runs on the
	parser thread.
- I need to pass the Id of the new Element to all nodes
- I may need to set up arbitrary array dimensions.

I could do this in one messaging call if I expand to 5 arguments, and put the
dimensions of the array in a vector< unsigned int > argument. Or I could
have a separate redimensioning call and do that separately.

Struggling a little now with formalising SrcFinfos and corresponding slots.
Idea is that slots should identify a communications channel. Each SrcFinfo
has a funcIndex, which looks up the Element::targetFunc_ array for the
funcId of the specified Msg.

Items still to formalise:
- Fix up SrcFinfos to use Conv.
- How is a shared message set up?
	- All the funcsIds get assigned. The size of the targetFunc_ array has
		to handle all the possible SrcFinfos even if they are 
		never used.
- How do we handle the typechecking?
	- Need a SharedFinfo much like before. It does all the individual
		typechecks during the 'add' function, which will be moved
		into Shell. See below.

- How do we handle cases where the same 'send' deals with different
		SharedFinfo message targets?
	See below. The SrcFinfo provides a distinct FuncId for each Msg in
	its Conn.
- For that matter, even with non-sharedFinfo targets, what do we do when  a
	single SrcFinfo has distinct FuncIds to call on different targets?
	- Note that the SrcFinfo has a single funcIndex to look up the
		targetFunc.
	- Currently, the target Conn is part of the SrcFinfo.
	: Put the extra info in a relay or hub if needed.
	: Put it back in the Conn, where it was in the earlier MOOSE.
	 	- Drawback: 
			- Conn ceases to be only a wire, now it has func info.
				It has dependencies on what goes over it.
			- If I want to do a piggyback msg I need to add a hack,
				such as making a new temporary msg.
			- For each entry in the MsgId vector in Conn, I need
				a whole vector of FuncIds, one for each
				dest entry in the sharedFinfo target.
			- Multiple SrcFinfos may point to the same Conn. So it
				should be the SrcFinfos who manage funcs, not
				Conns.
		- Positive: Conn now carries all msg info, and can be examined
			like old GENESIS.
	: Do the extra ones on the fly. Then where do I store the dest funcids?
	: Put an array of targets for each entry in targetFunc_.
		- How do I figure out which entry to look up?
	: Make duplicate dummy SrcFinfos. Nah. How would I call, manage, etc?
	: Linked list of SrcFinfos. But SrcFinfos are readonly statics.
	: in Eref::asend, pass funcIndex rather than targetFunc Id into Qinfo.
		- This goes to Conn::asend
		- This goes to Msg::addToQ
		- This goes to Qinfo::addToQ, but at this point the identity
			of the callerElement is lost.
		: I think Conn::asend is the one place where I know that there
			are multiple msg targets. I could successively look up
			Element::targetFunc for each of these targets. The
			Qinfo::FuncId could be set here.
	: Turn Conns into a virtual base class, having one or more Msgs.
		- This is only to make the scanning go a little faster, doesn't
		help with the function lookup stuff.
	: Turn targetFunc entries into virtual base classes, allowing 
		iteration for successive Msgs. Again, only an optimization hack
		and it will be fine to have the targetFunc_ entries just be
		vectors of FuncIds.
	SUMMARY: Conns unchanged. Element::targetFunc_ entries are now arrays
		of FuncIds. Pass not FuncId but SrcFinfo::funcIndex into
		send functions. Resolve FuncIds at Conn::asend, so when we scan
		through each Msg, we lookup Element::targetfunc_ and
		scan through target FuncIds.

- Is the backward direction from one target of the one-to-all Msg a one-to-one?
	Yes.

- How do we specify which type of message we want?
	- Should have an argument (string?) to specify type
		- Set up a static init for each msg to fill name-type mapping.
	Yes.
	- 'add' function needs to be incorporated in the Shell
	Yes.
	- Needs to be taken out of Message.h and Message.cpp
	Yes.
	- Create Message classes

=============================================================================
22 Feb 2010
Clearing the decks before the major rewrite for shared messages.
Compiles, but doesn't clear unit tests.
Checked in as 1610.
=============================================================================
23 Feb 2010
Fixing up the unit tests: turns out that there were issues with testGet
because of implicit assumptions both on the slot used for gets
(require requestGetSlot = 0) and also on the sequence of initialization
of SrcFinfos (require requestGet to be first, so that it has a ConnId of 0).
A bit more cleanup and it now clears unit tests.
Checked in as 1611.

=============================================================================
24 Feb 2010
Implemented the array of FuncIds for each funcIndex. This makes it messier
to delete messages. We need to:
- Scan through all Conns to find which holds doomed Msg. Get Msg index.
- Scan through all SrcFinfos that talk to the affected Conn.
	- erase/remove the FuncId at the affected index.

=============================================================================
25 Feb 2010
Thinking about this. We have a problem with funcIds, because the ability to
completely recreate a message is now compromised. In other words, the mapping
from message creation call to the simulation is now dispersed among
Conn, Msg, SrcFinfo, and the vector< vector< FuncId > > Element::targetFunc_
Points:
- Every SrcFinfo in a SharedFinfo points to the same Conn. Can we create the
	Msg with a subset of the SharedFinfos, even one SrcFinfo?
- Is every Conn accessed by exactly one SharedFinfo at most?
- A complex regionConnect call might decompose into multiple msgs on a Conn,
	but not all of them. For example, we have a projection to all subtypes
	of mitral cell from ORN, but another projection to all subtypes of PG
	cell. This would be fiendish to reconstruct from the outcome. If we
	do store setup info on the Conn, how do we store multiple such 
	regionConnect calls?
- If we have a separate entity storing the original messaging call,
	how do we relate back and forth?
- For economy, we may not want there to be permanent ConnIds or FuncIds
- For speed we do want to have permanent Conn and FuncIds.

All ugly. Deleting Msgs has also become ugly. I want to get back to single
funcId.

Try this:
- Eliminate Conn, Element::c_, Element::targetFunc_
- class SrcFinfo { unsigned short index };
	- If index < msgInfo.size() follow the msg.
- class MsgInfo { MsgId mid_; FuncId fid_; ushort next_ };
	- The next_ field points to a separate vector than the original msgInfo
		Or even pointers.
		Or even make MsgInfo a vector and eliminate next_
- SrcFinfo::send combines ops of Eref::asend and conn::asend, and
	iterates directly through the series of MsgInfos to put stuff in Qs.
- To delete a Msg we now need to just find the Msg on the MsgInfos.
- We may end up having multiple MsgInfos point to the same MsgId. Need to
	do reference counting so we kill it only when all ptrs go.
	Actually it is usually the other way around: msgs themselves are deleted
- MsgSpec lives in a separate indexed vector.
	- Each MsgSpec refers to its subordinate Msgs and SrcFinfo.
	- Each Msg (MsgInfo?) refers to a given MsgSpec ?
- The index off SrcFinfo also points to an entry in MsgSpec?
	- No, since we may have multiple MsgSpecs using the same SrcFinfo.
- Deletes are done at the MsgSpec level, not the low level of messaging.
- Multiple MsgSpecs may refer to the same Msg, for example, in a SparseMsg.
- Multiple Msgs may be referred to by the same MsgSpecs.
- SrcFinfo[0] is the parent->child msg
- SrcFinfo[1] is the Element->msgspec msg

=============================================================================
28 Feb
Suppose we have a SharedMsg with bidirectional func calls. Then both sides
have individual SrcFinfos indexing into their respective msgBinding vectors.
Nothing special needs to be done for the arriving msgs.

After much fiddling with the call sequence, I am now working on compilation.
Lots of templates have to be redone. I have eliminated a lot of
intermediate functions and the system just goes from SrcFinfo::send to
Element::asend to q::addToQ.
In SrcFinfo::send calls there is some work to be done to optimize and to 
perhaps use Conv rather than direct conversions. But there is an extra
data copy involved with the current form.
Need to implement BackSrcFinfo1 etc.

Now working on the SrcFinfo1::sendTo. Issue is that there are multiple
possible target Elements or MsgIds.
=============================================================================
2,3 Mar 2010
SendTo is used when sending data back to src, and when sending data to a 
completely specified target. In both cases the target elm is known.

Cleaned up Id to just have an index to the elm.

SendTo is currently a  problem. In the general case it has to provide the target
elm. So it now requires pretty much everything that the message creation 
itself requires, It may as well do what Set does: create a
temporary message and send data along it. Differences:
- If message already exists then verification not needed.
- May need to scan through target Msgs on msgBinding_ vec to find one that 
	matches. Usually only a few.

Cost of making new message probably bigger, but anyway, this is awkward.

Putting in a new Qinfo::addSendToToQ to put the data in the right place 
=============================================================================
4 Mar 2010
Now grinding through the compilation and cleanup process. Since so much
has been done, time to do a checkin. V1617.

=============================================================================
5 Mar 2010
Compilation grind. Currently fixing up OpFunc.h:GetOpFunc return values.

=============================================================================
6 Mar 2010

Still grinding through compilation. Now into Shell.cpp
=============================================================================
9 Mar 2010.
Compiled through basecode directory. Others pending. Checked in revision 1622.
Compiled entire thing. Doesn't clear unit tests. Checkin 1623.

Fixed minor bug. To my utter astonishment the thing now does clear unit tests.
Checkin 1624.
Also clears valgrind, with the usual caveat about pthreads.

Now to step back and look at the status with respect to multinode testing
* Redo benchmarking.
- Get Shell commands to work for basic functions
+ Get Shared Messages to work
	- Redesign ValueFinfos to be more like an automatic SharedMessage.
- Get Shell Messages to work for basic functions


Benchmarking on gj (opteron 2.4 GHz)
./moose		./moose -c 2	-c 4		-c 8
45		36.7/19.3	43.4/12		44.5/13.0
This is marginally slower than earlier, under 10%.

Minor shuffling to put Shell stuff in a separate directory. Will be handy
as the # of functions handled by Shell increases.

=============================================================================
11 Mar 2010
Two immediate things to sort out:
- Making a SharedFinfo for shared msgs
- Implement node-directed messages for shell operations.
	- Master to all nodes including self
	- Slave back only to master
	Also need harvester routine to check that all slaves have reported back.

Need to work out whether regular addMsg commands exist at all, or if they
are hidden under the MsgSpec ops.

Possible separate steps:
validateMsg: Checks that the src and destfields are compatible. 
	Only needs to go to master node.
createMsg: Makes Msg of specified type and specified MsgId, between 
	src and dest Elements
	Goes to all nodes, master waits till all ack.
	At this point this is a bare Msg, no functions associated.
assignMsgParms:
	Separate step where the Msg-type specific functions, e.g., 
	randomconnect or add_one_target, get called. Node-specific 
	setup stuff may happen here.
bindFuncToMsg: Binds the functions to the Msg, uses Element::addMsgAndFunc.
	May need to deal with a vector of FuncIds for shared Msgs.
	Goes to all nodes, master waits till all ack.
	Once this is done the Msg is live.
instantiateMsg: Fills in values. may want to have this as a wrap-up step
	so that the data allocation, node stuff and RNGs get done here.

These are all elementary functions. They will be coordinated by the function
that sets up the MsgSpecs. Shall that be the only one that the Shell exposes?
OK, let's try it that way.

Id Shell::addMsg( FullId src, string srcField, FullId dest, string destField, 
	);

Id Shell::addMsg( IdList src, string srcField, IdList dest, string destField );
Id Shell::addMsg( IdRule src, string srcField, IdRule dest, string destField );
Id Shell::addMsg( File mapping, string srcField, string destField );


Working on SharedFinfo, and how to validate messages with it.
=============================================================================
12 March 2010
Working on compilation of SharedFinfo. The current stuck point is 
message validation. Existing approach is to use the name of the FuncId.
This is a hack, done because ValueFinfos define two FuncIds: 
	set_<fieldname> and get_<fieldname>
May work out if I instead treat ValueFinfos as a shorthand for entering
two DestFinfos. Maybe SharedFinfos can set up all their contents? In
which case ValueFinfos are just a special case of SharedFinfo.
However, I don't want to exclude the possibility of connecting to just one
of the components of a SharedFinfo.  

Implemented the SharedFinfos and also a new scheme for validating match
for the purposes of setting up message.

Working on the create command. 
- The new elements Id:
	- In theory each node should keep track of everything and this should be
	unambiguous if we just increment the Id each time an Element is created.
		- Assumes fixed sequence of messaging - should avoid.
	- We could get the id on the master node and send it around to all.
		- Safer, let's do this.

=============================================================================
13,14 March 2010.
Compiled, runs, but valgrind not happy. Trying to clean up. Fixed. 
Checkin 1632.

Next steps:
	* Shared Msg between Shells should be set up. Checkin 1633, 1634.
		Issues again with message direction. If we have a reciprocal
		msg, the direction info is only from the calling Elm.
		Added isForward = 1 default arg to SrcFinfo<>::send calls.
		Checkin 1635.
		- Need to change Msg types between Shells so it isn't
		reciprocal: that is just too messy. It is a fairly obvious
		command/ack pair.
		Checkin 1638.
	* Unit test for SharedMsg. Things got too messy with the Shells.
		Checkin as 1636, still to clear tests.
		- We have a mess with registerOpFuncs for SharedFinfos.
			Fixed, now it registers each dest in the SharedFinfo.
		- We have a mess with registerBindIndex across the board.
			Fixed.
		- We still have a mess with isForward. Should not need user
			input at all, should figure out from elms.
		Clears tests, but isForward still a problem. Checkin as 1637.
	- The InterNodeMsg type should be defined, specially so that Shell
		can send stuff to itself and get back acks to master node.
	* Shell::doCreate should do a 'send' on an internode Msg to all Shells.
		It should wait till ack from all nodes.
		- Should the ack return success?
			- If it fails, what do we do to handle?
	- Shell::doDelete likewise
	- Test on MPI.
	- Set up Shell::warning, Shell::error as a stream.
	- Cleanup: go through and eliminate Message.cpp and its standalone 
		messaging functions. The Shell now handles it.

=============================================================================
15 March 2010
Working on msg-based doCreate. Goes into infinite loop. Hard to debug.
Working on implementing a showMsg. This regresses back to Cinfo::init
handling SharedMsgs to init all the entries.
Put in printf debugging fron tests. Compiles, but still stuck in loop.
=============================================================================
16 March 2010
Some sloppiness in handling Finfo registration has caught up with me, and
it is a spiral of problems. So I have to go right back to fixing this.
It will break a lot of the unit tests. Sigh.

Cinfo::init 
	- Copies out parent class Finfos.
		To what extent must we retain base Finfo and func indices?
		- How do we access them? If through name, no problem.
		- Can we always plop a derived class in? We do not have the
			concept of FinfoId. So should be safe.
		- But if we want to replace a class by its derived
			version, what happens? Need to rebuild linkages,
		-> will have to do so by name.
		- Can use a hint in the Finfo to say if it should be 
			registered with a predefined BindIndex. This is
			only to keep the preallocated bind_ vector small,
			useful when we have lots of SrcFinfos. But then why
			would we have a SrcFinfo if it isn't to be preallocated?
			The fields are what will proliferate, and those are
			only destFinfos.
	- To clone or to use original ptr?
		- Clone seems safer
		- Clone requires me to put in an op in all templated SrcFinfos
			as well as OpFuncs. Not on.
		- To use original ptr I'll have to use static creation of
			all Finfos, rather that 'new' allocation in each object.
		- Cinfo will no longer have to clean up Finfos when it is
			deleted.
		- Cannot now overwrite BindIndex and other Finfo fields.
		- Decide if to reuse old Finfo BindIndex. For now, do not.

	- Adds in current class Finfos. If there is a duplicate, free old one
		and plug in new one.
	- scans through Finfo map, registering each one.

Begun implementation. Compiles through to the testAsync.cpp.
Checkin 1640.

Compiles through all basecode files. 
Checkin 1642.

Compiles through Shell and IntFire. In sched there is an old hack come back
to haunt me: in Tick::advance, it uses the tick index_ to look up 
the msgBinding. With the more structured and automated scheme now in place,
this has to be done differently.
	- Set up an array of process SrcFinfos
	- Use the Tick::index_ to look up this array
	- Find the bindIndex from the identified SrcFinfo.
	- Use this bindIndex for calling process.
Done.

Need to deprecate Message.cpp, Message.h, SetGet.cpp, SetGet.h

Now compiles the whole project. Doesn't run yet. Checkin 1643.

Deeper into the murk. I was unable to debug the first unit test, insertIntoQ,
so I wrote a new one to just print out Element Finfos. That compiles but
also crashes.

Fixed issue with Finfo registration. Now clears first two tests. Checkin 1645

Now clears all unit tests except the last one, where I was earlier: using
messaging to request the create call. The problem is that the messaging is
trying to create two elements. Checkin 1646.

Clears all tests. The double create was because I had set up two identical
messages.
Valgrind is still not happy. Tracked most of it down to the Finfo definition of 
Synapse, which was still using heap rather than static Finfos. Fixed this,
also found another leak in testAsync.cpp. Now valgrind is happy. Checkin 1647.

Next steps: pending from 13 March.
	* Shell::doDelete also set up through messaging.
	- Test on MPI.
		- Start up MPI
		- Create Shell and basic Elms on both nodes.
		- Set up default Msg between shells
		- Figure out scheduling
	- Set up basic set of commands as do<Command> with messaging.
	- Set up numerical test with IntFire network.
		- Benchmark
	- Set up Shell::warning, Shell::error as a stream.
	- Cleanup: go through and eliminate Message.cpp and its standalone 
		messaging functions. The Shell now handles it.


=============================================================================
18 March. Put in wrapper stuff for parallel running using MPI. Checkin 1654.
Oops, added in the parallel directory and files. Checkin 1655.
=============================================================================
21 March.
MPI and MOOSE.
First, which parts of the unit tests do what:
- testAsync: Works fine, does more unit tests than the rest put together.
- testScheduling: Croaks badly with segv
- testShell: Doesn't croak, but never terminates either.

testShell:testCreateDelete: OK
testShell::testParserCreateDelete: hangs. This is now clear: it wants 
	as many acks as there are nodes. Currently we don't get acks off-node.
	Hence the hang.
testScheduling.cpp:setupTicks(): OK.
testScheduling.cpp:testThreads(): Croaks.
testScheduling.cpp:testThreadIntFireNetwork(): Croaks.

Fixed the issue in testScheduling: I was making a SimGroup for every node,
but actually each node makes its own. So I commented out the loop and the
testScheduling worked. testShell still hangs. Checkin 1658.

Now again looking at queue handling with MPI. See 3 Jan 2010.
Since threading is always (except for the BlueGene arch) going to be part
of such systems, lets do as follows:
- We maintain a single recv buffer. 
	- Use MPI_irecv to scan for incoming stuff, on the MPI thread. This
		goes on all the time. May need another thread than the 
		Shell thread, if Shell ops are likely to block.
	- Put GroupId in the MPI_tag. It is an unsigned short.
	- Append the received msg into the mpiQ of the specified Group.
- We also have to figure out when we can send the messages. The two options
	are to interleave with MPI_irecv, or to use the the master thread of
	each group. Problem 
	- If we use isends on thread0 within simGroup, we will need to do an 
	MPI_wait on this thread 
	before using the buffer again, to ensure the data is gone. This is
	not so bad, because we can do the iSend as soon as we have filled up
	the inQ, and then do the MPI_wait when the inQ processing is complete.
	Issue of thread safety.
	- If we use iSends on a separate MPI-thread, it will have to be told
	which SimGroups are ready to send stuff, and those simGroups have to
	block from returning (and hence touching the inQ_) till the iSend
	is done and also till the corresponding MPI_wait says the buffer is
	clear. At first we will just have two simGroups: the shell, and the
	processing group.
		- The MPI thread keeps in a loop 
			- test for iRecv, if it has come, assign it to mpiQ
			of appropriate SimGroup defined by tag.
				- A combined mpiQ for all external data is not
				practical for huge numbers of nodes.
				- Use a separate mpiQ for each of the nodes
				contributing to the SimGroup. Test for receipt.
				- As each extnodempiQ comes in, set a 
				mutexed flag to inform owner process thread.
			- check for flags indicating that a SimGroup is
			ready to send its inQ. Not sure if flag needs to be
			mutexed, perhaps volatile will do.
			- sends stuff if ready.
			- check for any sends having completed. 
				- Reports to SimGroup.
			- Check for flags from process threads to call iRecv
			for any completed mpiQs.
		------ repeat loop.------------
			- If a SimGroup is ready, the MPI thread does an
			iSend of its inQ. 
				- It puts in a flag to indicate that the
					process thread has to wait till the
					inQ is sent.
				- It adds the MPI_request to its list,
				and continues on its loop.
			- The MPI thread calls the MPI_test command or its
			variants to check on one or more requests at a time.
			- As soon as the MPI_request is cleared, the thread
			reports back to the process thread of the SimGroup that
			owns the inQ, and liberates it from a condition_wait. 
		- in Tick::advance, after Qinfo::mergeQ, the simGroup sets
		a Mutexed flag to tell the MPIthread to send the inQ. 
		The Tick continues on to process the local-node stuff.
			- After local-node stuff is done, the Tick checks
			if any off-node mpiQs have been received. It goes
			through them.
			- When each off-node mpiQ is done, a mutexed flag is
			set to tell the MPI thread to put them back on iRecv.
			- When all off-node mpiQs are serviced, use a 
			condition_wait to check if the inQ was sent yet.
			- continue with Tick process.
		

		

Unfortunate amount of data juggling here. Would be nice to have a list
	of buffers that we can rotate where needed. Later optimization.
Seems like the way to go is to have a separate set of mpiQs, indexed by
groupId.
Can we direct MPI messages to specific groups? Yes, there is the MPI-tag
which is currently an unsigned short, which will do.

Is MPI thread-safe? This appears to be implementation-dependent. OpenMPI
claims it is, but it is slightly ambiguous about whether this is here or a
goal still. MPI-LAM is not thread safe. I suspect we cannot rely on it for 
now.

=============================================================================
23 March
Working on implementation of above. Instead of regular MPI-sends/recvs
(of which I'd have to do one per node, on each node, I could do an 
MPI-broadcast on each node, but then I'd have to do one per node to receive
the broadast data. Or even more compactly, MPI_all_to_all and its variants
send info everywhere in one step. The bare MPI_all_to_all expects uniform
size buffers, but since I don't know ahead of time what the sizes are
going to be, and since they change each timestep anyway, I may as well
use it with the max likely size.

If we use the MPI_allToAll, then the loop becomes simpler:
mpiThreadFunc:
	Check that inQ is ready: condition_wait perhaps, or maybe barrier.
	AllToAll transmission.
	clear condition_wait for the Tick::advance. Perhaps barrier is better,
		as there are multiple threads to permit.
	repeat loop
Tick::advance:
	mergeQ // This gets inQ all set
	barrier// This is for processing threads, but could add mpiThread too?
	clear the condition_wait for the mpiThreadFunc.
	Start the local processing on inQ. Keep busy while the AllToAll happens.
	condition_wait for mpiThreadFunc to finish. Again, perhaps a 
		barrier is better.

Barriers:
	- We need a separate barrier for each SimGroup
	- We do NOT want to go into a SimGroup if it is not being Processed.
		- The Shell SimGroup is always being processed - or we have
			shell in both sim groups.
		- Can we put the Shell in both its own SimGroup (0) and also
		the numerical SimGroups? 
			- This would halve the need for AllToAll calls, also
			halve the barriers.
			- Will need to create the barriers at the same time
			as we create the SimGroups. So they may as well sit
			in the SimGroups.
			- But we really want the Shell comms to be out-of-band
			wrt the computational ones, so comput can start/stop
			whenever.
=============================================================================
24 March.
Is it good to have the Tick directly coordinate the mpiThreadfunc? Or
even have the mpi steps handled directly by Tick::advance?

If we have a rigid blocking call in the data exchange, then it is simpler
to put the mpi steps in Tick::advance. But we will need another barrier to
get the MPI_alltoall handling thread to complete.
The problem with doing this is that the blocking call should really sit and
wait on one thread while the local inQ is being cleared on other threads.
So we still want it on another thread.

Also another issue emerges with doing the alltoall: If the # of nodes is
large, then this takes a long time and brings in lots of data. This
means that the nodes do only communication during alltoall, and only 
computation once the data comes in. Will need to benchmark to test.


=============================================================================
25 March
Stages:
	- Get mpiThreadFunc to synchronize with Tick::advance
	- Figure out how to coordinate with shell::process.
	- Try out flaggable implementations for alltoall, broadcast, isend.
	- Benchmark.

Got the whole mess to compile again, begun process of implementing
Shell::mpiThreadFunc. Checkin 1661
Added in the mpiThread. It promptly stalled. I need to have a loop of the
correct number of cycles through the barriers. Perhaps better to be informed
about how long the calculations continue, rather than do the ugly stuff that
the ticks do.

Here we run into one of the issues with barriers. They are really designed for
lockstep calculations, and don't allow one to keep going around even if the
computation has stopped.

Hacked in a flag in Shell to indicate that the simulation is still running.
When the Clock finishes, it sets this flag to 0 as well. Seems to work.

Now on to extending the mpiThreadFunc with another barrier to clear up
the mpiQ. This compiles but fails unit test.

Passes unit test, but doesn't engage second or any further threads. They do
seem to form, though.
After a lot of messing around it transpires that all was OK, I just didn't run
the multithread calculation for long enough to see the other thread in
action. Just changed the runsteps from 5 to 50 in testScheduling.cpp, and
it is now visible when I run moose -c 2

Now the mpiThreadFunc does not kick in when I run 
mpirun -np 2 ./moose. Perhaps shell->isRunning_ is false?

No, it was just that I handn't compiled with the BUILD=mpi flag.
Now it hangs very satisfactorily, but without printing out lots of stuff.

This turned out to be an old (known) issue with the shells trying to
talk to each other without the mpi running. Commented it out. Now it
crashes in testScheduling.cpp on an assertion.

This is in testThreadSchedElement::process
=============================================================================
26 March
Confirmed that this happens only on node 0, at least over 10 trials.
Confirmed that it does not happen for mpirun -np 1
Confirmed that it does happen, on node 0, for mpirun -np 4

=============================================================================
27 March
Added USE_NODES flag. Turns out that the mpi problem happens even when the
mpiThread isn't running.

Fixed by commenting out the section in Tick::advance() where we go through
process again. This must be it: I should only call process once on each Element,
after all the incoming messages are dealt with. 
	Yes, it now clears the tests, and the
mpiThreadFunc is called as expected. But it now hangs with no CPU load 
after the job is done. Suspect imbalance in clearing barriers.

Try this: Before each barrier, the Tick sets a stage flag
After each barrier the mpiThreadFunc looks at it.
Problem is that it is possible for mpiThreadFunc to look
at stage flag before Tick has set it. We actually have to
wrap the flag assignment within barriers to be thread-safe.
Then we need to de-assign the flag within the other pair
of barriers.

B---S---B-------B---S---B
B-------B---U---B-------B

Unfortunately this isn't enough for MPI traffic. Here
we have an extra state.

B---S1--B---S2--B---S3---B  loop TickPtrBarrier
B---U3--B---U1--B---U2---B  loop TPB
Could also do with just S1 and S2 and use logical ops to work out
if we are in S3.
Problem is the MPI thread doesn't know when we're done with
the loop. Also I am myself not sure what happens with the 
end of a tickPtr cycle when there are multiple ticks.

Alternatively, perhaps cleaner, have three barriers plus the
termination barrier.

B1----B2---B3 loop ---B4
B1----B2---B3 loop ---B4

The other approach is to set off the mpiThread as just another Tick
thread. The problem with this is to guarantee that only one thread
ever calls MPI functions. I think this is safe. Let's try it.
We currently have a huge mess of things passing threading
info into the Clock::tStart. Some go through the ThreadInfo,
others are extracted from the GroupId, others directly set into Clock.

Tried to proceed anyway. Things are so messy it isn't doable.
Cleanup:
SimGroups: These contain the primary job decomposition info.
ProcInfo: Keeps track of stuff that current thread is doing.
	GroupId
	threadIndexInGroup
	nodeIndexInGroup? or current node#
	whether current thread is for mpi.
	clocke: can get globally.
	barrier
	sortMutex

Perhaps instead of passing in a ThreadInfo structure I should
pass in ProcInfos.

=============================================================================

28 March 2010: Stuck in silly assertion. Fixed up, now we
should have decent check for when we are in the mpiThread.
This uses the ugly old ThreadInfo structure for now.
But the assertion in testThreadSchedElement::process still fails.
OK, finally cleared it. I was testing numThreads, rather than 
numThreadsInGroup. The former also counts the extra mpiThread.
So the problem was indeed with the messiness of keeping track of threads.
Now hangs, but no crash.

Hangs in coming out of testThreads. When I comment out testThreads,
there is no problem.
Checkin as 1665.

Got rid of the hang. Checkin as 1666.
This is good. But I think I need to get the extra thread
running also for the Shell msg.
Then I can think about implementing it.

Setting up stuff for the Shell basic thread, in ShellThreads.cpp:
passThroughMsgQs
This framework is now OK: compiles and clears preliminary unit tests.
Checkin as 1669
=============================================================================
30 March.
Qinfo::sendAllToAll
Should be renamed, Qinfo::syncShells

The sendAllToAll will probably have to be implemented as a series
of broadcasts, which will give a chance to interleave comm and comp.

* Define size of data to go
* Allocate buffers.
	
Got a dummy data transfer to go via MPI. Checkin 1670.
Came across a surprising note, confirmed on the web: MPI2.2 deprecates
the C++ bindings. Given this situation, I need to go back at this
early stage and recode the MPI calls to C.
Done. Also put in a check for thread support, which turns out to
be missing on the current MPI implementation on my machine. Anyway,
my implementation is serialized so that I protect the MPI from 
simultaneous calls on different threads. Checkin 1672.

Added in info on inQ buffer size in the beginning of the buffer, as
an unsigned int. Surprisingly clean, all within Qinfo. Clears
unit tests. Checkin 1673.

Now beginning attempts to do actual data transfer. It does look like it
would be good to put the queue transfer stuff onto a separate thread.

Struggling. Segvs. Valgrind suggests that there is a problem when broadcasting
the inQ.
=============================================================================
31 March.
A clue: If I put in an MPI_barrier, the function hangs. So something is 
wrong with the sequencing. Trying to track this down.
=============================================================================
1 April
Looks like I've found the cause of the segvs. I had swapped
sendbuf and recvbuf in the MPI_Gather command. Now the program
can go on indefinitely without error, but I've cut it short.
Checking in 1678

Seems now to work through Create and Delete tests, but doesn't quit cleanly.
Part implementation of handleQuit etc. Need to poll on node0 for returns,
so that we can wrap up all processes.

=============================================================================
2 April
Looking at quit. Tricky because of lack of sync.

doQuit sends out msg
	It goes out to all nodes
	Whenever nodes get round to reading their Bcast, they get it.
		Nodes exit their loop after one more cycle of Bcast.
	Master gets Msg along with others.
So in theory all should exit at the same time.

Tracked it down: the msgLoop had a pthread_exit, which was bad as there
wasn't a thread going on for the main loop. Now for the first time I
have a little test which creates an element on multiple nodes, deletes it,
and quits cleanly.  Checkin as 1679
Works with any number of nodes (up to 17) and also in combination with
multiple threads (up to 4nodes x 4 threads).

Now need to set up 'doStart' to see if I can work with multiple nodes doing
computation.
Put it in, yet to check.
Next steps:
	- Confirm that clock scheduling is OK
	- Run test simulation with IntFire neurons.
	- Test on different architectures.


=============================================================================
5 April.
Trying to get the system to handle the 'start' command.
Control flow is now in shell hands, .cpp:357

=============================================================================
6 April.
Got start command to work very minimally in unit test. Issue was just that
the ackStart was not being called so there was no info about the job finishing.
Now it runs and stops nicely on 1, 2 nodes with 1 and 2 threads.

Incorporated testSchedThreads to keep track of clock ticks. Tried
out multinode. Two issues.
	1. Doesn't seem to be synchronizing between nodes
	2. Nodes seem to set off one more thread than I want.

Other than that it seems to work. Valgrind is OK too. Checkin 1682.

Now I understand both points. Item 2 was actually due to the
mpiThread being reported along with the worker threads.
Item 1 was because mpiThread doesn't currently do anything.

Some cleanup, followed by an hour of debugging because I couldn't figure out
why it didn't work any more. Now fixed. Checkin 1683.

Trying out mpi_Allgather in Qinfo to transfer data during compute time.
Seems to be doing the right thing, even for mpirun -np 2 ./moose -c 2
ie, 2 nodes and 2 threads on each.  Checkin 1684.

Now considering how to explicitly test internode message passing. Will have
to create objects with known node decomposition, and have the MsgSpecs
figure out how to direct data.
=============================================================================
7 April 


Set up a Sparse Msg that maps from A[] to B[] such that
A and B have one entry on each node.

Some test cases:
V1:
A[] has as many entries as there are nodes. One per node.
A[0] = 1
A[1] = 1
A[n] = A[n-1] + A[n-2]
In other words, a Fibonacci series.
The series will propagate in the appropriate # of timesteps.

V2:
Same, except now A[] has 100 entries, and we distribute among n nodes.
=============================================================================
8 April
Trying to get going again. Turns out that I hadn't gone back to fix up the
other unit tests. Lots of bugs. Fixed most, but now the system doesn't terminate
with mpirun -np 2.
Checkin 1685.

Figured out termination problem. The multinode Shell unit tests execute
from node 0, and send other nodes ahead to a loop to poll the queues.
This doesn't work when the next step is not the loop, but the sched
tests. Fixed this part by putting the Shell tests last. Now it fails
with an assertion in the IntFire tests in testThreadIntFireNetwork.

This problem is probably because the test is set up to run on one node
but the 'start' command checks # of nodes and tries to run it on multiple
nodes. For now I'll bypass the testScheduling. Now it clears unit
tests with multiple threads and multiple nodes, by not doing some of
the tests.

Implemented Shell::connectMasterMsg and set it up to execute at
MOOSE initialization. This connects up all shells on all nodes to
each other so that the do<Function> calls can be sent out on messages.
Checkin 1686

Implementing 5-argument MOOSE functions to handle data transfer for
Shell::doCreate. May have to do 6 argument to handle message creation.
Checkin 1687.

In a bit of a tangent, implemented a specialization for
Conv< vector< T > >.
This will be useful in passing arbitrary vectors around, but for
now I need it to pass the dimensions of the newly created Element.
Checkin 1688.

Working on implementing the 5 argument function for the Shell::create
command.
Also lots of cleanup of printf debugging.
Checkin 1689.

- Decide how to record partitioning info
	- Completely implicit. Based on node#, numNodes, and numData, derive
		local indices by a formula
		- lets one find desired node from index.
		- Could put in decomposition mode in order to get more variety.
- Decide how to instantiate Elements
	- Do so on creation, which is when the size should be known. 
		- Nice because then there is not just-in-time juggling when
		  fields etc are assigned.
		- Bad if there are different alternate decompositions to try.
	- Do so on 'start'
	- Do so whenever fields are assigned.
	- Do so on creation, and be willing and easy to redo at any point.
		- This would be helped if there is a terse specification of
		the current decomposition
- Should Messages be converted to creation with predefined MsgId?
	- If all is done in perfect sequence, no need. Same applies to Elements.
- How to spec Messages in a high-level way
	- Current set are simple enough that the spec can be extracted
	from the message
	- Exception is the SparseMsg which will need a high-level front-end.
	- Means that the message creation function needs some thought.
- When to set up the low-level messages
	- Wait till 'start' or 'reset'
	- Assuming Elements are done right away, do it right away.
- How to partition stuff in low-level messages
	- Look up partitioning rule on Element
	- Most low-levels follow right on.
	- SparseMsg needs some calculation.


Working on updated Element instantiation and creation. Lots of unit tests
in testAsync to be redone.
=============================================================================
9 April
Cleaned up compile after redoing Element creation code. Clears single node
tests, not MPI.
Checkin 1690

Ran valgrind to see if it picked up the problem with MPI. It picked up
something else, fixed. MPI still a problem.
Checkin 1691

MPI issue is the familiar one of having to have the worker nodes go in
a loop to clear Q entries for one test (testShellParserCreateDelete())
before we go on to other unit tests. It usually works on 2 nodes if I
put them in for 3 cycles, but this hack does not scale to 17 nodes.

Put in a relatively clean way to ensure that exactly the right # of
clearQ passes occur: test for creation of the Element. This works with
1, 2, 4, 17 nodes.
The partitioning of data entries among elements on different nodes seems
to be OK.
Checkin 1692.
Turns out it doesn't work when the 'doStart' begins right after it: hangs.
Fixed, just needed one more passThroughMsgQs on the worker nodes to 
send out the ack after creating elements.
Works for 2, 4 and 17 nodes.
Checkin 1693

Now on to runtime messaging.
- Set up messages with a predefined Id on the master node.
	- Is this the same as an Eid?
	- Do we have a single Message child on each Elm and using
		indexing for Message Spec data?
	- Do we redo MsgId to behave more like Id? 
		Or should we downgrade the Id to be like a typedefed int?

Begun process by putting in skeleton code for doCreateMsg,
and the various fields that implement it.
Checkin 1694.
Getting there. Need to make a test case of Shell::doAddMsg 
=============================================================================
10 April
Working on test case. Implemented 'builtins' directory. Implemented
Arith class there, currently it just adds its two inputs. Implemented a
very superficial test case for it.

Next: Implement a "DiagonalMsg". Should provide diagonal messaging with
a specifiable slope, width and offset. 
Then: Make DiagonalMsg node-aware, thread-aware.
Then: Try it out with the Arith class.
Checkin 1695.

Implemented DiagonalMsg. It sets up messages between arrays, where each
array entry connects to a matching entry in the target. The difference between
indices of the src and dest entries are the stride = destEntry - srcEntry.
To handle multiple such mappings, use multiple DiagonalMsgs.

Implemented the Fibonacci series as a good test for DiagonalMsg as well as
Arith. There is some mess with scheduling in that it retains legacy
clock ticks even though they are not used.
Checkin 1698

Something unpleasant happening with the moose processes. Lots of bad
interactions between the various unit tests, specially in mpi mode.

Put them all in the simplest way, and the situation is that the whole set
run fine except in mpi mode.

Bodged together a workaround: test for useMPI and skip the problem functions
if MPI is running. Now the thing clears all combos of thread and node tried.
Checking 1699
=============================================================================
11 April
Minor cleanup: separated the Msg.h and Msg.cpp file into separate files for
the individual msg types.
Two nasty subversion timeouts later, I've finally been able to check in this
minor update: 1700.

Putting in place the framework for Shell::doAddMsg.
Checkin 1701

Now need to tighten up the data decomposition, and what is stored on each
Element.

Perhaps a Decomposition/indexing class that does:
- Take the DataId and look up the appropriate Data entry.
	- DataId should be indivisible. Only the data() functions should
		query its parts?
- Reports whether a given Data Id is there
- Provide whatever info is needed by Messages to build their tables.
- Deal with array dimensions in lookup.
- Returns field, parent data, and other levels of nesting.
	- Currently the FieldElement class is templated on Field and Parent,
	and uses a lookup function provided by the Parent to find an 
	indexed Field.
- Deal with different decompositions.

=============================================================================
12 April.
Data Id uses an unsigned int to index data, and another to index field.
Actually should be an unsigned int and whatever else is needed to figure out
how to partition the dimensions.

I would like to also be able to take, say, a cell model and array-ize it.
And the whole thing has to work on N nodes.

Here are some of the use cases.
	- synapse[ IntFire# ][ syn# ]: Current deal. Can look up either
		parent IntFire, or any of a variable # of synapses.
		Summary: FieldElement class template.
	- compartment[ neuron# ][ compt# ]: This would work fine, even with
		different # of compts per neuron. But differs from 
		FieldElement in that the compts are not special fields, but
		a different index. Here the parent Element or the DataHandler
		should either do a vector of vectors, or some other 2-D array.
		Summary: Variant on Element or DataHandler class.
	- channel[ neuron# ][ compt# ]: Keep one Element per channel type.
		But not all compts have the channel. Can use an extra lookup
		table to go to the channel entry in the array, from the compt#.
		Summary: Another variant on Element or DataHandler.
	- compt[ glom ][ neuron ][ compt# ] Probably best done as a set of
		individual gloms.
	- molecule[ neuron# ][ compt # ][ mol# ]: similar to compartment or
		channel above, but will have to split one of the indices to
		look up mol#.
		Summary: Variant on Element/DataHandler, further split of
		DataId index.
	- Solver has taken over a neuron. Has an Element tree for the cell.
		Array of these solvers has the same Element tree, but each is an
		array. But indexing is backwards. 
		/model/cell[342]/dend23/KA
		The messaging to children would do the right thing.
	Access to indices:
	Msg::exec seems to call indices directly.
		DiagonalMsg: scans through data part, incrementing by 'stride'
		SparseMsg is worse: explicitly constructs 
			DataId( colIndex[j], fieldIndex[j] ).
		OneToAll: Checks for dim. If 1, scans through dim1. 
			If 2, scans through each of dim1 (data) then dim2(field)
		Here we want to create a DataId and tell _it_ to do the
		incrementing.

Now how to split among nodes. 
	Element->data( dataId ) should always be able to find the data, or
	report it off-node. 
		Likewise Element->data1( dataId )
	So, dataId does not know where data is located. That is Element's job.
	Element->findNode( dataId ) should be able to tell where to go.
		Or, could do with a DataHandler using a single Element class.
	Element->data( dataId, mynode ): possible? Should I require every
		attempt to acees the data to have myNoe available.

Use variants on Element, or delegate to DataHandler?
	- Element currently does too many things, notably handling messages,
		fields, and Cinfo.
	- DataHandler would have to be virtual and provide an interface to 
		

Q: Should Data itself do the lookup, ie., provide virtual lookup and dimension
funcs? This works well for nesting of arrays of X in Y. Doesn't help for
2-D arrays of X. Doesn't help if we want to nest two arrays, X1 and X2, in Y.
The FieldElement kind of arrangement also helps if we just want to have a
child Element that is fully contained but is seen as its own Element.
Ans: No, Data should not do any lookups. Leave it to the DataHandler.
	Data Handler will also take over the kind of role that FieldElement used to
	play.

DataId: Currently structured explicitly to look up data and field. Should be
more general. Should know what its dimensions are.



If we want the DataHandler and the Msg::Exec to both agree on how to do ops
on DataId, then we need an open and explicit definition for DataId as
a series of ints one for each dimension.

If we want a safer representation, we need to make DataId opaque, which 
is OK for DataHandler but tricky for Msg::Exec. But perhaps a good thing.
Will end up with more explicit specification of what the msgs are projecting to,
in some cases. Certain Msg types may need to be extended to do somewhat
different things for different kinds of projection. For example, SparseMsg
currently assumes a single target synapse on each target neuron. However,
if the first index is neuron, the second is compartment, and the third is 
synapse, we could have multiple synapses on different compartments but on 
the same neuron

This comes back to asking, how do we handle multiple levels of nesting?
We don't want to overdo this. The original explicit single Elements were
inefficient but simple.
Array Elements as here are efficient at one level, but add complexity.
Array fields in Array Elements are very efficient, but add further complexity.
2-D arrays would have a similar level of complexity.
We could stop here.

Summary:
1. Keep DataIds the way they are for now. Higher order indexing should be done
with caution. Perhaps add info about dimensionality of the DataId.
	1a. May need to put in some checking code for the messages.
2. Make a DataHandler virtual base class, templated for funny lookups that 
	FieldElement used to do. This deals with all the data access.
	2a. DataHandler requires node info as well as DataId to find data. This is
	so pervasive it should be a global static field of the Shell.
	2b. DataHandler requires info about decomposition of this specific Element.
	This is local info. Rather than weigh it down with options and ifs, the
	decomposition info is coded into derived classes of the DataHandler.
	2c. Each DataHandler knows how big the whole data set is to be. But it
	doesn't have to allocate it right away.
	2d. Can merge in Dinfo?
3. Element is now a single class, and its job is handling messages and
	field info. Has a DataHandler ptr to deal with the data.

=============================================================================
13 April.
Starting on implementation. Made DataHandler and two derived classes
ZeroDimensionData and OneDimensionData.
Checkin 1704.

Added skeleton code for FieldDataHandler.h.
Checkin 1705.

- Need to gear FieldDataHandler.h up for handling multiple nodes
	In progress.
* Need to rename ZeroDimensionData to ZeroDimHandler.h, etc.
* Need to do some unit tests for them.
* Need to do major overhaul so Element does not use its own data access fields.
* Did some cleanup of functions of DataHandler.
- Need to figure out how to do prototypes and other globals (all node) elements.
	If this is easy to flag, then all the initial setup should be in
	global mode (without allocation) and then the load balancing should
	take place. For now, proceed with immediate allocation so as to test
	the messaging.

=============================================================================
14 April.
Did renaming of ZeroDimensionData etc.
Checkin 1707.

Working on Element conversion. Pending points:
OneToAllMsg::exec needs to handle nodes. Should ideally pass the iteration
	into the DataHandlers.
Likewise SparseMsg and PsparseMsg.

FieldElement is turning messy. Ideally should be created automagically
as soon as the parent element is made. Problem crops up now with
Id allocation.
Working on automatic creation of FieldElement. This is done by having a
FieldElementFinfo with all the required access func info.
It will use the registerFinfo
to register a PostCreationFunc with Cinfo, and this will be used when the
parent Element is created, to make the FieldElement

Checkin 1708, 1709 as an intermediate step while I work on this.
Got the whole mess to compile. Doesn't clear tests. Checkin as 1711.

Starting to clear it up. Practical question: Do we allocate
data on the 'new Element' command or wait?

Starting to get some unit tests to clear. Currently stuck with instantiation
of a synapse on IntFire.
=============================================================================
15 April.
Lots of headaches with the FieldDataHandler. I think it needs to embed the
parent element's data handler. 
Intermediate commit 1712.
Done the embedding. Some more cleanups and progress into unit tests.
Intermediate commit 1715.

More fixes, now clears unit tests.  Checkin 1716.
Valgrind reports some leaks. Happens when deleting a FieldElement.
Hard to track down. For now I'll go on to send some internode messages.

Problem 1. Ticks are being split across nodes. Need a way to create objects
globally, with all entries present on all nodes.
Implemented this. 
Problem 2. Now I need to have FieldDataHandlers iterate through the parent
	node decomposition. Implemented skeleton using begin, end and
	iterator++ to do this.
Problem 3. Many unit tests fail with the regular construction
commands because creations are on only one node. Modest progress in
fixing. Checkin 1717

=============================================================================
16 April
Now unit tests work. Put in global Element construction for all testAsync tests,
and it seemed to come together painlessly. Checkin 1718.
Minor cleanup of printf debugging. Checkin 1719.

Converted the Fibonacci test in builtins to run multinode. It works!!!
Tried different # of nodes, automatically does it right. Checkin 1720.

Working on full IntFire network matix.
=============================================================================
17 April.
I've run into a lot of problems because older, low-level unit tests,
don't play well with MPI.
Conversely, I need to do the MPI unit tests in a manner which is completely
high-level, using the properly operating MPI infrastructure.

Setting up a framework. The non-mpi tests will just do their usual thing.
The MPI tests will have to work with the existing infrastruture for handling
multinode messaging.

=============================================================================
18 April.
Framework begun. The non-mpi test are fine, but the mpi tests have yet to
take shape. But it seems to work smoothly.
Checkin 1721.

We will need to set up Shell::doSetClock and doClearClocks.
For now, let's just eliminate these and directly assign to the 
Clock object. So what we really need is Shell::doSet and Shell::doGet,
and their vector equivalents.
Use Shell::doAddMsg to do the actual scheduling for now. Eliminate the 
useclock function as a legacy.

Before going much further, let me clean up the node-acknowledge system.
Most of the cases just need to return an ack to the specific request.
Given that the requests are issued in serial, at this point there is no
need to identify who the request came from either.

Should I ever permit multiple shells, this would have to change.
Updated the ack system to have a single ack, and pass back node # and status.
Checkin 1722.

Working on Set and Get. Later will need a close look at the
affected functions that rely on get especially. many in testAsync.

I have put the design for Set/Get in the DesignDocument. One pending issue is
serializing the Set calls. Options:
	DestFinfos send back acks:
		No, because the DestFinfos are normally called by 
		time-critical functions.
	Magic wrapper for all DestFinfos to be called by set
		Ugly
	Shell sends in a separate test function on the same temporary message,
		which returns an ack.
		Should be OK. The messages are executed in sequence and for 
		Shell calls we should operate always on thread 0.

First, let's sort out the single-node operations.
Need to cleanly attach msg to a given MsgId, so that we can use MsgIds
across nodes.
	- Provide a Msg::Msg function that specifies MsgId.
		- Use a special Msg class for the set/get msg that knows how to
		use this constructor.
	- Make a msg attached to the Shell at the start, with one end 
		attached to the shell and the other end dangling. The free
		end gets attached to whichever Element is being assigned.
		- Need to modify Msg base class
		- Need to make a special msg class.
I prefer the first option.

In the process of implementing this. Put in framework, currently haven't
tied it to the single-node set/get functions, let alone the multinode. 
Checkin 1723

=============================================================================
19 April 2010

Some longer-term perspectives. 

We need these things to work before merging in the mainline MOOSE
	+ Set/Get functionality: GetVec yet to be done.
	- Multinode message setup
	- Unit test for IntFire system
	- Benchmarks and optimization
	- Continuous messages
	- Element tree, wildcards
	- Move and copy of elements
	- Node balancing, even if it is done offline
	- Autosched
	
We need to work into these things as we bring in mainline MOOSE
	- Solvers
	- Parsers and threading
	- Graphics and threading

The conversion of mainline MOOSE will be tedious
	- object conversions
	- Unit tests as we go along.
	- Back conversion of MUSIC and related stuff.
	- Fix SBML, specially units

Release management has issues
	- Library dependency bloat
	- GUI cleanup

Finally, we get to interesting stuff
	- SBML and NeuroML
	- nkit, kkit, chankit, etc.
	- Tutorials and demos
	- A complete illustrated course
	- SigNeur

Back to the basic issues of set/get
Q: Does the command execute on worker nodes or on the master?
	We probably need to provide for all options. Start with strictly
	local version.
	Options:
	- Local node set/get for stuff it knows is local.
	- Local node calls all nodes for set/get
	- Master node calls all nodes for set/get
	Danger with the local node stuff is that the master node might want
	to also do a Set. Who is going to control the Msg if it is to persist
	for long enough for a return msg?
Conclusion: Only do master node control. If finer level set/get is needed then
we should be making an object with its own messages.

Done much of the implementing. Doesn't clear unit tests.
Checkin 1725.
=============================================================================
20 April 2010
Stuck in infinite loop doing the testSetGet unit test.
Valgrind showed problem.
Cleared up a problem with PrepackedBuffer.h and its converter Conv.
Added unit test for this. 
Valgrind now happy.
Still stuck in infinite loop doing the testSetGet unit test.
Checkin 1726.

Possibly the Master->worker message is not set up. No, it is.
Do we ever use SrcFinfo::addMsg? If not, get rid ofit.. Yes. we do.
Does the Msg::add function do the right thing with SharedMsgs?
	Usually not. Need to fix.

Another point is that the basic unit tests should not get stuck on this
high-level issue. Should I recode the tests to bypass it?

For now still struggling to get it to work. I have put in a function
Qinfo::reportQ(), which examines inQ[0]. Shows that the
FuncId going to the target element for field assignment, is
incorrect. The actual FuncId is 14, which is for handleSet.
Should be 0, and oddly seems to be 0 earlier in the stack.

Just noticed that the MsgId was always 1. This MsgId is reserved for
Shell connecting to Elements for SetGet. I think that it has been
overwritten for the ones going to the other Shells. Yes.
Fixed it, now clears a good set of unit tests before failing in testSparseMsg.
Checkin 1729

=============================================================================
21 April 2010
Another bizarre bug in testAsync. The Field::setVec command at line 754 
is changing the e2.e_->msgBinding_[0].size() from 1 to 0. 
In other words, messages are being deleted or something. 
Valgrind doesn't see anything odd.
The SetGet1::setVec does nasty things with the shell msgBind indices.
Should be fixed, perhaps that is part of the problem.
Fixed one issue with SetGet: Replaced all instances of iSetInner
with the current Shell::dispatchSet. Still fails at the same place.
Checkin 1731.
Found problem. Another nasty issue with the MsgIds. Turns out that
the deleted assignment Msg was being saved in the garbageMsg_, and
then the MsgId of 1 was being reused. 
Now it croaks somewhat further along.

Now it fails on an assertion in testScheduling.cpp:271, which is in the
function  "testThreadIntFireNetwork()". 

=============================================================================
22 April.
Checked messaging from Tick. Seems OK.
Seems the setclock function never actually delivers the command to
Clock::setupTick.

Struggling with this. Perhaps first let's fix two pending items:
- the Get function
- the setVec function, which works but is miserably inefficient.

Massive changes to implement the updated Get function. compiles, does not
clear unit tests.
Checkin 1733.
=============================================================================
23 April.
Got a first round of tests on Get to work. Checkin 1734.
Need to fix GetUpFunc. Fixed.
Also confirmed that one of the slow operations is SetVec. 
Will get to that soon.
Now it fails the unit tests where it did before I began working on the 
Set/Get stuff: in the testScheduling.cpp:testThreadIntFireNetwork.
Checkin 1737.

One option for SetVec: Put in a special flag for all of the data
or field indices. For example, ~0. 
	- We can use the entire same 'set' sequence with little or no
	change in the functions.
	- OpFunc::op will have to change in all cases, to handle
		single or a range of Ids.
	- This takes away the functionality of OneToAll type messages,
		and forces the OneToOne types to deal with both.
	- We could achieve the same result by adding a flag argument to
		the Set functions.
=============================================================================
24 April.
Good progress on SetVec. Nearly there except for 2 things:
- Need to put in a cleaner way to give the opfunc the Qinfo it expects. 
	This is in the AssignVecMsg.cpp file.
- Need to handle passing in data bigger than BLOCKSIZE. We're already 
	at that limit.

- Problem with handling 2-D vectors through a single vector assignment.
	On child nodes we won't be able to iterate through the whole lot, so
	won't know where to start.
- If we split up the msg, need to ensure that the ack part is last.

Checkin 1739

Bypassed this for now, simply made BLOCKSIZE bigger.
Now unit tests go on properly till the same old problem with testScheduling.
Seems that the IntFire::process is just not being called.
Tracked it down. There was a hangover of stuff in the queue from the 
testScheduling.cpp:testThreads. Inserting a Qinfo::mergeQ fixed it.
Checkin 1740.

Valgrind indicates lots of cleanup. The faster SetVec makes it much easier
to run.
One nasty set identified, not too happy with the fix but it helps: There were
two lowLevel functions for Set and Get respectively, each with a
different bindIndex. This bindIndex was used to clear the temporary msg
after use. However, as they were using different bindIndices, every time I
switched from Set to Get or vice versa, one msg was left uncleared.
Hacked around by using lowLevelGet for both.
Still lots of junk to clean up in valgrind, seems to have to do
with FieldElementFinfo.
Checkin 1742.

=============================================================================
25 April
Moving on to message setup. 
- Treat messages like other objects, except that instead of one parent they
	have two.
	- Provide a Cinfo and the usual field and setup stuff for messages
	- Regular field assignment stuff is OK.
		- This means we can send messages to messsages. Hm.
	- Merge in concept of Msgid with that of Id.
- Higher-level messageSpecs can manage a group of messages.

Put down lots of points for this in the paper notes.
Could try the following:
- Provide a Cinfo for each Msg, 
	- look up in the usual way to create msgs
	- Accessed through a virtual function that looks up the appropriate
		MsgClass::initCinfo.
- Provide a common Element for all Msgs
	- Element does an end-run to get Cinfo appropriate for the selected Msg
		How do we look up the right one?
		Many cases where we do a lookup Eref.element()->cinfo()->stuff
	- Element only allows Get-type messages.
	- Special DataHandler looks up message based on DataId( MsgId, 0 ). 
- Create Element on the fly whenever inspecting a given Msg.
	- Likely to cause problems with its life-cycle.
- Have a permanent Element, but change its Cinfo on the fly
	- Sure to cause problems with wrong Cinfo.
- Provide a separate Element for each of the Msg types.
	- Cinfo is set just once, on creation
	- Need to add and delete msgs as they are cycled.
	- Msg has to be able to identify either the Cinfo or Element that 
		it is associated with.

For now:
1. Set up a multinode doAddMsg.
	- Added infrastructure, yet to test. Checkin 1743.
2. Test out fibonacci
3. Test out IntFire

Implementing a big unit test for doAddMsg. This revealed an issue with
setRepeat, so I made a new unit test for that as well. Moved the setRepeat
out of the doAddMsg.
The message unit tests revealed a huge problem with SingleMsg and 
OneToAll msg. These have a single DataId as a starting point, and there
is no test for that DataId. I have hacked the two of them to do the test
at the exec stage, after the data is in the queue, but this is obviously
inefficient. Need to exclude the data before it gets sent. Anyway,
this clears the unit tests.
Checkin 1744.

Cleaning up the send operation with a test at the Element::asend and
Element::tsend. The latter turned out to have another bug, which I have
fixed, but which may well break a unit test. Let's see.
OK, clears tests. I do need to make a unit test specifically for sendTo.
For now, move on. Checkin 1745.

Next: make a SparseMatrix. Then go to multinode tests.
Also need to get rid of the legacy cruft in Message.cpp and the various
Msg<subtype>::add functions.

Made SparseMatrix, included it in unit tests. But this won't work on 
multiple nodes because I manipulate the matrix directly rather than through
a Shell command.  Checkin 1746.

Now trying out multinode stuff. In addition to the SparseMsg setup,
we have to get all the scheduling (setclock etc) done in parallel before
we can test this.
Struggling a bit here. The old setclock used simple 'set' commands to
do the assignment. How would these work across nodes?

=============================================================================
27 April 2010

Need to separate queues for Shell messages to local node and other nodes. By
default, all Shell messages should be to local node unless they are to 
the Shells on other nodes. Shell messages will typically be for Set/Get
kinds of operations.

Responses to Shell requests are either from 'get' functions or are acks.
In due course higher-order objects may generate Shell requests to orchestrate
complex ops such as gradient descent calculations. All these messages need
to be directed to the master node.

inQ[0] on master node gets input from queues on all other nodes.
	All messages to shell on any node must end up at master node.
inQ[0] on worker node gets input only from master node.
master node shell needs a special queue sending stuff to other shells
all node shells need a special queue for sending stuff to local node objects.

Also any messages to a global should stay on the local node and not
clutter up MPI. All our Shells and clocks are globals.

Currently we have two sets of input Qs: inQ and mpiQ. We have a single set of
outQs.

If we follow through on this, we will have to add 
	- localQ: for incoming data that is on local node only.
	- localOutQ: for dumping output that is to go to local node only.
	- Message-specific stuff for directing queue assignments in
		Element::asend.
An alternative is to have the msg on the target node decide if it should
	bother with the msg. It would have to ask if the srcIndex of the 
	src is on the current node.
	- Can be done for specific msg types, in the exec call itself.
	- Will add to internode traffic. Consider global calls to tables.
Both these look expensive. The additional queues will be a pain to manage,
	but have the advantage of not being bound by BLOCKSIZE.

While we're at it, let's consider queue block overflow.

Wrote out the possible Q arrangements on pen/paper. It proliferates 
unpleasantly, specially considering the multiplier effect of multiple threads.
Worst case would have, for each thread, one localQ, one outQ, and as many extQs
as there are SimGroups.

Option 1: Do the mergeQ in parallel, multithreaded. Each thread merges its own
outQ into the inQ, the localQ, and the respective extQs destined for other
COMMs. To do this, each thread scans through the outQ and fills up a vector
with start and end blocks for each destination, and simultaneously tallies
size. The threads provide each size in respective slots in a vector. Then
they hit a barrier or condition wait. The master (or zero) thread then 
tallies up sizes, allocates upper and lower bounds for each thread, and then
releases each thread to fill up its section of the queue.

Option 2: Actually this scanning could happen while the queues are being 
filled in the first place. This would be Element::asend or Element::tsend.

Decision: Go with option 2.
-----------------------------------------------------------------------------
Dealing with BLOCKSIZE. Most likely case is large setup message, copy or
other once-off. If the sustained messages have blocksize problems then it
will have to be increased.

Option 1:
	Locally, let inQ expand. Its first word contains bufsize, so we know
how big it will be. Send this out, up to the limit of Blocksize. The target
nodes now must scan all recieved msgs, and if any Blocksizes are too big
they (all) have to post another MPI_allgather. 
	- Still need to work out how to set up buffers to manage ths.
		- Could implement an overflowQ which fills up from the problem
		nodes. 
			- Need to have as many overflowQs as there are problem
			nodes. But this is dynamic.
- Could in principle do a different message passing, one which uses a specific
MPI call like MPI_bcast, with the known entire message size. This would get
the info over in one step, providing that we are still within the MPI size
limits. This is cleaner and no buffer juggling needed to assemble the final
message.

Option 2:
	While doing the classification of outQ entries, each thread also
keeps track of buffer size. Unfortunately the final tally will have to be
done on the single thread. Here we know if we're out of bounds for BLOCKSIZE.
Send along a size flag along with the block so all nodes know right away they
have to keep gathering data.
Split up Q entries to keep things down. Have an auxiliary Q for the overflow.
	- Doesn't help for a single giant assignment, like lots of syn wts.
	- Still doesn't help with setting up buffers to manage big input.
This will in any case reduce to the previous option for giant single messages.

Decision: I'll go with option 1.
Would like to have the rest of the mpiQ processed while all this extra
stuff is being transmitted. But that is a later refinement.

-----------------------------------------------------------------------------
Since I'm recording major design decisions, here is how I'll handle the
access to Msg fields.

1. Have a separate Element for each Msg class. 
	The cinfo for the Element handles field access for the Msgs.
	The Element can initially just manage MsgIds in the DataHandler array .
	Eventually it may help to have them actually allocated on the 
	DataHandler.

2. The Msg has a virtual function returning the Id of the managing Element.
	This Element has the appropriate Cinfo for this class of Msgs. 

3. The mapping from MsgId to specific Data entry happens through a map. I
	don't anticipate huge traffic in this direction. 
		Other options:
			- subdivide the MsgId address space to do this faster. 
			- have an indirection array to convert MsgIds into 
				the array index on their specific Element.
			- Allocate MsgIds based on Msg type.
	Summary: MsgId -> DataId( ArrayIndex, MsgId )
	If MsgId is badMsg, figure it out from ArrayIndex.
	If MsgId is reasonable, always use the MsgId to figure it out.

4. Path access to Msgs: MsgSrc_Element_path/SrcFinfo[index]. Should
	convert directly into a MsgId.

5. Projections (higher order Msgs) may manage several Msgs, including 
	ones between different Elements. Can do so through Msgs to the Elements
	that wrap the Msgs.

6. Field assignment: Regular SetGet calls. The Eref defines the holding Element
	and through the DataId we specify the Msg.

7. Message handling into MsgHandlerElements: same as usual.

-----------------------------------------------------------------------------
27 Apr 2010 
Back to debugging.
Implemented a busy loop option for starting moose, so I can get gdb to
examine each of the nodes.
Turns out that the data getting to node 1 is correct for setclock:
the arguments are correct and so is the function.
The problem is that for some reason the old message is left intact on 
MsgId 1, still going to the 'arith' element e1, through an AssignMsgVec.
Instead it should be redone to go to the Clock through a regular
AssignmentMsg.
Perhaps should check flushing of Qs.
=============================================================================
28 Apr 2010
Nasty debugging, issue of queue sequencing.

To help, here is the list of all funcIds.

Shell.set_name: 0
Shell.get_name: 1
Shell.set_quit: 2
Shell.get_quit: 3
Shell.completeGet: 4
Shell.start: 5
Shell.setclock: 6
Shell.loadBalance: 7
Shell.handleAck: 8
Shell.create: 9
Shell.delete: 10
Shell.handleQuit: 11
Shell.start: 12
Shell.handleAddMsg: 13
Shell.handleSet: 14
Shell.handleGet: 15
Arith.set_function: 0
Arith.get_function: 1
Arith.set_outputValue: 2
Arith.get_outputValue: 3
Arith.arg1: 4
Arith.arg2: 5
Arith.process: 6
Tick.set_dt: 0
Tick.get_dt: 1
Tick.set_localdt: 2
Tick.get_localdt: 3
Tick.set_stage: 4
Tick.get_stage: 5
Tick.set_path: 6
Tick.get_path: 7
Tick.parent: 8
Clock.set_runTime: 0
Clock.get_runTime: 1
Clock.get_currentTime: 2
Clock.set_nsteps: 3
Clock.get_nsteps: 4
Clock.set_numTicks: 5
Clock.get_numTicks: 6
Clock.set_numPendingThreads: 7
Clock.get_numPendingThreads: 8
Clock.set_numThreads: 9
Clock.get_numThreads: 10
Clock.get_currentStep: 11
Clock.start: 12
Clock.step: 13
Clock.stop: 14
Clock.setupTick: 15
Clock.reinit: 16
Clock.set_num_tick: 17
Clock.get_num_tick: 18

Here is an inspection of the queue as we wrap up the message Adds.

147                     readQ( proc );
(gdb) call Qinfo::reportQ()
0:      inQ: [0]=120    [1]=0   outQ: [0]=0     [1]=0
Reporting inQ[0]
Q::MsgId = 2, FuncId = 8, srcIndex = 0:0, size = 8, src = root, dest = root
Q::MsgId = 2, FuncId = 13, srcIndex = 0:0, size = 64, src = root, dest = root
(gdb) n
149             inQ_[ proc->groupId ].resize( BLOCKSIZE );
(gdb) call Qinfo::reportQ()
0:      inQ: [0]=0      [1]=0   outQ: [0]=32    [1]=0
Reporting outQ[0]
Q::MsgId = 2, FuncId = 8, srcIndex = 0:0, size = 8, src = root, dest = root

After the handleAddMsg is done, not surprisingly another handleAck is generated.

So we have a handleAck in the queue just _before_ the handleAddMsg.
That should never be. We should not have reached handleAddMsg while a 
handleAck was pending. I need to work back through the tests to see
where the ack may have been first generated.
Sequentially reinstated tests after commenting them out. Turns out that I
had put testShellAddMsg both in the testShell and testMpiShell. I'm not
sure why this matters. I tried having it done two times in a row and it
was fine. Perhaps testCreateDelete is the problem.
testCreateDelete should not be a single-node test. It uses Set/Get
functions which are now multinode.
OK, I see it. The mpi tests are done only on multinode systems. Duh.
Now confirmed that the testShallAddMsg has the seeds of its own destruction.
But a Qinfo::mergQ( 0 ) before it stops it from crashing.
Checkin for now since I've been making a lot of little changes and fixes.
Checkin 1751.
Ack call is indeed pending as you come in to testShellAddMsg.
First appears after testScheduling.
There is also a huge amount of stuff in the queue after testBuiltins.
Remains after testShell, which is an empty function.

Tracked down the issue to testScheduling:testThreads. The call to 
Shell::start sends out an ack msg, which is not required in this case.
Fixed, now seems to do all unit tests on single node. Fails on 2.
Checkin 1754.

Now trying to get things to work on 2 nodes. Nothing does.

=============================================================================
29 Apr 2010

Step back a bit. Implementing a multinode version of testShellSetGet. This
fails on 2 or more nodes.

After some tedious multinode MPI/gdb debugging it looks like the problem is
with the queue request sent out to the local message on node0, which gets
sent over to node 1 as well.
The msg referred to in this queue is present only on node0 at this time,
hence it dies. But a worse bug would be when an existing message from an
earlier assignment sits on the Shell. Then we would send data on msgId 1 to
the wrong target.

Options:
- use a localQ. This avoids sending the data at all.
- have node-specific MsgId rather than all at setMsgId==1
- encode node info somewhere in msg. Other nodes can ignore.
- Clear out temp msg quickly, so we can check for 0 msg.

I think I'll have to set up the localQ. Other options are hacks, and the
need will come up again. Prior to that, checkin 1755.

Now it clears the testAsync unit tests. Extremely slow for Set/Get,
specificaly in testSetGetSynapse. Not clear why. Fails at 
testScheduling:testThreadIntFireNetwork.
This is significant progress. Checkin 1756.

Put in localQ and some additional queue management code. Some debugging later,
clears unit tests with single thread. Checkin 1757.

Fails right away with 2 nodes. Looks like the system sees that the Shell->Shell
msg is going to a global, and puts the data on the local node.
Yes, fixing this helps.
Stuck now in 'Get'. Here we currently have the object send the data right back
to the master node. Now we'll have to relay through the local Shell.

Implemented this extra stage. Clears the single-node unit tests, croaks 
on 2 nodes.

Next: Get multinode tests to work
	Apply to IntFire network
	Implement MessageElements for field acces
	Implement big data packet sending.
	Clean up old messsage.cpp and like cruft.

=============================================================================
30 April.
Clears the SetGet on multiple nodes. Now stuck in doStart. The queues
are bad, and both nodes fail in readBuf at Qinfo.cpp:185 trying to access a
MsgId of 0.

testShellAddMsg: about to doStart
1:      inQ: [0]=388    [1]=0   outQ: [0]=0     [1]=352 mpiQ: [0]=4000000      [1]=4000000      localQ: 4
1: Reporting inQ[0]
Q::MsgId = 2, FuncId = 8, srcIndex = 0:0, size = 8, src = root, dest = root
Q::MsgId = 0 (points to bad Msg), FuncId = 0, srcIndex = 2:8, size = 0
Q::MsgId = 8, FuncId = 1, srcIndex = 4294967295:0, size = 2, src = tick, dest = a1
Q::MsgId = 0 (points to bad Msg), FuncId = 524288, srcIndex = 65536:4294901760, size = 65535
1: Reporting mpiQ[0]
Q::MsgId = 2, FuncId = 8, srcIndex = 0:0, size = 8, src = root, dest = root
Q::MsgId = 6, FuncId = 2, srcIndex = 0:0, size = 1668248176, src = d1, dest = d2


0:      inQ: [0]=0      [1]=4   outQ: [0]=0     [1]=192 mpiQ: [0]=4000000      [1]=4000000      localQ: 4
0: Reporting mpiQ[0]
Q::MsgId = 2, FuncId = 8, srcIndex = 0:0, size = 8, src = root, dest = root
Q::MsgId = 6, FuncId = 2, srcIndex = 0:0, size = 1668248176, src = d1, dest = d2

Went through one round of stepping through the system. Didn't help.
I think one issue is that we initiate the runtime state while still within
Qinfo::readBuf::m->exec. This leaves the existing buffer dangling and other
bad things. Want instead to change over gracefully.

Did that. That uncovers lots of problems with how we do 'process'.
currently the testThreadSchedElement::process is bad.
=============================================================================
1 May.
OK, the problem is that the Element class is no longer virtual. So the
'process' in testThreadSchedElement doesn't get called, and instead the
original Element::process does.

Put in a proper TestSched class, derived from Data, with all the usual
initCinfo and so on. Now it works for one thread but not for two.
Checkin 1758.

Seems like the system goes through ticks, in setupTicks, but doesn't call
target.
=============================================================================
2 May.
Solved the setupTicks problem, it was a bit of a waste. I was using absolute
lookup to find the Tick Element in Clock::start. However in the setupTicks
test, I was creating both the Clock and the Ticks locally. Hence I was
not seeing any of the targets of the test Tick.
Checkin 1759

Now the system clears single-thread tests. The 2-thread tests run into
problems because I now explicitly check threads before executing process on
any Data entry. The old test design assumed not. Need to change the 
TestSched class to be a proper multi-dimension type.
Checkin 1760.

Tightened up test for TestSched. Now compares time alignment between threads.
Checkin 1761

Little fix done for OneDimHandler::process. Now it often clears the tests
for 2 threads, but for 3 or more it either hangs or fails the assertion on
the output. Checkin 1762.

In order to tighten up the Tick::advance logic, I have introduced a second
barrier. Earlier there was just one barrier, being used twice. This doesn't
seem to make any difference to the running, but perhaps debugging will be
cleaner.

Fixed a bug that came up because the Clock was not initialized to zero 
properly. But the earlier symptoms still persist: usually clears
2-thread tests, sometimes fails. Always fails with more threads.
Valgrind doesn't find anything wrong, other than about 2K of memory leak.
Checkin 1763.

Checked output. The values are usually pretty close but not right on,
sometimes one of the values is right, sometimes it is quite far off.
Nothing pending in the queues.
This looks to me like possible race conditions: data slopping between threads.

The thread balancing is as follows:
Process: Thread X out of N threads access data[ X + Y*N ] where Y is a 
non-negative integer.
PsparseMsg: Thread X out of N access row[ X + origRow * N ]
targetThread = ( colIndex * numThreads ) / ncols.
So successive targets are in the same thread:

Array index	Array size	# threads	Thread #	Low	High
colIndex	ncols		numThreads
0-1023		1024		1		0		0	1024
0-511		1024		2		0		0	512
512-1023	1024		2		1		512	1024
0-341		1024		3		0		0	341*
342-682		1024		3		1		341*	682
683-1023	1024		3		2		682*	1024
etc.
So my Process load balancing was wrong. Not too much to fix, though:

Msg type	Handles threads?
Single		No
OneToAll	No
OneToOne	No
Single		No
Diagonal	No
Sparse		No
Psparse		Yes.

Now the inverse calculation: What is the range of Array indices for a
given array size, # threads, and thread #:
lowIndex = ( thread# * array size ) / numThreads
highIndex = ( ( thread# + 1 ) * arraySize ) / numThreads
	For high index we use C convention of one more than the last one used.
This will give:

Array index	Array 	#thrds	Thrd#	Low	High	lowV2	HighV2
colIndex	size	
0-1023		1024	1	0	0	1024	0	1024
0-511		1024	2	0	0	512	0	512
512-1023	1024	2	1	512	1024	512	1024
0-341		1024	3	0	0	341*	0	342
342-682		1024	3	1	341*	682	342	683
683-1023	1024	3	2	682*	1024	683	1024

This isn't right. Low and high indices are rounded down where they should not
be.

Try version 2: See columns lowV2 and highV2
lowIndex = ( thread# * arraySize + numThreads -1 ) / numThreads
highIndex = ( (thread# + 1)* arraySize + numthreads - 1 ) / numThreads
This looks OK. But it is messy to index. Let's try it to see if the 
threads work. No, they still don't. The error looks much the same as before.
The are either very close, or one is close and the other is off.
This time the 4-thread version seems to work OK at least half the time.
3-threads works occasionally. Suggests that something else is also wrong.
Checkin 1764.

Another option is to change how PsparseMatrix decomposes things. 

=============================================================================
3 May
Went through another round of checks for the thread decomposition. See
may03_2010_thread_decomp_test.xls
The formulas above seem OK.

Did a printout of each tick for 1 to 5 threads.
This shows that the problems set in quite early, and seem to be confined to 
thread# 2, which starts out by not having any pending events, and
this trickles through to other problems.
Oddly enough even with 4 threads, #2 seems to be a problem
For 5 threads, #2 and #4 are problems.
Pending events are the job of the message. From the tables it is clear
that the process operations are all being called.
I should flag to indicate the event (outgoing msgs) and their threads to
see if the alignment is good.

=============================================================================
4 May
A table of where the zero msgs come in.
#threads	Blank threads
1		-
2		-
3		2
4		2
5		2,4 or 4
7		0,1,3,4 or 1,2,5,6 or 6

If it were a fixed set of blank threads I would suspect the PSparseMatrix
decomposition. As it is, I need to look for some thread/race issue that
systematically blanks out an entire block of messages, and once done, the
config continues across multiple timesteps. Suggests a setup issue.

There were numerous problems with multiple threads accessing and assigning
fields in shared memory. Tried to fix this but it still causes problems.
The frequency of failures is now lower, but the nature is the same:
blocks of messages don't communicate. 

Put in a printf onthe PsparseMsg::exec function too. Turns out that
the offending threads simply do not call the exec function.

OK, I think I got one problem: we are not really treating the queue as 
readonly. Instead in the readQ (or readLocalQ) call itself, we zero out the
bufsize. Need to separate out the zeroing and protect it.

OK, seems I can put it in mergeQ, which is also protected.
Note that mergeQ is protected in an awkward way using two barriers.
This may go much faster as follows:

in mutex:
	increment counter for barrier
	Check for completion
		If complete:
			mergeQ
			clear condition.
	condition_wait
everybody emerges at the right time, only a single barrier-type
function.

Anyway, that is for the future. For now, I put the zeroing out of the queues
into mergeQ, right where it resizes the same queues, and it now works.
I have run a series of threads up to 19, 4 repeats each, in a little foreach
loop, and moose never complains. Above 19 it goes over the sparse matrix 
size limit. Major step forward. Checkin 1766.

Cleaned up memory leaks: all were due to post-creation of 
FieldElements. Valgrind now happy, barring the pthread leak. Checkin 1767.

============================================================================
5 May
Now on to tackling multinode runs. It fails because of a mid in Qinfo being
zero.

I think one good approach is similar to
what I did for the threads: Just put out enough debug information to
find where problems happen.

Going into mpiClearQ the queues look OK.
Then things go wrong.
Looks like the contents of QueueBlock are bad.
OK, I think I've tracked it down. QueueBlock is OK but part of the data is
in outQ[0] and part in outQ[1]. Presumably there were some setup messages
still pending, so they got put in the other queue. Yet more thought needed
to straightening out queueing system.

We currently have a set of groups, each of which can occupy some threads
and some nodes. There is one inQ for each group. The inQ has whatever
data needs to go to other nodes in the group.

We also have outQs, one per thread. Each outQ is matched by a QueueBlock.
Need to combine. There is some confusion here about how groupIds look them up.

We have the localQ, which is just one queue handling all strictly local
messages. But this doesn't take care of many possible local calls in
different groups. But it doesn't matter. They can all be done together.

We also have mpiQs, one for each node or SimGroup sending info to current node.
Somewhat poorly defined for other SimGroups.

In a nutshell, the SimGroup organization is quite messy here. This turns out
to cause problems when trying to fix up the QueueBlock system.
A hacked together version finally there, checkin 1768.

OK, clears multithread tests.
And also finds a new place to crash in the mpi run. So some progress.

Got through the message execution in testShellAddMsg. The outputs don't match
though. Checkin 1769.
============================================================================
6 May 2010

Big progress: When I change the runtime from 1 to 2 in testShell:355
(testShellAddMsg) then all the outputs match up, except the already known
problem with the SparseMsg. We're in business.
Checkin 1770.
- Why does the system require two clock steps? One worked on a single node.
	Seems like the first step doesn't actually pass any of the
	process time message info.
	OK, this is what seems to happen:
		- The message passing happens before the 'process' in each tick.
		- The 'process' call is needed to dump the msg data into queues.
		- On one node, the subsequent scanning for the field values
			clears the queues.
		- On multiple nodes, the field value scan uses special MPI
			calls between shells and does not clear the queues.
	So, a second timestep is needed to complete the message passing 
	between nodes.

- Make valgrind happy.
* Set up the Msg field stuff.
	- Implemented SingleMsgWrapper. Compiles, clears tests, yet to 
		exercise it. Checkin 1772.
	- Went through with the implementation. Lots of cleanups. 
		Clears tests even though it isn't complete. Check 1773.
	- Cleaned up destructors. Checkin 1774.
	- Made a unit test (on SingleMsg) to check the fields it digs up,
		and to reassign targets of the message. Works. Checkin 1774.
* Confirm the functioning of the current tests
	- Got the testShellAddMsg to work with 1 thread, 1 node, using the
		new field access to the PsparseMsg.
	- Got it to work with 2 threads, 1 node. Had to add several additional
		utility functions to the Element interface to PsparseMsg 
		including loadBalance
	- Got it to work with 1 thread, 2 nodes. Simple fix on PsparseMsg::exec,
		and now that the assignment of the matrix is good it went
		cleanly.
	- Not yet working with 2 threads, 2 nodes.
- Set up a multinode IntFire network, and test.
	- Doing setup. Compiles but croak in unit tests.
- Set up multinode/multithread IntFire network.
- Benchmarks and optimization?

============================================================================
7 May 2010
Checked in working version from yesterday. Network had failed. 1776.
Working on setting up IntFire network using message fields. This still isn't
clearing.

Got setup of IntFire to work, using message fields and Shell-based commands
only. This works with 1 thread. Good progress there, but 2 threads crashes,
and 2 nodes stalls and I can't tell if it is in an infinite loop or just 
extremely slow. Checkin 1777.

Fixed issue with node stall: it was just that I had 'quit' the simulation and
as far as the other nodes were concerned, they were wainting on MPI_finalize.
With this the IntFire network simulation runs and Vm100 is Ok
But Vm900 differs. 
============================================================================
8 May 2010.
Looking at IntFire circuit and the PSparseMsg setup.
Very peculiar.
Even on 1 node, it isn't displaying rows 100 and 600.
Even on 1 node, it is only showing up at t = 0.4.

On 2 nodes, it seems to miss rows 0, 100 and 600. Seems to do row 500 twice.

1. Why it only shows up at t = 0.4: at other times only  a few axons are 
firing. So none of the selected (%100) set is hit.
2. Actually it does all rows that the single-node version does. The #
of input msgs does match up between 1 and 2 nodes.

Looks like there is a problem with the setup of the sparse matrix
on node 1.

Checking the sizes array. There is a stark contradiction in the printfs in
PsparseMsg::exec. Need to do a printf to see how the node 1 array is set up.
Confirmed problem. All colindices for node 1 are 1. Fieldindices are big
numbers, unlike the fieldindices for node 0 which are usually small single 
digits.
============================================================================
9 May 2010.
Walking through the code with gdb.
Matrix looks good on both nodes before transposition.
The various lookup operations for resizing the SynIndex look OK.
Matrix transpostion fails. Astonishing, I thought I had dealt with this
and had unit tests for it. rowStart looks OK but colIndex on node 1 is all 1.

Confirmed problems with transpose. I can cause the matrix code to crash with
a test matrix that starts with lots of empty spaces.
Some ugly, careful indexing and housekeeping later, I think I've fixed the
matrix transposition code. It was so broken that it is amazing it worked
for the earlier, smaller unit tests.

But it _still_ doesn't work with the multinode IntFire test.
After fighting with GNOMEs keyring for quite a while, finally managed to
subdue it, and was able to checkin as 1778

Looks now like the weight assignment was completely off. Need to figure out
how to get the correct # of synapses across all nodes, in order to do this
assignment. Then the check becomes interesting too.

Design issue: Suppose there is a field like NumSynapses, which has a different
value on each node. How do we get individual NumSynapses? How do we get the
total?
We could set up a vector of the values... But that assumes we know how the
nodes are decomposed.
We should know all this at the time that the synapses are set up. If it is done
using a global function like the SparseMsg randomConnect, then each
node will know where it starts from. Need to get this info into the 
appropriate DataHandler.

AssignMsgVec is also messed up for 2-D data assignment.

============================================================================
10 May 2010
For fields like numSynapses, which have input from all nodes, we need to
set up a distinct Finfo which may need a distinct OpFunc to run it.
This can do the gather and summation of data from all over the place.

Also need to merge the variants of the SparseMsg to give a single one.
============================================================================
11 May 2010

There seems to be a memory overwrite bug, causing things to crash. For once
valgrind isn't much help as it reports a different problem in a different place
than gdb. But it seems likely that one of the Element or DataId,
contents is being overwritten.

Here is some tighter debugging info. Looks like parentDataHandler ptr is also
funny:
(gdb) p se.e_->dataHandler_->begin()
$4 = 0
(gdb) p se.e_->dataHandler_->end()
$5 = 0
(gdb) p temp->dataHandler_->end()
$6 = 100
(gdb) p temp->dataHandler_->begin()
$7 = 0

So this is something nasty about the FieldDataHandler class and how it has
inherited from the DataHandler.

Changed the DataHandler to use a pure virtual function for the 
startDim2index. Now it works. Don't ask why.
Had to clear up a number of unit tests involving the random connected sparse
matrix. This follows from the fixes to the SparseMatrix::transpose() function.
Clears unit tests on 1 thread, 1 node. Other cases fail.
Went to regular make (as opposed to the MPI version). Did a valgrind test.
No bad accesses, but lots of leaks. These turn out to be due to the 
MsgManagers. When they are cleared out valgrind is happy.

This version still doesn't work with multiple threads. Fails when trying to
doStart in testScheduling.cpp:testMultiNodeIntFireNetwork.
Seems I need to do a loadbalance.... Done, now it works for 2 threads.

Trying to set up for multinode. Need to hard-code in checks for weights/delays.
There is something funny happening with the start_: should be 0, comes out 10.
This turned out to be a nasty little bug in randomConnect. Fixed, and now the
sparse matrix and IntFire solutions are back to what they used to be.

Implemented some tests for synaptic weights and their assignments. 
1 thread clears it. 2 nodes fails even before it gets there.

============================================================================
12 May 2010
Checkin of all the above pending stuff, 1781.

Fixed yet another bug in the randomConnect. Now the connectivity is set up
OK but it fails in the tests for the weights, on 2 nodes.

Checked, found that the startSynapse number was not set correctly for worker
node. Some more minor fixes in the randomConnect. Now it works. IntFire network
seems to be set up and compute correctly across nodes. But it still fails
for 2 nodes each with 2 threads. Checkin 1782.
Also works for 3 and 4 nodes.

Cleanup: Folded the old SparseMsg code into PsparseMsg, and renamed the 
PsparseMsg class to SparseMsg.  Checkin 1783.

Cleanup: Renamed the PsparseMsg files too. Checkin 1784.

Cleanup: printf debugging. Checkin 1785.

Cleanup: Got rid of Message.cpp:add. Used only in testAsync. Checkin 1786.

Cleanup: Got rid of SingleMsg::add. Checkin 1787.

Cleanup: Getting rid of all dependencies on Message.cpp. Checkin 1788

Cleanup: Getting rid of Message.cpp and Message.h. Checkin 1789.

Cleanup: Make a separate directory for Msgs. Checkin 1790.

New stuff: Implemented check for buffer size, followed by special send of
	expanded data. This works for 1 and 2 nodes but not 3 or 4.
	Makes a huge speed difference on multinodes.
============================================================================
13 May 2010
Trying to figure out why it fails for multinodes with the expanding buffer code.
This happens just after the big data broadcast.
Nothing obvious at this point. Using printf debugging I can see it handles the 
expanded broadcast just fine, and the subsequent 'gather' seems to be OK
as well.
What makes it odder is that it fails for > 2 nodes.
Tried valgrind with mpirun -np 3 valgrind ./moose.
Doesn't report anything amiss. The system fails an assert.

Looks like there are some leftover/spurious message packets in queue 1, 
when the system sends the big messages which are on node 0.
Tried to add in an mpiClearQ after the last cycle of Process. Doesn't seem
to work. Queue 1 still has lots of stuff.
============================================================================
14 May 2010
Tried clearing out Queue1 hard coded at the end of clock::tstart. Doesn't
work.

Found another problem with the handling of readMpiQ. Basically, on worker nodes
I don't really want it to inspect any nodes other than node 0. 
For now, at least. Also implemented a function Qinfo::emptyAllQs to clear
up stuff after a processing run. Together these make it work on n = 2, 3, 4
nodes.
Checkin 1792.
Recompiled with optimization, threads, MPI. Timings
Try	Machine		1thread	2thread	2 nodes	4thread
1	laptop		37.7	23.4	23.17
2	laptop		37.9	23.4	23.01

This is pretty wild. The unoptimized MPI stuff is actually faster than
the threading?! Need to check on a serious machine.
Checkin 1794.

I needed to redo the IntFire network to get it to generate some usefully
diverse Vm values at 1000 steps. Managed by adding -ve weights. Put in a
test - not assertions, but an if statement. Confirmed that things work for
1 core, 2 nodes and 2 threads respectively. Timings have now changed:

Try	Machine		1thread	2thread	2nodes	4thread	4nodes
1	laptop		29.8	18.7	18.4	17.7	32.1
2	laptop		29.6	18.2	18.5	17.7	32.1

Importantly, it handles the higher node/thread cases correctly, even if slower
for more nodes.

Tested mpirun -np 2 ./moose -c 2: Incorrect output.
Checkin 1795.

============================================================================
16 May 2010
From 19 April: Some longer-term perspectives.  
	+ Set/Get functionality: GetVec yet to be done.
	* Multinode message setup
	* Unit test for IntFire system
	+ Benchmarks and optimization
	- Continuous messages
	- Element tree, wildcards
	+ Move and copy of elements
	- Node balancing, even if it is done offline
	- Autosched
	- Documentation
	
We need to work into these things as we bring in mainline MOOSE
	- Solvers
		- Replace Cinfo and DataHandler on Elements that are solved.
		- The Finfos in Cinfo all get replaced.
		- The DataHandler points right to the solver.
		- The messages and other stuff remain untouched.
		- The Fids are all local to the Cinfo, so they now just refer
			to their new functions.
	- Parsers and threading
	- Graphics and threading
	- Extended fields
		- coordinates, generated rule-based on DataId
		- arbitrary new fields, one per entry
		- Element-globals: fields that are one per Element.

Got rid of leftover stuff from FieldElement. Checkin 1796

Decide how to place the subdirectory for Element heirarchy: Neutral, 
the wildcard stuff, copy, and possibly the various data handlers.

I think this should be in its own subdirectory. Call it tree.

Wildcards: Vector of FullIds. For compactness, have special cases
	to indicate that all entries are included.
	
Operations for wildcard traversal:
	- Implement a wildcard Filter object
		This will be needed in due course also for complex messaging.
	- Do local node traversal for Elements
		Filter manages field-dependent operations, such as name/value
		lookups.

Copy: Too many permutations, need to tighten:
	Local/Global src, Local/Global dest, src single/array,
	dest x1/xn.  Msgs within/outside tree.

Move: Change parent is one, but may have to convert singles to arrays? Later.
	Invisible move between nodes

Load balance: Move data between nodes, while retaining original tree

Serialize: Used for dumping data. Copy may use. 

Read: Formal ML based load of model structure/parameters/state

Write: Formal ML based dump of model structure/parameters/state.

============================================================================
17 May 2010

Practicalities: Moved Neutral into the shell directory to begin the process.
Set up the Parent/child msg to pass an int.

Checkin 1797
============================================================================
18 May 2010
Working on benchmarking. Got it to compile on an 8-core Nehalem.

Try	Machine		1thread	2thread	2nodes	4thread	4nodes	7thread	7node
1	laptop		29.8	18.7	18.4	17.7	32.1	
2	laptop		29.6	18.2	18.5	17.7	32.1
3	Nehalem		20.1	9.5	9.3	5.3	Failed	3.3	3.8
4	Nehalem		20.8	10.2	10.6	4.9	Failed	3.1	3.7
5	Opteron1node	33.6	16.1	17.8	9.7	10.5	9.7	13.6
6	Opteron1node	33.7	16.1	18.3	9.8	10.5	9.4	11.8

	1 job/node	1thr	2node	4node	8node	16node	32node
7	Opteron clus	30.9	16.8	9.3	5.7	5.5	5.7
8	Opteron clus	30.9	16.1	9.3	5.0	5.5	5.1

	2 job/node	1thr	2node	4node	8node	16node	32node
9	Opteron clus	30.9	17.8	10.1	6.4	4.8	6.8
10	Opteron clus	30.9	17.9	9.6	6.4	4.9	6.9

Note that the Nehalem was running something else 100% on one of the cores.
So the 7thread/7node cases are actually faster than if I use all 8 cores.

This problem at least does not scale too well on the cluster past 8 nodes. 
But it runs!

Made the problem 2x bigger: 2x as many IntFire neurons, 4x as many connections.
This was done by setting 'size' to 2048 instead of 1024.
We have connectionProbability = 0.2, so the # of synapses ~ 0.84 million
There are 1000 timesteps and 7/20 neurons have fired in the previous timestep,
so about 2048 * 1000 * 7/20 = 0.72 million APs have fired.
With 0.2 connectivity, this comes to 2048 * 0.2 * 0.72e6 = 294e6 
synaptic events.

Try	Machine		1thread	2thread	2nodes	4thread	4nodes	6thread	6node
1	Nehalem		93.9	45.8	failed	23.0	23.2	19.7	15.8
2	Nehalem		93.9	45.8	failed	25.3	25.5	16.2	15.9

	1 job/node	1thr	2thr	2node	4node	8node	16node	32node
3	Opteron clus	142	69.1	failed	31.3	16.2	11.1	11.0
4	Opteron clus	136	72.7	failed	31.2	16.1	10.8	11.2

	2 job/node	1thr	2thr	2node	4node	8node	16node	32node
5	Opteron clus			failed	38.5	19.5	14.2	17.4
6	Opteron clus			failed	38.8	20.3	11.4	17.2

This is interesting. The cluster scaling beyond 8nodes is pretty poor. 
No benefit at all in going to 32 nodes. This is using an infiniband network.
Would be interesting to see scaling with more detailed neurons.
I wonder how much of this is setup time... Let's rerun with 2000 sec 
runtime.

Also there is a puzzling crash that seems to happen at a certain
number of nodes, and across machines. Need to track down.

Checkin 1798

Redoing for 2000 sec runtime to estimate setup time.

Try	Machine		1thread	2thread	2nodes	4thread	4nodes	6thread	6node
1	Nehalem		187.0		failed	55.6	49.9
2	Nehalem		171.8			50.3	48.6

	1 job/node	1thr	2thr	2node	4node	8node	16node	32node
3	Opteron clus	271	124.7	fail	61.2	31.5	19.7	17.4
4	Opteron clus	275.8	134.9	fail	61.0	30.9	18.5	19.7


10000 neuron model, 200 steps
Try	Machine		1thread	2thread	2nodes	4thread	4nodes	6thread	6node
1	Nehalem		556.8	308		189.8	217.2	127.2
2	Nehalem		680	336		207.6	185.1	129.9

		1thr	2thr	4node	8node	16node	32node	48node	64node
3	GJclus	1367	603.7	236.5	90.6	42.9	27.1	26.0	26.8
4	GJclus	1408	580.3	236.3	90.7	42.7	26.9	25.9	26.6
	no overlap					25.0	23.3	22.5
							24.5	22.8	21.3

Firing rate in 10000x200 model is 22 out of 81 per timestep. 

============================================================================
19 May 2010
Working on Neutral.cpp. Trying to implement Field access to name 
(which is stored on Element)

============================================================================
20 May 2010
Continuing with implementation. Removing the 'name' field from the Neutral and
putting it into the parent Element has led to all sorts of updates needed
in the unit tests, about halfway through. Checkin 1800.

Went through the remaining tests, now clears all. Checkin 1801.
Ran valgrind. It is happy.

Now setting up Neutral as the base class for all Elements. Odd bug pops up in
testScheduling.cpp:69, where the timings don't match.
Turns out to happen when Tick is derived from Neutral. Will deal with it later.
Checkin 1802.

Implemented creation-time message from parent to child. Surprisingly it 
Just Works. Yet to do full testing on it, but it clears old unit tests.
============================================================================
21 May 2010
First "simple" function in doing msg traversing turns out to be messy.
I want to get the FullId of the parent of any Eref. Looks like I have to:
- scan through Element::m_ array of all destMsgs. 
	Likely that it is entry 0.
- Find the srcElement from e1()
- go to SrcElement, scan through Element::msgBinding_ till we find one that
	mathches the MsgFuncBinding, that is, both the mid and the fid are OK.
	Check if this 
	

============================================================================
22 May 2010
Msg::findSrcEref: Find the full Eref of the src, given the dest. In most 
Msg cases this is the src Element. Sometimes, like the SparseMsg, we will 
need to look up msg internals to find src from dest and its DataId part.

============================================================================
23 May 2010
Implementing Neutral::getParent. This requires some message traversal 
functions, so lots of implementation in the different Msg classes. 
Checkin 1804.

Partial implementation of unit tests for traversal.  Checkin 1805.
Implemented all unit tests for Msg::findOtherEnd. Checkin 1806.

Added tests for Element::findCaller. Checkin 1807.

Added tests for Neutral::parent (to look up parent id). Checkin 1808.

Added 'children' field to return a vector of child Ids. Tested OK. Checkin 1809.

Added 'path' field, added 'me' field. Tested OK. Checkin 1810.

Added 'getChild' function. Need a LookupFieldFinfo to handle such cases,
where the field uses and extra argument. In array type lookups I could use
the index field of the dataId, but this isn't general.

============================================================================
24 May 2010
Put in a unit test for raw getChild, without a Finfo wrapper. Works.
Checkin 1811.

Trying to set up LookupValueFinfo and associated funcs. 
Working on LookupEpFunc.

============================================================================
25 May 2010.
Tracked through SetGet and Field operations for doing a 'get' call. 
It goes through Shell::innerDispatchGet. There
is no easy way to pass in an argument to the Get function. So the 
LookupEpFunc isn't much use.

============================================================================
27 May 2010.
Starting move and copy operations, which are handled by the Shell. Checkin 1813.

Implemented move operation, and its unit tests. Works. Checkin 1814.

============================================================================
28 May 2010
Working on 'copy' func. Trying to compile.

============================================================================
29 May 2010
compiles but I don't have any data in the new Element tree. Need to 
implement pure virtual function DataHandler::copy( n ).
Implemented most of the stuff for Shell::doCopy, compiles, does not
clear unit tests. Checkin 1823.

============================================================================
30 May 2010
Fixed up Shell::doCopy. Clears unit tests, but they do not yet stress
the data handling of the copy operation. Checkin 1824.
Big things still to fix:
- delete operation to clear all children recursively
- Copy operation to copy all messages.

Implemented Neutral::isDescendant, and unit test. Clears. Checkin 1825.
Separated out Copy operations into ShellCopy.cpp. Checkin 1826.

Need to implement Msg::copy( Id orig, FuncId fid, Id newElement, Id newTgt, 
	unsigned int n );

============================================================================
31 May 2010

Message copying into array turns out to be messy.
		What happens to:
Msg type	OrigSrc		OrigDest	NewMsg type
SingleMsg	ZeroDim		ZeroDim		OneToOne
SingleMsg	ZeroDim		OneDim		OneToSlice
SingleMsg	OneDim		ZeroDim		SliceToOne
SingleMsg	OneDim		OneDim		SliceToSlice

OneToOne	Zero or One	Zero or One	OneToOne I think.

OneToAll	Zero or One	Zero or One	SliceToAll: parents
						SliceToFatSlice: comput
						OneToAll: master-worker control

Diagonal	One		One		SliceDiagonal

Sparse		One		One		SliceSparse

The outcome of this, of course, is that n-copies won't work till I sort out
the issues of 2-D arrays and add the Slice msg types.

Did a first pass, pending implementation of these issues. Compiles but
does not clear unit tests. Checkin 1827.

Was duplicating the parent->child msgs. Now clears unit tests.
============================================================================
1 June 2010

Confirmed functioning of messages in copied tree, and also correct 
copying over of initial values. There is a big revisit of all this 
pending to deal with the issues of arrays: dimensions, start, end
of copied data on multiple nodes, and so on.
I've gone through some of this on pg 49 of the handwritten notes.
Basic conclusion is that adjacent indices should remain together.
Also it looks like the current DataId will do with a linear lookup: it is
up to the DataHandlers and possibly Msgs to keep track of dimensions.
Also think about promoting locals to globals if they are going to be
copied.
Other big pending issue is proper cleanup of children after delete of
parent.
In addition I need to rebuild the signaling solver. That is a major use case
to be converted over, and then I need to look at parallelizing it.
Also ext fields. This comes up in deleting element trees.
The stages in the earlier version were: 
Mark for deletion
Clear Msgs
Delete.
This works out because it is far faster to clear Msgs if we know that the
whole lot are to be wiped out. Otherwise there is overhead to find and
clear dangling ends. In principle I could first build the delete tree as
a set, and use that instead.

To do ext fields, we need another hook, or to replace the Cinfo. 
The kinds of ext fields we need are:
- shared. All the entries on the Element should see them. The 'name' is an
	example. 
- One per object entry. Coords are an example.
- Purely synthetic. Coords in a regular array are an example here too.

Why not do as child Elements?
	- Slow. This is primarily because of linear search by name.
	- Parsimonious. Adds nothing to the existing structure.

So Child Elements it is. But it is kind of strange to decorate an entire tree
with additional child Elements. Also assumes everything is derived from 
Neutral. If we were going to make this strong assumption, then it could have
been built into the Element class.

Turns out I already had a good indicator for deleting elements: the cinfo_
field. When set to 0 it tells the Msgs not to bother to traverse.
Coded up, clears unit tests, but the destroy op isn't actually being called.
need to decide if deleting an Id should refer to the Neutral::destroy.

To be symmetrical with the Element creation convention, let's have it so that
Shell::doDelete really does destroy the element and its children, but the
Id operations do not.
also we need to be careful to put the doDelete operations on a safe queue
separate from the Messaging queue.

The Shell::doDelete seems to be OK, but it has uncovered at least two problems:
- The previously pending issue of Synapse descent (or not) from Neutral.
* the testMove() function turns out to have problems. While the moved child
	thinks it has moved, its old parent still wants to hold on.
	Further confirmed by putting in a count for # of children.
  Fixed the Shell::doMove. It was not deleting the old msg, just disconnecting 
  	Now all the unit tests clear. Rather a lot of stuff now to checkin.
	Checkin 1831

============================================================================
2 June.
Fixed up many of the memory leaks with valgrind. Still need to tackle the
issue with the Synchan inheriting from Neutral: I think I can then clear the
rest.
============================================================================
3 June.
Nasty bug came up with the scheduling: When Tick is inherited from
Neutral, it inherits the Parent SrcFinfo. This messed up the hard-coded
indexing in the tests for process messages out of the Tick.
Fixed, clears unit tests again.  Checkin 1833.
Excellent, clears valgrind too without further messing around.

From 19 April: Some longer-term perspectives.  
	+ Set/Get functionality: GetVec yet to be done.
	* Multinode message setup
	* Unit test for IntFire system
	+ Benchmarks and optimization
	- Continuous messages: Should we abandon these? 
		- Enormous simplification of system.
		- High-traffic continuous stuff should use solvers
		- High-traffic stuff is pretty efficient using queues.
		- Likely speed penalty.
	- Element tree, wildcards
	+ Move and copy of elements
	- Node balancing, even if it is done offline
	- Autosched
	- Documentation
	
We need to work into these things as we bring in mainline MOOSE
	- Solvers
		- Replace Cinfo and DataHandler on Elements that are solved.
		- The Finfos in Cinfo all get replaced.
		- The DataHandler points right to the solver.
		- The messages and other stuff remain untouched.
		- The Fids are all local to the Cinfo, so they now just refer
			to their new functions.
	- Parsers and threading
	- Graphics and threading
	* Extended fields
		- coordinates, generated rule-based on DataId
		- arbitrary new fields, one per entry
		- Element-globals: fields that are one per Element.
============================================================================
4 June.
Another thought for a sweeping change: How about eliminating the regular
messaging form ( GENESIS legacy) for signaling simulations? Instead have all
molecules go directly over to solver calculations?

I'm implementing a skeleton for the kinetics directory, which has been
present but not compiled for a long time. Seems that it would be far more
efficient to go straight to a solver-based design here.
For now, put in Mol and Reac implementations and complete compilation.
Checkin 1835.

Design for signaling calculations:
- Molecules grouped into compartments and 'groups', like GENESIS.
	- Note that compartments != groups
	- Compartments point to their spatial definition, but the compt
		itself could be inside or outside, or a surface.
		- Would it make sense to have functionless messages: just
		indicate connectivity without any implied operations?
			- Issues with some of the builtin functions, that assume
			one end is on the msgBinding_ vector.
			- How do we identify the msg if there is no associated
   			  funcId? We have gone to some lengths to ensure that
   			  Msgs do Not carry func info. Here we would be stuck.
		So for now, don't go there.
Q: Which is 'parent' for this organization, and which is handled by messaging?
	Option 1: Parent = compt, groups as msgs, compt pts to geom.
	- allows for multiple levels of grouping
	- compt natural container for reacs/mols.
	- Problem with kkit convention, needs effort to convert
	Option 2: GENESIS grouping, assign compts by Msgs.
	- Same convention as kkit: possibly cleaner conceptually?
	- Awkward messaging for volume issues, compartments get muddled.
	- Note that the solver handles volume issues
Pick option 1.
Q: Organization on solver, vs GENESIS backward compatibility
	Option 1: Solver is parent cell/compt. Compts or molecules
	are ElementFields.
		- Nasty nested ElementFinfos to handle 
		  channels/synapses etc.
		- Nasty SparseMsg to stand in for various inter-compt
		  and other msgs. If we don't expect to support this 
		  level of backward compatibility, we could eliminate.
	Option 2: Implement whole mess on solver with whatever
	data organization is good for computation. 
		- Implement entire arbitrary Element tree elsewhere,
		  with whatever data organization is good for user. 
		- If necessary make the tree structure readonly so 
	  	  that we don't get into nasty callbacks when adding 
		  or removing molecules.
		- Use smarter versions of FieldElements to access
		  the data. These can make their own heirarchy as
		  they like, but use a FieldDataHandler which takes
		  extra args to know which portion of the data set
		  it should handle.
	Option 3: Same as option 2, except:
		- Use messaging to map data entries from solver to
		  aribtrary Element tree.
		- How often do we need fast field access?
		- Explicit but messy map may be worse than implicit one.
	Issues
		- The link between data and access is implicit if it uses
		  ptrs for the Data, and messy if it uses messages.
		- When and how are the two versions of the
		  model interface synchronized?
		  - At Reset on the solver
		  - As part of the function that adds reactions to the solver
		  - At creation time using kkit or SBML reader
		- Do I rebuild the entire Element interface if I implement a
		  solver with a different data structure?
		- Which is the reference model/data struct?
			- The full mol element tree is expensive esp in arrays
			- The 

Most fundamentally: how essential is it to preserve backward compatibility?
Easy: drop it.
Medium: Preserve field and Element browsing compatibility, and any scripting
	that uses this. Field assignments but only partial Element creation.
Medium-ugly: Preserve message browsing compatibility too, specially message
	scanning. But all readonly. A few special messages, such as for I/O
	could still be possible to create/remove in old form.
Hard: Preserve full messaging and Element creation compatibility.

I think that though the centre of gravity should shift to a Python-optimized
view of the model, the BC is important enough to deploy people to get the
medium and medium-ugly aspects to continue to work.

If we want this to happen, we need something on the order of option 2 or 3,
where an entire apparent Element tree is visible.

Some of this comes to the question: can we map a large number of distinct
Elements with individual names, such as molecules or electrical compartments,
onto an array? If this is done then various issues can be sorted. 
Key features:
	- Names managed separately
	- Data through any of the >0 dimension DataHandlers, including
		FieldDataHandlers
	- Element traversal knows how to deal with this. That is really a
		Shell issue.
	- Wildcard and allchild traversal also knows how to deal with it.
	- Provision for unique children for individual entries.
	- Separate out node-specific subsets.
Key issues addressed:
	- Scalability
	- Backward-compatible interface
	- Browsable interface.

Much work. We need a really good reason to do this. Scalability is the only 
one that existing Element structures do not handle. Here too it might not
be such a problem:
	- # of individual Elements is usually finite and small, even in things
		like a complex neuronal model
	- Scaling usually involves making huge arrays of the originals. This
		can be done efficiently in MOOSE.
In the existing MOOSE, Raamesh had implemented a transform for array indexing.
It went like this: If we make an array of a prototype tree, then each Element
in the new tree is an array. However, to the old Genesis it looked like the
parent of the tree was an array and it had individual children.
Options:
	- We don't need to replicate this in MOOSE.
	- We could replicate this only in the GENESIS interface.

Related issue: predefined indices for Element::m_ vector of MsgIds.
Index 0 should be parent.
Index 1 could be the explicit link to the solver.

		-----------------------------------
Some decisions, from the need to maintain a uniform interface.
1. Will maintain medium to medium-ugly levels of backward compatibility for
	purposes of browsing, both within SLI and Python.
2. Will maintain regular Element tree for things like cells and reac networks.
2a. Existing array functionality will handle scaling issues.
2b. Special msgs keep track of grouping, other system-specific conventions.
	Src is on group organizer, dest on components of group.
3.? When loading models use the minimal interface class.
4. Minimal interface class will include dummy messages for connectivity info
4a. Doesn't hurt to make the Msgs functional, but then the 'process'
	has to be turned off.
4b. Need to provide simple and complete API for traversal e.g., 
	vector< FullId >targets( string msgName ); // Looks up outputs of MsgSrc
	vector< FullId >sources( string msgName ); // Looks up inputs of MsgDest
5. Solver classes must be able to build their model definitions from the
	interface classes.
6. For runtime updates, solver classes must 'take over' interface classes
	in situ by replacing cinfos and data handlers.

These are essentially the same decisions as in the earlier MOOSE.
		-----------------------------------

Steps:
- Implement standalone kkit parser to build models.
	- Move out SUMTOTALS, buffered mols and other oddities into special
	entities like arbitrary math op, buffered subclass of mol, etc.
	- Build it on compartment heirarchy, with grouping defined by 
		additional msgs.
	

Implementing OpFuncDummy for use in grouping. Done.

============================================================================
6 June.
Implementing ChemCompt. Some points.
- It would be nice to have a piggyback msg so that child Molecules could easily
	look up their 'size' so as to report conc. The context of piggyback 
	use is when there is already a message connecting two objects but we
	would like an additional function.
- It would be nice to have a fast lookup across Elements, avoiding the 
	messaging. Again, consider conc calculations. Perhaps this is
	legal only in FieldElements.

Setting up parsing using the CellParser code from moose_g3.
============================================================================
7 June 2010
Checking in the kinetics code changes. Checkin 1842.

============================================================================
8 June 2010
Extended fields revisited.
If we can automatically promote child Elements to fields, we're in business.
This is like the converse of FieldElements.
Does it have to be hard-coded, or only in the Shell functions?

One option:
	- Make Element->findFinfo function to replace current lookup
		through cinfo.
	- If it fails to find finfo, tries for child object name
		matching the field.
		- Perhaps a flag to indicate that the child is to be
			used as a field?
		- The name of the base object is just ""
		- Shell::dispatchSet will have to do stuff.

	- Alternatively, Shell::innerSet could go after the child
		Element, except that all we have here to find the
		correct child is the FuncId. 
		- This would be a good place if we could manage it
		because it handles the incoming Msgs.

============================================================================
10 June 2010
The SetGet::checkSet function has all the info needed to decide if the
field is actually a child Element. Supposing we define fid == 0 as a
predefined fid for setting value of entire object. fid == 1 would have to
do the 'get' function on it.

What could go wrong:
	- mis-name a field, and as a result try to 'set' a child Element.
		This is unlikely because of type safety rules.
	- Waste of a couple of good FuncIds: for set and get resp.
	- It is a bit of a hack.
	- Hard coding this is ugly.
	- Will need to redo stuff here for adding msgs to these 'fields'
	- How to handle SetVec etc.
	- Type of same fid changes with Element class: this is OK
	- Figure out how to do 'get'

At init, Cinfo could automagically adapt Dinfo to assign the base set/get
Finfos.
This is messy. Simpler to have Neutral define set_this and get_this as
dummy funcs, and then override these in classes which are allowed to be
used as ext fields. Might also be useful in data transfer across nodes
and other kinds of serialization.

============================================================================
11 June 2010

Working on ext field elements. Implemented a 'Real' class. I would be
happier if I could figure out how to do the operations
double foo = Real(1.234);

Also I am unsure how far I can go if Real is not derived from Data.

With this in place I can now start to munge the field assignment code to
look for child Elements to act as ext fields.
============================================================================
12 June 2010
One impediment in doing this is that we still derive all MOOSE classes from
Data. This derivation is only used in DataHandler<subclass>::process, 
which typecasts the data to a Data*, and then calls its Process.
We could do better by using a Process call in Dinfo. This would be typesafe
and would also allow for classes that do not support Process.
Later.

To do child Element field assignment:
- Add Element return to SetGet::checkSet
- Fix Shell::dispatchGet to look for the finfo on the child element if needed
- Later: Fix Shell::doAddMsg to handle set/get driven by message.

Checkin 1852.
Now need to put in unit test
Put in test for 'set' command. To my astonishment, it works. Checkin 1853.
Actually the unit test was buggy. Fixed. Still works. Also handles SetVec
correctly. Checkin 1854.

Added in functionality for ExtField 'get'. Unit tests happy. Checkin 1855.
Fixed memory leak found by valgrind. Checkin 1856.

Renamed Real to Mdouble. The string class will then be Mstring, etc. 
Checkin 1857.

For tomorrow:
	- Get rid of Data class.
	- kkit parser
	- Solver infrastructure.
Then I can go back to parallel coding.

============================================================================
13 June 2010
Starting on ReadKkit. Skeleton coded and it runs. Checkin 1858.
Incremental progress on framework for argument conversion. Checkin 1859.
Further progress and checks on ReadKkit argument conversion. Checkin 1860.

Now I need to implement the tricky part of this process, the grouping and
the compartment assignment.
Planned design for kkit successor is that the compartments form primary
groups. Within these, there can be organizational groups that do not have
any compartmental implications, but are just for placement. 
Orthogonal to this, we can have other organizational groups that may span
compartments. For example, synapses.

Conversion has some frequent issues. For example, PKC exchanges between
the cytosolic and membrane-bound forms. I often do not even have a volume
difference between them. These are legacy models, so I may not be able to
do the right thing automatically in all cases.
In the synapse, AMPAR cycles between PSD and bulk. kkit tries to assign
a compartment (geometry and geometry[1]) to them. May as well use this, but
sometimes these have to be merged if the volumes are the same. 
Problem is that the grouping is orthogonal. For now make sub-groups within
the geometry. Avoid third layer of grouping.

So the heuristic is like this:
- Build up the kkit tree the original way
- Check for compartments as the model is loaded. Build these on /kinetics.
	At this point I can't deal with nesting. Maybe never should.
- Groups that are intact in one compartment: no problem, they just move into
	the compartment.
- Groups that have components in multiple compartments: duplicate group names
	in the sub-compartments.

Merged compilation into main tree. Checkin 1861.

Added Shell/Neutral calls for tree traversal.
Like May 9, had to fight with GNOMEs keyring for quite a while, 
finally managed to subdue it, by adding a line in  .subversion/config:
password-stores =
and was able to checkin as 1862.

Added functions for field access to Shell::cwe_. 
Clears tests for Shell::doFind. Checkin 1863.

ReadKkit now creates real live Groups and Molecules. Checkin 1864.
ReadKkit now creates Reacs. Checkin 1865.
ReadKkit now creates messages between mols and reacs. Checkin 1866.

============================================================================
14 June 2010
Implemented Enz, incorporated into ReadKkit. Msgs pending. Checkin 1867.
Put in most of the Enz msgs too. Checkin 1868.
Still to do the MMenz case.

============================================================================
15 June 2010
First pass MMEnz implementation. Compiles. Checkin 1869.
Second pass: Sets up the MMenz messaging. Checkin 1870.
Added in the test file dend_v26.g that I used. Checkin 1871.

Minor cleanups for unused variables vol and slave_enable. For now I've
just put in hooks for their future use.
Also implemented Pool::diffConst field. Checkin 1874.

- Define compartment objects and some default geometries.
- Make /kinetics a compartment with the default volume
- The assignCompartment function maintains a list of compartments
	with different volumes. Every time a new volume appears,
	it creates a new compartment. For now assume that all
	objects with the same volume are in the same compartment.
	Returns compartment id.
- After build is done, then we scan all groups to see if all contents
	are in same compartment. We need to build up a map of
	all mol ids, which refer to their compartment.
	Are all mols in same compartment?			(A)
	- Yes: If same compt as parent of group, retain.
	- Yes: If differs from parent of group:
		Has new compt been instantiated?
		- Yes: move group to new compt
		- No: Make new compt. Move group into it.
	- No: Split group into same/diff compartment parts.
		With each part of group, goto (A). 

- Groups are invisible to a compartment. Any molecule looking for its
	volume or geometry goes ancestorwise till it finds a compt.
- 


compartment has a collection of boundaries
	- Each is a geom.
	- Simple compartments are just enclosed. Single boundary.
	- Complex compartments have different diffusion rates on
		each boundary. E.g., cylindrical compartment part
		of a long cylinder.
	- At each boundary there is a possible adjacent compartment.
		- Can we have both the surface compt and also the
			next one out?
		- Adjacent compartments may be dimension shifted.
			(e.g., cytoplasm to membrane)
	- All compartments have a size.

	- A membrane compartment may have 
		membranes on others. Multiple boundaries.
	- Compartment as a whole has a volume

	- How do we square with 'outside' concept of current SBML?

Worked out ChemCompt and Boundary to set up compartmentalization and 
interfaces between compartments. Will also need a geometry class for these
to connect to. Checkin 1875.

I could scan through and find all volumes.
I could scan through and put all Ids in a list by volume
I could create compartments on the fly as soon as I find a new volume.
	It could be on the current group
	It could be on /kinetics
		But if the compt
I want /kinetics to have the biggest volume, others inside it.

First scan: find all volumes
Assign /kinetics to biggest, put all compts right on /kinetics
Second scan: find groups that have a single volume. Put on appropriate
	compartment. May lose nesting?
	Groups that have multiple volumes: split up into volume-defined groups.
	Put onto appropriate compartment.

Massive struggle to get the compartmentalizing done. For now I'll skip it
to test that the model gets loaded and can run. Next:
* Implement tables to test output
- Implement enough scheduling to run model and test it.
- Implement the solver. This is what this exercise was really about.
- Then I can come back to the compartmentalization.
- Look at the sbml reader.

============================================================================
16 June 2010

Starting on tables. Implemented them from scratch, not bothering with too
much backward compatibility in this round. The table entries are 
handled as FieldElements. Checkin 1878.

Put in first pass unit test for Table acting as a buffer for incoming
data. Checkin 1879.

Put in a manual check that the table can dump data into xplot form. 
Commented out after checking.
Coded in the details for ReadKkit to talk to tables as plot substitutes.
Checkin 1880.

Now to implement Shell::doUseclock, the hook for which is now implemented in
Shell.cpp.
============================================================================
17 June 2010
Thoughts on generic wildcarder, one that can do network messaging too.
- Object
	- Portion that does traversal. Follows wildcard tree.
	- Portion(s) that handle constraints and return a double. For example,
		distance from source, membrane potential, or combination.
	- Portion(s) that operate on constraints. For example, create a
		message, set a synaptic strength, connect up a clock tick.
	
Converted in previous moose wildcard code, threw out the indexing stuff.
Lots of Set/Get fixes needed to do string set/gets without prior knowledge
of types. Checkin 1886.

Compiling in the unit tests for wildcard, again based on earlier version.
Still huge number of compiler errors.

============================================================================
19 June 2010
Wildcard.cpp finally compiles. Fixing up unit tests.
Turns out that StrSet/StrGet are not working. Set up unit tests for 
them.

Still struggling to get them to work.

Now past the StrSet/StrGet unit tests, back to the wildcard tests.
Slow but steady progress through wildcard tests. Currently at 
Wildcard.cpp:545. Checkin 1891.

============================================================================
20 June 2010
Cleared next round of unit tests for wildcards. This was painful. Checkin 1892.
Minor issue: The wildcard tests for fields issue a warning. Don't want it.
Valgrind is not at all happy.
Cleared up two big issues, the deletion of the test trees for wildcards and
for /kinetics. Remaining stuff is a lot of uncleared pointers from getSetGet.
I think this can be replaced with a call to the function that uses getSetGet,
(innerStrGet), rather than returning the pointer.
This will require lots of changes. Checking in the fixes so far. Checkin 1893.
removed all getSetGet calls, replaced with memory-safe Finfo::strSet and 
Finfo::strGet calls in Finfo and all derived classes. Valgrind reports
_still_ more leaks. Checkin 1894.
Further fixes to ReadKkit, improved, still leaks. Checkin 1895.
Yet more cleanup. Finally leaks gone. Checkin 1896.

Now to get back to testing chem kinetics. For starters, 
* run model, dump data, and compare with genesis.
* Bring in solver to scan model tree and zombify it.
* Test out solver too
Then in no particular order:
- Eliminate Data class
- Implement arbitrary math handler
- Fix up compartmentalization
- Implement readSBML
- Parallelize

Set up code to run model. At this point it just stalls.
Fixed the stall, now it crashes. It is calling process on a mol,
even though the src of the message is a reac and the arguments do
not seem to be right.
One subtle possible issue that SharedMsgs can be added either way,
but the code requires that the src and dest have a specific
direction. This could get messy if we wanted to use one of the
messages in a standalone way at the same time. For example, consider
Mol::nOut. It could be the src message for several things, so it
should always be on e1. Suppose its target thought the same in 
a shared message. There would be confusion.
I had earlier handled this simply by checking whether the src was
e1. Replicated this approach. Lots of changes in SrcFinfo, Qinfo and so on.
Compiles but crashes. Checkin 1898.
Progress. Now clears all unit tests and even the ReadKkit run.
Answers are wrong, though. Checkin 1899.

============================================================================
21 June 2010
Enzyme model test working now. Checkin 1900.

Thoughts on eliminating Data class and handling process and reinit:
- The Data class is used only to provide 'process' calls to Tick. 
- We actually also need to provide a reinit.
- Earlier idea had been to use Dinfo to provide process and reinit.
- Alternative: 
	- use the regular message framework for setting up the 
		msgBinding_ entry on the Tick.
	- Process becomes a shared msg with reinit.
	- Tick goes explicitly to this entry and scans through it directly,
		eschewing use of the queuing system.
	- This keeps the infrastructure to the minimum, and also bypasses the
		issues of putting process calls in the queue.
	- This also makes it easy to handle reinit, init, and other arbitrary
		target functions from process.
	- When an Element is zombified its process msgs should be eliminated.
	

In the meantime, lots of cleanup to get simple test programs for enz, reac,
mmenz to work. Now clears the Kholodenko.g model. Checkin 1901.

Had a look at KineticHub. Almost all of its contents will vanish, since the
DataHandler will be shared directly with the zombies and the zombie access
functions will do all the necessary lookups. No messages will change either,
except the Process ones.

- It would be nice to have a notification (another message?) when a message
	is added or removed. This will let me change the structure of a model
	while keeping the solver informed.
- I need a clean interface to the DataHandlers.


Adding solver code. Created subdir ksolve. Checkin 1902.
============================================================================
22 June 2010
A little benchmark: ran the kholodenko model for 60K sec at dt = 0.1.
Identical run in GENESIS: 2.4 sec. MOOSE: 12 sec. Pretty heavy costs for
the queued messaging. May be good to profile.

Did so. Here is the top of the profile:

Each sample counts as 0.01 seconds.
  %   cumulative   self              self     total           
 time   seconds   seconds    calls   s/call   s/call  name    
  9.78      0.88     0.88 22200390     0.00     0.00  Element::asend(Qinfo&, unsigned short, ProcInfo const*, char const*)
  8.60      1.65     0.77 37200000     0.00     0.00  SingleMsg::exec(char const*, ProcInfo const*) const
  7.54      2.32     0.68 37200390     0.00     0.00  Qinfo::addToQ(unsigned int, MsgFuncBinding, char const*)
  6.59      2.91     0.59 37200390     0.00     0.00  Qinfo::assignQblock(Msg const*, ProcInfo const*)
  6.20      3.47     0.56 37854727     0.00     0.00  std::vector<char, std::allocator<char> >::_M_fill_insert(__gnu_cxx::__normal_iterator<char*, std::vector<char, std::allocator<char> > >, unsigned long, char const&)
  5.42      3.95     0.49 90001576     0.00     0.00  Msg::getMsg(unsigned int)
  5.36      4.43     0.48  1236668     0.00     0.00  readBuf(char const*, ProcInfo const*)

This is somewhat difficult to work on, as there isn't any single huge 
bottleneck. Messed around with it a bit, but the improvement is negligible.
Now MOOSE takes 11.5 sec. Anyway, checked it in for reference. Checkin 1904.
Cleaned out benchmarking stuff. Checkin 1905.

Starting out on the ZombieMol. This turns out to be easy and clean to
implement if it is subclassed from Stoich.  Checkin 1907.
Pending issues:
- Swapping out contents of Element for zombie
- Stoich should handle arbitrary arrays, or explicitly take care of sims
	where the molecules are going to turn into reac-diff vectors.
	The latter is easier and far more powerful. Only the molecules
	need to add dimensions: reacs remain fixed through the volume.
	I need to then add the diffusive calculations.


Added in ZombieReac implementation. Checkin 1908.
Added in ZombieEnz implementation. Checkin 1909.
Added in ZombieMMenz implementation. Checkin 1910.

All this is good baseline stuff. Now the crux of the matter: swapping
Elements. Simple approach is a Element::zombify function,
to be called by the functions that scan the existing tree and put in the
new one.

Working on this. Mostly OK, except how do we decide what kind of DataHandler
to build when unzombifying an Element? The preliminary code is in 
ZombieMol.cpp and Element.cpp. Got the zombify stuff set up. Checkin 1912.

Put in zombify stuff in Reac, Enz and MMenz. Checkin 1913.

Other part of zombification: Scan path and figure out stoichiometry matrix
from messages.
============================================================================
23 June 2010
Stoich::setPath begins to handle traversal. Yet to deal with messages.


============================================================================
24 June 2010
Still working on traversal. Checkin 1918.
Traversal coming along. Major updates including eliminating the Element
calls relating to sync messaging. Not yet there, but checkin 1919.
Putting in the RateTerm creation stuff for MMEnz. Checkin 1920.
Putting in the RateTerm creation stuff for Reac. Checkin 1921.

Put in general functions in Element for getInputs and getOutputs for
specified Finfos. Replaced these in ZombieReac, seems OK. Yet
to carry through on other Zombies. Checkin 1922.
Next: Clean up zombies. Build the stoich matrix. Run. More unit tests.
Cleaning up ZombieReac and ZombieMMenz. Checkin 1924.
Cleaning up ZombieMol. Checkin 1925.
Cleaning up ZombieEnz. Checkin 1926.

Putting in the stoich matrix stuff. Printed it out, seems OK.
Added in stoich matrix stuff for enzyme too. Printed out 
barebones enzyme, seems OK. Handles the bigger dend_v26.g model,
but I won't try to decipher the stoich matrix. Checkin 1927.

Now on to the solver proper. Added in the Stoich funcs to handle it.
Need a clean way to exchange the data structures.


============================================================================
25 June 2010
First pass at GslIntegrator. Compiles but doesn't do anything as I haven't
set the USE_GSL flag. Checkin 1929.

Fixed up USE_GSL flag. Compiles. Checkin 1930.

Trying to set up message from Stoich to GSL. Like the plot messages, I want
to harness the 'Get' command capabilities to do this. Examples:
				Initiate		Handle return
- Script/shell 'get' request:	Shell			Shell
- Plot 'get' request:		Plot			Plot
- GSL stoich request:		Shell or Manager	GslIntegrator.

Defer for a bit. Focus on the solver. Seems to be getting close. But it dies.
Turned out there was a sign flip for the Stoich matrix terms for Reac.
Fixed. Runs. Does Kholodenko oscillatory model correctly. Checkin 1932.

Implemeinting hooks for Table to be able to request a datum,
typically used when trying to plot values on distinct timesteps.
Trying to compile...
Compiled, clears tests without hanging, but need to test the new functions.

============================================================================
26 June 2010
Implemented unit test for table to request a datum. A bit of a hack, but the
whole 'get' system is a bit of a hack. Checkin 1936.

Some cleanup of diagnostic messages during unit tests. Checkin 1937.
Checking memory leaks using valgrind. Many fixes. Checkin 1939

From 20 June, we had this list, which I have extended with dependencies
* Eliminate Data class			<- full reinit/proc msgs
- Implement arbitrary math handler
- Fix up compartmentalization
- Array and diffusive kinetics		<- Multidim arrays
- Implement readSBML
- Parallelize				<- Fix scheduling
- Fix scheduling 			<- full reinit/proc msgs
- Try replacing y_ with S_		<- Benchmark solver 
- complete ReadKkit			<- Buffers, Sumtotals

============================================================================
27 June 2010
Lets start with the Data class.
Q: Should I extend the process( Eref, Qinfo*, ProcPtr ) to have an extra
argument to decide between reinit, process, and other stages?
A: No. It is tempting but the other stage argument would need to be tracked
	in the Tick. Messy.
Q: Can I extend the SharedMsg with arbitrary extra msgs on the same line?
	In other words, piggyback?
A: Should be possible, but probably irrelevant, as different Ticks will
	be doing other stages.

There was a hiccup in the implementation because I had not fully worked
out how to get the DataHandlers to call 'process' for all objects. Did an
implementation, this is how it could go:
- Create a ProcOpFunc<T>: public OpFunc
- It does the usual OpFunc stuff, and has an extra function 
	proc( char* obj, Eref e, ProcPtr p )
	which does the type casting into T and then calls the T::*func
	for the process/reinit/init and other actions.
Pros and cons of this arrangement over earlier Data base class:
- Can handle arbitrary # of functions of the Proc func form: proc, reinit, etc.
	This can be hacked using Data, but not cleanly.
- No Data base class needed
- Need to maintain an entire specialized structure for process. This is true
	for both versions, but more so for the Data base class.
- Uglier. Typecasts etc during operations, though the Data form also needed
	typecasts.
- Possibly an extra operation or two. e->cinfo()->getOpFunc( fid ) as opposed to
	the Data::process virtual function.
- With the Data::process virtual function I could have passed in an extra
	argument to specify which of the ops to do. Need to trickle back to the
	calling Tick, and it would have to uniquely associate the correct func
	number to it. Not too different from passing in an fid.
Overall, items 1 and 3 are the crucial ones. Go with the conversion.

Conversion now compiles, but doesn't clear unit tests.
Checkin 1941.
Amazingly, just one more fix and the unit tests clear. Checkin 1942.

I would like to also fix the passing of Eref as the
full object. Better to pass by reference, though the compiler might
be doing that anyway.

Next, to fix scheduling, I need to standardize on a shared Finfo
for Proc/Reinit and set up the scheduler to deal with this.
- ReadKkit to default create the solver and stuff, and to
schedule it.
- Solver to remove process messages if present.
- Solver expanding into arrays
- Solver expanding into arrays with diffusion using RK
- Parallellization of all this.
- Multithreading of all this.
============================================================================
28 June.
Consolidating process and reinit into a shared message. Done for
kinetics and ksolve directories. Checkin 1948.
Done for builtins and biophysics. Checkin 1949.

Converting over to the consolidated form for Ticks. Croaks in unit tests.
============================================================================
29 June.
Tracked down bug. Unit tests now clear. Checkin 1952.

Applied the 'proc' shared message in unit tests. Croaks. This led to 
consderable fixing up of the Shell control of scheduling, specifically,
implementation of reinit, stop and terminate functions. Checkin 1956.

============================================================================
30 June.
Set most objects to use the shared Msg "proc" rather than the old "process"
message. Unit tests showed up a funny failure, finally tracked down to
failure to clean up the 'doReinit_' flag within Shell. Fixed, now clears
unit tests again. Checkin 1959.

Plans on going to array stoich solver, leading soon to stupid diffusion
calculations.
- The S_ vector becomes a temporary holder for the current compartment.
- The RateTerms and the vector V do not need to expand, and they can keep
	pointing into the S_ vector.
- updateV does not change what is there. 
	However things get unpleasant on multithreads.
- UpdateV is extended with a loop that fills in the diffusive flux. This
	will need just another table for diffusion constants.
- innerGslFunc cycles through multiple rounds of copying successive
	compartments worth of data back and forth into S_, and doing
	updateV each time.
The issue with the above set is that it won't work on multithreads.

Could consider the following bigger changes:
- Replace the S_ vector with a pointer to the current chunk of the y_ vector.
- Replace the RateTerm pointers with indices. This is slower but means
	that they can be used multithreaded for different chunks of data.

Refinements to updateV may still be relevant, but the details of how to
set up compartment updates are not, given that we need to implement a
completely different reac-diff solver. More important is to get the
data structures right.

============================================================================
6 July 2010
Went through converting function calls using Eref to using const Eref&.
Checkin 1975.

Some cleanup of ksolve tester. I had tried to use a ptr into the S_ vector 
directly for the GSL routine, but it didn't work. Checkin 1978.

Trying to put in buffered molecules, the inheritance seems to be temperamental.
============================================================================
7 July 2010
Putting in BufMols. The test for this has thrown up a lot of bugs with
scheduling, limitations of wildcarding, and so on. For now it
works but need to clean up. Checkin 1980.
* Make it so the 'ISA' flag in wildcards applies for ancestor classes too.
- Get rid of 'stage' in ticks. Their index and dt should do.
* Fix Gsl integrator to handle buffers.

Implemented Cinfo::isA function. Checkin 1981
Incorporated into Wildcard.  Checkin 1982
Added ZombieBufMol. Checkin 1984.

Fixes to Stoich and ZombieEnz, now it does buffered and regular enz reactions
OK. Checkin 1985.
I thought I was set for testing a big simulation, but ReadKkit croaks because
it doesn't know how to handle SumTotals. Need to put in the arbitrary math
expression calculator.

Brought in Raamesh's MathFunc. It seems pretty complete.
General idea:
- Have a RateTerm wrapper for the MathFunc
- Have another RateTerm table for handling updates that occur without 
	integration, but affect molecules that the reactions may use.
	e.g., sumtotals and tables
Looked at the RateTerms. Don't really fit well with the MathFunc. I
need a tighter form, say MathTerm, that handles the reactants and the
function string. I should set up a dummy version to do the SumTotals first.

Converted MathFunc over to current code base. Set up its unit test, it clears. 
Checkin 1988.

============================================================================
8 July 2010
Function handling taking shape. On the solver side, there will be a vector
of FuncTerms to handle all functions. FuncTerms are the base class and have
an operator()( double t ) so that they can accommodate functions of time
as well as molecule levels.

On the MOOSE object side, we need a base class FuncBase. This will basically
encapsulate a FuncTerm of the appropriate type, and provide hooks between
the messaging and the FuncTerm. Also will need to provide Zombification 
functionality.

Implemented framework on solver side. Compiles, clear tests, but doesn't
exercise any of the new code. Checkin 1990.

Implemented SumFunc as a standalone object. Not sure that the framework is
suitable... let's see how it looks as I scale up. Checkin 1992.

Implemented FuncMol as a derived class of Mol, designed to relay its input
to the molecule count. Updated ReadKkit to deal with it. Correctly loads
and runs model in ee mode, but the solver is yet to handle it. Checkin 1995.

At tail end of compiling ZombieSumFunc and colleagues, needed to get the
data into the solver.
============================================================================
9 July 2010
Completed compilation, solver now sees it and seems to work. Checkin 1999.
Tried with the bigger production model. ReadKkit still fails.

Fixed ReadKkit, but the simulation resuts for dend_v26.g model are wrong.
Checkin 2000.
Looks like PKC-active has something funny going on. It is the target of
the sumtoal. It is flat in the new MOOSE but rises steeply with the old one.

============================================================================
11 July
The GSL version fails on the PKC model from DOQCS, acc48.g...

		Genesis		GSL
PKC-active	0.0719806	0
PKC-cytosolic	0.825743	0.878
PKC-basal*	0.0165155	0.0176
PKC-Ca-memb*	0.0286744	0.0305
PKC-Ca-AA*	0.00385561	0.00415
PKC-DAG-memb*	0.00624439	3e-5
PKC-AA*		0.00432258	0.0046
PKC-DAG-AA*	0.0123681	6e-5
Ca		0.08		0.08
DAG		11		0.0569
AA		5		5

So, DAG is doing something odd.
The EE calculation in MOOSE looks OK.


A bit side-tracked: I did a valgrind to see if I could track this down.
Valgrind reveals a huge mess in MathFunc. Memory leaks galore. Need to fix,
but will have to defer.  Checkin 2013.

Fixed a minor leak in Stoich, commented out MathFunc test. Valgrind now
clean. This also means that the problem with the model above is not 
memory-leak related. Checkin 2014.

Tracked problem down to Stoich.cpp. I had used the wrong offset for 
updating the FuncMols. Now works with pkc model. Checkin 2018.

Checked again: dend model _still_ does not match up.
============================================================================
12 July
The old moose does handle it OK. Going through mol by mol:
Ca: OK
DAG: OK
AA: Goes down to zero in new code, initial part is OK
CaN: OK
AC2: OK
cAMP: Way too low. About 3 orders of mag.
CaM.Ca4: OK
GTP-Ras: 10x too low.
Raf-GTP-Ras: 50x too low.
PKC-active: Starts OK, but fails to build up.
PP1-active: OK
tot-auton_CaMKII: OK
tot-CaM-CaMKII: OK
MAPK*: zero. 

Tested the obvious possible problem from this list, of MMenz with a 
buffered substrate. GSL is OK.

Stripped off other stuff to make a model of AC, AC.g.
This also shows a discrepancy but the other way:
AC2*-Gs:	OK
AC1-Gs:		OK
AC1-CaM:	A little low
AC2*:		OK
cAMP:		3x too high.

Removed the PDE stuff, to make model AC_noPDE.g. This matches up. Is it the
PDE stuff on its own?

Removed the Gs stuff, to make model AC_no_Gs.g. This does not match.
cAMP:		3x too low (in gsl)
AC2*:		OK
AC1-CaM		OK, a bit of what looks like numerical error at ~5%.

Removed AC1 also, to make model AC_no_GS_no_AC1.g. This is way off.
cAMP:		None at all in gsl
AC2*:		OK.

Funny, the system doesnt seem to call the MMenz zombification routine.
============================================================================
13 July
Compared the AC_no_GS_no_AC1.g output for ee and ksolve modes. The cAMP
differs, and is wrong in both.
Fixed difference between ee and gsl methods. Now cAMP is zero in both. 
	Checkin 2035.
	- Can't be just MMenz. The Kholodenko oscillatory model works, and
		is full of MMenz.
	- Enz on its own and MMenz on its own work.

Reduced problem to just one enzyme, in AConly.g

Tracked it down. Unexpected: it was bad comment handling in ReadKkit.cpp,
which was discarding lines with the string "*/". AC2*/kenz was therefore
always discarded. Fixed this. The AConly.g file now matches, and so does the
original dend_v26.g. Checkin 2038.

Fixed problem with missing first data point in plots. Added operation to
request data at Table::reinint. Checkin 2040.

Quick benchmark on dend_v26.g run for 2000 sec:
current MOOSE: 97.5 sec, 
old optiMOOSE: 87.5 sec 

Unsure why there is this difference. All the work should be done by the
gsl solver, which is the same.

Next major cleanup is for volume handling and compartments.

One fundamental issue here is that if I refer to a different Element for
the volume in order to calculate conc, I do NOT want to use messaging to
access the data. The queueing data transfer would cause scheduling issues.

Another issue: explicit vs spatial indexing. I should really ask the system 
what the conc is at a given point in space, rather than specify a compartment
through indexing.

Things to ask of a molecule:
- n throughout the entire compartment
- n in a subvolume based on the grid indexing
- n in a specified region of space
- conc likewise

============================================================================
15 July.
For some of the more complex cases, implement an 'inspector' object.
For starters, just obtain local conc. Assume mol as a whole has access to
vol info. Would like a object-wide field in the data handler, one that isn't
duplicated when it becomes an array. At reinit this field is updated by the
compartment. I could implement this by having a derived class with the global
field. A temporary of this is created when the data is looked up, and the
global field is filled in. This would involve some nasty memory management.

For now, put in the extra double for the volume. Wasteful but the alternatives
are not much better. Note that we have other similar things in the pipeline,
like diffusion constant. Note also that all these go away when the solver
gets rid of individual entries. Checkin 2052.
============================================================================
16 July.
Setting up scan for volumes for generation of compartments. Vol categorization
code puts the test model dend_v26.g into 3 compartments, clearly incompatible:
# cases		Vol		Notes
129		2.767e-27	Likely the molecules as there were 129 of them
				But the value is wrong. Should be 1e-15 m^3
49		600000		Likely enzymes.
5		1		?

Fixed various issues here: the vol scaling is corrected, and the enzyme complex
vols are set by that of the parent enzyme. Checkin 2053.

Now return to the issue of organizing compartments and groups.
- Should 'containment' be represented by group hierarchy?
- Should 'neighbour' be represented by peers?
- Should compartmentalization take precedence over grouping?

We have three kinds of tree structure to deal with:
- Grouping
- Containment
- Neighbours.
Can we merge the containment and neighbour tree structure? 
	Should we treat a compartment as a single entity with an array of
	parts that have diffusive coupling?
	- It may have structure (spines, etc) that need to be positioned off
	individual compt entries.

No, I think we keep each of these distinct. There can be separate messages
to keep track of each of these relationships.

Options:
1. Use current grouping as parent/child relationship.
2. Use Containment as parent/child relationship
3. Use above merged containment/neighbour tree as parent/child.
4. Use tree hierarchy in whatever grouping seems sensible, let 
	Containment/neighbour messages deal with themselves. By default
	put containers in flat hierarchy like in neuron model defaults, but
	this is not mandatory. 'Sensible' in this case also implies that
	the grouping should be easy to array-ify.

1. is simple for kkit use, but at odds with desired formalizing of containment.
2. is good for SBML and other containment forms, but is not informative about
	multiple array entries each containing something. 
3. is probably not good to mix concepts. We want very clear-cut info from
	the messaging.
4. is free-form and puts the burden of organization on separate message-linked
	trees.

Use cases: 
	Dendrite with linear diffusion and lots of spines
	Presynaptic terminal with lots of vesicles
	Neuron model paralleled to electrical model

Seems like all of these can be done with option 2. Other levels of organization
are all easy, and all that we have done is to state that the 'containment'
concept is the primary one.

For kkit conversions: 
Largest compartment is not always container. Consider PSD and diffusive models.
Thus, /kinetics should not be assumed to be a compartment. This, though, will
make it extremely hard to handle backward compatibility in scripts.

Option 4 it is then. The next thing is to standardize the location and
organization of the grouping, container, and geometry objects.
ReadKkit puts stuff under a specified modelname, which can be under any
parent. 
/pa/modelname/kinetics
is where stuff now goes.
So all these things should go under /pa/modelname:
/pa/modelname/compartments		: Msg to all child Elements.
/pa/modelname/geometry			: Msg from/to compartments
/pa/modelname/grouping			: Msg to relevant child Elements.
/pa/modelname/kinetics			: Neutral based groups and kinetic model

============================================================================
17 July.
Put in first step at implementing the compartmental stuff. Molecules
are adopted by their compartments. Seems OK.
Should I also put reactions into compartments? See what SBML does. 
Checkin 2054.

From 28 June we had:
- ReadKkit to default create the solver and stuff, and to
schedule it.
- Solver to remove process messages if present.
- Solver expanding into arrays
- Solver expanding into arrays with diffusion using RK
- Parallellization of all this.
- Multithreading of all this.

1. I need to fix the naming. ChemCompt refers to a cellular compartment,
	which may be spatially extended. I should have a separate naming
	with the spatial compartment, which is internally uniform and is used
	for numerical calculations. Let's call it voxel.
2. Different cellular compartments are very likely to discretize differently.
	They are even likely to use different solvers. So solvers should be
	compartment-specific.
3. If solvers are compartment-specific, then we need to set up a clean way
	to interface between them. Brute force is forward Euler to estimate
	fluxes. I've done a little calculation to work out a Crank-Nicolson
	scheme with an intermediate transfer 'compartment' that doesn't
	react, but I haven't figured out how to deal with the mol concs in it.


Consider voxels A-1, A, A-1 where 
	A is the inserted transfer voxel,
	A-1 is the last voxel on the first compartment
	A+1 is the first voxel on the second compartment.

Explicit calculation:
A(t+1) = A(t) + dt*( A-1(t) + A+1(t) - 2A(t) )

Implicit calculation:
A(t+1) = A(t) + dt*( A-1(t+1) + A+1(t+1) - 2A(t+1) )

To make it Crank-Nicolson, we take the average of the above two. 

For now, just use forward Euler for flux. Possibly exp Euler even.
Each timestep, get 

Flux = A-1(t)*D/dx - A+1(t)*D/dx

vtot(A-1) = v - Flux
vtot(A+1) = v + Flux

Another complication: Stoich will need a different flux for each boundary.
So boundaries have to be ElementFields. As an example, a branching dendrite
will need 3 flux entries: one for parent, and two more for the two branches.

Stoichs will have to match up the molecules that diffuse, and send out
only those. This looks like a set of extra 'v' terms.

Now that this is defined, I need to go back a couple of steps to set up
array stoichs within a compartment. This is needed before I can figure out 
how to deal with the end-terms for flux.

============================================================================
18 July 2010
Starting on array-ification of Stoich. 
- For GslIntegrator, we now need to build a y_ vector big enough for entire
	model.
- For Stoich, we need to keep assiging S_ and passing in the correct
	part of y_
- For zombies, we need to access y_ rather than S_. 
- Should we move the y_ vector into the Stoich?


============================================================================
19 July
Did a first pass implementation. This further underlines the desirability
of using indexing rather than pointers in RateTerms. Specifically, the
loop begs to be parallelized, but can't be as long as the native data
structure of the Stoich is being used for all calculations. This could
be avoided with indexing, as we then only need to base the indices off
different points in the mol conc vector. The remaining 
difficulty with this idea is that in diffusive models, we need to look
up buffer and func molecules in a different way from the variable 
molecules. Offsets in mol Conc need to go to different parts of the 
overall varMol vector, whereas the buffers and func mols can be shared.
Options
	- fold buffers/funcMols into the RateTerms?
	- Have distinct kinds of RateTerms for buffered mols.
	- Do memcopy into thread-unique buffers
	- Put the buffer/func mols at the end of the entire varMol set.
First two options will need multiple distinct kinds of RateTerms.
Fourth option will work and will also deal with individual access to all fields.

Anyway, get the first pass working first. This needs changes to GslIntegrator
to move the y_ vector out of it into the Stoich.
Did this, but more updates needed. The gsl output isn't coming because I
had changed the 'Mol' class. Fixed that, but now I get all zeroes because the
plot is trying to look up concentration, and the Stoich system doesn't
currently keep track of volumes. So all the Zombies return 0.
Checkin 2060.

============================================================================
20 July

Will need 3 modes of operation at least:
1. general Stoich class, picks up entire simulation for GSL or Gillespie
	calculation applied across whole model. This one will need to refer
	to all of the compartments.
2. Compartmental Stoich class. Fills in a single compartment and can
	discretize it in various ways. May even take over from the Compartment
	class. This one will have to provide flux terms for interfaces,
	as well as do something sensible about reactions spanning boundaries.
3. Compartmental nonStoich solvers, such as Smoldyn. Fills in a single
	compartment and does whatever it does internally. May take over from
	Compartment class. Provides flux terms.

For starters we need to implement 1, to be able to load and run generic
models. 
Then we need to convert this and RateTerms to use array indices rather than
pointers.
Then we derive the compartmental Stoich class from it and set up to do
1-D arrays with branching etc.

Did item 1 which was to pick up all compartments and deal with the volume
and conc stuff. This required several fixes in ReadKkit. Checkin 2076.

Fails on regression test with psd12.g, which has 2 volumes.

There is a problem with the kinetics code that cascades over the later test
of the shell messaging. Unclear why, but there are lots of memory leaks.

============================================================================
25 July
Turns out that the problem is in other models as well, but surely with the
kinetics code. Also the valgrind problem is probably just because we exit
from MOOSE with an assertion, leaving lots of stuff lying around. So thing
to focus on is why the kinetics stuff causes problems with later messaging.
Note that the ksolve is not the problem, it also happens with the regular
kinetic calculations.
Looks like there is a problem with Shell::setclock, or at least setting
clock 1. Clock 0 seems OK to set.
============================================================================
27 July
Still struggling with the setclock issue. Some random cleanups in the vicinity
of the problem, but they haven't helped. Checkin 2093.

Eliminated the Tick::stage field. Went through and cleaned up all the effects
of this change, with the idea that a global clean of the tick handling code
might fix the setclock bug. The scheduling unit tests clear with the new
code, but the setclock issue with ReadKkit persists. Checkin 2094.

The unit test clears if we run the test for 2 timesteps rather than 1. Seems
that the key thing here is to clean up the execution of all ops in each
timestep. This is still pending.

With this, ran psd12.g. Now the gsl version works but the original one 
doesn't. Valgrind is clean.

============================================================================
29 July
2 minor pending things before going to change the structure of the ksolver.
- Find why only 4 graphs are being plotted. The ones on moregraphs are not.
	I checked, and the plotpath is OK, and all 7 graphs are made.
	There are issues with setting up lookup for PlotId
	Fixed. Also fixed plotpath in dumpPlots. Oddly it dumps now for
	EE but not GSL. Tracked down to another plotPath issue in testKsolve.
- Find why the EE method fails in MOOSE
	- works for enz.g
	- Works for a reduced version of psd with only one compartment.
	- Works for a reduced version of psd with one mol added in bigger compt.
	- Works for a reduced version of psd with two mol added in bigger compt.
	Started over with psd12.g
	- Fails when the exocytosis step is removed.
	- Fails when the PKA conc is set to zero.
	- Fails when PKA is deleted.
	- Fails when both exo and endo reactions are deleted. Now we have
		no exchange between compts.
	- Fails when dangling Ca and PKA are also deleted.
	- Rebuilt plots. Now there is naming confusion in plots, and I need to
		fix it. However, the underlying problem persists.
		Call this one twocompt.g for reference.
	- Removed M of the big compartment. Problem remains.
	- Removed all reactions in big compartment, leaving just RP and M*
		pools. Now output is all zero, not informative.
	- Set init conc of M in small compartment to 1. Outputs still match.
	- Set init conc of M* in big compartment to 1. Outputs still match.
	- Set init conc of M in small compartment to 0. Outputs still match.
	- Made a reac between Rp and M* in big compartment. Outputs still match.
	- Set init conc of M* in big compartment to 0. Outputs still match.
	- Replaced the reac between Rp and M* with an enz. Outputs now 
		differ, but only by a factor of 2. Note that this means
		that M* exceeds total init conc in the EE case.
	- The enz above was in compartment B. Moved it to /kinetics.
		No change, outputs as before.
		differ, but only by a factor of 2.

This gets tedious. Let me fix up the graphs for now.
Fixed. But the problems with the EE calculations remain.

Let's start removing the small compartment objects.
	- Removed A/M*. Problem persists.
	- Removed phosphatase. Problem persists.
	- Removed kinase. Problem persists.
	- Removed RP. Problem persists. Now we just have A/M. 
	Save this reduced problem version as minimal.g
	- Removed M as well. Now the A compartment is empty. Problem persists.
	- Removed the A compartment. Problem persists. Copy to minimal.g
	- Shuffled the order of definition of addmsgs to match those in enz.g
		Problem persists.
	- Shuffled the order of definition of pools to match those in enz.g.
		Specifically, the kpool handling the enzyme is now first. Was
		last. Problem persists.
	- Plotted all molecules. Bizarre stuff happening with enz pool and
		substrate, looks like jump at TRANSIENT_TIME. Are we just
		running into numerical issues? The rates are not big.
	
	Reduced dts. The EE method now works, when there is no variable DT.
	With same dts, where finedt == simdt, ran EE with variable DT. 
	Still OK.
	With simdt == 2x finedt, ran EE with variable DT. Still OK.
	Still OK.
	Ran in GENESIS with suggested dt of 0.01. Works fine. Fails in MOOSE.
	So I have an error somewhere in the EE calculations. Ugh.


During reinit:
	Mol::reinit: n=ninit, A_ = B_ = 0, nout.send.
	Enz::reinit: r1 = k1
During messaging: 
	Mol:: A and B increment on incoming A,B. Which doesn't happen.
	Enz:: r1, r2 and r3 update on incoming n. This happens.
Mol::proc1:  
	Mol::proc: integrate then send. First integ is pointless.
	Enz::proc: send, then reset r1. First send finally has data.
During messaging: 
	Mol:: A and B increment on incoming A,B
	Enz:: r1, r2 and r3 update on incoming n

To change: The Enz is on a later tick. It is executed only after
all data from t0 has arrived. So we may as well start calculations
right away.
This doesn't work. Reverted.
I checked if there is something bad happening with volumes. Doesn't seem to be.

Derived another test case from the generic enz.g, simply by scaling
volumes up by 1000x.

Check what enz does on creation with its cplx molecule.
Showed fields. Nothing obvious.
Checked what the timing of the 'tick' is like. Seems OK.
============================================================================
01 Aug 2010

Going to try again, this time just printing out values at each place each
timestep. 
In the meantime, checked in the updates as 2104.

Printf debugging to the rescue. The problem seems to be that the reinit
call sends out messages from all mols, and then the process call
also sends out the same messages. The 'sub' function is therefore called
twice for each molecule, doing a multiplication into the internal variable
r1_ each time. During normal process operations r1 gets reinitialized, but
not during this initial timestep problem. That is odd.

OK, this is what the scheduling does:
During reinit, it just calls reinit on all objects, in scheduled order.
During process, it first deals with all the pending queue stuff, then 
	calls process on all objects.

So there is no clearing of pending stuff after reinit. If there are multiple
sends during this phase, they all just pile up till the first 'process'.
Should rethink and clean up.

Can I keep all 'sends' within a timestep? No, the 'process' comes second
and there will usually be spillover to the next timestep.

in that case I may as well set up the rule that the only 'sends' that reinit
is allowed to do are 'sends' that span timesteps.

In reactions, the reacs and enzymes cannot send useful data till
they have done their 'process'. So the sequence should be:

REINIT:
Molecule reinits and sends out data.

PROCESS:
Stage 0:
	Reacs/Enz handle msgs
	Reacs/Enz send update
Stage 1:
	Molecules handle msgs
	Molecules send update.

This also has implications for resume and cleanup of processing.
It is cleaner to have a separate 'resume' or 'restart'
function called when a sim is beginning or resuming. If so,
reinit just sets boundary conditions.
restart sends out the preliminary data to start the cycle.
Should decide if reinit should fold in restart.
Restart always needed when resuming simulation midstream.

Another way to put it, is that at the end of any timestep, the
system should be in a state where all pending data can be flushed,
and where the state variables of the system can easily regenerate
the messages needed to continue calculations where they were left
off.

OK, worked. This took a long and weary time to fix, but I've now
set policy for how to handle multi-stage process calculations.
Checkin 2105.

OK, now change gears to look at implications of indexing the stoich
calculations, rather than direct pointer refs.
Phase 1: Benchmarks of existing version.
Binary		File		Runtime		Plotdt		User time
moose_msg	Kholodenko.g	6e6		5		19.1
moose_msg new	Kholodenko.g	6e6		5		19.9
moose_g3	Kholodenko.g	6e6		5		28.6
moose_msg	Dend_v26.g	200		1		9.7
moose_msg new	Dend_v26.g	200		1		14.2
moose_g3	Dend_v26.g	200		1		8.4

Interesting divergence here. The smaller model runs way faster on current code,
bigger model a bit slower.
Checkin 2106.

Now go through and reimplement Stoich and RateTerms to use indexing.
Done. Looks like a pretty severe speed penalty specially with the bigger
model. Checkin 2107. Let's see if I can track down the speed issue.

Did profiling. Nothing obvious.

============================================================================
2 Aug
Try more careful benchmarking. I started out with a really big model (acc81.g),
except this gave me still more bizarre results, with the new MOOSE being
astonishingly fast on it. Then another mid-size model.

Binary: moose_msg new (with array indexing). On: Mishti: (Lenovo X301)
	U9400@1.4GHz.

File		#Mol	Runtime	Plotdt	t1	t2	t3	Comments
Kholodenko.g	15	6e6	5	18.8	18.8	18.8	2.25 on 500s dt
spine_v61c.g	104	500	10	18.5	18.6	18.5
Dend_v26.g	129	200	1	14.0	13.9	14.0	Surprising slow	
acc81.g		1050	200	1	12.6	12.6	12.6	Surprising fast

Binary: moose_msg old (with array ptr lookup). On: Mishti: (Lenovo X301)
Kholodenko.g	15	6e6	5	18.8	18.6	18.9	2.1s on 500s dt
spine_v61c.g	104	500	10	13.7	13.7	13.7
Dend_v26.g	129	200	1	9.5	9.5	9.5
acc81.g		1050	200	1	12.6	12.5	12.5

Binary: moose_g3 (Jul 21 64-bit build). On: Mishti: (Lenovo X301)
Kholodenko.g	15	6e6	5	27.9	27.9	27.8	Surprising slow
								2.2 on 500s dt
spine_v61c.g	104	500	10	11.8	11.8	11.8
Dend_v26.g	129	200	1	8.3	8.3	8.3	Eh?
acc81.g		1050	200	1	28.4	30.4	28.6	Eh??

These results are all over the place. The only consistent finding is that
the old moose_msg binaries (pointer version) work faster than the new 
array index variants. Sometimes by 50%, sometimes not at all. The old 
moose_g3 binaries are all over the place, sometimes better still, 
sometimes dreadful.  I really need to compile and run these on another machine.
Just to be sure, I also went and compared the output of the calculations
with those on Genesis. Seems OK.

============================================================================
4 Aug
Now running benchmarks on Nehalem

Binary: moose_msg new (with array indexing). On: Ghevar: (Xeon E5520@2.27GHz)

File		#Mol	Runtime	Plotdt	t1	t2	t3	Comments
Kholodenko.g	15	6e6	5	10.6	10.7	10.7	1.1s on 500s dt
spine_v61c.g	104	500	10	10.6	10.6	10.6
Dend_v26.g	129	200	1	8.7	8.7	8.7
acc81.g		1050	200	1	8.8	8.9	8.9

Binary: moose_msg old (with array ptr lookup, R2106). On: Ghevar
Kholodenko.g	15	6e6	5	10.7	10.9	10.8	1.2s if plotdt=
									500
spine_v61c.g	104	500	10	7.7	7.7	7.7
Dend_v26.g	129	200	1	5.8	5.8	5.8
acc81.g		1050	200	1	8.9	8.9	8.9

Binary: moose_g3 (Oct 14 2008 64-bit build). On: Ghevar
Kholodenko.g	15	6e6	500	1.8	1.8	1.8	Note long plotdt
spine_v61c.g	104	500	10	8.4	8.5	8.4
Dend_v26.g	129	200	1	6.3	6.2	6.4
acc81.g		1050	200	1	9.3	9.3	9.3


Conclusions
1. The array indexing does have a significant impact in medium-sized models.
	This is between 30% and 50% slowdown.
	My interpretation is that this is cache-related. Really small models
	have everything in cache, or are otherwise limited. Really big models
	are out of cache regardless.
2. The new solver compares well with the old one in most cases.
2a. The Kholodenko model initially gave odd results, but it turns out that
	these were due to file dump time. Without it, things are comparable.

Discussed with Subha, he doesn't have any additional insights other than
agreeing that array indexing should not cause such a big speed hit.

Let's do one more cycle of profiling to see if there is something else
obvious happening.

A lot of work later, turns out that the models are not producing exactly
the same output. There are issues with the Ca handling, and the # of 
calculations differ in the profiling. This may be the problem.
Specifically, it looks like CaM-Ca4 is different and so are its downstream
molecules. Possibly a high-order reaction bug.

Working through by elimination of the pathways. The critical one seems not to
be CaM, but PP2B.
A bit more tracking, and I can get the discrepancy just with the PP2B (CaN)
model.

Reduced still further, it is just the two Ca binding steps, each of
which is a reaction 2nd order in Ca. I get a steadily near linear
increasing CaNAB-Ca2.Co with time in the 2107 checkin case.

Tracked down to the single reaction 
2 Ca + CaNAB <===> CaNAB
with Kf 1000 and Kb 0.1. (kf in this vol of 1e-15 is 2.78e-9)

Found error - nasty bug where virtual function args had been changed,,
but the one for the derived class had not. So the default virtual func
was being called and giving the wrong answer.
With this fixed the timings are:

Kholodenko.g	15	6e6	5	19.4	19.4	19.8	2.4 on 500s dt
spine_v61c.g	104	500	10	13.4	13.4	13.4
Dend_v26.g	129	200	1	9.7	9.7	9.7
acc81.g		1050	200	1	12.7	12.7	12.8	

These new times are almost exactly like the old (array pointer) ones.
Also confirmed that the output now matches perfectly (md5sum) with the old
one for the Dend_v26.g model. So we are in business. No barrier to going
ahead with the indexed approach to calculate diffusion models. Checkin 2112.

============================================================================
10 Aug 2010
Instead of trying to do different geometry calculations independently, I'll set
up a data structure that keeps track of junctions. Each junction has a scaling
factor derived from cross-section area and length of the compartment, and
applies this individually to each diffusing molecule. Will need a bit more info
for motors and non-uniform diffusion.

class jn {
	double scaling;
	unsigned int vox1;
	unsigned int vox2;
};

However, many of the jn entries could be generated on the fly based on higher-
level knowledge of model shape and voxelization. # of entries is quite
large, ~# voxels. 

Other consideration is to make the representation general enough that other
solution methods than Runge-Kutta could use it.

Should I index the junctions as independent entities, or have the first 
index refer to the first voxel? Issue of multiple references.

Seems like
a) All reactions within a compartment are the same. Same Stoich. 
	- But we might use different solvers for the same compt
	- We might want to share stoichs for different identical compts: spines.
		- alternative is to treat them as diff geoms.
b) Each compartment (ChemCompt) supplies an iterator to internal junctions.
	Or a lookup function to let us get the relevant junctions within and
	at the boundaries of the compartment.
c) The compartment holds one or more geoms. 
	- Used internally to figure out junctions
	- Used externally to draw.
d) Compts talk to each other by another set of junctions, implemented as
	FieldElements. These contain info about how to exchange molecules,
		and include mappings of mol indices between compts.
e) Motors are a special case of inter-compt reactions. It is a bit premature
	to specify them in full, but the general idea would be to use the
	full SBML capabilities to define the reactions, and add in a scaling
	using XA and L of each voxel. Don't mix up with the diffusive stuff.
f) Buffered gradients. Another nasty one to specify. Tedious to
	implement: a vector or func for init values, and housekeeping to
	fit it into the numerical calculations with each voxel.

Some specific cases:
A neuron model could have four compts: the soma, the dendrites, the spines
	and the psds
	These would have special geometry refs that look at the neuronal 
		morphology spec. Need to handle multiple separated geoms for
		spines/psds.

============================================================================
10 Sep 2010
We need another layer of info; specifying which solver to use where.
Option 1: just use another compartment, even if the stoich is identical
Option 2: Use the definition of cellular compartments as domains where 
	reactions are the same (even if there are spatial gradients). Then
	I need to subdivide the cellular compartment into numerical 
	compartments.

15 Sep 2010
Note that either of these is messy. The model is being confounded with the
algorithm. Alternatively one could argue that we are just setting the model
up into domains of deterministic and stochastic behaviour.

If we use option 2, then we really want the ability to use a higher-level
spec to tell the system which algo to choose where. It could for example be 
based on dendrite diameter, or it could be a simple geometrical spec.
Big advantage is that I can swap around numerical approaches independent of
basic model definition.

I also need to build in the interface capabilities with other solvers: adaptors.

chemCompt ------/ Model
		/ Stoich
		/ metaGeom ---/	Geoms
		/ innerJunctions
		/ outerJunctions
		/ spatialReactions
		/ spatialParameters
		/ spatialAlgorithms [---/ GeomList]
		/ adaptors[]

Some details:
	- metaGeom could refer to a class that takes a path to a neuronal model
	and manages a chemical decomposition to match it. Or it could be
	a vector of regular geoms. Has to provide discretization info. 
		- Need a vector of discretization objects, which specify
		gory details of junctions as well as a lot of the spatial stuff.
		Also handle geoms. Hm.
	- innerJunctions and outerJunctions need to refer to assorted geoms
	as well as to discretization 


============================================================================
16 Sep 2010
Subha and others are having difficulties with the current MOOSE. So I need
to change gears and focus on getting the new base-code ready for adoption.

Key issues:
- Thread handling needs some cleanup
	- Need to set up a separate GUI thread and possibly another for 
		the openGL stuff. Or they could go out by MPI.
	- Generic cleanup
- Queue stuff needs cleanup
	- See about streamlining the allocation into local/offnode queues.
	- Have another look (later) at optimization. This is a big bottleneck.
- Array handling and indexing and lookup needs some work
	- need also to sort out policy for multidim arrays.
		- Could just add ops for DataId to take a vector of 
			indices and vector of sizes to compute linear index.
		- Will need to add ArbDimGlobalHandler.
		- Clean up numData assorted set of ops in DataHandler.
	- Sort out field vs element array logic.
+ Unit testing
	- Need to resurrect the whole bank of unit tests.
	+ need to create regression tests driven by C++ for system-wide stuff.
- Create and copy need work, specially for arrays
- Complete list of shell functions. This may need to provide an
	interface to the Set/Get operations through strings.
- Move may need work
- All Shell functions need to be made thread and MPI-compatible
- I need to build some unit tests at the Shell level for all operations,
	and an automatic way of running them through multithread and
	multinode and combinations thereof.
- Proper Design Document
- Fix up dimension junk in the create function: give a simpler version for
	0 and 1 dim arrays.
- Treat arrayFieldElements so that we can easily look up their dimensiosn.

============================================================================
18 Sep 2010
Finally started to code. Renamed Shell::create into the more systematic
Shell::handleCreate. Compiles. Turns out that I have an error now because
I deleted a file 'foo.g' which the ReadKkit was using to test.
Should separate out unit tests from regression tests.

============================================================================
19 Sep 2010

Setting up MOOSE regression test system
First order is to ensure that it can correctly load and run specific models.
In addition, From Wikipedia:
"Therefore, in most software development situations it is considered good 
practice  that when a bug is located and fixed, a test that exposes the bug
is recorded and regularly retested after subsequent changes to the program"

For MOOSE, I'm putting this second kind of issue into both the regression and
unit test mandates, as appropriate. The general scope of regresion tests
is entire system function. The scope of unit tests is function of
specific small subparts.

I'll implement a function in Table to read in files including single and
double-column xplot files, scanning through for a specific xplot name,
and doing interpolation in due course.

This needs a regression test.
I need a regression test now for testing ReadKkit too.
I need a regression test for testing ksolve too.
So I've commented out these operations as they utilize an external model file
	for their tests, and hence are more appropriate for regression testing.

Finally, all compiles and clears _unit_ tests. Checkin 2179.

Next: Make a table comparison function or Class. Class it is, since there
will be a range of parameters to assign for comparison. This is a bit
redundant when considering the capabilities of NumPy, but we need it for
low-level runtime calculations so it stays.

Done. Implemented as a function wthin the table to avoid class proliferation.
Does RMSdifference  and scaled RMS difference calculations. Use the latter
for measuring goodness of fit in a dimensionless manner. Checkin 2180.

With this in hand I can implement some regression tests. First to test
the testers.

I've now set up some of the framework for the regression tests. Still need
to clean up the base Makefile so they are not compiled in at all unless
a flag is set. Still the table test doesn't work. Checkin 2181.

The test doesn't work because I haven't fixed up StrSet and StrGet.
Currently they go dangling from the DestFinfo. Need to decide if I should
use the virtual forms or go direct to statics.
============================================================================
20 Sep 2010

Implemented the StrSet. Involved fixing lots of functions and OpFunc derived
classes. Checkins up to 2183.

With that, the regression tests started to give real error messages. Fixed
those. Checkin 2184.

Added a few more regression tests for the Table. This revealed yet more
bugs, including a nasty one for getting the outputValue. Fixed. Checkin 2185.

At this point I think Tables are pretty well covered, and with them, many
of the utilities needed for regression tests. On now to fix up
some of the model-based regression tests.

Need to implement a general Shell command to load in a model from a spec file.
Shell::doLoadModel( fname, destpath );

Turns out that Id::Id( const string& path, const string& separator )
	has yet to be defined. Ugh. Fixed. 
Implemented some unit tests for components of Shell::doLoadModel. Other
parts will need regression tests.

Checkin 2188.

Worked through regression tests for checking file type and then for
loading and running Kholodenko model using ReadKkit. Added a few
more unit test cases for bugs that turned up.
Checkin 2189.

Now we have the base regression test framework working.
============================================================================
23 Sep 2010
Immediate two cleanups:
- Array indexing, both elements and fields
	- Always have array as linear array. The Data Handler currently 
	does a lot of stuff to return different slices/indices of it. It
	can probably generate an indexer class that the Shell uses to
	make sense of the array index.
	I would like to be able to index arrays by text strings too, but
		not critical.
	Options:
		- Function that takes arbitrary args and generates linear index.
		- Function that takes vector of uints like the dimensions vec.
	Problems:
		- Linear lookup of something with ragged arrays. 
			Here we need to transform quickly from the linear
			index to the multidimension one.
		- Resizing arrays
		Options:
			Use rectangular dimension assumption.
- Implement Shell::doResize to change dimensions of an Element.



So the interface looks like this:
	unsigned int linearIndex( vector< unsigned int > dimIndex );
	unsigned int size( unsigned int dimension, vector< unsigned int > dimindex );
	unsigned int numDimensions();

	In addition the DataHandler needs generic functions for setting these
	things. setSize would have to pass in a vector of sizes
	Iterators.

============================================================================
30 Sep
Slowly working on AnyDimHandler which is meant as a prototypical data handler
that fully exercises the capability of DataDimensions class.
Need to incorporate node allocation in AnyDimHandler::resize();

Use case for 3-D array is obviously diffusion.


Dangling functions for the AnyDimHandler:
- copying to globals from non-global. This can be deferred. It requires
	a way for all parts of the object to be consolidated. The Shell
	may need to do an all-to-all send where each node sends out its
	own data contents.

An issue is: who sets multinode policy? Ideally should separate this out
from the DataHandlers. Comes up because the copy and allocation operations 
involve multinode decomposition. This decomposition will often need to
talk to the shell:
	Allocate: Need to decide 'start' and 'end' of the data chunk. More
		generally, there may be many ways to chop up the data.
		Should this policy be decided by different subclasses of
		DataHandler, or should the DataHandlers just do this generally
		and have the Shell tell them how data is done?
	Copy into Globals: Shell needs to arrange that all nodes contribute
		their data to the composite global
	Copy into another dimensions: Typically will want to assemble
		all entries into a block, and then duplicate this into the
		new dimension so that each node gets one or more blocks.
		But there are lots of policy issues such as the granularity
		of the blocks and the # of nodes.

Option 1: Proliferate DataHandlers to embody all this policy
Option 2: Implement DataHandlers generally enough that they can handle any of
	the policies, but leave the policy decisions to the Shell and up.

What is in a policy?
	-> Given a vector of dimensions, and N nodes with T threads each, 
	which object indices should be on a given node? 
	- Note that all threads have access to all the object indices on the
		node. So the threads are needed to figure out policy, but we do
		not subdivide objects by thread.
	- Policy also depends on the messaging structure. But that is a still
		higher-level decision, coming from the solver or model builder.
Policy operations:
	Allocation / resizing
	Duplication/copy, including adding more dimensions
	isDataHere... 
	Looking up data from multidim index

If we assume all DataHanders are n-dim arrays, then example policies are:
	- linear block along data array: Easy to look up, matches the Id.
	- cuboid block in n-dim array: good for neighbour connected models
		- columns and other low-dim versions are variants of this.
		- Spatial mapping need not match index mapping, but the
		duplication of blocks in new dims is likely to retain structure.
	- scattered entries along data array: Easy to resize. Hard to redim.
		But redim is tricky anyway.
	- Sticky dim0. This means that we try to keep objects on dim0 
		(the fastest changing dim) together, as they are likely
		highly connected. e.g., neurons in a cortical column or glom.

Policy		Allocation	Lookup		IsDataHere	Resize	add dim
Linear block	find start/end	[id - start]	start<=id<end	OK	hard
scattered	1 + size/N	[id/N]		(id+offset)%N	OK	?
cuboid block	size/N multidim	vec(id - start)	cuboid		Messy	Easy

Sticky dim0:
Linear block: as before, with start and end being calculated accordingly.
scattered: 	dim0 * (1 + size/(N*dim0) )
				dim0		((id + offset)/dim0)%N
				

Multilevel arrays:
	Bulb model case
		Compts in each cell are individual Elements, as each has
			different channel composition.
		Have a set of distinct prototype Cells, with their solvers.
		Make arrays of each in order to set up one glom.
		Have a set of distinct prototype gloms, each arrays of
			assorted mitral and PG cells.
			Say mit[0-7] in glomTypeA, mit[0-4] in glomTypeB
		Make arrays of each of these gloms for whole bulb.
			mit[0-500][0-7] in glomTypeAs, mit[0-600][0-4] in B.

	This won't work with a single base array. Need to make two base mits.

============================================================================
5 Oct 2010
AnyDimHandler taking shape, and I've begun the process of getting the newly
defined interface into the base DataHandler class.

Pending issue is to globalize or unglobalize DataHandlers, which also requires
the ability to pass contents around. This is in principle also enough to
do dynamic load balancing, but there are issues with the ordering.
Specifically, if we want to shift some objects from node A to node B,
we are likely to mess up the ordering of objects on the data handlers of both
nodes.

============================================================================
6 Oct 2010
Starting the compile process. Since there have been many incremental changes,
I'll checkin things. Checkin 2199.

More fixes. Checkin 2200.
More fixes. Checkin 2201
============================================================================
7 Oct 2010
More fixes. Checkin 2201.

More fixes, now compiles ZeroDimHandler.cpp.

Continuing with fixes. Need to figure out how setData works. It has to be able
to handle block assignments.

============================================================================
9 Oct 2010

Slowly continuing with compilation. Checkin 2207.

Redid interface for setData, now it is setDataBlock and the constraints are
set much more tightly.

Issue: What to do in setDataBlock when copying into block that is decomposed
among multiple nodes? See OneDimHandler::copyToNewDim for this situation.
	- Should each individual part of the OneDimHandler be replicated 
		newDimSize times? Actually at present this does NOT happen. 
	- Should pass in an entire data block, or only the sparsely filled
		data block on the current node? If the former, how?

Usual scenario should be that we only copyToNewDim from a global.


Some test functions:
make test object global, add dimensions till a 4-dimensional thing.
make test object local, add dimensions till a 4-dimensional thing.
make test object global, add dimensions, and last dimension is decomposed.
globalize model
unglobalize model
make 100 cells from a global prototype
Make a planar (2-D) array of cells
move some objects between nodes (part of load balance)

============================================================================
10 Oct 2010
Compilation now past the DataHandler classes, and running into the expected
issues in the rest of the code. Checkin 2208.

Now moving onto data1 as used in Eref and thence in many other places.
It is to "Returns data entry of parent object of field array"
Perhaps if I want the parent entry I should request the parent explicitly?
But it is true it is rapidly obtained from the Ids.
It is used in UpFunc.h, in cases where the parent object has to be called
to execute a function in the child. The child is usually an array entry.
Examples are syn input to an IntFire.

Key issue is this: when we have an array of fields, then the data entries
for the fields are distinct from the parental data entries, and the fields
are within a C++ array on the parental data entries.

Array of glom
	-> Array of mit cells
		-> Array of compts
			-> array of channels (but these are different elements)
				-> Array of syn inputs on receptors

Elements associate messages and Finfos with data.
	Except sometimes Elements are used to hold extended fields
	and Elements sometimes hold other fields with subparts.
	Here they still associate things with data...
Finfo handles function types for Elements using OpFuncs.
Messages associate functions with data.
DataHandlers associate Elements with data.
Dinfo handles data types for DataHandlers.
Elements are looked up by Ids
Data within an Element is looked up by DataIds.
	This isn't fully right. The whole object is looked up by the DataId,
	except that sometimes we also look up Fields with the DataId,
	when it is a FieldElement.
OpFuncs provide a common interface for arbitrary Object functions.

============================================================================
11 Oct 2010
FieldDataHandlers vs regular DataHandlers
When do we need the field data, and when the parent object data?
	- For the purposes of OpFuncs we always need the parent object rather
		than the field object, as the functions are based off the
		parent. Could get rid of UpFuncs if this were resolved.
============================================================================
12 Oct 2010.
Grind through all the files to figure out where the Eref::data() refers to the
parent and where it refers to the field itself.

Finally it compiles through the entire basecode directory. Checkin 2209.

Finally also we have a clear direction for the FieldDataHandler interface.
Here we need to ensure that it has a similar iterator behaviour to the 
regular DataHandlers. In other words, if we go from begin to end we should
see all the field entries on all the Data that happens to be on current node.
Would also like a way to jump partway into the data sequence, say the first
field entry on Object[2][3]. This comes up in AssignVecMsg.cpp.

Another thing came up: DataHandler::addOneEntry. Same as push_back. This
is only used in one place, on the MsgManager.cpp. There are issues
with doing this on non-globals, since the node allocation could get
scrambled from the default. The MsgManager is a global, so this concern
doesn't apply, but I shouldn't introduce a one-use feature.

Got into the gory details of how we manage Elements corresponding to Msgs.
Somewhat messy, let's see if there is a way to clean up. The fundamental 
problem is that there are many Msg subclasses and each has its own fields,
so one grand array of the whole lot will not work. The current approach has
independent Manager for each subclass with an array of MsgIds, and then the
Msg.h base class manages another static vector to help the msgs look up their
index on their respective Managers.
============================================================================
13 Oct 2010.

Bodged this to work by using resize and setDataBlock. This is horribly 
inefficient, involves a reallocation and copy of the entire set of existing
messages for the addition of each one. Will need to revisit.

Got it to compile through the msg directory. Now stuck with Shell.

============================================================================
14 Oct 2010
Compiled through all files, then finds lots of undefined virtual functions
in the various DataHandler subclasses. Checkin 2211.
============================================================================
15 Oct 2010

- Check that all the Handler::Handler( Handler*) constructors alloc data
- Replace assimilateData with setDataBlock

Compilation coming along, working on eliminating 
'assimilateData' and merging it into setDataBlock

setDataBlock( const char* data, unsigned int begin, unsigned int end,
	unsigned int dimNum, unsigned int dimIndex

Use cases:
To set a single value in a 0-dim dataset: 
	setDataBlock( data, 0, 1, 0, 0)

To set a single value at index 'i' in a 1-dim dataset: 
	setDataBlock( data, i, i+1, 0, 0) or
	setDataBlock( data, 0, 1, 0, i)

To set a single value at index [i][j] in a 2-dim dataset:
	setDataBlock( data, j, j+1, 1, i) or
	setDataBlock( data, 0, 1, 0, i * numDim0 + j )

I don't like this.
Here is the logic. Suppose we have a DataHandler data[x][y][z]
where sizes are 4, 5, 6.
The z dimension varies the fastest.
A zero dim slice would be [x1][y1][z1]: in other words, it fully specifies the
	entry.
A one dim slice would be [x1][y1]: It specifies x and y fully and z is the
	slice dimension. This would have size 6.
A 2-dim slice would be [x1]: It specifies a plane in y and z located at x1. 
	This would have size 5*6 = 30.
A 3-dim slice is silly, but it would be defined by a zero size slice vector.
	This has size 4*5*6 = 120.

Now within these slices, the setDataBlock function specifies a range in
	the slice, which is assumed linearized.

To set a single value in a 0-dim dataset: 
	setDataBlock( data, 0, 1, {} ) where {} is an empty slice vector.

To set a single value at z1 in a 1-dim dataset: 
	setDataBlock( data, 0, 1, {z1} ) or
	setDataBlock( data, z1, z1+1, {} )

To set a single value at y1, z1 in a 2-dim dataset: 
	setDataBlock( data, 0, 1, {y1,z1} ) or
	setDataBlock( data, z1, z1+1, {y1} ) or
	setDataBlock( data, y1*nz+z1, y1*nz+z1+1, {} )

To set a single value at x1, y1, z1 in a 3-dim dataset: 
	setDataBlock( data, 0, 1, {x1,y1,z1} )

and so on. The preferred form is the first one in each case.

To set a vector of values from z1 to z2 in a 1-dim dataset:
	setDataBlock( data, z1, z2, {} )
	setDataBlock( data, 0, z2 - z1, { z1 } ) We disallow this because it
		makes the slice lookup ambiguous.

To set a vector of values from z1 to z2 on y1 in a 2-dim dataset:
	setDataBlock( data, z1, z2, {y1} )

To set a vector of values from z1 to z2 on x1, y1 in a 3-dim dataset:
	setDataBlock( data, z1, z2, {x1, y1} )

To set a 2-d array of values from y1,z1 to y2, z2 on a 2-dim dataset:
	Cannot do.

To set a 2-d slice of values from y1,0 to y2,nz on a 2-dim dataset:
	setDataBlock( data, y1*nz, (y2+1)*nz, {} )

To set a 2-d slice of values from y1,0 to y2,nz on a 3-dim dataset at x1:
	setDataBlock( data, y1*nz, (y2+1)*nz, {x1} )
This does not really work so cleanly. 

To set the complete 3-D block on a 3-d dataset:
	setDataBlock( data, 0, nx*ny*nz, {} )

Do these forms now permit arbitrary node decompositions to be merged into
a single vector, e.g., for globalization? Not really. We'll need the global
DataHandler to ask its localDataHandler counterpart to figure out how to put
values into the correct places on a transfer vector, which is then copied
in its entirety onto the GlobalDataHandler. Which brings us back to the
assignData function.

getSlice( numDimsInSlice, sliceStartDim, vector< unsigned int > slice
	unsigned int sliceStartDim = slice.size();
	unsigned int numDimsInSlice = dims_.size() - slice.size();

This should give a slice of data including the lower (rightmost) indices.

Trying it out in AnyDimGlobalHandler.cpp

Decided against a recursive tree of DataHandlers for successive
dimensions, at least for now.
============================================================================
16 Oct 2010
Implementing simple version of setDataBlock :
setDataBlock( const char* data, unsigned int numData,
	const vector< unsigned int >& startIndex )
setDataBlock( const char* data, unsigned int numData,
	unsigned int startIndex )
which just starts the data assignment at the specified startIndex.
Lots of files changed, so I'm checking it in even though it doesn't
yet compile. Checkin 2216.

Compiled but does not link, the usual vtable issues for DataHandlers.
Checkin 2217.

Sorted the vtable issue as hinted on the discussion lists. Made a 
DataHandler.cpp to define the only non-pure-virtual functions. Now compiles.
Checkin 2218.

Finally starting to go through unit tests and clean up the DataHandler code
along the way. Checkin 2219.

============================================================================
17 Oct 2010.
Doing unit tests. Run into issue with size and indexing in FieldDataHandler.
	Consider synapses being set up. The addition of a synapse may cause
resizing of the dimensions of the FieldDataHandler, unless we hard-code in
some limits here. This affects the way we index and identify entries.

Approach 1: Don't index stuff this way. Don't ask for common size for
field entry arrays. 
	This gives the possibility of autonomous, ragged vectors for fields.
	This gives up single-integer indexing.
	This implies that all field arrays are on a single node, as it is
	otherwise very hard to keep track.  This makes coding sense anyway, 
		as fields are meant to be coded serially as parts of an
		individual object.
Approach 2: Put in callbacks for every function that resizes a field array,
	to update the current field size. 
	Note that this gets out of hand very quickly in a multinode array.

Chose option 1, which is the older version anyway. This means that the
setDataBlock index should be changed to a DataId. Done. Compiled, still 
crashes.  Checkin 2221.

in AssignVecMsg.cpp:exec around line 50: Issue is how to map
entries in PrepackedBuffer pb to DataHandler entries, especially
FieldDataHandlers. Fixed.

Many bugs squashed, but still stuck on a test for setting a
vector of synaptic fields, in testAsync.cpp:566. Incremental checkin 2222.

Further bug squashing in FieldDataHandler iterators, now it clears the
test for assigning Synaptic delay. More bugs loom. Checkin 2223.


============================================================================
18 Oct 2010
New bug turns out to be the confusion between parent data and field data
in UpFunc. 
Current approach: UpFunc uses special access functions to get at the parentData
Option: use a special DataId::fieldId to indicate that the regular
	OpFunc should return ordinary data. Otherwise OpFunc looks up field
	data. 
Decision: Too messy. Stay with current system. This means filling out
	parentData() function in all DataHandlers.

