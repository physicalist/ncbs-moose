MOOSE redesign for messaging.
Goals:
1. Cleanup.
2. Handle multithreading and MPI from the ground up.

Why do this?
It is a huge amount of work to refactor a large existing code base. This is
needed here, and was in fact anticipated, for two reasons:
- We needed the experience of building a fairly complete, functioning system
	to know what the underlying API must do.
- The original parallel stuff was a hack.

This redesign does a lot of things differently from the earlier MOOSE messaging.
- Introduces a buffer-based data transfer mechanism, with a fill/empty
	cycle to replace the earlier function-call mechanism. The fill/empty
	cycle is needed for multithreading and also works better with multinode
	data traffic.
- All Elements are now assumed to be array Elements, so indexing is built into
	messaging from the ground up.
- There is a split between synchronous messaging (exact amount of data
	transferred on every timestep) and asynchronous messaging (variable
	amounts of data transferred). This split is at the data transfer
	level but the message specification should be nearly the same.
- Field access function specification is cleaner.
- Separation of function specification from messaging. This means that any
	message can be used as a communication line for any function call 
	between two Elements. This gives an enormous simplification to message
	design. However, it entails:
- Runtime type-checking of message data, hopefully in a very efficient way.
	As message setup is itself runtime, and arbitrary functions can sit
	on the message channels, it turns out to be very hard to
	do complete checking at compile or setup time.
- Wildcard info merged into messages.
- Three-tier message manipulation hierarchy, for better introspection and
	relating to higher-level setup calls. These are
	Msg: Lowest level, manages info between two Elements e1 and e2. 
		Deals with the index-level connectivity for the Element arrays.
	Conn: Mid level. Manages a set of Msgs that together make a messaging
		unit that takes a function call/data from source to a set of
		destination Elements and their array entries. Has introspection
		info. Is a MOOSE field.
	Map: High level. Manages a set of Conns that together handle a
		conceptual group. Equivalent to an anatomical projection from
		one set of neurons to another in the brain. Equivalent to what
		the 'createmap' function would generate. Has introspection.
		Is a MOOSE Element.

- Field access and function calls now go through the messaging interface. Not
	necessarily the fastest way to do it, but simplifies life in a 
	multinode/multithreaded system, and reduces the complexity of the
	overall interface.


------------------------------------------------------------------------------
Control flow.

1. Scheduling.
As before, we have a hierarchy of Sched, Clock, and Tick.
The Tick now becomes an ArrayField of the Clock. This means that the clock
can manipulate Ticks directly, which is much easier to understand. However,
Ticks still look like Elements to the rest of the system and have their
own messages to all scheduled objects.
The Tick object is the one that connects to and calls operations on target
Elements.

Ticks call the 'process' function on Elements. This is built into the Element
base class, and is the only function called directly. 

Ticks also call the global Qinfo::clearQ() function. This handles all
asynchronous message passing including spike events and setup calls.
The old reinit/reset/resched is gone, it will be a function call triggered 
through clearQ.

ASYNCHRONOUS MESSAGING>
Scheduling:
clearQ: Manages the event queue for the async messaging. Here are the calls:

Tick::advance(): // Calls Qinfo::clearQ on the global event queues.
Qinfo::clearQ: marches through the queue, each entry of which identifies the
	operative message and hence target Element. Also identifies function
	to call, and the arguments.
Msg::exec: specialized for each Msg subclass. Calls func on all target entries.

Still to do: Sort out flow control to interface threads such as graphics
and console. Presumably these would go through other Jobs. There is some
trickiness in how they fill in queues for the regular objects to clear.

Functioning:
This is how asynchronous messaging works.
slot s = Slot1< double >(Conn#, Init<TargetType>Cinfo()->funcInfo("funcName"));
	This slot constructor does an RTTI check on the func type, and if OK,
	loads in the funcIndex. If bad, emits warning and stores a safety
	function.
	Limitation: handling multiple types of targets, whose Functions are
	not just inherited variations. e.g., conc -> plot and xview.
	To get round it: these are simply distinct Connections.
s->send( Eref e, double arg ); // Uses s to send the data in a typesafe way.
	Converts the funcId and argument into a char buffer.
void Eref::asend( Slot s, char* arg, unsigned int size); 
	//Looks up Conn from Slot, passes it the buffer with funcId and args.
void Conn::asend( funcIndex, char* arg, unsigned int size); 
	// Goes through all Msgs with buffer. 
void Element::addToQ( FuncId funcId, MsgId msgId, const char* arg, unsigned int size ); 
	Puts MsgId, funcId and data on queue. May select Q based on thread.
	Multi-node Element needs a table to look up for MsgId, pointing to 
	off-node targets for that specific MsgId. At this time it also inserts 
	data transfer requests to those targets into the postmaster buffers.
	Looks up opFunc, which is also an opportunity to do type checking in 
	case lookup shows a problem.
OpFunc Element::getOpFunc( funcId ); // asks Cinfo for opFunc.
OpFunc cinfo::getOpFunc( funcId );
<Need to clear up arguments of functions above>
At this point all the info is sitting in the asyncQueue of the target Element.
Eventually the scheduling gets round to calling clearQ as discussed above.
<Need to put in handling of synaptic input>


SYNCHRONOUS MESSAGING
Scheduling:
process: Manages synchronous messaging, which is similar to the traditional
	GENESIS messaging in that objects transfer fixed amounts of data every
	timestep. Here are the calls:
Tick::process(); // Calls Process on targets specified by Conn.
Conn::process( const ProcInfo* p ); // Calls Process on all Msgs.
Msg::process( const ProcInfo* p ); // Calls Process on e1.
Element::process( const ProcInfo* p );// Calls Process on all Data objects.
	Partitions among threads.
virtual void Data::process( const ProcInfo* p, Eref e ); 
	Asks e to look up or sum the incoming data buffers, does computation,
	sends out other messages.  This involves dumping data directly into 
	the target sync buffers. Note that the memory locations are fixed 
	ahead of time and are distinct, so this can be done simultaneously 
	from multiple threads.

Functioning:

Here are the calls that the Elements/Data use to send out data:

<Here Slot means an index to the hard-coded location for the buffer, different
from the new Slot class above. Will need to clear up nomenclature.>
void ssend1< double >( Eref e, Slot s, double arg ); // Calls Eref::send.
Eref::send1( Slot slot, double arg); // Calls Element::send
Element::send1( Slot slot, unsigned int eIndex, double arg ); 
	Puts data into buffer: sendBuf_[ slot + i * numSendSlots_ ] = arg;
Note that we assume all sync data transfer is doubles. Probably a good
assumption, may help with data alignment.

Here are some of the calls that Elements use to examine the dumped data:
double Eref::oneBuf( Slot slot ); 
	Looks up value in sync buffer at slot:
	return *procBuf_[ procBufRange_[ slot + eIndex * numRecvSlots_ ] ];
double Eref::sumBuf( Slot slot ); 
	Sums up series of values in sync buffer at slot. Uses similar operation
	except that it iterates through the offset variable:
		offset = procBufRange_.begin() + slot + i * numRecvSlots_;
double Eref::prdBuf( Slot slot, double v ); 
	multiples v into series of values in sync buffer at slot, similar
	fast iteration through buffer using offset.

< To clarify: How to set up these buffers when defining messages >


SPORADIC FUNCTION CALLS
Case 1: Existing message between Elements, using all targets:
Slot* s = Slot1< double >(...)
s->send( Eref e, [args] )
Case 2: Message does not exist
bool set< double >( Id tgt, FieldId f, double val );
bool set< double >( Id tgt, const string& fieldname, double val );
Both of these functions create a temporary object and add a temporary message.
<More details to come>
Case 3: Message exists, but want to send to a single target:
s->sendTo( Eref me, Id tgt, [args] );

FIELD ACCESS
Always use the Shell object to carry out these functions. Put the Shell as
/root. Shell is present on all nodes.

Set: Use the same set functions as above.
bool set< double >( Id tgt, FieldId f, double val );
bool set< double >( Id tgt, const string& fieldname, double val );
Get:
bool get< double >( Id tgt, FieldId f, double& val );
bool get< double >( Id tgt, const string& fieldname, double& val );
Again, we create a temporary object, send off the request with the originating
temporary object as one of the args. On the remote node the request function
calls:
s->sendTo( Eref me, Id requester, [args] );
and then it has to do lots of cleanup afterward.
<Later: Look into doing an array 'set' and 'get' command.>

Field access is set up using ValueFinfo and its variants.
ValueFinfo: uses 
	setFunc of form  void ( T::*setFunc )( F )
	getFunc of form  F ( T::*getFunc )() const
to access fields. Wraps these functions into OpFunc1 and GetOpFunc. Generates
functions named "set_" + fieldname and "get_" + fieldname for the Finfo array.

ReadOnlyValueFinfo: Similar, only doesn't use the setFunc.

We have a small problem here when handling array fields that also need to 
refer to the parent object. For example, when dt values on clock ticks are
modified the parent clock needs to do some re-sorting.

A related problems is to set up zombies. In all these cases, things would be
easier if we always had access to the full DataId as one of the function
arguments, as in UpFunc. This is untidy for regular function calls.
For that matter we might want to use yet more information arguments, such
as the target Element and the Qinfo, as in Epfuncs.

------------------------------------------------------------------------------
Setup
FIELD DEFINITION
Similar to earlier MOOSE, have an initClassInfo function for each class,
that is meant to be called in order at static initialization. The ordering
is ensured by having the static initializers call the static initializers
of their own base classes. 

With initClassInfo, set up a static array of finfos.  Entries like:
new Finfo( async1< Reac, double, &Reac::setKf >, "Kf" )
Function handling is cleaner in at least three ways:
- It does not require static typecasting of functions, 
- It does static typecasting of the char buffer within a templated function
- It uses member functions of the Data class directly, rather than static
	functions.

Note that a single Finfo might refer to more than one OpFunc. For example,
a ValueFinfo will typically define a set func and a get func.
A LookupFinfo will define a set func with index, a get func with index, a
	get func to return size of table, and a get func to return the whole
	table.

<Still need to fully define how the Finfos control runtime message setup>

ValueFinfo: Stores functions for set and get operations on a specific field.
LookupFinfo: Stores functions for set with index, get with index, a
	get to return size of table, and a get to return the whole table.
DestFinfo: Stores a single function.
SrcFinfo: Defines origin of a message. Manages the send and sendTo operations.
	Typed. Provides index for FuncId lookup.
Note that the Finfos do NOT map one-to-one to the funcs.

The Finfo may have multiple functions in it. Each function is defined using
an Ftype that provides 3 functions: 
constructor: Loads in the function of the form &Reac::setKf.
checkSlot( const Slot* s): Does RTTI check on the slot to ensure compatibility.
op( Eref e, const char* buf): converts the data entry and buffer and calls func.

MESSAGE SETUP
bool add( Element* src, const string& srcField,
	Element* dest, const string& destField );
	Decides if fields are for simple or shared messages.
Case 1: Simple Msg:
bool addMsgToFunc( Element* src, const Finfo* finfo, Element* dest, FuncId fid )
	Makes a new Msg, with Src and Dest Elements. 
Msg::Msg( src, dest ): constructor registers the new Msg on the src and dest:
	Element::addMsg( m )
	This part needs thought, to decide what kind of Msg to make. 
Conn::add( m): To put the Msg on the Conn.
src->addConn : To put the conn on the Src.
src->addTargetFunc: To put the target Func info on the Src.

Case 2: Shared Msg.
addSharedMsg( Element* src, const Finfo* f1, Element* dest, const Finfo* f2 )
Yet to implement.


------------------------------------------------------------------------------
OpFunc lookup
Every function used by MOOSE has a unique FuncId, consistent across nodes. 
This is assigned at static initialization time when the Cinfo constructor
scans the FinfoArray.
There is a static global vector of OpFuncs in the Cinfo class that has entries
for every function so defined. The FuncId is the index into this vector.
Additionally, each Cinfo instance has a vector of OpFuncs applicable to itself,
again indexed by FuncId. Invalid FuncIds for the class have a zero pointer.

FuncId 0 is a dummy function.

------------------------------------------------------------------------------
Solvers
Original Data is replaced completely with a solved version
The Element uses a replacement Cinfo to handle this, so that the operation:
	OpFunc Element::getOpFunc( funcId ); // asks Cinfo for opFunc.
is now redone. This handles all field access and async messages.
< Need to work out what to do with sync messages >







Slot::sendTo
	- Sets up a temporary buffer for the argument and target index.
	- only it doesn't use it. Could replace the target index stuff
		done below in Conn::tsend.
Eref::tsend
	- Creates a Qinfo with the useSendTo flag
	- Element looks up a Conn based on ConnId.
Conn::tsend
	- Looks for a Msg with a target Element matching the target Id.
	- Appends the target index after the arg in the buffer.
	- Updates the Qinfo to indicate the new size
Msg::addToQ
	- the Qinfo gets the msgId of the target
Element::addToQ
Qinfo::addToQ called with the queue vector from the Element.
	- copies the Qinfo and then the arg into the queue.

...................... There it sits till Element::clearQ	

Element::clearQ
	Marches through buffer
Element::execFunc
	- Extracts Qinfo
	- Looks up func and Msg
	- If useSendTo:
		OpFunc::op Executes the function.
	- else:
		Msg::exec
			- Sorts out which target to use. 
				Unnecessary as it is on it
		Opfunc::op: Executes the function.
		

------------------------------------------------------------------------------

MULTITHREADING.
Async:
Setup:
Nothing special
Send:
Each thread has a separate section of the Queue on each target Element. So
there is never any need for individual mutexes.
clearQ: Three possible levels of separation.
	- Separate at the level of Msgs for large Msgs handling lots of
	targets. Separate chunks of the targets could be assigned to
	different threads. Needs that no other threads are modifying target.
	- Separate at the level of Elements; so that each Element is on a
	different thread. Need to set up enough Elements to balance this
	properly. Can we set up multiple Elements on the same node with the
	same Id? This would work well for cases where there are huge numbers
	of Elements.
	- Separate at the level of solvers. Works where one big solver handles
	lots of Elements. This becomes very solver-specific, and we would
	have to define rules for how the solver is allowed to do this.
Sync:
No problems because the memory locations are distinct for all data transfer.


------------------------------------------------------------------------------

MPI:
Async:
Setup: need to figure out off-node targets. See below for send.

Send:
Msg has been preconfigured to add to the regular Q, the outgoing Q, or both.
Shouldn't this be separate Msgs? But it is to the same Element, just to
a different buffer on it.

ClearQ:
On-node stuff is as usual.
Postmasters (or Elements themselves) clear out off-node Qs in a straightforward
way, since everything is now serialized.
Once these arrive on the target node, they simply get dumped into the incoming
Q of the target Element.


------------------------------------------------------------------------------
