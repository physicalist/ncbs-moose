12 Oct 2013.
This is the async13 branch. It is based on the current (buildQ) branch and
is designed to test out asynchronous messaging in MOOSE. This will drop
all the multithreading and queueing code framework with a view to simplifying
and speeding up calculations. It is also meant to implement parallelization
from the ground up, using either MPI or Charm++ messaging.
It is meant to keep the old buildQ API as much as possible.

Compiled the reduced code base, got most of the unit tests to work. 
Checkin 4792.

Fixed a dangling 'else' statement in the main.cpp that was preventing the
unit tests from closing. Now it is OK. Checkin 4793.

Next: eliminate threading. 
	much eliminating later, it compiles but doesn't clear unit tests.

Stuck in Shell::doReinit in the Qinfo::buildOn/buildOff around line 686
Eliminated all buildOn/buildOff calls. Now it croaks much sooner in the
unit tests.

===========================================================================
13 Oct 2013.
Further cleanup. Moved the process loop into Clock. Gets some way through
unit tests but fails in testShellAddMsg(), seems that between Clock::doReinit
and doStart the message info isn't going around.
Checkin 4794.

===========================================================================
14 Oct 2013

Clever trick to break a python script at a specified place to use gdb:

import os
import signal

PID = os.getpid()

def do_nothing(*args):
    pass

def foo():
    print "Initializing..."
    a=10
    os.kill(PID, signal.SIGUSR1)
    print "Variable value is %d" % (a)
    print "All done!"

signal.signal(signal.SIGUSR1, do_nothing)

foo()

From Stack overflow by Michael Hoffman in 2011.
===========================================================================
16 Oct 2013.
Cleaning up threading references from Clock. Still doesn't get through the
unit tests, some issue with doReinit crops up in testBuiltins.
Some progress.
Current status is that the requestData()->send call is invoked, but data doesn't
get back in time for the reinit to complete. Need to investigate how the 
send call is handled.

===========================================================================
17 Oct 2013
Some more cleanup, it finally clears all unit tests. Checkin 4796.
Valgrind is totally happy too.

Next items:
Make benchmark for IntFire network.
Message rebuild/eliminate Qinfo.
Scheduling rebuild
Id and data handler rebuild
Synapse rebuild
Conv elimination
Prepacked buffer elimination
Parallel rebuild

===========================================================================
20 Oct 2013

Design of key sections. 

Data and field access:
Elements manage data as well as node info. All data are arrays. 
Msgs managed too. No FieldElements. No DataHandlers. Cinfo deals with Dinfo.
class Element {
	private:
		string name_;
		Id id_;
		const Cinfo* cinfo_;
		char* data_;
		unsigned int numData_;
		vector< indexRange > indexRange_; // Looks up indices by node.
		vector< MsgId > m_;
		vector< MsgTraverse > mt_;
};

Synapses are an array of fields on a Receptor. Not FieldElements.

Ids specify element and index of object entry.

Msgs specify Id and field index to fully specify Obj and field. Msg ops call 
back to Element, possibly Object.  For fast traversal, Msgs are digested into 
{funcPtr, Element ptr} sets with { Obj ptr, field} arrays. Each Obj index looks
up one such set of arrays, which can be shared for different funcs.

Scheduling: Tick 0 is base clock with a specified dt. All others are integral
multiples of it. Order within a given timestep is by tick#. No fieldElements,
just 10 MsgSrcs emanating from Clock, and internal array for the Multiples.

Conv: Used only to serialize off-node msgs, and to convert to and from strings,
and to provide rttiType to MOOSE.

Eref {
	Element* e_;
	DataId i_;	// An unsigned int for now
	// An unsigned int, used to pass field index but does not affect data 
	// lookup. Instead field index is extracted by the target EpFunc.
	FieldIndex f_;	
	char* data();
};

Prepacked buffer goes away

Parallel calls: At time of Msg digest, converted and condensed to single call
	per msg per node for outgoing. Likewise incoming msgs expanded out
	as if from original src Element, to all tgts on recipent node.
	Outgoing bufsend right away.
	Incoming polled at end of Tick by src element proxy.
	Transfer of objects by serialization + rebuild of all element node info
	+ rebuild of digested msgs.

I think that the Element and Msg stuff are pretty intertwined. But lets start
with the Msg and see how far we can go.

............................................................................
Alternative to this set is to have Synapses and other FieldElements
as independent Elements, using pointers cleverly to refer back to parent.
Possible disadvantage is some juggling to refer back to parent quickly.
Advantage is to eliminate FieldIndex.
Disadvantage is to proliferate Elements, many proxy Elements on all nodes.

For now trying fieldIndex for synapses.
===========================================================================
20 Oct 2013.
Begun massive refactoring of Msgs. To start with, redid the OpFuncs, 
EpFuncs and SetGet headers. Threw out UpFuncs and FieldElement stuff.
Now the headers seem consistent, but SrcFinfo::send calls need to be 
populated, which can only happen after I redo Element.

Compilation begun, far more to do. Checkin 4801.

===========================================================================
21 Oct 2013.
Marching through refactoring. Messaging there in skeleton but I'm not
satisfied. I would like a generic way to handle the following cases:
Traverse through single msgs, each with a unique Eref.
	This is what I've implemented for now.
Traverse through synapses, same Element but many indices and fieldIndices
Traverse through obj arrays: same Element, many indices, all fieldIndices = 0.
Traverse through 'all' obj: same element, auto through indices.

Won't try to do this through all field indices. Unclear where that would be
relevant.

I've eliminated the FieldElements and DataHandlers. This caused some
issues with the Cinfo element instances, which used to have FieldElements
for the SrcFinfos, destFinfos and so on.
I've hacked around it by explicitly creating Finfo elements as children of the
Cinfo elements. It compiles but looks precarious. Need to fix up Finfo
field access.
Checkin 4802.

===========================================================================
22 Oct 2013
Tediously slogging through compiling testAsync.cpp. Done, but the many
tests with IntFire and synapses are not so clean and will need some thought
on how better to set up this interface.
===========================================================================
25 Oct 2013

Back to Element lookup options. 

With just an integer data entry lookup:
- Synapses would need to be FieldElements, one per receptor
- Copies of neurons either to a new element tree, or arrayed in dataId.
- Need to be able to create child Elements on every data entry.
- Msg cleaner
- 

With data entry plus field entry lookup:
- Synapses could be to be FieldElements, one per receptor, or tables.
- Copies of neurons either to a new element tree, or arrayed in dataId.
- Do not need to be able to create child Elements on every data entry, but handy
- Msg has extra field.

===========================================================================
28 Oct

Some memory estimates.

Integer data lookup:
- Elements
	Suppose I have 1e6 neurons and 1e10 synapses.
	Assume I have 100 synapses per receptor, this is still about 1e8 synapse
	Elements. Somewhat strenuous if I attempt to put this all on each node.
- Msgs
	In sparse Msg form, it is essentially 2 longs per synapse, but only
	for synapses on local node. The higher level Msg would be negligibly 
	small.
- Synapse data
	2 doubles per synapse on the node. A modest circular buffer for the 
	queue.

Field entry lookup.
- Elements
	Here I would have 1 'synapse' per receptor on the parent neuron. A few
	hundred at best. Rest is just a matter of looking up indices.
	I could put the relevant Elements on each node easily.
- Msgs
	Same as above. 3 longs per synapse makes 
- Synapse data
	2 doubles per synapse on the node. A modest circular buffer for the 
	queue.


So it is pretty clear that the integer data lookup would require a different
way to set up messaging, one where the skeleton simulation cannot fit on one
node.

Note that I have to do Data entry mapping to nodes in either case.
Let's see what that looks like.

===========================================================================
1 Nov 2013
Data entry mapping.
Goals: 
	From any node, find quickly which node holds data.
		This doesn't have to be real-time: not used for sending msgs,
		but used for setup.
	Keep it compact. Can't have a full index of entire data list.

Design: 
	Element Ids: On all nodes
	Data Ids: Decomposed between nodes
	Field indices: Only on one node, that of the parent Data Id.

Use cases:
	- Neuronal model decomposed. Small # of neurons on each node, many 
	nodes, some degree of spatial organization helps, load balancing may 
	occur.  Little data transfer due to spiking.
	- 3D space decomposed in rdesigneur. Unpleasant large matrix, simple
	reacs but lots of diffusion. Heavy data transfer, each timestep for
	matrix solution. Very spatially organized, unlikely to do funny 
	balancing.
	- rdesigneur cell models: Each model on its own, doesn't need to talk
	to others except via spiking.
	- Spiking neuron model decomposed. Huge # of neurons and synapses on
	each node, many nodes, some spatial orgn. Load balancing may not be
	so useful. Lots of data transfer.
	- Large cluster vs small cluster/cores on a machine.

// Returns blocks
getNode( unsigned int numNodes, unsigned int numData, unsigned int dataId ) {
	return dataId * numNodes / numData;
}
// Returns nicely arrayed.
getNode( unsigned int numNodes, unsigned int numData, unsigned int dataId ) {
	return dataId % numNodes;
}
Now, what if load balancing has happened? Add check for that.
getNode( unsigned int numNodes, unsigned int numData, unsigned int dataId ) {
	if ( auto i = dataMap.find( dataId ) ) {
		return i->second;
	}
	return oldNodeMethod( numNodes, numData, dataId );
}
// May also have a situation where the data is on every node: a global.
getNode()
{
	return GlobalNode
}

So we just need a virtual class or function for this to sit on every Element.
Quite easy. Eventually will want to have a way to garbage collect if lots of
object movement has happened. Could even have a policy switch field, ugly though
it may seem.

Other thing is to put back the FieldElements, but have them point to the parent
data_.

Checkin 4818.
For Msgs, since the MsgId is no longer needed for fast msg sending, I'll just
have the MsgId be a regular ObjId pointing to the message. 

Should also add callbacks to Elements for add and delete messages.  These
are done as extra args in the Cinfo constructor, typically zero. 
The callback functions pass in Element and Msg target info.

Created FieldElements. Nothing there yet for FieldElementFinfos. Not yet
compiled through SparseMsg.cpp.

===========================================================================
2 Nov 2013
Grinding through compile. ElementValueFinfo and similar things need sorting.

===========================================================================
3 Nov 2013
Grinding through compile. Cleared Shell.cpp, but many things to do with path
are postponed.
I have completely redone the scheduling. Now the Clock does it all, and
much more simply and possibly faster. Eliminated Tick related objects.
Now compiles through the scheduling directory, but tests only as placeholders.
Now compiles through the builtins directory. 
Now a bit stuck with the Synapse and FieldElement, which I have sidestepped
so far. 

Desiderata: 
- Very fast Synapse handling. This requires that both the parent object
	and the synapse entry are rapidly accessed.
	I had thought I would do this by returning the base data entry, with
	the index from eref and lookup func for the Field from EpFunc.
	Needs an extra lookup compared to the regular Msg because we need to
		engage the parent class as well as the specific Synapse.
	addSpike( double time ) {
		ringBuffer->addEvent( time + delay_, weight_ );
	}
	This requires the Synapse to a) have a pointer to the ring buffer,
		and b) the parent class have a ring buffer in it to use.

	A more complicated one might also look at association with postsyn Vm,
	but that would be computed at time of arrival, handled by the Receptor.
	If we do this by passing pointers in from the UpFunc we could refer
	to the Receptor level:
	Receptor::addSpike( const Eref& e, double time ) {
		const Synapse& syn = synapses_[e.fieldIndex()];
		ringBuffer.addEvent( time + syn.delay(), syn.weight() );
	}
	This requires that there be a common base class for the Receptors that
	all Synapses could use.
	Or we could refer to the Synapse level:
	Synapse::addSpike( const Eref& e, double time ) {
		auto rec = reinterpret_cast< Receptor* >( e.plainData() )
		rec->ringBuffer.addEvent( time + delay_, weight_ );
	}
	This too requires a common base class for the Receptors.

- Easy access to the Field, preferably deal directly with its pointer.
	This is certainly doable with the lookup, and even more so with
	the precomputed Msg ptrs, but then how to handle parent?
	In cases where the parent isn't needed, we could design an EpFunc
	to directly call the Field.
	The first option above would do this directly.

The first option is nicest, also fits well with the idea of single precomputed
target pointers for messages. Costs an extra pointer in each Synapse. Minor.
This implements very smoothly, but there is another consideration. 
The synapse class needs to provide a callback so that when additional spike 
messages come in, the synapse buffer is expanded to match.
- Callback detects message addition and removal
- Callback would modify # of synapses.
- Callback may assign a fieldIndex to the message itself.

This is fine, but in the Synapse we have a problem because the callback
needs to talk to the parent SynHandler if it wants to resize the Synapse array.
Easily handled by looking up the parent ObjId of the Synapse.

In the process of setting up the SynHandler. Next:
+Fix up Msgs to do the precompiled fast lookup
+Fix up FieldElement stuff to access synapses from Eref
Fix up FieldElement and Element stuff to deal with callbacks
Fix up Msgs to use ObjIds to identify
+Fix up Msgs to handle the specification of fieldIndex.
Work out how to talk to nodes on parallel machines. Socket?
Work out how to do SetGet on parallel machines
Work out parallel messaging.

===========================================================================
4 Nov 2013.
Fixing up testAsync.cpp:testSetGetSynapse, now that I've restructured
the FieldElement stuff.

May need to go back and redefine ObjId so it specifies the fieldIndex too, 
just like the Eref.

Getting close to compilation completing. Had to go back to TableBase.

Stuck a bit in Table.cpp, figure out what to do with recvDataBuf.
===========================================================================
5 Nov 2013
Finally cleared first pass of compilation, link still fails with lots of 
unresolved functions. Checkin 4824.
Finally compiles. Doesn't run.

Many nasty bugs later, I have now to deal with the MsgDigest for fast 
messaging. In testAsync::testSendMsg.
===========================================================================
6 Nov 2013
The 'send' command has now taken shape with the MsgDigest. It is far from
as efficient as envisioned:

1. SrcFinfo->getBindIndex()
2. Compute offset of specific DataId * specific src func.
3. Look up MsgDigest vector from Eref.
4. Iterate through MsgDigest
5. Lookup function
6. Iterate through targets
7. Check if they are ALLDATA
	8a: Make temp eref
	8b: lookup numData
	8c: Create reference k
	8d: Iterate over ALLDATA
	8e: call function
else
	8a. Call function.

Here are some cases:
				# ops
Single object target:		8 + F
N unique object targets		5 + 3N + NF
Single Alldata target 
	with X entries		10+2X + XF
N Alldata target 
	with X entries each	5 + 5N + 2X + NXF

F is cost of function.

Immediate goal: Have the Msg do its own digest... No, need to store
this in Element because there would be a number of targets in each digest.

Now setting up Msg::sources and Msg::targets() to build digest.
===========================================================================
7 Nov.
Seems like some messages are beginning to go...
Now about 10 unit tests in. 

Stuck in SparseMsg.cpp:211. At this point there are no fields allocated,
so we're not finding any targets.
===========================================================================
8 Nov.
Steady progress. Some things to consider:
- Should we retain ext fields? Python deals with this kind of use mostly.
	If not we could simplify the function for checkSetGet.
- Need to redo the Msg as object tests in the unit tests

Now clears testAsync.
Fails in testMsg.cpp, but both those tests require some major pending updates:
to the ObjIds handling Msg, and for automatic updating of MsgDigest.

So commented them out.

Implemented the ObjId based Element tree structure. This lets individual
DataEntries have their own child Elements.
Yet to compile and the check is in testShell.cpp:121

===========================================================================
9 Nov.
A design and semantics issue with the capability to have each ObjId be parent
to a sub-tree: What happens when we replicate the entire tree?

Suppose, on a/b[100], we create a unique element child c[50] on 
	a/b[23]

Now we use createmap or equivalent to make 10 copies of a. This does:
	a[10]/b[1000]
What about c in this? My reading is that we should get

	a[1..10]/b[23,123,223,323,423,523,623,723,823,923]/c

where in each case c is a separate Element. 
The reason is that we do not permit a given Element to be parented by many
different objects. One could in principle do even this through sparse msgs,
but I think the semantics get to be too untidy.
Other aspect of the semantics is that the parent of any entry in b would be 
a[0]. So there is a special status given to those Elements parented off zero.

Yet another point: if I have a FieldElement, its parent dataIndex should be
identical to its own dataIndex. Implemented.

Now the Neutral::path is working well. Also went through and cleaned out
all uses of Id::operator()() since they were confusing.

Further progress, now clears testScheduling.

A bit stuck on destroying Elements. Currently cinfo_ is set to zero to tell
the Elements that they shoult NOT remove the msgs. cinfo_ isn't a good way
to do this because it is needed later. So we could set the id_ to zero.
But I may need to bifurcate the Element class so that the base class of 
FieldElement doesn't try to destroy the data.

Tried it. It was rather elegant: all the messaging stuff went into the
ElementBase, and the data stuff into Element - just like the FieldElement
was just doing data. But all the rest of the infrastructure refers to
Element and assumes that FieldElement is derived from it.

One way out might be to retain the split, but have the base class be Element,
and have a DataElement as counterpart to FieldElement.

Create functions would refer though to the DataElement and the FieldElement.
Lots of that.
I suspect that the DataElement will need considerable expansion with the
new multinode code. May as well anticipate.

So I redid this with the base class as Element, and DataElement that which
handles regular object arrays as data. FieldElements stay as is, now
quite symmetrical with DataElements.
Compiles and gets a little further with unit tests.

===========================================================================
10 Nov 2013.
Need to sort out semantics of copying with the provided multiplier.

consider: /a[10]/b[20]
copy /a /z
The bare copy should traverse the tree and produce an identical tree:
	/z/a[10]/b[20]

copy /a /z 5
Should I reduce a to 5, or make 5 copies of the whole thing? Or just 
forbid the multiple copy operation and reserve it for createmap type calls?

Clearly any deep copy should preserve the child indexing. So I cannot
do any reductions, will have to copy integral multiples of the whole tree.
Should I only allow unit copies? No, keep the feature, and use it for
more complicated createmap type calls.

Another case. I may need to do a specific object copy:

Copy a[3] /z
Here the rest of the tree is ignored unless the there is a tree rooted on a[3].
This precludes doing a copy of a[0] unless I implement a special parental
message that goes from the element as a whole. But even that would return an
index if queried.
Don't worry about this for now.

Did substantial cleanup on path handling.
Now completes testShell.

Trying to get clock to run a simulation in testBuiltins. The reinit::send
command fails because it doesn't find anything in the MsgDigest.

Ran into trouble with requestData calls. These were handled specially, 
and the new framework doesn't like them. There is the opportunity to do them
really simply, for example, pass the function a reference to be filled in
by the target 'get' function. Backward compatibility means that I can't now
change all the A get() functions to get( A& ), that would have been the 
simplest.
I've implemented a skeleton of a very simple GetOpFunc which just takes
a pointer as an argument, and dumps the value into the pointer. Problem
comes if I want to call such a function across nodes. Will everything
grind to a halt while I wait for the request to go and the response to come?
What if there is a deadlock: two nodes waiting on each others' returns?
Will have to set up the off-node 'get' call as a loop polling for the
answer, and updating any calls that the off-nodes make.

In the meantime, carried on with debugging, and now it clears unit tests but
with a segv at the end. Checkin 4837.

Much work with valgrind later, now it is completely clean. No segv either.
Checkin 4838.

List from 3 Nov, updated here:
*Eliminate multithreading
*Change to integral ticks for scheduling.
*Fix up Msgs to do the precompiled fast lookup
*Fix up FieldElement stuff to access synapses from Eref
Fix up FieldElement and Element stuff to deal with callbacks
*Fix up Msgs to use ObjIds to identify
*Elements should know when to call the MsgDigest.
*Fix up Msgs to handle the specification of fieldIndex.
Work out how to talk to nodes on parallel machines. Socket?
Work out how to do SetGet on parallel machines
Work out parallel messaging.


Starting on the ObjId identification of Msgs.
Did a bit in OneToOneMsg.cpp and OneToAll.h

===========================================================================
11 Nov 2013.
Callbacks
	- Message changes: add, drop, reconfigure: need message id.
	- Scheduling/clockdt changes
	- Changes in node decomposition (?)


Eliminated MsgIds, replaced with ObjIds. The change makes it much easier to
handle Msgs. Compiled but doesn't clear any tests yet. Checkin 4839.

Discrepancy in ptr looked up from mid to created ptr.

Fixed. Then much messing around trying to get the system to quit cleanly.
Eventually did by brute force, rather than the recursive removal of objects,
since the removal of the objects handling messages caused problems with
subsequent removals.
Something like an anti-bootstrapping.
Now terminates cleanly and valgrind is happy. Checkin 4840.
Some tests to do with this before going on to automatically deciding when to
do digestMessages.
Restored a few unit tests for the various msg classes. Not too bad.

For: Elements should know when to call the MsgDigest.
I'll have it so that the Msgs call each of their target Elements to set
a 'dirty' flag when the Msgs change. This flag is checked at the time of
reinit or process, but no. Should not be handled by the Object.
Options:
	- put in a conditional in every Send. In fact the act of 
	getting the MsgDigest vector could do this. e.msgDigest.

Starting with an 'isDirty' flag for messages that change. Need to fill in.
===========================================================================
12 Nov 2013.
Implemented the isDirty flag, but called it isRewired. 
Reactivated some of the unit tests on message field assignment. All cleared.

Considering the callbacks.
- Bypass altogether. Let the user track changes and ensure that the right
	number of synapses are defined. Possibly give a helper function.
	Possibly help with offsets for each incoming SparseMsg so one does
	not lose all assigned Weights/delays of one when the other changes.
- Give FieldElements an automatic scan of incoming msgs to check for field
	size. Activate this whenever the fields are accessed.
- Put in a function in SparseMsg::randomConnect and its
	ilk, to assign synapse::numField.
	Problem is that there may be other messages. Could cobble something
	together for the SparseMsg objects, but there are other possible Msg
	types which should not need to know this stuff.
- There are prototype callback functions in Synapse: 
	addMsgCallback and dropMsgCallback. But the message involved would
	have to know what to do with the info about existing synapses.

Summary: Need to provide offsets for each Msg. Rest is up to user. Need use
	cases.

General things to fix:
	- Use new 'send' command for all the Srcfinfos.

For now, bypass altogether. Move on to multinode.
- Fill in send buffers:
	- Message digest consolidation
	- Arg conversion.
- Figure out best design for internode comms. 
- simple block mapping of DataId to rawIndex.
- Global objects and synchronization
- Communication between nodes for setup operations
- Communication between nodes for Set/Get operations
- Clock ticks between nodes.
- Messaging between nodes.

Working on standalone prototype for the MPI. Stuck.
Some useful tricks for debugging:

mpicxx -g proc2.cpp
mpirun -np 2 xterm -e gdb a.out

Possibly the problem is that the local node has not filled in any entry
in recvReq. Yes. Now compiles, but the run goes into an infinite loop.
===========================================================================
13 Nov 2013.
Incremental debugging, now looks close. proc2.cpp.
Works but it isn't clear to me why there is the present limit on the number
of nodes it can handle. I would have thought N^2 = numCalls would be the limit.
But it works for powers of 2 up to 128, which is as far as my laptop will
manage. Possibly I shouldn't have so many Irecvs dangling at a time.

Next to set up an AllToAll type transfer to see if that goes faster. Certainly
looks simpler.
Working in proc3.cpp

Then put in handling for sporadic big mesages.

Then do benchmarking on ghevar and on cluster.

===========================================================================
14 Nov 2013. proc3 works. The
time tests are all over the place but proc3 does seem a little slower than
the proc2. Perhaps I could do another version with non-blocking calls.

Looking at how regular msgs put stuff in send bufs for off-node delivery.
- Element::putFuncsInOrder: Check if tgt is off-node, put in a dummy
	OpFunc that stores MsgBindIndex. Tgt will come from eref in 
	SrcFinfo::send.
- Derivative of OpFuncBase that also stores msgBindIndex. Need virtual 
	constructor
	function from OpFuncBase so that the PutFuncsInOrder can synthesize it.
	- Derivative needs manage the convs and buffer
- Converter that takes arguments and bungs into provided double buffer.
	Conv.h needs to be stripped down and redone.
	static const T& buf2val( const double* buf, unsigned int& size )
	static const unsigned int val2buf( const T& val, double* buf );
- Collapse all same-node targets for the same func, into just one entry
	for any given node.  In Element::putTargetsInDigest
- Someone to manage the buffers. Even if they are global. Could have a
	special class, postmaster?
	- Presumably same object (postmaster) to send stuff to other 
	nodes on tick.
	- Same object to parse arrived buffers and send out.
	- Quite a bit more here.
- Need to set up the node map on Elements too.
- Decide on buf structure. Say 1 double for size + 1 double for originating
	ObjId, + double for bindIndex. Assorted doubles for arguments.
	Actually make a header structure and use that, however many doubles
	it takes.

Started with Conv.h



===========================================================================
16 Nov 2013.
Filling in blanks for multinode stuff.
OpFuncBase now has something
DataElement now knows more about node handling.
Need to put buffers somewhere standard: postmaster?
postmaster needs to be elaborated.

- Global objects need to have assignments and sends go to all nodes.
	Actually just define a special DataElement subclass.

PostMaster::clearPending:
	Goes through recvBuf. 
		calls SrcFinfo::sendBuffer with buffer
			SrcFinfo converts to arguments
			Sends to all targets on specified bindIndex
			(which is a function of this SrcFinfo)
			Originating Eref is fully specified so
			there is no ambiguity about which subset of
			msgs srcs were involved.
			The MsgDigest must digest off-node stuff into
			same array.


 Here is an outline of how messages go between nodes, from PostMaster.h
			.................
This is how internode message passing works. I'll describe this at two
levels: the movement of data, and then the setup.

Level 1: Movement of data.
1. Object Sender sends out an argument A in a regular message.
2. The SrcFinfo scans through targets in the MsgDigest via func,tgt pairs
3. The off-node (Func,tgt-Eref) pair holds a HopFunc and a special
     Eref. The main part of the Eref is the originating object. The
     FieldIndex of the Eref is the target node.
4. The HopFunc fills in the Send buffer of the postmaster. It uses
     the target node info, stuffs in the ObjId of the originating object,
     and converts and stuffs each of the arguments using Conv< A >.
     Thus the Send buffer contents are a header with TgtInfo, and then the
     actual arguments.
5. This happens for all outgoing messages this cycle.
6. Postmaster sends out buffers in process. It then waits for incoming
     stuff in the recvBufs.
7. The scene now shifts to the PostMaster on the remote node. In its
     'process', the clearPending call is executed. It looks at the recvBuf
     and extracts the tgtInfo. This tells it what the originating object
     was, and what SrcFinfo to use for the outgoing message. !!!! fixme
             (Currently I use BindIndex which ought to be right but is done
             very indirectly. I need to check.)
8. we call the SrcFinfo::sendBuffer call from the originating object.
 The sendBuffer call converts the arguments
 back from the bufer to their native form and despatches using the
 regular messaging. Note that the MsgDigest will have done the right
 thing here to set up the regular messaging even for off-node 
 DataIndices on this element.
9. Messages reach their targets.

Level 2. Setup.
1. Objects and messages set up the regular way. Objects may have 
subsets of their Data arrays placed on different nodes. Messages
are globals, their contents are replicated on every node.
2. When a SrcFinfo::send call is about to execute for the first time, 
it is digested: Element::digestMessages. During this step each of the
messages is expanded by putTargetsInDigeest into target Erefs.
3. The target Erefs are inspected in filterOffNodeTargets. Off-node
targets are removed, and we record each pair of Source DataId and node.
4. We now call putOffNodeTargetsInDigest. This generates the
HopFunc by calling OpFunc::makeHopFunc with the fid of the SrcFinfo.
5. putOffNodeTargetsInDigest then examines the Source/Node pairs and 
creates HopFunc/Eref pairs which are stuffed into the msgDigest.
Note that these Erefs are the hacked version where the Eref specifies
the originating object, plus using its FieldIndex to specify the target
node.

Possible optimization here would be to have a sendToAll buffer
that was filled when the digestMessages detected that a majority of
target nodes received a given message. A setup time complication, not
a runtime problem.
			.................


I've now put together most of the code. Compilation in progress.
Checkin 4851.
===========================================================================
18 Nov 2013. Trying to compile HopFunc.cpp.
19 Nov 2013. Compiled. Trying to debug in testConvVectorOfFcetors.
20 Nov 2013. going through unit tests. Convs were messy but now OK.
	Need to handle copy of Msgs to n targets.

Other than that pending matter, the unit tests clear. Checkin 4856.

===========================================================================
22 Nov: See if the mpi_scatter will work with larger recv buffers than send
sizes. The documentation isn't hugely clear on this.
Tried it out in proc4.cpp, which is based on the scatter code in proc3.cpp. 
Seems to work at first, but then fails once up to 16 nodes and higher.
Let me check the mpirecv can handle this asymmetry. In proc5.cpp, which is
based on proc2.cpp. Yes, this works. It is documented too.

From the 12 Nov list:
* Fill in send buffers:
	- Message digest consolidation
	- Arg conversion.
+ Figure out best design for internode comms. 
* simple block mapping of DataId to rawIndex.
- Global objects and synchronization
- Communication between nodes for setup operations
- Communication between nodes for Set/Get operations
- Clock ticks between nodes.
+ Messaging between nodes.
	- MPI implementation

Next is to apply the MPI implementation, use the MPI_Irecv approach as the
MPI_Scatter won't handle variable-length messages.

Added most of code for PostMaster MPI stuff, yet to compile.


===========================================================================
22 Nov: Compiles.  Checkin 4857.
Beginning unit tests. Goes OK until it has to issue a Set call to an off-node
data entry in testShell. Will need to fix SetGet. Also looks like unit
tests are not going across nodes.

Looking at SetGet. While the use of a HopFunc takes us most of the way,
there are 5 issues remaining:
- I may set a field. The hack in the send command knows that the src eref 
	is not a field, so it uses the fieldIndex as the node identifier.
	As I can just query the eref about the target node, this is can be
	handled provided the system knows it is a SetGet operation.
* PostMasteraddToSendBuf doesn't know that this is a SetGet rather than msg.
	Subclasses of the HopFunc? Switch statements off a flag passed in by
	bindIndex?
	Created a HopIndex class that lets met track both.
+ How to set up the bindIndex to refer to the correct SetGet operation on the
	target node?
	Make a static global vector of all OpFuncs, refer to this.
	Need to be sure that the dynamically assigned OpFuncs come strictly 
	after the statically assigned ones.
+ On the target node, SrcFinfo::sendBuffer was used to convert back. Here it
	won't work as the sendBuffer refers to the regular send call.
	Need to refer to something quite different. This could be handled
	if the TAG on the mpi msg tells the target node to do something else.
	I would need to get the OpFuncBase or Hopfunc to convert buffer to 
	arguments and call the OpFunc->op().
- How to handle return values for get funcs?

Stages in off-node 'set':
1. call SetGet::set. Checks if isDataHere(). If so, easy. If not, make a 
	HopFunc with the flag to say it is a Set operation. Call hop->op.
2. hop->op does the generic addToBuf.
3. The postmaster sees the special flag to tell it that it is a Set operation
	in p->addToSendBuf.
4. PostMaster sends the data off immediately [and polls for return]. 
	Appropriate SETTAG.
5. Tgt PostMaster is polling for any SETTAG from any source. Gets it.
6. Uses TgtInfo to look up appropriate OpFunc from BindIndex, and Eref.
7. Calls OpFunc1Base<A>::op->opBuffer( Eref, buf). This does the 
	conversion from the buffer and calls the virtual OpFunc with the 
	converted argument.

This would be it for the Set unless I wanted to make it a blocking Set.

Stages in off-node 'get':
Same as 1 to 6 but the originating PostMaster has to do polling for the return.
7. Calls GetOpFunc1Base< A >::op->opBuffer. This creates a temporary A field
	for the return value and gets it.
8. Converts into a buffer explicitly.
9. Sends back using yet another tag.
10. The return poll harvests the value. The GetOpFuncBase knows what type to 
	expect back and use in the buffer.

This is going to be tough. First things first with the set. Perhaps should
separate out the messaging in the unit tests, too.

For the Shell operations, I think it should be yet another SHELLTAG or maybe
even separate tags for the various shell ops. There are only about a dozen
of them. They all seem to be forward only ops, no returns.
Could we use a variant of Set?

I also need to figure out how to do setVec and getVec.
===========================================================================

Week 1: Threading out
Week 2: Messaging refactoring
Week 3: Element and field indexing refactoring. Data Handler out.
Week 4: Scheduling refactoring, recompile and unit tests. 
	Copy refactoring.
Week 4.5: MsgIds replaced with ObjIds. Automatic message compilation.
Week 5: Standalone tests on mpi framework
Week 6: Multinode messaging design, implementation, compilation, no tests.
Week 7: Multinode Set/Get. Compile and test.

Major changes in MOOSE:
1. Refactored messaging and eliminated queueing. Messages now use the same 
high-level structure, but at runtime they do not dump 'send' calls and
arguments to a queue. Instead the messages are compiled into function-object
pairs and use these to directly call the target objects with the arguments.
This permits the use of pointers as arguments in messages.
This means that message calls are not synchronized by the queue - they occur
as called.
This means that the stack can be traversed to find what called what.
This is likely to be much faster, haven't yet benchmarked.
2 Eliminated threading.  As a necessary precondition for the above, threading
was eliminated. In addition to obviating the need for queues, this greatly 
simplified the process loop and scheculing.
3. Used integral scheduling. Earlier the clock emitted ticks which could have
arbitrary doubles for the timestep. Now there is a base clock dt, and all
tick events are integral multiples of this base. There is no ambiguity about
ordering nor any issue with rounding. Much simpler too.
4. Indexing of data is simplified. There are three integers to look up data:
	Id -> Identifies the Element which is a wrapper for data and messages.
	DataId or DataIndex -> Identifies a data entry in the Element. A simple
		one-dimensional array, can have a single entry.
	FieldIndex -> Identifies a FieldObject on a data entry. The Field Object
		must be associated with a FieldElement. Most objects do not
		have any field objects. Cases which do include synapses, which
		are subsidiary objects sitting on each receptor.
	A corollary of this is that there is no more multidimensional indexing
	of data entries. No more field masks.
5. MsgIds are no more. Instead Msgs are identified by ObjIds just like any other
	object.

6. The Set commands now directly call functions on the target object to assign
	values. But see parallel specialization.
7. The Get commands now directly call functions on the target object to obtain.
	values. But see parallel specialization.

8. Any DataEntry can now be the parent of an Element. Earlier this was a bit
	ambiguous and many parts of the code required instead that only Elements
	could be parents.
9. Synapses now use a ring buffer instead of a sorted queue to handle spike 
	arrival times.

In addition to these changes which are evident in serial code, there are many
changes to enable parallelization.
10. There are multiple Element classes, all derived from Element. Each has its
own rules for node decomposition. This is automatic: upon creation the 
appropriate data entries are put on the appropriate nodes. Note that every
node has the full Element tree, but only subsets of the DataEntries are 
put on different nodes.
11. Parallel messaging is implemented by using special flags during the 
'compilation' of messages. This intercepts off-node messages and puts the
message request and arguments into a buffer such that any given node only
gets a single request for the message, even if there are a hundred targets for
the message on the remote node. The buffer for each node is sent off at the
end of each clock tick in a non-blocking manner. Arriving buffers are digested
and the messages sent out to all targets on the local node.
12. Parallel field assignment and once-off function calls (set) is implemented
by detecting if target is off-node, and if so, putting into a different buffer.
This is dispatched in a non-blocking manner, but only after clearing all 
pending incoming field assignment calls.
13. Shell simulation control operations are implemented through 'set' calls.
14. Get calls work in a similar manner to 'set', except that data also gets
	sent back.

===========================================================================

25 Nov 2013: Starting to compile with the Set calls included. But this is a
bad way to develop. Should instead do multinode messaging separately,
and then do the MPI-based real test.
Compiled, clears single-node unit tests. Checkin 4858.
Now to separate out the MPI-based tests.


Setting up cross-node Shell::doCreate by going through the newly
implemented SetGet functions.
Need to unit check the argument passing and fitting into buffers,
by HopFunc.

Compiled, too sleepy to tackle furter.
===========================================================================
26 Nov 2013.
Finally tracked down the problem with off-node Set calls. The
order of evaluation of the Conv functions in OpFunc6Base::opBuffer()
is backwards. The last argument, arg6, gets evaluated first, and so on.
Presumably backwards in all cases. Need to redo conversion syntax.
Turns out C++ explicitly does NOT specify an evaluation order.

So did the less elegant but simple code to extract arguments. Now seems to
work for creating objects, but the unit tests don't seem to go all the way 
through.
===========================================================================
27 Nov 2013.
MPI is being difficult about reporting where it croaks. By stepping through
it looks like it is testShell.cpp:1718, testTreeTraversal()
From what I can see it looks like an innocuous call to Shell::doCreate 
of a Neutral on line 50.
Turns out i was being careless with the indexing of the MPI_Request array,
which is continguous rather than indexed by node number. Fixed. 

Next bug is with path traversal. This croaks on both nodes. I think 
what is happening is:

Node 0: The path traversal does not handle the rawIndex/DataIndex properly
Node 1: The 'get' call is sent across as the baseclass 'set' and this 
causes problems. (This possibility is somewhat of a guess).

So I need to tackle both the 'get' call and the path traversal logic.
But it would be good to have the 'set' call properly unit tested.

Worked on the path traversal. Should now be able to do it without having to
inquire across nodes.
Clears the path unit test. Now fails in testShell.cpp::testCopyFieldElement
which needs to assign the vector of sizes just to set up the system.
All very well, but at this point I don't have the capability to copy except
from Globals.

Some pending cross-node things:
Proper unit test for Set across nodes. I think it works.
Get. If I get this to work I can check the 'set' call too.
SetVec + unit test. Try the testCopyFieldElement with the objects created as
	locals.
GetVec
The whole list of Shell commands.


===========================================================================
30 Nov 2013
Subha suggests having elements which put contents on specific nodes.
Perhaps a better thing to do for automation is to have Elements which keep
the entire contents on one node. I would prefer to be able to do models without
reference to size of computer.

Now working on Get. In Postmaster.cpp.
===========================================================================
1 Dec 2013.
Some input on handling random numbers.
Hi,
  Got some answers to the second problem: (1) if the number of random
numbers to be created for each node is known beforehand "jump ahead"
will work (just mailed Upi about it)
(http://www.math.sci.hiroshima-u.ac.jp/~m-mat/MT/JUMP/index.html)
  (2) And for the online case, there seem to be methods for this
(http://software.intel.com/en-us/forums/topic/283349): MT2203 gives up
to 6024 independent streams and Charles Leiserson (the one who wrote
"Introduction to Algorithms" with Cormen, and Rivest) published a
paper on another system for this:
http://supertech.csail.mit.edu/papers/dprng.pdf

I believe Pritish will have more insight into what is best suited for
our purpose.

Best,
  Subha

On Sat, Nov 30, 2013 at 4:14 PM, Upinder S. Bhalla <bhalla@ncbs.res.in> wrote:
> Hi, Pritish,
>     Good to hear from you. It is good timing and we could use your inputs.
>
> Based in good part on the discussions we had during your visit, I've redone
> the MOOSE basecode to be single-threaded and done lots of other pending
> fixes along the way. I'm desiging at present for a simple MPI interface,
> easily adapted to Charm++.
>
> Couple of key design questions have come up, again motivated by our earlier
> discussions, and I don't remember the suggestions you had on them.
>
> 1. The python/scripting interface to MOOSE. With the current design, we need
> to have node zero talk to the Python script on the one hand, and to all the
> other nodes via MPI. The other nodes do not need to know Python. As I
> recall, you had suggested an alternative architecture where there was some
> kind of socket communication to one node. In principle we could have the
> Python-aware node outside the cluster, but then I'm not sure how one would
> tunnel MPI through to the cluster nodes. Do advise.
>
> 2. Parallel random numbers. Suppose synaptic weights need to be assigned by
> a random seed across all nodes. Each node knows what to do with its own
> subset of numbers, but the problem is that we may have to grind through a
> few million numbers in the random sequence before we get there. Even worse
> is how to handle random numbers emitted at runtime for node-specific
> calculations. How does one ensure that the outcome is the same independent
> of number of nodes? I'm sure this is a classic problem in parallel
> simulations.
>
> Best,
>     Upi

Now compiles but doesn't even start to clear unit tests.
===========================================================================
2 Dec 2013
Now clears serial unit tests. Fails on target node when called from 
remoteGet in parallel unit tests. Indexing issue.
Oddly, the other node croaked in clearPendingSet.

===========================================================================
3 Dec 2013.
Understood bug, now to fix. I was trying to use 'get' for the path of
an object that was off-node. The good news is that it does indeed
correctly do most of the work of going off-node to get the object.
The bad news is that it loses the fieldIndex somewhere along the way.

Put in a bit of a hack so that the TgtInfo puts the fieldIndex into
the 'size' field for Sets and Gets, which do not use the size field.

Now carries on to Shell::testCopyFieldElement.
Lots of errors on Node01, for inability to create objects. Line 401
in Shell::testCopyFieldElement.
Fixed the cpoy at that point, but a few lines later it goes bad.

Problems start in testShell.cpp::testChildren.
Seems that I need to fix up Shell:::doDelete.
Did that. Now there is a problem in testShell.cpp:450, all I'm doing
is trying a 'get' on numField.
===========================================================================
4 Dec 2013.
Incrementally going through unit tests. 
Currently a bit stuck in testShell.cpp::copyFieldElement, not because
of real errors but because SetVec and GetVec are not yet implemented.

Next error is due to trying low-level functions to look up values, in 
testShell::testObjidToAndFromPath()
Easy to fix one part of it in line 511, but would be good to
have a general solution.
===========================================================================
5 Dec 2013. Checkin 4878.
Now it is a heisenbug. I'm trying to do a simple set/get on an offnode field,
works when I step through it using gdb, fails when run without stepping.
In testShell.cpp:517.

Possibly the set is not complete when I ask for the 'get'.

Redid SetGet a bit so that both use the same MPI tag. As hoped, this fixes
the sequencing problem of set and then get.

Next problem in testShellSetGet is another SetVec issue. Postpone.
On now to the message tests. This requires further fixing of the
Shell commands.

Man Shell commands now fixed. The program now goes on till 
testShell.cpp:912, where again it runs into the SetVec problem.
Checkin 4880.

Working on SetVec. In SetGet.h and HopFunc.h: 60 ish.
Pending:
- What to do with globals
- Propagate changes through to the dispatchBuffers call and the postmaster
- Handle stuff on remote nodes.

===========================================================================
6 Dec 2013.
Working in FieldElement.cpp:85 to put in the skeleton for getNumOnNode.
The serious version of this function sould look up remote nodes to get
the values.
===========================================================================
7 Dec 2013.

Compiling the changes in for the SetVec.

Discussions with the MOOSE team, seems like there is a feeling that the
path semantics need to change for the FieldElements. Specifically, use only 
the fieldIndex part to index the FieldElement, since they are always children
of an Element whose DataIndex applies both to the base element and to the
FieldElement value.
pare
the FieldElements as always a child of a regular Element 

Silly bug in unit tests, the messages maintain their own indexing for
lookup. and this is incremented when a new message is created. But the
unit tests on the master node make and delete several messages, leaving
their indices different from those on the slave nodes.

split the unit tests into nonmpi and mpi parts,
Provide a global index when creating messages.
Clear out the indices after each block of unit tests.

Messy thing comes up with msgs. The DataId for look up of the msg objects
can get misaligned on different nodes, already does due to unit tests on
node 0. I've implemented a hack to use the DataId for the msgs as generated
by the master node. Unfortunately all the Msg::copy functions and likely 
many others need to make msgs on the fly, they are not able to use something
generated centrally. Current messy solution is to let the copy operations use
automatic incrementing.

===========================================================================
8 Dec.
Accumulating issues
'send' from a global: the MsgDigest needs to only do stuff to local node.
'send' to all: Need to use ALLDATA for targets on local node, and then 
send single calls to remote ones.

Major cleanup done on the MsgId generation, not perfect but the key commands
issue a direct specification of the mid on all nodes and the rest of the time
it does an increment which should happen identically on all nodes.

Checkin 4890.
In the meantime, on with the unit tests to testShell.cpp:942 at which point
we need getVec.

Bypassed that using a loop of 'get' calls. Now it starts messaging, which
of course crashes.
===========================================================================
9 Dec
Spent a lot of time adding a unit test for filterOffNodeTargets. Which works,
but doesn't fix the problem with the messages not going out.

Two bugs now: 
- in testShell.cpp::testCopyFieldElement:460. Number of copies
is wrong for 4 nodes, but OK for 2.
- in testShell.cpp::testShellAddMsg:1001. Basically the messages don't go.

Seems that the Clock does not have any messages to go out.


===========================================================================
10 Dec
Sorted outgoing messages of clock. The ALLDATA flag in the targets had 
confused it regarding which node to send to.

Now it seems that although there is a message from a1 to a2, the msgDigest
doesn't show any outgoing node.

Nasty. Tracked down to error in how SingleMsg was handling sources() and
targets().

More subtleties needed fixing in how the off-node targets were identified.

Now it is down to getting the postmaster to schedule message sending.
First problem is that the reinit call isn't propagating to the off-node
clocks.

Much struggling later, turned out to be just that I needed to make clean
and recompile. But now it is hanging in a confusing way, in a section 
before the 'start'. Also it thinks reinit has been called twice on the
worker node.
===========================================================================
11 Dec.
Fixed an issue with the clearPending() command so that it can safely be
called recursively. This helps.

Now it is trying to send a message on node 1 but the list of targets includes
off-node entries.

Much painful debugging of the filterOffNodeTargets later, now it clears
the message sending. Hooray! But it promptly fails in the next unit test,
where I copy messages. Checkin 4900.

Next place of failure seems to be when the system tries to send messages
to nonexistent objects, deleted when the testShell::testShellAddMsg
function ended, but accessed by the next unit test testCopyMsgOps
in Shell::doReinit. Somewhere in clearPendingSend.
===========================================================================
12 Dec.
Turns out we're accessing objects a1 and b1: these were the msg sources of
messages on the respective nodes. So it seems likely that the postmaster is
reprising the old message calls. Similar to what happened with SetGet.

Step 1: Put Postmaster on Clock 9, guaranteed to be the last called. Doesn't
fix it, but this was a necessary cleanup anyway.
Question is, do the deleted objects somehow send out stuff on the originating 
node, or is it something left over in the recvBuf on the target node?

Since the thing croaks on doReinit, which does NOT trigger a send call on the
Arith object, it is most likely something in the recvBuf. But possibly
the send buffer isn't being cleared either.

Fixed it up. The send buffer was not being cleared and also there was no
check that all sends had completed, before going on to the next cycle.
With this we advance further till the next instance of getVec.
Actually the problem is with the copy command. 
Turns out I was not correctly offsetting the original data when doing the
copy. Now clears that test. Checkin 4907.
Next, in testSyncSynapseSize: Fixed a low-level lookup that would only work
on a single-node calculation. Then yet another getVec is commented out.
With this it clears all unit tests on 2 nodes. Major milestone.
Checkin 4908.
On 4 nodes it croaks in testShellAddMsg:968, which is our old friend where
it starts off the little test of all the different message types.
Fails on 2 of the 4 nodes: one with a bad msg index in Arith::send, and 
another trying to finish up a SetVec. Perhaps we're not synced properly?

Working on GetVec. Mostly there with the code framework. Still need to
tie the Field< A >::getVec function to HopFunc.h::GetHopFunc::opVec.
===========================================================================
13 Dec
Trying to compile with the getVec. Issue with how the class is derived from
OpFunc1Base< A* >, where the pointer causes friction in the definition of 
opVec.

Trying to compile.

===========================================================================
14 Dec.
Trying to clean up the mess with GetVec. Compiled but buggy.
Turns out this is a bit of a dead-end. I had earlier subclassed 
GetOpFuncBase< A > off OpFunc1Base< A* > which was very clean and elegant for
returning values. In trying to do GetVec I've done it off opFuncBase1< A >,
but that has messed up the earlier return structure and seems to be unnecessary
anyway. I may need to revert back to 4908
===========================================================================
15 Dec.

GetVec now works on 2 nodes, and the whole unit test clear. 
But GetVec still does not work for 4-node tests.

Some more fixing, now getVec does seem to work but again there are problems in
testShellAddmsg. Chekin 4919.

I think there may be a problem with setVec on multiple nodes.

===========================================================================
18 Dec.
SetVec it is. Still to track down. I'm concerned it isn't even
getting caught at HopFunc.h:103. Is the control flow bypassing this?

===========================================================================
19 Dec
Found two problems on the 'send' side when trying to send a 5-entry
array to 4 nodes in the subdivision 2, 2, 1, 0.
1. The e.getNode() reports node 2 as the target for e={elm, 5, 0}
	Should either be node 3, or no node.
2. The setSendSize is 4, but reports a dataSize of 2. Actually
	both should be zero as nothing is to be transmitted. In fact
	the system should flag this and not even attempt to send anything.

Huge mess in HopFunc.h:OpVec and co. There is a confusion between 
serial index of FieldElements, between DataId and FieldIndex.

Sort of fixed. Now it fails in some of the messaging calls on 4 nodes,
typically on the last entry, but in other cases too.
===========================================================================
21 Dec
The ones that fail are: bdefg
b: OneToAll. Fails on last entry
d: Diagonal. Fails on last entry.
e: Sparse. Fails on first and last entry.
f: OneToAll, where all entries go to all others. Fails on all.
g: Sparse, where all entries go to all others, except self. Fails on all.

All these could be explained if the last entry, which is on node 2, has
problems both sending and receiving.
Look at what happens on its postMaster.

From Process on PostMaster on Node 2:
(gdb) p *(TgtInfo*)(&sendBuf_[0][0])
$47 = {static headerSize = 3, id_ = {id = {id_ = 264}, dataId = 4, 
    fieldIndex = 0}, bindIndex_ = 1, dataSize_ = 1}
(gdb) p *(TgtInfo*)(&sendBuf_[0][4])
$48 = {static headerSize = 3, id_ = {id = {id_ = 266}, dataId = 4, 
    fieldIndex = 0}, bindIndex_ = 1, dataSize_ = 1}
(gdb) p *(TgtInfo*)(&sendBuf_[0][8])
$49 = {static headerSize = 3, id_ = {id = {id_ = 268}, dataId = 4, 
    fieldIndex = 0}, bindIndex_ = 1, dataSize_ = 1}
(gdb) p *(TgtInfo*)(&sendBuf_[1][0])
$50 = {static headerSize = 3, id_ = {id = {id_ = 266}, dataId = 4, 
    fieldIndex = 1}, bindIndex_ = 1, dataSize_ = 1}
(gdb) p *(TgtInfo*)(&sendBuf_[1][4])
$51 = {static headerSize = 3, id_ = {id = {id_ = 268}, dataId = 4, 
    fieldIndex = 1}, bindIndex_ = 1, dataSize_ = 1}
(gdb) p *(TgtInfo*)(&sendBuf_[3][0])
$52 = {static headerSize = 3, id_ = {id = {id_ = 266}, dataId = 4, 
    fieldIndex = 3}, bindIndex_ = 1, dataSize_ = 1}

Looks very reasonable. The sendSize in all cases is also reasonable
Stepped through, looks like node zero didn't get the data that was sent
by node 2. I was monitoring in PostMaster::clearPendingRecv.

Turned out an error in how I understood the MPI_TestSome arguments. The 
status index I was using was wrong.

With this fixed, I now clear the testShell.cpp:testShellAddMsg function
which is the crucial test of message passing. But now stuck in 
testCopyMsgOps.

This bug is because I don't let DataElement start with zero entries. I 
force it to be 1. Let's examine this logic. No, seems bad. I should
permit zero entries on the DataElement.

With that it clears unit tests on 4 nodes. Hooray! But the quit call does
not succeed in getting it to quit.

Also fails on 5 nodes now. in testShell.cpp:925
This is due to my trying to do a direct access to a data entry on the
master 0 node. The data is not on this node.

Fixed this little bug. 
Now it fails again in testShellAddMsg. That is turning out to be
the most definitive unit test.

Just for completeness, ran on 3 nodes. Clears unit tests there too.

Checkin 4924. Now to tackle why it fails on 5 nodes.
Failure is due to erroneous message outcome as detected on master node.

Here the message outcome is like this:
(0, 0) (1, 2) (2, 2) (3, 3) (4, 4) 
(5, 5) (4, 4) (3, 3) (2, 4) (1, 1) 
(15, 15) (15, 11) (15, 16) (15, 13) (15, 15) 
(14, 14) (13, 9) (12, 13) (11, 9) (10, 10) 

This is massages d,e,f and g.
d: Diagonal. Fails on second entry.
e: Sparse. Fails on second-last entry
f: OneToAll, where all entries go to all others. Fails on all but first and last
g: Sparse, where all entries go to all others, except self. Fails as above.

Important check: is it consistent? No.
d (0, 0) (1, 1) (2, 4) (3, 3) (4, 0) 
f(15, 15) (15, 15) (15, 13) (15, 15) (15, 13) 
g(14, 14) (13, 13) (12, 10) (11, 11) (10, 8)

Ran it a number of times without gdb. Fails in most cases for 3 and above.
Again, not consistent in how it fails.

I would guess that this happens because the processes launch off without
a barrier at the start. Let's try.
No, that doesn't fix it.

===========================================================================
22 Dec 2013
Another try: Put a barrier on PostMaster::reinit.
That doesn't work either. From the variety of error messages, it is clear that 
at least part of the problem is synchronization. In some cases it claims that
I'm trying to set off a new simulation while an old one is still running.
In many cases the message sending in the testShellAddMsg comes out wrong.

I may need to make 'set' a blocking call. However, all the 'get' calls have
worked.

Put in a barrier in PostMaster::reinit and process, after all the loops
and operations. With this it clears unit tests for 1 through to 16 nodes.
Also quits cleanly. Checkin 4925.

So this is how long it took:
Week 1: Oct 12-19: Threading out
Week 2: Oct 19-26 Messaging refactoring
Week 3: Oct 26-Nov02: Element and field indexing refactoring. Data Handler out.
Week 4: Nov 02-12. Scheduling refactoring, recompile and unit tests. 
	Copy refactoring. MsgIds replaced with ObjIds. Automatic message compilation.
Week 4.5-5: Nov12-15 Standalone tests on mpi framework
Week 6: Nov 16-23 Multinode messaging design, implementation, compilation, 
	no tests.
Week 7: Nov 23-30 Multinode Set. Compile and test.
Week 8: Nov 30-Dec 7. Multinode Get. Most of the Shell commands work. SetVec.
Week 9: Dec 7-14. Multinode messaging. Much cleanup. Clears all unit tests 
	for 2 nodes. Began GetVec.
Week 10: Dec 14-21: GetVec works. Much bug stomping through scaling up to 
	any number of nodes. 
Dec 22: Clears all unit tests on any number of nodes from 1 to 16.

Did a valgrind on the single node. Looks quite filthy. All seems to be in MPI.
Uninitialized variables, leaks, the works.

Now to look ahead. One minor item is to have Id::path return a string without
trailing braces.  Done. Checkin 4926.

Next steps:
	- Get it working with python
	- Benchmarks

===========================================================================
25 Dec 2013.
Tried to get a script-based test model to go. Very awkward, no luck.
Resurrected a benchmark/unit test for and int fire network. This is now in 
testBiophysics.cpp::testIntFireNetwork. Doesn't match values accurately for
the basic run test. Also, fails even with 2 nodes.
===========================================================================
26 Dec 2013.

Fixed issue with SparseMsg::randomConnect: It now more properly figures out
which matrix entries need to be filled on local node.
Next issue is that of overflow of the send/recv buffers when setting a very
large matrix for the synaptic weights.

Fixed that simply by making it bigger. But now I need to do something about the
get command and it isn't so simple because we need one buffer per node.

===========================================================================
27 Dec 2013
in testIntFireNetwork (runsteps=5)
    at testBiophysics.cpp:125

Getting the weight vec fails for 4 nodes, values are wrong.

===========================================================================
28 Dec
Traced the problem with getvec to the setvec most likely. But this reveals
a more fundamental issue: What range should the set/getVec calls span for
FieldElements? Currently I'm trying to scan everything. 
Given that the Python interface treats FieldElements as equivalent to the
regular Elements (with the proviso that the DataId is pre-specified), we
should use the set/getVec calls only spanning the FieldIndex for FieldElements.
Not as handy as being able to access all the entries in a single call. But
perhaps that kind of access should be a separate function that uses a wildcard.

Put in a little fix so that objects cannot be renamed if there is an existing
object with the same name. Unfortunately one of the unit tests creates
a parentless object... testAsync.cpp:192
===========================================================================
29 Dec.
Easily fixed, arranged adoption of the parentless object. Clears unit tests.
Valgrind is not so happy, but I think most of this is MPI being awful.
For the SetVec, I'll change call semantics so it only goes to one array.
If it is a FieldElement, then it is indexed by the FieldIndex. If it is
a DataElement, it is indexed by the DataIndex.
As a more general Set call I should look into having a specification of an
ObjId list with provision for a specific or an 'all' entry.

Some more cleanup on the name change and also on the Shell::doCreate, so that
the tests are done at the start of the function rather than after it has
been dispatched to the nodes. Now correctly returns Id() on failure.
Added unit tests for this.
Checkin 4937. After a minor fix for the back-ported SpikeGen, 
Valgrind seems clean.

Things to do to clean up the SetVec etc.

- Iterator for Erefs/elements.
- Add the node-specific Element class
- Redo HopFunc::opVec.
- Redo SetGet::setVec to take ObjId argument.

I've examined the iterator options. Currently not worth it.

Reimplemented the SetGet::vec functions for the updated semantics, where
the vector returned is the last array. For DataElements it is the
array of values from all DataEntries, and for FieldElements it is the
array of values from all FieldEntries on a particular DataIndex. Many
unit tests need fixing now.

Checkin 4939.
Cleared unit tests on single node. Fails on 2, doing testShell.cpp::testCopy. 
I think the copy itself fails, not the setget. But possibly the values oon
the remote node were not set in the first place.

===========================================================================
30 Dec
In Python, 'element' refers to a single object. A bit like GENESIS.
There is 'ematrix' for the vector of data or the vector of fields: in each
case a one-dimensional vector of objects. Suggest to rename the Python 
'ematrix' to evec elevec elmarray earray erray elmtab etab elmvec wrapper
eptr elmptr emulti elmulti multielm etuple elmtuple elmvec eset elmset 
elmatrix arrayelm multielm elmarray elmindex elmdex elemdex elemulti elookup


Traced the problem for the testCopy. As far as I can see the copy itself is
OK, but the original assignment of values to the Global has failed on the
remote node.

Now clears unit tests with up to 8 nodes. But the IntFire test isn't
complete yet. Looking at the output values. They do not match. One systematic
thing: the values for Vm get steadily smaller with larger numbers of nodes.
1. Check if off-node IntFires are clocked.		Yes
2. Check if off-node IntFires get off-node msgs.	No.
===========================================================================
31 Dec.
Now this is strange. The smaller scale test for multinode message worked,
see testShellAddMsg.

Made fixes to path and doFind functions to convert to and from string paths.
This is to fix things with the pymoose code and conventions.

Examining the message passing. Turns out no data was passing from node 0 to
node 1, though the postmaster functions were indeed called.

OK, I think I have it. When the SparseMsg sets up using randomConnect, it
only fills up local node messages. So there is no info of off-node msgs for
Element::disgestMsgs to work on.

What I need to do is recode this a bit so that the msgs themselves generate
a list of nodes to which they project, and have this used by digestMsgs.

As a quick test, I reinstated the entire SparseMatrix so all the messages
are represented. Outcome is a bit puzzling. The node 0 response is identical
to the single-node version, but the last node value isn't and it varies with
number of nodes. I wonder if this is related to the problem with the match
to the previous version.
Scaled the above run up to 12 nodes, at which the Vm100 moves over to other
than node 0. Now the response changes.

Tried increasing the synaptic delay so that it is at least one timestep long.
Otherwise we might get within-timestep delivery of spikes to node0, but not
to any other node. Doesn't work.

So the clue here is that the node0 value Vm100 is independent of # nodes, 
whereas the last node value Vm900 changes every time. Are the local
messages going out correctly on non-node-zero?

Tried a number of values from node0. Turns out that at least one of them
(Vm101) goes back and forth between a couple of values depending on numNodes. 
None of the other node0 reported values changes.
All of the node1 reported values change. Is there node misdirection?

Tried on 3 nodes. Each node is receivng the same number of bytes as was
sent, respectively to the other nodes. So it doesn't look like we're missing
on one message or another.

Something wrong about what is sent, it seems.

===========================================================================
1 Jan 2014.
Things to try:
- Send the DataId and node of the sender instead of the timestamp. Work 
backwards to figure out what is missing in different node configurations.

Here it is with only node 0:
        11      14      22      23      28      34      36      38      45     48       50      51      56      60      66      67      71      73      77     84       86      96      99      107     109     112     117     124     126    127      135     145     149     150     156     157     160     172     175    178      183     185     192     198     205     213     216     237     244    245      246     248     264     269     270     273     274     281     291    297      300     304     316     318     324     325     331     341     348    349      353     357     358     365     369     375     383     390     397    400      402     412     414     421     427     430     435     448     450    454      456     473     474     475     488     490     491     492     495    497      498     501     506     514     517     522     526     530     533    539      546     547     556     566     570     582     586     587     594    596      604     607     613     614     622     627     634     636     639    650      654     655     656     668     671     676     682     686     690    692      700     701     704     710     713     714     719     722     724    725      726     730     734     735     737     738     739     740     741    745      748     750     754     756     762     769     783     788     792    793      798     801     811     815     818     824     833     837     841    843      845     854     856     857     865     866     876     880     888    892      899     902     903     906     908     911     912     915     917    929      932     965     968     980     982     983     987     992     994    998      1000    1001    1006    1009    1011    1012    1021

Then: Confirmed we get same set when split into 2 nodes.
Then: Check what arrives. 
	a. The value of what arrives (dataId) matches the
	dataId of the sender. As it should.
	b. The arrivals seem to match all the ones that were sent.
Then:	Check that the arrivals go to the right targets OK.
	Check that the locals go to the right targets OK.
	Check that the weights and delays get set correctly.

Here are some debug prints to check these. Each entry is the dataId,FieldIndex:
The call args are the func#, srcDataId.
(gdb) call e.e_->printMsgDigest( 1, 0 )
0:      0:              9,0     10,0    14,0    20,0    53,0    54,0    55,0   66,0     67,0    72,0    74,0    80,0    81,0    89,0    105,0   124,0   132,0  134,0    135,0   138,0   139,0   140,0   162,0   189,0   197,0   208,0   222,0  233,0    243,0   245,0   248,0   261,0   286,0   324,0   337,0   355,0   369,0  375,0    378,0   381,0   392,0   394,0   415,0   421,0   426,0   442,0   451,0  458,0    473,0   476,0   483,0   501,0   503,0   507,0   530,0   540,0   543,0  546,0    590,0   593,0   618,0   632,0   636,0   638,0   655,0   671,0   688,0  690,0    696,0   703,0   711,0   717,0   732,0   751,0   755,0   764,0   770,0  778,0    781,0   786,0   807,0   826,0   868,0   896,0   907,0   921,0   924,0  925,0    927,0   934,0   941,0   950,0   959,0   963,0   970,0   976,0
(gdb) call e.e_->printMsgDigest( 1, 1 )
1:      0:              3,0     26,0    30,0    38,0    58,0    60,0    106,0  108,0    109,0   110,0   129,0   142,0   143,0   147,0   157,0   166,0   172,0  176,0    177,0   198,0   206,0   215,0   217,0   225,0   235,0   265,0   267,0  269,0    278,0   289,0   291,0   303,0   311,0   327,0   328,0   353,0   357,0  391,0    399,0   406,0   408,0   412,0   425,0   468,0   476,1   478,0   487,0  489,0    490,0   501,1   502,0   507,1   509,0   536,0   540,1   568,0   576,0  578,0    579,0   591,0   597,0   603,0   624,0   625,0   655,1   658,0   664,0  698,0    713,0   729,0   747,0   749,0   753,0   765,0   771,0   802,0   803,0  806,0    820,0   829,0   841,0   847,0   857,0   860,0   874,0   881,0   885,0  893,0    894,0   900,0   912,0   915,0   919,0   922,0   923,0   924,1   929,0  944,0    947,0   951,0   959,1   960,0   965,0   966,0   975,0   979,0   987,0  1002,0   1017,0  1018,0
(gdb) call e.e_->printMsgDigest( 1, 2 )
2:      0:              2,0     7,0     8,0     19,0    35,0    36,0    43,0   47,0     58,1    60,1    61,0    70,0    79,0    88,0    90,0    108,1   115,0  120,0    124,1   125,0   130,0   149,0   156,0   157,1   158,0   162,1   181,0  190,0    192,0   196,0   199,0   202,0   203,0   221,0   223,0   264,0   288,0  300,0    319,0   320,0   325,0   353,1   359,0   371,0   382,0   383,0   393,0  399,1    421,1   423,0   425,1   427,0   442,1   446,0   450,0   454,0   455,0  466,0    476,2   487,1   497,0   507,2   514,0   516,0   518,0   526,0   533,0  534,0    548,0   560,0   568,1   570,0   572,0   578,1   581,0   606,0   631,0  638,1    651,0   671,1   673,0   674,0   678,0   680,0   697,0   704,0   706,0  727,0    738,0   745,0   752,0   779,0   785,0   789,0   796,0   800,0   850,0  858,0    866,0   868,1   872,0   885,1   899,0   905,0   913,0   923,1   955,0  958,0    974,0   976,1   997,0   1004,0  1007,0
(gdb) call e.e_->printMsgDigest( 1, 900 )
900:    0:              18,91   26,95   29,91   33,82   43,108  45,87   49,94  78,97    81,98   89,96   91,84   110,88  133,88  140,103 143,92  166,81  176,86 180,84   190,83  192,94  197,84  230,101 237,95  243,94  249,74  256,88  259,99 269,93   277,86  303,83  311,104 312,83  314,94  322,78  332,86  347,89  393,101402,87   417,90  458,99  474,95  483,98  485,90  500,87  523,76  525,100 536,86 539,96   544,91  546,99  583,96  584,102 610,74  620,91  623,89  638,80  644,82 645,97   648,85  662,118 666,82  689,85  691,101 692,96  697,79  716,101 719,94 730,83   742,65  751,82  756,92  757,81  760,86  768,97  769,102 796,83  797,65 800,87   811,97  813,92  824,81  825,87  844,94  858,86  870,95  874,107 875,88 895,100  911,88  917,88  927,88  928,77  933,77  940,97  967,74  988,82  995,98
(gdb) call e.e_->printMsgDigest( 1, 901 )
901:    0:              22,77   24,97   46,102  67,81   81,99   86,102  91,85  99,80    112,95  136,90  155,82  171,90  181,84  234,93  238,98  250,88  269,94 278,94   287,95  311,105 313,87  323,84  348,98  349,87  357,89  361,76  377,84 378,91   399,87  406,80  422,92  438,117 446,90  453,80  459,81  466,83  477,70 486,103  493,106 497,85  500,88  505,82  513,100 518,95  529,100 531,103 535,93 537,97   542,82  544,92  556,83  559,87  563,96  565,91  567,87  583,97  588,78 596,83   600,65  612,87  635,99  641,91  715,88  743,83  748,84  766,76  791,83 792,72   794,92  799,82  801,100 804,101 841,88  846,86  865,96  868,101 874,108877,100  903,89  933,78  938,89  952,82  983,92  985,89  1002,95 1015,107       1019,85  1021,81
(gdb) call e.e_->printMsgDigest( 1, 902 )
902:    0:              0,81    10,91   16,104  39,89   50,106  57,86   61,76  69,89    75,91   82,88   83,70   91,86   96,90   101,91  102,89  103,92  106,92 108,85   110,89  118,87  132,87  141,89  153,106 158,106 160,98  166,82  169,83 185,105  228,81  231,99  249,75  267,86  280,78  287,96  289,82  321,96  322,79 325,84   328,120 342,103 380,95  387,81  388,104 401,82  405,81  420,83  424,80 434,99   471,93  476,82  482,90  486,104 488,92  504,76  517,83  522,89  525,101526,78   539,97  542,83  548,102 564,94  566,101 570,79  577,82  584,103 592,87 593,90   595,119 605,88  612,88  622,95  623,90  644,83  647,94  654,91  670,92 694,100  703,74  705,86  711,104 722,83  728,92  734,96  750,107 757,82  762,83 769,103  786,103 790,87  808,72  819,97  828,109 832,93  833,82  846,87  847,97 852,96   871,88  888,101 893,78  901,98  904,84  914,101 934,94  940,98  980,72 991,94   995,99  1005,86 1020,85

Now to see what happens on the nodes in a 2-node simulation.
Node 0:
(gdb) call e.e_->printMsgDigest(1,0)
0:      0:              9,0     10,0    14,0    20,0    53,0    54,0    55,0   66,0     67,0    72,0    74,0    80,0    81,0    89,0    105,0   124,0   132,0  134,0    135,0   138,0   139,0   140,0   162,0   189,0   197,0   208,0   222,0  233,0    243,0   245,0   248,0   261,0   286,0   324,0   337,0   355,0   369,0  375,0    378,0   381,0   392,0   394,0   415,0   421,0   426,0   442,0   451,0  458,0    473,0   476,0   483,0   501,0   503,0   507,01:         0,1
(gdb) call e.e_->printMsgDigest(1,1)
1:      0:              3,0     26,0    30,0    38,0    58,0    60,0    106,0  108,0    109,0   110,0   129,0   142,0   143,0   147,0   157,0   166,0   172,0  176,0    177,0   198,0   206,0   215,0   217,0   225,0   235,0   265,0   267,0  269,0    278,0   289,0   291,0   303,0   311,0   327,0   328,0   353,0   357,0  391,0    399,0   406,0   408,0   412,0   425,0   468,0   476,1   478,0   487,0  489,0    490,0   501,1   502,0   507,1   509,01:         1,1
(gdb) call e.e_->printMsgDigest(1,2)
2:      0:              2,0     7,0     8,0     19,0    35,0    36,0    43,0   47,0     58,1    60,1    61,0    70,0    79,0    88,0    90,0    108,1   115,0  120,0    124,1   125,0   130,0   149,0   156,0   157,1   158,0   162,1   181,0  190,0    192,0   196,0   199,0   202,0   203,0   221,0   223,0   264,0   288,0  300,0    319,0   320,0   325,0   353,1   359,0   371,0   382,0   383,0   393,0  399,1    421,1   423,0   425,1   427,0   442,1   446,0   450,0   454,0   455,0  466,0    476,2   487,1   497,0   507,21:         2,1
(gdb) call e.e_->printMsgDigest(1,900)
900:    0:              18,91   26,95   29,91   33,82   43,108  45,87   49,94  78,97    81,98   89,96   91,84   110,88  133,88  140,103 143,92  166,81  176,86 180,84   190,83  192,94  197,84  230,101 237,95  243,94  249,74  256,88  259,99 269,93   277,86  303,83  311,104 312,83  314,94  322,78  332,86  347,89  393,101402,87   417,90  458,99  474,95  483,98  485,90  500,87
(gdb) call e.e_->printMsgDigest(1,901)
901:    0:              22,77   24,97   46,102  67,81   81,99   86,102  91,85  99,80    112,95  136,90  155,82  171,90  181,84  234,93  238,98  250,88  269,94 278,94   287,95  311,105 313,87  323,84  348,98  349,87  357,89  361,76  377,84 378,91   399,87  406,80  422,92  438,117 446,90  453,80  459,81  466,83  477,70 486,103  493,106 497,85  500,88  505,82
(gdb) call e.e_->printMsgDigest(1,902)
902:    0:              0,81    10,91   16,104  39,89   50,106  57,86   61,76  69,89    75,91   82,88   83,70   91,86   96,90   101,91  102,89  103,92  106,92 108,85   110,89  118,87  132,87  141,89  153,106 158,106 160,98  166,82  169,83 185,105  228,81  231,99  249,75  267,86  280,78  287,96  289,82  321,96  322,79 325,84   328,120 342,103 380,95  387,81  388,104 401,82  405,81  420,83  424,80 434,99   471,93  476,82  482,90  486,104 488,92  504,76

Node 1:
(gdb) call e.e_->printMsgDigest(1,0)
0:      0:              530,0   540,0   543,0   546,0   590,0   593,0   618,0  632,0    636,0   638,0   655,0   671,0   688,0   690,0   696,0   703,0   711,0  717,0    732,0   751,0   755,0   764,0   770,0   778,0   781,0   786,0   807,0  826,0    868,0   896,0   907,0   921,0   924,0   925,0   927,0   934,0   941,0  950,0    959,0   963,0   970,0   976,0
(gdb) call e.e_->printMsgDigest(1,1)
1:      0:              536,0   540,1   568,0   576,0   578,0   579,0   591,0  597,0    603,0   624,0   625,0   655,1   658,0   664,0   698,0   713,0   729,0  747,0    749,0   753,0   765,0   771,0   802,0   803,0   806,0   820,0   829,0  841,0    847,0   857,0   860,0   874,0   881,0   885,0   893,0   894,0   900,0  912,0    915,0   919,0   922,0   923,0   924,1   929,0   944,0   947,0   951,0  959,1    960,0   965,0   966,0   975,0   979,0   987,0   1002,0  1017,0  1018,0
(gdb) call e.e_->printMsgDigest(1,2)
2:      0:              514,0   516,0   518,0   526,0   533,0   534,0   548,0  560,0    568,1   570,0   572,0   578,1   581,0   606,0   631,0   638,1   651,0  671,1    673,0   674,0   678,0   680,0   697,0   704,0   706,0   727,0   738,0  745,0    752,0   779,0   785,0   789,0   796,0   800,0   850,0   858,0   866,0  868,1    872,0   885,1   899,0   905,0   913,0   923,1   955,0   958,0   974,0  976,1    997,0   1004,0  1007,0
(gdb) call e.e_->printMsgDigest(1,900)
900:    0:              523,76  525,100 536,86  539,96  544,91  546,99  583,96 584,102  610,74  620,91  623,89  638,80  644,82  645,97  648,85  662,118 666,82 689,85   691,101 692,96  697,79  716,101 719,94  730,83  742,65  751,82  756,92 757,81   760,86  768,97  769,102 796,83  797,65  800,87  811,97  813,92  824,81 825,87   844,94  858,86  870,95  874,107 875,88  895,100 911,88  917,88  927,88 928,77   933,77  940,97  967,74  988,82  995,981:                900,0
(gdb) call e.e_->printMsgDigest(1,901)
901:    0:              513,100 518,95  529,100 531,103 535,93  537,97  542,82 544,92   556,83  559,87  563,96  565,91  567,87  583,97  588,78  596,83  600,65 612,87   635,99  641,91  715,88  743,83  748,84  766,76  791,83  792,72  794,92 799,82   801,100 804,101 841,88  846,86  865,96  868,101 874,108 877,100 903,89 933,78   938,89  952,82  983,92  985,89  1002,95 1015,107        1019,85 1021,811:               901,0
(gdb) call e.e_->printMsgDigest(1,902)
902:    0:              517,83  522,89  525,101 526,78  539,97  542,83  548,102564,94   566,101 570,79  577,82  584,103 592,87  593,90  595,119 605,88  612,88 622,95   623,90  644,83  647,94  654,91  670,92  694,100 703,74  705,86  711,104722,83   728,92  734,96  750,107 757,82  762,83  769,103 786,103 790,87  808,72 819,97   828,109 832,93  833,82  846,87  847,97  852,96  871,88  888,101 893,78 901,98   904,84  914,101 934,94  940,98  980,72  991,94  995,99  1005,86 1020,851:               902,0


Well, even this looks OK. That leaves assignment of weights and delays. The
weights at least are checked already as part of the unit tests.
Another thing I can think of is there being an error in how the 
SpikeRingBuffer handles incoming at different times. Nothing apparent, the code
is really simple.

Next: monitor what the spikeRingBuffers contain on each cycle.
Here is the buffer on node 0 for IntFire0 at time = 0.4:
(gdb) p *(&buf_.weightSum_[0])@20
$6 = {0, 0, 0, 0, 0.0011231906618922949, 0.028195377006195489, 0, 0, 0, 
  0.011927361548878253, 0.012636251500807702, 0.011373002212494612, 
  0.02413556597195566, 0.030104355947114525, 0.014308227128349244, 
  0.0051488068467006086, 0, 0, 0.0080248786695301527, 0}

Here it is for IntFire900:
(gdb) p *(&buf_.weightSum_[0])@20
$9 = {0.0039365481445565818, 0, 0.019248148943297565, 0.0095276742568239578, 
  0.012616655579768122, 0.015804447340779008, 0.031027889060787857, 
  0.020450418312102557, 0, 0.018955300822854043, 0.029514752686955036, 
  0.025616842862218617, 0.010785101428627968, 0, 0.023539618398062886, 0, 
  0.013914890796877443, 0.029127159682102499, 0.0079548832122236495, 
  0.022189490515738725}

Here we are on 2 nodes:
IntFire0: (on node 0)
(gdb)  p *(&buf_.weightSum_[0])@20
$3 = {0, 0, 0, 0, 0.0011231906618922949, 0.028195377006195489, 0, 0, 0, 
  0.011927361548878253, 0.012636251500807702, 0.011373002212494612, 
  0.02413556597195566, 0.030104355947114525, 0.014308227128349244, 
  0.0051488068467006086, 0, 0, 0.0080248786695301527, 0}

IntFire900: (on node 1)
 p *(&buf_.weightSum_[0])@20
$6 = {0.0039365481445565818, 0, 0.019248148943297565, 0.008282735007815064, 
  0.001244939249008894, 0.012616655579768122, 0.013555899155326187, 
  0.020450418312102557, 0, 0.018955300822854043, 0, 0.046050151288509367, 
  0.019866545689292251, 0, 0.023539618398062886, 0, 0, 0.015623985570855439, 
  0.035372948120348156, 0.022189490515738725}

OK, finally we see some differences. Interestingly, it is all-or-none. Many
values are identical. Suggests that individual messages are not getting 
through. Now the problem is to trace these back to where they came from.
See the one which has zero in one and nonzero in another, should be able
to find the offending weight.
Given that many of the entries are correct, it seems that the weights are
probably OK. It is just messages not arriving.

Added yet another printf debug the Synapse::addSpike call, to see what
arguments come through. Specifically, some of these seem to be missing.

============================================================================
2 Jan 2014
On one node, reporting src DataId, dest FieldIndex.
monitoring synapses on IntFire 100:
        28,3    34,4    51,6    216,18  248,21  297,28  383,36  454,43  495,45 498,46   586,51  734,71  750,73  783,75

monitoring 900:
        157,14  172,15  178,16  300,27  375,33  435,39  450,41  475,45  498,46 582,51   607,53  650,58  682,65  722,69  724,70  737,73  748,74  762,76  793,78 801,79   837,83  845,86  857,87  865,88  908,91  911,92  929,95  1011,103

On 2 nodes,  monitoring IntFire 100: (output only on node 0):
        28,3    34,4    51,6    216,18  248,21  297,28  383,36  454,43  495,45 498,46   586,51  734,71  750,73  783,75

On 2 nodes, monitoring IntFire 900: (Output only on node 1):
582,51  607,53  650,58  722,69  724,70  737,73  762,76  793,78  801,79 837,83   845,86  857,87  865,88  908,91  911,92  929,95  1011,103
157,14 172,15   178,16  300,27  375,33  435,39  450,41  475,45  498,46

There are two incoming events missing on node1, 900. Importantly, they are
missing from what should be the local set of activities on node 1. 
The events coming from the sub-512 set are all there.
The missing events are: 682,65 and 748,74

Better directly check that all Vms and other values are set correctly.
Should print out the time-series for 682 and 748.

One node:
Here it is for 100:
0.2,0.00710756  0.4,0.00568605  0.6,0.00454884  0.8,0.0138681   1,0.0110945
For 682:
0.2,0.783242    0.4,-1e-07      0.6,0   0.8,0.0247299   1,0.0417334
For 748:
 0.2,0.778782    0.4,-1e-07      0.6,0   0.8,0   1,0
For 900:
0.2,0.276274    0.4,0.221019    0.6,0.176816    0.8,0.156851    1,0.133103

Two Nodes:
For 100: 
0.2,0.00710756  0.4,0.00568605  0.6,0.00454884  0.8,0.0138681   1,0.0110945
For 682:
0.2,0.783242    0.4,0.626594    0.6,0.515236    0.8,0.420256    1,0.365541
For 748:
0.2,0.778782    0.4,0.623026    0.6,0.51831     0.8,0.422253    1,0.337803
For 900:
0.2,0.276274    0.4,0.221019    0.6,0.176816    0.8,0.156851    1,0.132107

Differences: 682 and 748 seem to have spiked and rebounded on 1 nodes and not
on 2. There is a subtle difference for the 900 final value as well.

Let's look at other values on the IntFire:
On 2 nodes:
For 100:
Vm_ = 0.0071075635496526957, thresh_ = 0.80000000000000004, tau_ = 1, 
  refractoryPeriod_ = 0.40000000000000002, lastSpike_ = -0.40000000000000002, 
  bufferTime_ = 4
(gdb) p synapses_[0]
$3 = (Synapse &) @0x34e54d0: {weight_ = 0.0083155348012223847, 
  delay_ = 1.4613027032464743, buffer_ = 0x34af128}

For 682:
Vm_ = 0.78324243589304388, thresh_ = 0.80000000000000004, tau_ = 1, 
  refractoryPeriod_ = 0.40000000000000002, lastSpike_ = -0.40000000000000002, 
  bufferTime_ = 4
(gdb) p synapses_[0]
$4 = (Synapse &) @0x2c8ee60: {weight_ = 0.011778635610826314, 
  delay_ = 0.78097819723188877, buffer_ = 0x2c5a868}

On 1 node:
For 100:
Vm_ = 0.0071075635496526957, thresh_ = 0.80000000000000004, tau_ = 1, 
  refractoryPeriod_ = 0.40000000000000002, lastSpike_ = -0.40000000000000002, 
  bufferTime_ = 4}
(gdb) p synapses_[0]
$2 = (Synapse &) @0x2cc1ff0: {weight_ = 0.0083155348012223847, 
  delay_ = 1.4613027032464743, buffer_ = 0x2c6c0d8}

For 682
Vm_ = 0.78324243589304388, thresh_ = 0.80000000000000004, tau_ = 1, 
  refractoryPeriod_ = 0.40000000000000002, lastSpike_ = -0.40000000000000002, 
  bufferTime_ = 4}
(gdb) p synapses_[0]
$5 = (Synapse &) @0x2dafe30: {weight_ = 0.011778635610826314, 
  delay_ = 0.78097819723188877, buffer_ = 0x2c7d1a8}

I don't see any difference. Worrying. Why are they behaving so differently?
Is there something different in the buffers that causes it to fire on one
node but not on two?

On single node, looking at 682:
(gdb) p synapses_[65]
$2 = (Synapse &) @0x2db0448: {weight_ = 0.011979404771700501, 
  delay_ = 2.7584519190713763, buffer_ = 0x2c7d1a8}
(gdb) p *(&synapses_[65].buffer_->weightSum_[0])@20
$6 = {0.017451648577116431, 0, 0.020829295343719426, 0.027436947519890967, 
  0.011310790828429163, 0.0075863821897655731, 0.0028163404529914261, 0, 
  0.0060605108505114918, 0.0035257226461544635, 0, 0.0042454327363520864, 
  0.017304209023714066, 0, 0, 0.030237772795371709, 0, 0, 0, 
  0.018134191795252264}
Current bin is zero. 

On 2 nodes, looking at 682:
$2 = (Synapse &) @0x2c8f478: {weight_ = 0.011979404771700501, 
  delay_ = 2.7584519190713763, buffer_ = 0x2c5a868}
(gdb)  p *(&synapses_[65].buffer_->weightSum_[0])@20
$3 = {0, 0, 0, 0.015841135480441155, 0, 0, 0, 0, 0.0060605108505114918, 
  0 <repeats 11 times>}
Current bin is zero. 
 
So, at start, there are a whole lot of values in the spike queue in the 
one-node case that are absent in the two-node case. Key point is that some
are present in zero delay bins, where they should not be allowed as the
delay value is 2.something. Presumably some of these were going to be 
delivered later. I wonder if this is aliasing, going all around the delay
buffer for some long delays. As a quick check, I've doubled the buffer time.
One node:
 Vm100 = 0.00232901, 0.241473, 0.182856, 0.0102305, 0.0857292
Vm900 = 0.105924, 0.259907, 0.0230763, 0.107449
Two nodes:
Vm100 = 0.00232901, 0.241473, 0.182856, 0.0102305, 0.0857292
Vm900 = 0.0958305, 0.260413, 0.0324635, 0.107449

Still not fixed. Actually even the old buffer time was longer than the 
simulation run itself, so there should never have been any entries on the
first few bins.

Just realized that there was a hack where the 'time' field was used to 
send out the DataIndex of the calling object. Fixed.
1 node case:
 Vm100 = 0.00734036, 0.243398, 0.196683, 0, 0.0857292
Vm900 = 0.115276, 0.290259, 0.00774015, 0.107449
Two nodes:
 Vm100 = 0.00734036, 0.243398, 0.196683, 0, 0.0857292
Vm900 = 0.116182, 0.294987, 0.00774015, 0.107449

Nope, note the first and second entry in Vm900 don't quite match. So
on with the debugging.
Here is the buffer on Vm900 for 1 node:
(gdb) p *(&weightSum_[0])@40
$7 = {0, 0, 0.020633713547140359, 0.020450418312102557, 0, 
  0.0090267524495720869, 0, 0.016535398601554334, 0.027418064908124505, 
  0.0091998224612325433, 0.01710238739848137, 0.017760914247483016, 0, 0, 
  0.008282735007815064, 0.022528946460224686, 0.01175383843947202, 
  0.015918520665727555, 0, 0, 0.028642979008145629, 0 <repeats 19 times>}

Here it is for 2 nodes:
$4 = {0, 0, 0.013555899155326187, 0.020450418312102557, 0, 
  0.0090267524495720869, 0, 0.016535398601554334, 0, 0.0079548832122236495, 
  0.01710238739848137, 0, 0, 0, 0.008282735007815064, 0.022528946460224686, 0, 
  0, 0, 0, 0.016026323428377509, 0 <repeats 19 times>}

Actually this isn't a good test, because the time of the sample is after
all messaging for lower entry values in the first case, but before the
node0 entries reach the IntFire in the second.

Nevertheless, when I trace through (on 2 nodes) till the point where the 
first nonzero entry is actually used, it is still 0.013555899155326187.
So some spikes are just not reaching.

Perhaps I should change the nature of the problem and set it up so that
the IntFire calculations are independent of update order. At this point
this is supposedly done by the delay time but that is fragile. If the delay
is less than the timestep then it will fail. The ring buffer is also fragile,
not to mention wasteful and clumsy in how it is implemented.

For the IntFire, the obvious solution is to split the Vm computations off
from the message sending. It would be
	Init: Harvest the ring buffer for incoming spikes.
	Process: send if needed.
	Clock 9: exchange with nodes.

============================================================================
3 Jan 2014
I think I may have identified a (the?) problem. The spike gets delivered to
a different bin depending on whether it arrives before or after the 'pop'
of the ring buffer. Let's fix by passing in the current time to the ring
buffer upon pop, so that all incoming spikes have an absolute time reference.
This will also enable automatic buffer resizing, highly desirable.

This seems to work. Don't have to do the separation into init and process.
Also the ring buffer is much more robust now. The only thing we need to
guarantee is that the delay exceeds the step-size for the internode sync.

Checkin 4950.
If we revisit the time this took, it looks like this:
Week 1: Oct 12-19: Threading out
Week 2: Oct 19-26 Messaging refactoring
Week 3: Oct 26-Nov02: Element and field indexing refactoring. Data Handler out.
Week 4: Nov 02-12. Scheduling refactoring, recompile and unit tests. 
	Copy refactoring. MsgIds replaced with ObjIds. Automatic message compilation.
Week 4.5-5: Nov12-15 Standalone tests on mpi framework
Week 6: Nov 16-23 Multinode messaging design, implementation, compilation, 
	no tests.
Week 7: Nov 23-30 Multinode Set. Compile and test.
Week 8: Nov 30-Dec 7. Multinode Get. Most of the Shell commands work. SetVec.
Week 9: Dec 7-14. Multinode messaging. Much cleanup. Clears all unit tests 
	for 2 nodes. Began GetVec.
Week 10: Dec 14-21: GetVec works. Much bug stomping through scaling up to 
	any number of nodes. 
Dec 22: Clears all unit tests on any number of nodes from 1 to 16.
Week 11-12: Key semantic change that affected paths as well as SetVec and 
	GetVec: All element 'arrays' are treated as indexing of the last vector
	only. Thus FieldElements use fieldIndex and regular Elements use
	dataIndex. Many bugfixes. Set up unit test with IntFire array, 
	horrendous debugging, but it finally works on multiple nodes. The
	Python interface also updated to reflect all of these.

Next steps.
- Work with subha to get the Python interface up to speed
	+ Need to add a field to use vector of pairs of xy values, for filling 
		in the Sparse matrix.
	+ Write python version of IntFire test.
+ Fix Obj creation so one can't make up an invalid ObjId that way.
	Checkin 4958.
- Naming conventions. Need to dig up and redo.

============================================================================
4 Jan.
	Here are the naming conventions as noted by Niraj:
 - CamelCase, as before.
 - Append src fields with "Out".
 - Don't do anything special for dest fields and shared fields.
 * For set/get fields, start with 'set'/'get', followed by the field name exactly the way it is. For example, 'getn' for 'n', and 'getVm' for 'Vm'.

I don't remember the thought behind the last one: is the idea that I should
remove the automatic insertion of an underscore by the ValueFinfo code?

Confirmed this. Also put in code to automatically change the first char of the
field name to upper case, i.e., staty with camel case. Many unit test
fixes after this. Checkin as 4959.

============================================================================
5 Jan
Implemented change in name for src fields. I'm leaving 'output' as is though.
Went through cleaning out legacy naming and patching up the unit tests to
match. Now clears unit tests on 1-5 nodes.
Two other legacy SrcFinfo names are retained: 'process' and 'reinit'.
Checkin 4960.

Now porting over the ghastly kinetics directory. In the process redid the
Zombie function, this will need significant fix-ups in the code.

On the semantics of the mesh handling: 
- I want to be able to access volume reasonably fast, but not necessarily at
	 msg rates.
- I may need to have the mesh update all contained pools and reactions,
	including changing parameters and telling them to expand their arrays.

Msg-based handling:
	explicit
	immediate completion of operations as soon as mesh changed
	need to always set up messages when pools created, won't work without.
	Could end up with odd tree arrangement
	Coding needed now.

tree-based handling
	Implicit
	Would need to implement an internal traversal if mesh changes.
	A bit slower to get vols from pool.
	Dont' need the pool messages, could handle without.
	Could get funny scaling if pool moved to another compartment
	Coding deferred somewhat.
	Will need some nasty node-specific ops, perhaps new implementation?

I should really avoid having two ways to do things. If we wish to have a
	guarantee that all pools are within their parent compartment then
	the tree handling should apply.

============================================================================
6 Jan 2014
Discussed preferences on compartment policy with Harsha. She prefers the
tree handling. So let's go with that. It will break a lot of scripts.

Implemented this, relatively easy since all volume and compartment queries
were going through the lookupVolumeFromMesh set of commands.

Managed to compile all of the kinetics directory. Unfortunately there are 
rather a lot of dependencies on things like mesh and SimManager. Would
prefer to avoid. In doing so I fixed up the zombie handling code. The
policy is very simple now: the zombieSwap function allocates the data for
the new class, destroys the old data, and replaces the Cinfo on the Element,
all without touching any of the messaging. Perhaps should put in a test to
make sure that the swapped classes are message-compatible.

Checkin 4961.

Went through and cleaned up SrcFinfo names. Checkin 4962.
Went through and implemented the zombify functions. Checkin 4963.

Ported back the Mesh classes. Trying to compile.

============================================================================
7 Jan 2014
Fixed but in MsgElement.h: had not implemented zombieSwap. Checkin 4965.

Discussion with Subha. Things to fix.
- Implement bool ObjId::bad() const.
This handles cases of returning a bad ObjId (rather than root) and also cases
where a user makes an ObjId using direct indexing, and needs to check if it 
is valid.
- Fix up ShowMsg functions in Msg handler classes
- Clearly specify message semantics, particularly as they relate to 
	FieldElements. Single and Sparse are OK, but others need work.
		OneToAll: should traverse both the dataIndex and fieldIndex.
		OneToOne: Should go to only the last index: data or field as
			case may be.
		Diagonal: Also to last index.

OK, implemented bool ObjId::bad. 
Also replaced ObjId::dataId with ObjId::dataIndex for consistency.
============================================================================
