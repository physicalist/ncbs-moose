12 Oct 2013.
This is the async13 branch. It is based on the current (buildQ) branch and
is designed to test out asynchronous messaging in MOOSE. This will drop
all the multithreading and queueing code framework with a view to simplifying
and speeding up calculations. It is also meant to implement parallelization
from the ground up, using either MPI or Charm++ messaging.
It is meant to keep the old buildQ API as much as possible.

Compiled the reduced code base, got most of the unit tests to work. 
Checkin 4792.

Fixed a dangling 'else' statement in the main.cpp that was preventing the
unit tests from closing. Now it is OK. Checkin 4793.

Next: eliminate threading. 
	much eliminating later, it compiles but doesn't clear unit tests.

Stuck in Shell::doReinit in the Qinfo::buildOn/buildOff around line 686
Eliminated all buildOn/buildOff calls. Now it croaks much sooner in the
unit tests.

===========================================================================
13 Oct 2013.
Further cleanup. Moved the process loop into Clock. Gets some way through
unit tests but fails in testShellAddMsg(), seems that between Clock::doReinit
and doStart the message info isn't going around.
Checkin 4794.

===========================================================================
14 Oct 2013

Clever trick to break a python script at a specified place to use gdb:

import os
import signal

PID = os.getpid()

def do_nothing(*args):
    pass

def foo():
    print "Initializing..."
    a=10
    os.kill(PID, signal.SIGUSR1)
    print "Variable value is %d" % (a)
    print "All done!"

signal.signal(signal.SIGUSR1, do_nothing)

foo()

From Stack overflow by Michael Hoffman in 2011.
===========================================================================
16 Oct 2013.
Cleaning up threading references from Clock. Still doesn't get through the
unit tests, some issue with doReinit crops up in testBuiltins.
Some progress.
Current status is that the requestData()->send call is invoked, but data doesn't
get back in time for the reinit to complete. Need to investigate how the 
send call is handled.

===========================================================================
17 Oct 2013
Some more cleanup, it finally clears all unit tests. Checkin 4796.
Valgrind is totally happy too.

Next items:
Make benchmark for IntFire network.
Message rebuild/eliminate Qinfo.
Scheduling rebuild
Id and data handler rebuild
Synapse rebuild
Conv elimination
Prepacked buffer elimination
Parallel rebuild

===========================================================================
20 Oct 2013

Design of key sections. 

Data and field access:
Elements manage data as well as node info. All data are arrays. 
Msgs managed too. No FieldElements. No DataHandlers. Cinfo deals with Dinfo.
class Element {
	private:
		string name_;
		Id id_;
		const Cinfo* cinfo_;
		char* data_;
		unsigned int numData_;
		vector< indexRange > indexRange_; // Looks up indices by node.
		vector< MsgId > m_;
		vector< MsgTraverse > mt_;
};

Synapses are an array of fields on a Receptor. Not FieldElements.

Ids specify element and index of object entry.

Msgs specify Id and field index to fully specify Obj and field. Msg ops call 
back to Element, possibly Object.  For fast traversal, Msgs are digested into 
{funcPtr, Element ptr} sets with { Obj ptr, field} arrays. Each Obj index looks
up one such set of arrays, which can be shared for different funcs.

Scheduling: Tick 0 is base clock with a specified dt. All others are integral
multiples of it. Order within a given timestep is by tick#. No fieldElements,
just 10 MsgSrcs emanating from Clock, and internal array for the Multiples.

Conv: Used only to serialize off-node msgs, and to convert to and from strings,
and to provide rttiType to MOOSE.

Eref {
	Element* e_;
	DataId i_;	// An unsigned int for now
	// An unsigned int, used to pass field index but does not affect data 
	// lookup. Instead field index is extracted by the target EpFunc.
	FieldIndex f_;	
	char* data();
};

Prepacked buffer goes away

Parallel calls: At time of Msg digest, converted and condensed to single call
	per msg per node for outgoing. Likewise incoming msgs expanded out
	as if from original src Element, to all tgts on recipent node.
	Outgoing bufsend right away.
	Incoming polled at end of Tick by src element proxy.
	Transfer of objects by serialization + rebuild of all element node info
	+ rebuild of digested msgs.

I think that the Element and Msg stuff are pretty intertwined. But lets start
with the Msg and see how far we can go.

............................................................................
Alternative to this set is to have Synapses and other FieldElements
as independent Elements, using pointers cleverly to refer back to parent.
Possible disadvantage is some juggling to refer back to parent quickly.
Advantage is to eliminate FieldIndex.
Disadvantage is to proliferate Elements, many proxy Elements on all nodes.

For now trying fieldIndex for synapses.
===========================================================================
20 Oct 2013.
Begun massive refactoring of Msgs. To start with, redid the OpFuncs, 
EpFuncs and SetGet headers. Threw out UpFuncs and FieldElement stuff.
Now the headers seem consistent, but SrcFinfo::send calls need to be 
populated, which can only happen after I redo Element.

Compilation begun, far more to do. Checkin 4801.

===========================================================================
21 Oct 2013.
Marching through refactoring. Messaging there in skeleton but I'm not
satisfied. I would like a generic way to handle the following cases:
Traverse through single msgs, each with a unique Eref.
	This is what I've implemented for now.
Traverse through synapses, same Element but many indices and fieldIndices
Traverse through obj arrays: same Element, many indices, all fieldIndices = 0.
Traverse through 'all' obj: same element, auto through indices.

Won't try to do this through all field indices. Unclear where that would be
relevant.

I've eliminated the FieldElements and DataHandlers. This caused some
issues with the Cinfo element instances, which used to have FieldElements
for the SrcFinfos, destFinfos and so on.
I've hacked around it by explicitly creating Finfo elements as children of the
Cinfo elements. It compiles but looks precarious. Need to fix up Finfo
field access.
Checkin 4802.

===========================================================================
22 Oct 2013
Tediously slogging through compiling testAsync.cpp. Done, but the many
tests with IntFire and synapses are not so clean and will need some thought
on how better to set up this interface.
===========================================================================
25 Oct 2013

Back to Element lookup options. 

With just an integer data entry lookup:
- Synapses would need to be FieldElements, one per receptor
- Copies of neurons either to a new element tree, or arrayed in dataId.
- Need to be able to create child Elements on every data entry.
- Msg cleaner
- 

With data entry plus field entry lookup:
- Synapses could be to be FieldElements, one per receptor, or tables.
- Copies of neurons either to a new element tree, or arrayed in dataId.
- Do not need to be able to create child Elements on every data entry, but handy
- Msg has extra field.

===========================================================================
28 Oct

Some memory estimates.

Integer data lookup:
- Elements
	Suppose I have 1e6 neurons and 1e10 synapses.
	Assume I have 100 synapses per receptor, this is still about 1e8 synapse
	Elements. Somewhat strenuous if I attempt to put this all on each node.
- Msgs
	In sparse Msg form, it is essentially 2 longs per synapse, but only
	for synapses on local node. The higher level Msg would be negligibly 
	small.
- Synapse data
	2 doubles per synapse on the node. A modest circular buffer for the 
	queue.

Field entry lookup.
- Elements
	Here I would have 1 'synapse' per receptor on the parent neuron. A few
	hundred at best. Rest is just a matter of looking up indices.
	I could put the relevant Elements on each node easily.
- Msgs
	Same as above. 3 longs per synapse makes 
- Synapse data
	2 doubles per synapse on the node. A modest circular buffer for the 
	queue.


So it is pretty clear that the integer data lookup would require a different
way to set up messaging, one where the skeleton simulation cannot fit on one
node.

Note that I have to do Data entry mapping to nodes in either case.
Let's see what that looks like.

===========================================================================
1 Nov 2013
Data entry mapping.
Goals: 
	From any node, find quickly which node holds data.
		This doesn't have to be real-time: not used for sending msgs,
		but used for setup.
	Keep it compact. Can't have a full index of entire data list.

Design: 
	Element Ids: On all nodes
	Data Ids: Decomposed between nodes
	Field indices: Only on one node, that of the parent Data Id.

Use cases:
	- Neuronal model decomposed. Small # of neurons on each node, many 
	nodes, some degree of spatial organization helps, load balancing may 
	occur.  Little data transfer due to spiking.
	- 3D space decomposed in rdesigneur. Unpleasant large matrix, simple
	reacs but lots of diffusion. Heavy data transfer, each timestep for
	matrix solution. Very spatially organized, unlikely to do funny 
	balancing.
	- rdesigneur cell models: Each model on its own, doesn't need to talk
	to others except via spiking.
	- Spiking neuron model decomposed. Huge # of neurons and synapses on
	each node, many nodes, some spatial orgn. Load balancing may not be
	so useful. Lots of data transfer.
	- Large cluster vs small cluster/cores on a machine.

// Returns blocks
getNode( unsigned int numNodes, unsigned int numData, unsigned int dataId ) {
	return dataId * numNodes / numData;
}
// Returns nicely arrayed.
getNode( unsigned int numNodes, unsigned int numData, unsigned int dataId ) {
	return dataId % numNodes;
}
Now, what if load balancing has happened? Add check for that.
getNode( unsigned int numNodes, unsigned int numData, unsigned int dataId ) {
	if ( auto i = dataMap.find( dataId ) ) {
		return i->second;
	}
	return oldNodeMethod( numNodes, numData, dataId );
}
// May also have a situation where the data is on every node: a global.
getNode()
{
	return GlobalNode
}

So we just need a virtual class or function for this to sit on every Element.
Quite easy. Eventually will want to have a way to garbage collect if lots of
object movement has happened. Could even have a policy switch field, ugly though
it may seem.

Other thing is to put back the FieldElements, but have them point to the parent
data_.

For Msgs, since the MsgId is no longer needed for fast msg sending, I'll just
have the MsgId be a regular ObjId pointing to the message. 
