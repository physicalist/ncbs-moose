12 Oct 2013.
This is the async13 branch. It is based on the current (buildQ) branch and
is designed to test out asynchronous messaging in MOOSE. This will drop
all the multithreading and queueing code framework with a view to simplifying
and speeding up calculations. It is also meant to implement parallelization
from the ground up, using either MPI or Charm++ messaging.
It is meant to keep the old buildQ API as much as possible.

Compiled the reduced code base, got most of the unit tests to work. 
Checkin 4792.

Fixed a dangling 'else' statement in the main.cpp that was preventing the
unit tests from closing. Now it is OK. Checkin 4793.

Next: eliminate threading. 
	much eliminating later, it compiles but doesn't clear unit tests.

Stuck in Shell::doReinit in the Qinfo::buildOn/buildOff around line 686
Eliminated all buildOn/buildOff calls. Now it croaks much sooner in the
unit tests.

===========================================================================
13 Oct 2013.
Further cleanup. Moved the process loop into Clock. Gets some way through
unit tests but fails in testShellAddMsg(), seems that between Clock::doReinit
and doStart the message info isn't going around.
Checkin 4794.

===========================================================================
14 Oct 2013

Clever trick to break a python script at a specified place to use gdb:

import os
import signal

PID = os.getpid()

def do_nothing(*args):
    pass

def foo():
    print "Initializing..."
    a=10
    os.kill(PID, signal.SIGUSR1)
    print "Variable value is %d" % (a)
    print "All done!"

signal.signal(signal.SIGUSR1, do_nothing)

foo()

From Stack overflow by Michael Hoffman in 2011.
===========================================================================
16 Oct 2013.
Cleaning up threading references from Clock. Still doesn't get through the
unit tests, some issue with doReinit crops up in testBuiltins.
Some progress.
Current status is that the requestData()->send call is invoked, but data doesn't
get back in time for the reinit to complete. Need to investigate how the 
send call is handled.

===========================================================================
17 Oct 2013
Some more cleanup, it finally clears all unit tests. Checkin 4796.
Valgrind is totally happy too.

Next items:
Make benchmark for IntFire network.
Message rebuild/eliminate Qinfo.
Scheduling rebuild
Id and data handler rebuild
Synapse rebuild
Conv elimination
Prepacked buffer elimination
Parallel rebuild

===========================================================================
20 Oct 2013

Design of key sections. 

Data and field access:
Elements manage data as well as node info. All data are arrays. 
Msgs managed too. No FieldElements. No DataHandlers. Cinfo deals with Dinfo.
class Element {
	private:
		string name_;
		Id id_;
		const Cinfo* cinfo_;
		char* data_;
		unsigned int numData_;
		vector< indexRange > indexRange_; // Looks up indices by node.
		vector< MsgId > m_;
		vector< MsgTraverse > mt_;
};

Synapses are an array of fields on a Receptor. Not FieldElements.

Ids specify element and index of object entry.

Msgs specify Id and field index to fully specify Obj and field. Msg ops call 
back to Element, possibly Object.  For fast traversal, Msgs are digested into 
{funcPtr, Element ptr} sets with { Obj ptr, field} arrays. Each Obj index looks
up one such set of arrays, which can be shared for different funcs.

Scheduling: Tick 0 is base clock with a specified dt. All others are integral
multiples of it. Order within a given timestep is by tick#. No fieldElements,
just 10 MsgSrcs emanating from Clock, and internal array for the Multiples.

Conv: Used only to serialize off-node msgs, and to convert to and from strings,
and to provide rttiType to MOOSE.

Eref {
	Element* e_;
	DataId i_;	// An unsigned int for now
	// An unsigned int, used to pass field index but does not affect data 
	// lookup. Instead field index is extracted by the target EpFunc.
	FieldIndex f_;	
	char* data();
};

Prepacked buffer goes away

Parallel calls: At time of Msg digest, converted and condensed to single call
	per msg per node for outgoing. Likewise incoming msgs expanded out
	as if from original src Element, to all tgts on recipent node.
	Outgoing bufsend right away.
	Incoming polled at end of Tick by src element proxy.
	Transfer of objects by serialization + rebuild of all element node info
	+ rebuild of digested msgs.

I think that the Element and Msg stuff are pretty intertwined. But lets start
with the Msg and see how far we can go.

............................................................................
Alternative to this set is to have Synapses and other FieldElements
as independent Elements, using pointers cleverly to refer back to parent.
Possible disadvantage is some juggling to refer back to parent quickly.
Advantage is to eliminate FieldIndex.
Disadvantage is to proliferate Elements, many proxy Elements on all nodes.

For now trying fieldIndex for synapses.
===========================================================================
20 Oct 2013.
Begun massive refactoring of Msgs. To start with, redid the OpFuncs, 
EpFuncs and SetGet headers. Threw out UpFuncs and FieldElement stuff.
Now the headers seem consistent, but SrcFinfo::send calls need to be 
populated, which can only happen after I redo Element.

Compilation begun, far more to do. Checkin 4801.

===========================================================================
21 Oct 2013.
Marching through refactoring. Messaging there in skeleton but I'm not
satisfied. I would like a generic way to handle the following cases:
Traverse through single msgs, each with a unique Eref.
	This is what I've implemented for now.
Traverse through synapses, same Element but many indices and fieldIndices
Traverse through obj arrays: same Element, many indices, all fieldIndices = 0.
Traverse through 'all' obj: same element, auto through indices.

Won't try to do this through all field indices. Unclear where that would be
relevant.

I've eliminated the FieldElements and DataHandlers. This caused some
issues with the Cinfo element instances, which used to have FieldElements
for the SrcFinfos, destFinfos and so on.
I've hacked around it by explicitly creating Finfo elements as children of the
Cinfo elements. It compiles but looks precarious. Need to fix up Finfo
field access.
Checkin 4802.

===========================================================================
22 Oct 2013
Tediously slogging through compiling testAsync.cpp. Done, but the many
tests with IntFire and synapses are not so clean and will need some thought
on how better to set up this interface.
===========================================================================
25 Oct 2013

Back to Element lookup options. 

With just an integer data entry lookup:
- Synapses would need to be FieldElements, one per receptor
- Copies of neurons either to a new element tree, or arrayed in dataId.
- Need to be able to create child Elements on every data entry.
- Msg cleaner
- 

With data entry plus field entry lookup:
- Synapses could be to be FieldElements, one per receptor, or tables.
- Copies of neurons either to a new element tree, or arrayed in dataId.
- Do not need to be able to create child Elements on every data entry, but handy
- Msg has extra field.

===========================================================================
28 Oct

Some memory estimates.

Integer data lookup:
- Elements
	Suppose I have 1e6 neurons and 1e10 synapses.
	Assume I have 100 synapses per receptor, this is still about 1e8 synapse
	Elements. Somewhat strenuous if I attempt to put this all on each node.
- Msgs
	In sparse Msg form, it is essentially 2 longs per synapse, but only
	for synapses on local node. The higher level Msg would be negligibly 
	small.
- Synapse data
	2 doubles per synapse on the node. A modest circular buffer for the 
	queue.

Field entry lookup.
- Elements
	Here I would have 1 'synapse' per receptor on the parent neuron. A few
	hundred at best. Rest is just a matter of looking up indices.
	I could put the relevant Elements on each node easily.
- Msgs
	Same as above. 3 longs per synapse makes 
- Synapse data
	2 doubles per synapse on the node. A modest circular buffer for the 
	queue.


So it is pretty clear that the integer data lookup would require a different
way to set up messaging, one where the skeleton simulation cannot fit on one
node.

Note that I have to do Data entry mapping to nodes in either case.
Let's see what that looks like.

===========================================================================
1 Nov 2013
Data entry mapping.
Goals: 
	From any node, find quickly which node holds data.
		This doesn't have to be real-time: not used for sending msgs,
		but used for setup.
	Keep it compact. Can't have a full index of entire data list.

Design: 
	Element Ids: On all nodes
	Data Ids: Decomposed between nodes
	Field indices: Only on one node, that of the parent Data Id.

Use cases:
	- Neuronal model decomposed. Small # of neurons on each node, many 
	nodes, some degree of spatial organization helps, load balancing may 
	occur.  Little data transfer due to spiking.
	- 3D space decomposed in rdesigneur. Unpleasant large matrix, simple
	reacs but lots of diffusion. Heavy data transfer, each timestep for
	matrix solution. Very spatially organized, unlikely to do funny 
	balancing.
	- rdesigneur cell models: Each model on its own, doesn't need to talk
	to others except via spiking.
	- Spiking neuron model decomposed. Huge # of neurons and synapses on
	each node, many nodes, some spatial orgn. Load balancing may not be
	so useful. Lots of data transfer.
	- Large cluster vs small cluster/cores on a machine.

// Returns blocks
getNode( unsigned int numNodes, unsigned int numData, unsigned int dataId ) {
	return dataId * numNodes / numData;
}
// Returns nicely arrayed.
getNode( unsigned int numNodes, unsigned int numData, unsigned int dataId ) {
	return dataId % numNodes;
}
Now, what if load balancing has happened? Add check for that.
getNode( unsigned int numNodes, unsigned int numData, unsigned int dataId ) {
	if ( auto i = dataMap.find( dataId ) ) {
		return i->second;
	}
	return oldNodeMethod( numNodes, numData, dataId );
}
// May also have a situation where the data is on every node: a global.
getNode()
{
	return GlobalNode
}

So we just need a virtual class or function for this to sit on every Element.
Quite easy. Eventually will want to have a way to garbage collect if lots of
object movement has happened. Could even have a policy switch field, ugly though
it may seem.

Other thing is to put back the FieldElements, but have them point to the parent
data_.

Checkin 4818.
For Msgs, since the MsgId is no longer needed for fast msg sending, I'll just
have the MsgId be a regular ObjId pointing to the message. 

Should also add callbacks to Elements for add and delete messages.  These
are done as extra args in the Cinfo constructor, typically zero. 
The callback functions pass in Element and Msg target info.

Created FieldElements. Nothing there yet for FieldElementFinfos. Not yet
compiled through SparseMsg.cpp.

===========================================================================
2 Nov 2013
Grinding through compile. ElementValueFinfo and similar things need sorting.

===========================================================================
3 Nov 2013
Grinding through compile. Cleared Shell.cpp, but many things to do with path
are postponed.
I have completely redone the scheduling. Now the Clock does it all, and
much more simply and possibly faster. Eliminated Tick related objects.
Now compiles through the scheduling directory, but tests only as placeholders.
Now compiles through the builtins directory. 
Now a bit stuck with the Synapse and FieldElement, which I have sidestepped
so far. 

Desiderata: 
- Very fast Synapse handling. This requires that both the parent object
	and the synapse entry are rapidly accessed.
	I had thought I would do this by returning the base data entry, with
	the index from eref and lookup func for the Field from EpFunc.
	Needs an extra lookup compared to the regular Msg because we need to
		engage the parent class as well as the specific Synapse.
	addSpike( double time ) {
		ringBuffer->addEvent( time + delay_, weight_ );
	}
	This requires the Synapse to a) have a pointer to the ring buffer,
		and b) the parent class have a ring buffer in it to use.

	A more complicated one might also look at association with postsyn Vm,
	but that would be computed at time of arrival, handled by the Receptor.
	If we do this by passing pointers in from the UpFunc we could refer
	to the Receptor level:
	Receptor::addSpike( const Eref& e, double time ) {
		const Synapse& syn = synapses_[e.fieldIndex()];
		ringBuffer.addEvent( time + syn.delay(), syn.weight() );
	}
	This requires that there be a common base class for the Receptors that
	all Synapses could use.
	Or we could refer to the Synapse level:
	Synapse::addSpike( const Eref& e, double time ) {
		auto rec = reinterpret_cast< Receptor* >( e.plainData() )
		rec->ringBuffer.addEvent( time + delay_, weight_ );
	}
	This too requires a common base class for the Receptors.

- Easy access to the Field, preferably deal directly with its pointer.
	This is certainly doable with the lookup, and even more so with
	the precomputed Msg ptrs, but then how to handle parent?
	In cases where the parent isn't needed, we could design an EpFunc
	to directly call the Field.
	The first option above would do this directly.

The first option is nicest, also fits well with the idea of single precomputed
target pointers for messages. Costs an extra pointer in each Synapse. Minor.
This implements very smoothly, but there is another consideration. 
The synapse class needs to provide a callback so that when additional spike 
messages come in, the synapse buffer is expanded to match.
- Callback detects message addition and removal
- Callback would modify # of synapses.
- Callback may assign a fieldIndex to the message itself.

This is fine, but in the Synapse we have a problem because the callback
needs to talk to the parent SynHandler if it wants to resize the Synapse array.
Easily handled by looking up the parent ObjId of the Synapse.

In the process of setting up the SynHandler. Next:
+Fix up Msgs to do the precompiled fast lookup
+Fix up FieldElement stuff to access synapses from Eref
Fix up FieldElement and Element stuff to deal with callbacks
Fix up Msgs to use ObjIds to identify
+Fix up Msgs to handle the specification of fieldIndex.
Work out how to talk to nodes on parallel machines. Socket?
Work out how to do SetGet on parallel machines
Work out parallel messaging.

===========================================================================
4 Nov 2013.
Fixing up testAsync.cpp:testSetGetSynapse, now that I've restructured
the FieldElement stuff.

May need to go back and redefine ObjId so it specifies the fieldIndex too, 
just like the Eref.

Getting close to compilation completing. Had to go back to TableBase.

Stuck a bit in Table.cpp, figure out what to do with recvDataBuf.
===========================================================================
5 Nov 2013
Finally cleared first pass of compilation, link still fails with lots of 
unresolved functions. Checkin 4824.
Finally compiles. Doesn't run.

Many nasty bugs later, I have now to deal with the MsgDigest for fast 
messaging. In testAsync::testSendMsg.
===========================================================================
6 Nov 2013
The 'send' command has now taken shape with the MsgDigest. It is far from
as efficient as envisioned:

1. SrcFinfo->getBindIndex()
2. Compute offset of specific DataId * specific src func.
3. Look up MsgDigest vector from Eref.
4. Iterate through MsgDigest
5. Lookup function
6. Iterate through targets
7. Check if they are ALLDATA
	8a: Make temp eref
	8b: lookup numData
	8c: Create reference k
	8d: Iterate over ALLDATA
	8e: call function
else
	8a. Call function.

Here are some cases:
				# ops
Single object target:		8 + F
N unique object targets		5 + 3N + NF
Single Alldata target 
	with X entries		10+2X + XF
N Alldata target 
	with X entries each	5 + 5N + 2X + NXF

F is cost of function.

Immediate goal: Have the Msg do its own digest... No, need to store
this in Element because there would be a number of targets in each digest.

Now setting up Msg::sources and Msg::targets() to build digest.
===========================================================================
7 Nov.
Seems like some messages are beginning to go...
Now about 10 unit tests in. 

Stuck in SparseMsg.cpp:211. At this point there are no fields allocated,
so we're not finding any targets.
===========================================================================
8 Nov.
Steady progress. Some things to consider:
- Should we retain ext fields? Python deals with this kind of use mostly.
	If not we could simplify the function for checkSetGet.
- Need to redo the Msg as object tests in the unit tests

Now clears testAsync.
Fails in testMsg.cpp, but both those tests require some major pending updates:
to the ObjIds handling Msg, and for automatic updating of MsgDigest.

So commented them out.

Implemented the ObjId based Element tree structure. This lets individual
DataEntries have their own child Elements.
Yet to compile and the check is in testShell.cpp:121

===========================================================================
9 Nov.
A design and semantics issue with the capability to have each ObjId be parent
to a sub-tree: What happens when we replicate the entire tree?

Suppose, on a/b[100], we create a unique element child c[50] on 
	a/b[23]

Now we use createmap or equivalent to make 10 copies of a. This does:
	a[10]/b[1000]
What about c in this? My reading is that we should get

	a[1..10]/b[23,123,223,323,423,523,623,723,823,923]/c

where in each case c is a separate Element. 
The reason is that we do not permit a given Element to be parented by many
different objects. One could in principle do even this through sparse msgs,
but I think the semantics get to be too untidy.
Other aspect of the semantics is that the parent of any entry in b would be 
a[0]. So there is a special status given to those Elements parented off zero.

Yet another point: if I have a FieldElement, its parent dataIndex should be
identical to its own dataIndex. Implemented.

Now the Neutral::path is working well. Also went through and cleaned out
all uses of Id::operator()() since they were confusing.

Further progress, now clears testScheduling.

A bit stuck on destroying Elements. Currently cinfo_ is set to zero to tell
the Elements that they shoult NOT remove the msgs. cinfo_ isn't a good way
to do this because it is needed later. So we could set the id_ to zero.
But I may need to bifurcate the Element class so that the base class of 
FieldElement doesn't try to destroy the data.

Tried it. It was rather elegant: all the messaging stuff went into the
ElementBase, and the data stuff into Element - just like the FieldElement
was just doing data. But all the rest of the infrastructure refers to
Element and assumes that FieldElement is derived from it.

One way out might be to retain the split, but have the base class be Element,
and have a DataElement as counterpart to FieldElement.

Create functions would refer though to the DataElement and the FieldElement.
Lots of that.
I suspect that the DataElement will need considerable expansion with the
new multinode code. May as well anticipate.

So I redid this with the base class as Element, and DataElement that which
handles regular object arrays as data. FieldElements stay as is, now
quite symmetrical with DataElements.
Compiles and gets a little further with unit tests.

===========================================================================
