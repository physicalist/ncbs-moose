12 Oct 2013.
This is the async13 branch. It is based on the current (buildQ) branch and
is designed to test out asynchronous messaging in MOOSE. This will drop
all the multithreading and queueing code framework with a view to simplifying
and speeding up calculations. It is also meant to implement parallelization
from the ground up, using either MPI or Charm++ messaging.
It is meant to keep the old buildQ API as much as possible.

Compiled the reduced code base, got most of the unit tests to work. 
Checkin 4792.

Fixed a dangling 'else' statement in the main.cpp that was preventing the
unit tests from closing. Now it is OK. Checkin 4793.

Next: eliminate threading. 
	much eliminating later, it compiles but doesn't clear unit tests.

Stuck in Shell::doReinit in the Qinfo::buildOn/buildOff around line 686
Eliminated all buildOn/buildOff calls. Now it croaks much sooner in the
unit tests.

===========================================================================
13 Oct 2013.
Further cleanup. Moved the process loop into Clock. Gets some way through
unit tests but fails in testShellAddMsg(), seems that between Clock::doReinit
and doStart the message info isn't going around.
Checkin 4794.

===========================================================================
14 Oct 2013

Clever trick to break a python script at a specified place to use gdb:

import os
import signal

PID = os.getpid()

def do_nothing(*args):
    pass

def foo():
    print "Initializing..."
    a=10
    os.kill(PID, signal.SIGUSR1)
    print "Variable value is %d" % (a)
    print "All done!"

signal.signal(signal.SIGUSR1, do_nothing)

foo()

From Stack overflow by Michael Hoffman in 2011.
===========================================================================
16 Oct 2013.
Cleaning up threading references from Clock. Still doesn't get through the
unit tests, some issue with doReinit crops up in testBuiltins.
Some progress.
Current status is that the requestData()->send call is invoked, but data doesn't
get back in time for the reinit to complete. Need to investigate how the 
send call is handled.

===========================================================================
17 Oct 2013
Some more cleanup, it finally clears all unit tests. Checkin 4796.
Valgrind is totally happy too.

Next items:
Make benchmark for IntFire network.
Message rebuild/eliminate Qinfo.
Scheduling rebuild
Id and data handler rebuild
Synapse rebuild
Conv elimination
Prepacked buffer elimination
Parallel rebuild

===========================================================================
20 Oct 2013

Design of key sections. 

Data and field access:
Elements manage data as well as node info. All data are arrays. 
Msgs managed too. No FieldElements. No DataHandlers. Cinfo deals with Dinfo.
class Element {
	private:
		string name_;
		Id id_;
		const Cinfo* cinfo_;
		char* data_;
		unsigned int numData_;
		vector< indexRange > indexRange_; // Looks up indices by node.
		vector< MsgId > m_;
		vector< MsgTraverse > mt_;
};

Synapses are an array of fields on a Receptor. Not FieldElements.

Ids specify element and index of object entry.

Msgs specify Id and field index to fully specify Obj and field. Msg ops call 
back to Element, possibly Object.  For fast traversal, Msgs are digested into 
{funcPtr, Element ptr} sets with { Obj ptr, field} arrays. Each Obj index looks
up one such set of arrays, which can be shared for different funcs.

Scheduling: Tick 0 is base clock with a specified dt. All others are integral
multiples of it. Order within a given timestep is by tick#. No fieldElements,
just 10 MsgSrcs emanating from Clock, and internal array for the Multiples.

Conv: Used only to serialize off-node msgs, and to convert to and from strings,
and to provide rttiType to MOOSE.

Eref {
	Element* e_;
	DataId i_;	// An unsigned int for now
	// An unsigned int, used to pass field index but does not affect data 
	// lookup. Instead field index is extracted by the target EpFunc.
	FieldIndex f_;	
	char* data();
};

Prepacked buffer goes away

Parallel calls: At time of Msg digest, converted and condensed to single call
	per msg per node for outgoing. Likewise incoming msgs expanded out
	as if from original src Element, to all tgts on recipent node.
	Outgoing bufsend right away.
	Incoming polled at end of Tick by src element proxy.
	Transfer of objects by serialization + rebuild of all element node info
	+ rebuild of digested msgs.

I think that the Element and Msg stuff are pretty intertwined. But lets start
with the Msg and see how far we can go.

............................................................................
Alternative to this set is to have Synapses and other FieldElements
as independent Elements, using pointers cleverly to refer back to parent.
Possible disadvantage is some juggling to refer back to parent quickly.
Advantage is to eliminate FieldIndex.
Disadvantage is to proliferate Elements, many proxy Elements on all nodes.

For now trying fieldIndex for synapses.
===========================================================================
20 Oct 2013.
Begun massive refactoring of Msgs. To start with, redid the OpFuncs, 
EpFuncs and SetGet headers. Threw out UpFuncs and FieldElement stuff.
Now the headers seem consistent, but SrcFinfo::send calls need to be 
populated, which can only happen after I redo Element.

Compilation begun, far more to do. Checkin 4801.

===========================================================================
21 Oct 2013.
Marching through refactoring. Messaging there in skeleton but I'm not
satisfied. I would like a generic way to handle the following cases:
Traverse through single msgs, each with a unique Eref.
	This is what I've implemented for now.
Traverse through synapses, same Element but many indices and fieldIndices
Traverse through obj arrays: same Element, many indices, all fieldIndices = 0.
Traverse through 'all' obj: same element, auto through indices.

Won't try to do this through all field indices. Unclear where that would be
relevant.

I've eliminated the FieldElements and DataHandlers. This caused some
issues with the Cinfo element instances, which used to have FieldElements
for the SrcFinfos, destFinfos and so on.
I've hacked around it by explicitly creating Finfo elements as children of the
Cinfo elements. It compiles but looks precarious. Need to fix up Finfo
field access.
Checkin 4802.

===========================================================================
22 Oct 2013
Tediously slogging through compiling testAsync.cpp. Done, but the many
tests with IntFire and synapses are not so clean and will need some thought
on how better to set up this interface.
===========================================================================
25 Oct 2013

Back to Element lookup options. 

With just an integer data entry lookup:
- Synapses would need to be FieldElements, one per receptor
- Copies of neurons either to a new element tree, or arrayed in dataId.
- Need to be able to create child Elements on every data entry.
- Msg cleaner
- 

With data entry plus field entry lookup:
- Synapses could be to be FieldElements, one per receptor, or tables.
- Copies of neurons either to a new element tree, or arrayed in dataId.
- Do not need to be able to create child Elements on every data entry, but handy
- Msg has extra field.

===========================================================================
28 Oct

Some memory estimates.

Integer data lookup:
- Elements
	Suppose I have 1e6 neurons and 1e10 synapses.
	Assume I have 100 synapses per receptor, this is still about 1e8 synapse
	Elements. Somewhat strenuous if I attempt to put this all on each node.
- Msgs
	In sparse Msg form, it is essentially 2 longs per synapse, but only
	for synapses on local node. The higher level Msg would be negligibly 
	small.
- Synapse data
	2 doubles per synapse on the node. A modest circular buffer for the 
	queue.

Field entry lookup.
- Elements
	Here I would have 1 'synapse' per receptor on the parent neuron. A few
	hundred at best. Rest is just a matter of looking up indices.
	I could put the relevant Elements on each node easily.
- Msgs
	Same as above. 3 longs per synapse makes 
- Synapse data
	2 doubles per synapse on the node. A modest circular buffer for the 
	queue.


So it is pretty clear that the integer data lookup would require a different
way to set up messaging, one where the skeleton simulation cannot fit on one
node.

Note that I have to do Data entry mapping to nodes in either case.
Let's see what that looks like.

===========================================================================
1 Nov 2013
Data entry mapping.
Goals: 
	From any node, find quickly which node holds data.
		This doesn't have to be real-time: not used for sending msgs,
		but used for setup.
	Keep it compact. Can't have a full index of entire data list.

Design: 
	Element Ids: On all nodes
	Data Ids: Decomposed between nodes
	Field indices: Only on one node, that of the parent Data Id.

Use cases:
	- Neuronal model decomposed. Small # of neurons on each node, many 
	nodes, some degree of spatial organization helps, load balancing may 
	occur.  Little data transfer due to spiking.
	- 3D space decomposed in rdesigneur. Unpleasant large matrix, simple
	reacs but lots of diffusion. Heavy data transfer, each timestep for
	matrix solution. Very spatially organized, unlikely to do funny 
	balancing.
	- rdesigneur cell models: Each model on its own, doesn't need to talk
	to others except via spiking.
	- Spiking neuron model decomposed. Huge # of neurons and synapses on
	each node, many nodes, some spatial orgn. Load balancing may not be
	so useful. Lots of data transfer.
	- Large cluster vs small cluster/cores on a machine.

// Returns blocks
getNode( unsigned int numNodes, unsigned int numData, unsigned int dataId ) {
	return dataId * numNodes / numData;
}
// Returns nicely arrayed.
getNode( unsigned int numNodes, unsigned int numData, unsigned int dataId ) {
	return dataId % numNodes;
}
Now, what if load balancing has happened? Add check for that.
getNode( unsigned int numNodes, unsigned int numData, unsigned int dataId ) {
	if ( auto i = dataMap.find( dataId ) ) {
		return i->second;
	}
	return oldNodeMethod( numNodes, numData, dataId );
}
// May also have a situation where the data is on every node: a global.
getNode()
{
	return GlobalNode
}

So we just need a virtual class or function for this to sit on every Element.
Quite easy. Eventually will want to have a way to garbage collect if lots of
object movement has happened. Could even have a policy switch field, ugly though
it may seem.

Other thing is to put back the FieldElements, but have them point to the parent
data_.

Checkin 4818.
For Msgs, since the MsgId is no longer needed for fast msg sending, I'll just
have the MsgId be a regular ObjId pointing to the message. 

Should also add callbacks to Elements for add and delete messages.  These
are done as extra args in the Cinfo constructor, typically zero. 
The callback functions pass in Element and Msg target info.

Created FieldElements. Nothing there yet for FieldElementFinfos. Not yet
compiled through SparseMsg.cpp.

===========================================================================
2 Nov 2013
Grinding through compile. ElementValueFinfo and similar things need sorting.

===========================================================================
3 Nov 2013
Grinding through compile. Cleared Shell.cpp, but many things to do with path
are postponed.
I have completely redone the scheduling. Now the Clock does it all, and
much more simply and possibly faster. Eliminated Tick related objects.
Now compiles through the scheduling directory, but tests only as placeholders.
Now compiles through the builtins directory. 
Now a bit stuck with the Synapse and FieldElement, which I have sidestepped
so far. 

Desiderata: 
- Very fast Synapse handling. This requires that both the parent object
	and the synapse entry are rapidly accessed.
	I had thought I would do this by returning the base data entry, with
	the index from eref and lookup func for the Field from EpFunc.
	Needs an extra lookup compared to the regular Msg because we need to
		engage the parent class as well as the specific Synapse.
	addSpike( double time ) {
		ringBuffer->addEvent( time + delay_, weight_ );
	}
	This requires the Synapse to a) have a pointer to the ring buffer,
		and b) the parent class have a ring buffer in it to use.

	A more complicated one might also look at association with postsyn Vm,
	but that would be computed at time of arrival, handled by the Receptor.
	If we do this by passing pointers in from the UpFunc we could refer
	to the Receptor level:
	Receptor::addSpike( const Eref& e, double time ) {
		const Synapse& syn = synapses_[e.fieldIndex()];
		ringBuffer.addEvent( time + syn.delay(), syn.weight() );
	}
	This requires that there be a common base class for the Receptors that
	all Synapses could use.
	Or we could refer to the Synapse level:
	Synapse::addSpike( const Eref& e, double time ) {
		auto rec = reinterpret_cast< Receptor* >( e.plainData() )
		rec->ringBuffer.addEvent( time + delay_, weight_ );
	}
	This too requires a common base class for the Receptors.

- Easy access to the Field, preferably deal directly with its pointer.
	This is certainly doable with the lookup, and even more so with
	the precomputed Msg ptrs, but then how to handle parent?
	In cases where the parent isn't needed, we could design an EpFunc
	to directly call the Field.
	The first option above would do this directly.

The first option is nicest, also fits well with the idea of single precomputed
target pointers for messages. Costs an extra pointer in each Synapse. Minor.
This implements very smoothly, but there is another consideration. 
The synapse class needs to provide a callback so that when additional spike 
messages come in, the synapse buffer is expanded to match.
- Callback detects message addition and removal
- Callback would modify # of synapses.
- Callback may assign a fieldIndex to the message itself.

This is fine, but in the Synapse we have a problem because the callback
needs to talk to the parent SynHandler if it wants to resize the Synapse array.
Easily handled by looking up the parent ObjId of the Synapse.

In the process of setting up the SynHandler. Next:
+Fix up Msgs to do the precompiled fast lookup
+Fix up FieldElement stuff to access synapses from Eref
Fix up FieldElement and Element stuff to deal with callbacks
Fix up Msgs to use ObjIds to identify
+Fix up Msgs to handle the specification of fieldIndex.
Work out how to talk to nodes on parallel machines. Socket?
Work out how to do SetGet on parallel machines
Work out parallel messaging.

===========================================================================
4 Nov 2013.
Fixing up testAsync.cpp:testSetGetSynapse, now that I've restructured
the FieldElement stuff.

May need to go back and redefine ObjId so it specifies the fieldIndex too, 
just like the Eref.

Getting close to compilation completing. Had to go back to TableBase.

Stuck a bit in Table.cpp, figure out what to do with recvDataBuf.
===========================================================================
5 Nov 2013
Finally cleared first pass of compilation, link still fails with lots of 
unresolved functions. Checkin 4824.
Finally compiles. Doesn't run.

Many nasty bugs later, I have now to deal with the MsgDigest for fast 
messaging. In testAsync::testSendMsg.
===========================================================================
6 Nov 2013
The 'send' command has now taken shape with the MsgDigest. It is far from
as efficient as envisioned:

1. SrcFinfo->getBindIndex()
2. Compute offset of specific DataId * specific src func.
3. Look up MsgDigest vector from Eref.
4. Iterate through MsgDigest
5. Lookup function
6. Iterate through targets
7. Check if they are ALLDATA
	8a: Make temp eref
	8b: lookup numData
	8c: Create reference k
	8d: Iterate over ALLDATA
	8e: call function
else
	8a. Call function.

Here are some cases:
				# ops
Single object target:		8 + F
N unique object targets		5 + 3N + NF
Single Alldata target 
	with X entries		10+2X + XF
N Alldata target 
	with X entries each	5 + 5N + 2X + NXF

F is cost of function.

Immediate goal: Have the Msg do its own digest... No, need to store
this in Element because there would be a number of targets in each digest.

Now setting up Msg::sources and Msg::targets() to build digest.
===========================================================================
7 Nov.
Seems like some messages are beginning to go...
Now about 10 unit tests in. 

Stuck in SparseMsg.cpp:211. At this point there are no fields allocated,
so we're not finding any targets.
===========================================================================
8 Nov.
Steady progress. Some things to consider:
- Should we retain ext fields? Python deals with this kind of use mostly.
	If not we could simplify the function for checkSetGet.
- Need to redo the Msg as object tests in the unit tests

Now clears testAsync.
Fails in testMsg.cpp, but both those tests require some major pending updates:
to the ObjIds handling Msg, and for automatic updating of MsgDigest.

So commented them out.

Implemented the ObjId based Element tree structure. This lets individual
DataEntries have their own child Elements.
Yet to compile and the check is in testShell.cpp:121

===========================================================================
9 Nov.
A design and semantics issue with the capability to have each ObjId be parent
to a sub-tree: What happens when we replicate the entire tree?

Suppose, on a/b[100], we create a unique element child c[50] on 
	a/b[23]

Now we use createmap or equivalent to make 10 copies of a. This does:
	a[10]/b[1000]
What about c in this? My reading is that we should get

	a[1..10]/b[23,123,223,323,423,523,623,723,823,923]/c

where in each case c is a separate Element. 
The reason is that we do not permit a given Element to be parented by many
different objects. One could in principle do even this through sparse msgs,
but I think the semantics get to be too untidy.
Other aspect of the semantics is that the parent of any entry in b would be 
a[0]. So there is a special status given to those Elements parented off zero.

Yet another point: if I have a FieldElement, its parent dataIndex should be
identical to its own dataIndex. Implemented.

Now the Neutral::path is working well. Also went through and cleaned out
all uses of Id::operator()() since they were confusing.

Further progress, now clears testScheduling.

A bit stuck on destroying Elements. Currently cinfo_ is set to zero to tell
the Elements that they shoult NOT remove the msgs. cinfo_ isn't a good way
to do this because it is needed later. So we could set the id_ to zero.
But I may need to bifurcate the Element class so that the base class of 
FieldElement doesn't try to destroy the data.

Tried it. It was rather elegant: all the messaging stuff went into the
ElementBase, and the data stuff into Element - just like the FieldElement
was just doing data. But all the rest of the infrastructure refers to
Element and assumes that FieldElement is derived from it.

One way out might be to retain the split, but have the base class be Element,
and have a DataElement as counterpart to FieldElement.

Create functions would refer though to the DataElement and the FieldElement.
Lots of that.
I suspect that the DataElement will need considerable expansion with the
new multinode code. May as well anticipate.

So I redid this with the base class as Element, and DataElement that which
handles regular object arrays as data. FieldElements stay as is, now
quite symmetrical with DataElements.
Compiles and gets a little further with unit tests.

===========================================================================
10 Nov 2013.
Need to sort out semantics of copying with the provided multiplier.

consider: /a[10]/b[20]
copy /a /z
The bare copy should traverse the tree and produce an identical tree:
	/z/a[10]/b[20]

copy /a /z 5
Should I reduce a to 5, or make 5 copies of the whole thing? Or just 
forbid the multiple copy operation and reserve it for createmap type calls?

Clearly any deep copy should preserve the child indexing. So I cannot
do any reductions, will have to copy integral multiples of the whole tree.
Should I only allow unit copies? No, keep the feature, and use it for
more complicated createmap type calls.

Another case. I may need to do a specific object copy:

Copy a[3] /z
Here the rest of the tree is ignored unless the there is a tree rooted on a[3].
This precludes doing a copy of a[0] unless I implement a special parental
message that goes from the element as a whole. But even that would return an
index if queried.
Don't worry about this for now.

Did substantial cleanup on path handling.
Now completes testShell.

Trying to get clock to run a simulation in testBuiltins. The reinit::send
command fails because it doesn't find anything in the MsgDigest.

Ran into trouble with requestData calls. These were handled specially, 
and the new framework doesn't like them. There is the opportunity to do them
really simply, for example, pass the function a reference to be filled in
by the target 'get' function. Backward compatibility means that I can't now
change all the A get() functions to get( A& ), that would have been the 
simplest.
I've implemented a skeleton of a very simple GetOpFunc which just takes
a pointer as an argument, and dumps the value into the pointer. Problem
comes if I want to call such a function across nodes. Will everything
grind to a halt while I wait for the request to go and the response to come?
What if there is a deadlock: two nodes waiting on each others' returns?
Will have to set up the off-node 'get' call as a loop polling for the
answer, and updating any calls that the off-nodes make.

In the meantime, carried on with debugging, and now it clears unit tests but
with a segv at the end. Checkin 4837.

Much work with valgrind later, now it is completely clean. No segv either.
Checkin 4838.

List from 3 Nov, updated here:
*Eliminate multithreading
*Change to integral ticks for scheduling.
*Fix up Msgs to do the precompiled fast lookup
*Fix up FieldElement stuff to access synapses from Eref
Fix up FieldElement and Element stuff to deal with callbacks
*Fix up Msgs to use ObjIds to identify
*Elements should know when to call the MsgDigest.
*Fix up Msgs to handle the specification of fieldIndex.
Work out how to talk to nodes on parallel machines. Socket?
Work out how to do SetGet on parallel machines
Work out parallel messaging.


Starting on the ObjId identification of Msgs.
Did a bit in OneToOneMsg.cpp and OneToAll.h

===========================================================================
11 Nov 2013.
Callbacks
	- Message changes: add, drop, reconfigure: need message id.
	- Scheduling/clockdt changes
	- Changes in node decomposition (?)


Eliminated MsgIds, replaced with ObjIds. The change makes it much easier to
handle Msgs. Compiled but doesn't clear any tests yet. Checkin 4839.

Discrepancy in ptr looked up from mid to created ptr.

Fixed. Then much messing around trying to get the system to quit cleanly.
Eventually did by brute force, rather than the recursive removal of objects,
since the removal of the objects handling messages caused problems with
subsequent removals.
Something like an anti-bootstrapping.
Now terminates cleanly and valgrind is happy. Checkin 4840.
Some tests to do with this before going on to automatically deciding when to
do digestMessages.
Restored a few unit tests for the various msg classes. Not too bad.

For: Elements should know when to call the MsgDigest.
I'll have it so that the Msgs call each of their target Elements to set
a 'dirty' flag when the Msgs change. This flag is checked at the time of
reinit or process, but no. Should not be handled by the Object.
Options:
	- put in a conditional in every Send. In fact the act of 
	getting the MsgDigest vector could do this. e.msgDigest.

Starting with an 'isDirty' flag for messages that change. Need to fill in.
===========================================================================
12 Nov 2013.
Implemented the isDirty flag, but called it isRewired. 
Reactivated some of the unit tests on message field assignment. All cleared.

Considering the callbacks.
- Bypass altogether. Let the user track changes and ensure that the right
	number of synapses are defined. Possibly give a helper function.
	Possibly help with offsets for each incoming SparseMsg so one does
	not lose all assigned Weights/delays of one when the other changes.
- Give FieldElements an automatic scan of incoming msgs to check for field
	size. Activate this whenever the fields are accessed.
- Put in a function in SparseMsg::randomConnect and its
	ilk, to assign synapse::numField.
	Problem is that there may be other messages. Could cobble something
	together for the SparseMsg objects, but there are other possible Msg
	types which should not need to know this stuff.
- There are prototype callback functions in Synapse: 
	addMsgCallback and dropMsgCallback. But the message involved would
	have to know what to do with the info about existing synapses.

Summary: Need to provide offsets for each Msg. Rest is up to user. Need use
	cases.

General things to fix:
	- Use new 'send' command for all the Srcfinfos.

For now, bypass altogether. Move on to multinode.
- Fill in send buffers:
	- Message digest consolidation
	- Arg conversion.
- Figure out best design for internode comms. 
- simple block mapping of DataId to rawIndex.
- Global objects and synchronization
- Communication between nodes for setup operations
- Communication between nodes for Set/Get operations
- Clock ticks between nodes.
- Messaging between nodes.

Working on standalone prototype for the MPI. Stuck.
Some useful tricks for debugging:

mpicxx -g proc2.cpp
mpirun -np 2 xterm -e gdb a.out

Possibly the problem is that the local node has not filled in any entry
in recvReq. Yes. Now compiles, but the run goes into an infinite loop.
===========================================================================
13 Nov 2013.
Incremental debugging, now looks close. proc2.cpp.
Works but it isn't clear to me why there is the present limit on the number
of nodes it can handle. I would have thought N^2 = numCalls would be the limit.
But it works for powers of 2 up to 128, which is as far as my laptop will
manage. Possibly I shouldn't have so many Irecvs dangling at a time.

Next to set up an AllToAll type transfer to see if that goes faster. Certainly
looks simpler.
Working in proc3.cpp

Then put in handling for sporadic big mesages.

Then do benchmarking on ghevar and on cluster.

===========================================================================
14 Nov 2013. proc3 works. The
time tests are all over the place but proc3 does seem a little slower than
the proc2. Perhaps I could do another version with non-blocking calls.

Looking at how regular msgs put stuff in send bufs for off-node delivery.
- Element::putFuncsInOrder: Check if tgt is off-node, put in a dummy
	OpFunc that stores MsgBindIndex. Tgt will come from eref in 
	SrcFinfo::send.
- Derivative of OpFuncBase that also stores msgBindIndex. Need virtual 
	constructor
	function from OpFuncBase so that the PutFuncsInOrder can synthesize it.
	- Derivative needs manage the convs and buffer
- Converter that takes arguments and bungs into provided double buffer.
	Conv.h needs to be stripped down and redone.
	static const T& buf2val( const double* buf, unsigned int& size )
	static const unsigned int val2buf( const T& val, double* buf );
- Collapse all same-node targets for the same func, into just one entry
	for any given node.  In Element::putTargetsInDigest
- Someone to manage the buffers. Even if they are global. Could have a
	special class, postmaster?
	- Presumably same object (postmaster) to send stuff to other 
	nodes on tick.
	- Same object to parse arrived buffers and send out.
	- Quite a bit more here.
- Need to set up the node map on Elements too.
- Decide on buf structure. Say 1 double for size + 1 double for originating
	ObjId, + double for bindIndex. Assorted doubles for arguments.
	Actually make a header structure and use that, however many doubles
	it takes.

Started with Conv.h



===========================================================================
16 Nov 2013.
Filling in blanks for multinode stuff.
OpFuncBase now has something
DataElement now knows more about node handling.
Need to put buffers somewhere standard: postmaster?
postmaster needs to be elaborated.

- Global objects need to have assignments and sends go to all nodes.
	Actually just define a special DataElement subclass.

PostMaster::clearPending:
	Goes through recvBuf. 
		calls SrcFinfo::sendBuffer with buffer
			SrcFinfo converts to arguments
			Sends to all targets on specified bindIndex
			(which is a function of this SrcFinfo)
			Originating Eref is fully specified so
			there is no ambiguity about which subset of
			msgs srcs were involved.
			The MsgDigest must digest off-node stuff into
			same array.


 Here is an outline of how messages go between nodes, from PostMaster.h
			.................
This is how internode message passing works. I'll describe this at two
levels: the movement of data, and then the setup.

Level 1: Movement of data.
1. Object Sender sends out an argument A in a regular message.
2. The SrcFinfo scans through targets in the MsgDigest via func,tgt pairs
3. The off-node (Func,tgt-Eref) pair holds a HopFunc and a special
     Eref. The main part of the Eref is the originating object. The
     FieldIndex of the Eref is the target node.
4. The HopFunc fills in the Send buffer of the postmaster. It uses
     the target node info, stuffs in the ObjId of the originating object,
     and converts and stuffs each of the arguments using Conv< A >.
     Thus the Send buffer contents are a header with TgtInfo, and then the
     actual arguments.
5. This happens for all outgoing messages this cycle.
6. Postmaster sends out buffers in process. It then waits for incoming
     stuff in the recvBufs.
7. The scene now shifts to the PostMaster on the remote node. In its
     'process', the clearPending call is executed. It looks at the recvBuf
     and extracts the tgtInfo. This tells it what the originating object
     was, and what SrcFinfo to use for the outgoing message. !!!! fixme
             (Currently I use BindIndex which ought to be right but is done
             very indirectly. I need to check.)
8. we call the SrcFinfo::sendBuffer call from the originating object.
 The sendBuffer call converts the arguments
 back from the bufer to their native form and despatches using the
 regular messaging. Note that the MsgDigest will have done the right
 thing here to set up the regular messaging even for off-node 
 DataIndices on this element.
9. Messages reach their targets.

Level 2. Setup.
1. Objects and messages set up the regular way. Objects may have 
subsets of their Data arrays placed on different nodes. Messages
are globals, their contents are replicated on every node.
2. When a SrcFinfo::send call is about to execute for the first time, 
it is digested: Element::digestMessages. During this step each of the
messages is expanded by putTargetsInDigeest into target Erefs.
3. The target Erefs are inspected in filterOffNodeTargets. Off-node
targets are removed, and we record each pair of Source DataId and node.
4. We now call putOffNodeTargetsInDigest. This generates the
HopFunc by calling OpFunc::makeHopFunc with the fid of the SrcFinfo.
5. putOffNodeTargetsInDigest then examines the Source/Node pairs and 
creates HopFunc/Eref pairs which are stuffed into the msgDigest.
Note that these Erefs are the hacked version where the Eref specifies
the originating object, plus using its FieldIndex to specify the target
node.

Possible optimization here would be to have a sendToAll buffer
that was filled when the digestMessages detected that a majority of
target nodes received a given message. A setup time complication, not
a runtime problem.
			.................


I've now put together most of the code. Compilation in progress.
Checkin 4851.
===========================================================================
18 Nov 2013. Trying to compile HopFunc.cpp.
19 Nov 2013. Compiled. Trying to debug in testConvVectorOfFcetors.
20 Nov 2013. going through unit tests. Convs were messy but now OK.
	Need to handle copy of Msgs to n targets.

Other than that pending matter, the unit tests clear. Checkin 4856.

===========================================================================
22 Nov: See if the mpi_scatter will work with larger recv buffers than send
sizes. The documentation isn't hugely clear on this.
Tried it out in proc4.cpp, which is based on the scatter code in proc3.cpp. 
Seems to work at first, but then fails once up to 16 nodes and higher.
Let me check the mpirecv can handle this asymmetry. In proc5.cpp, which is
based on proc2.cpp. Yes, this works. It is documented too.

From the 12 Nov list:
* Fill in send buffers:
	- Message digest consolidation
	- Arg conversion.
+ Figure out best design for internode comms. 
* simple block mapping of DataId to rawIndex.
- Global objects and synchronization
- Communication between nodes for setup operations
- Communication between nodes for Set/Get operations
- Clock ticks between nodes.
+ Messaging between nodes.
	- MPI implementation

Next is to apply the MPI implementation, use the MPI_Irecv approach as the
MPI_Scatter won't handle variable-length messages.

Added most of code for PostMaster MPI stuff, yet to compile.


===========================================================================
22 Nov: Compiles.  Checkin 4857.
Beginning unit tests. Goes OK until it has to issue a Set call to an off-node
data entry in testShell. Will need to fix SetGet. Also looks like unit
tests are not going across nodes.

Looking at SetGet. While the use of a HopFunc takes us most of the way,
there are 5 issues remaining:
- I may set a field. The hack in the send command knows that the src eref 
	is not a field, so it uses the fieldIndex as the node identifier.
	As I can just query the eref about the target node, this is can be
	handled provided the system knows it is a SetGet operation.
* PostMasteraddToSendBuf doesn't know that this is a SetGet rather than msg.
	Subclasses of the HopFunc? Switch statements off a flag passed in by
	bindIndex?
	Created a HopIndex class that lets met track both.
+ How to set up the bindIndex to refer to the correct SetGet operation on the
	target node?
	Make a static global vector of all OpFuncs, refer to this.
	Need to be sure that the dynamically assigned OpFuncs come strictly 
	after the statically assigned ones.
+ On the target node, SrcFinfo::sendBuffer was used to convert back. Here it
	won't work as the sendBuffer refers to the regular send call.
	Need to refer to something quite different. This could be handled
	if the TAG on the mpi msg tells the target node to do something else.
	I would need to get the OpFuncBase or Hopfunc to convert buffer to 
	arguments and call the OpFunc->op().
- How to handle return values for get funcs?

Stages in off-node 'set':
1. call SetGet::set. Checks if isDataHere(). If so, easy. If not, make a 
	HopFunc with the flag to say it is a Set operation. Call hop->op.
2. hop->op does the generic addToBuf.
3. The postmaster sees the special flag to tell it that it is a Set operation
	in p->addToSendBuf.
4. PostMaster sends the data off immediately [and polls for return]. 
	Appropriate SETTAG.
5. Tgt PostMaster is polling for any SETTAG from any source. Gets it.
6. Uses TgtInfo to look up appropriate OpFunc from BindIndex, and Eref.
7. Calls OpFunc1Base<A>::op->opBuffer( Eref, buf). This does the 
	conversion from the buffer and calls the virtual OpFunc with the 
	converted argument.

This would be it for the Set unless I wanted to make it a blocking Set.

Stages in off-node 'get':
Same as 1 to 6 but the originating PostMaster has to do polling for the return.
7. Calls GetOpFunc1Base< A >::op->opBuffer. This creates a temporary A field
	for the return value and gets it.
8. Converts into a buffer explicitly.
9. Sends back using yet another tag.
10. The return poll harvests the value. The GetOpFuncBase knows what type to 
	expect back and use in the buffer.

This is going to be tough. First things first with the set. Perhaps should
separate out the messaging in the unit tests, too.

For the Shell operations, I think it should be yet another SHELLTAG or maybe
even separate tags for the various shell ops. There are only about a dozen
of them. They all seem to be forward only ops, no returns.
Could we use a variant of Set?

I also need to figure out how to do setVec and getVec.
===========================================================================

Week 1: Threading out
Week 2: Messaging refactoring
Week 3: Element and field indexing refactoring. Data Handler out.
Week 4: Scheduling refactoring, recompile and unit tests. 
	Copy refactoring.
Week 4.5: MsgIds replaced with ObjIds. Automatic message compilation.
Week 5: Standalone tests on mpi framework
Week 6: Multinode messaging design, implementation, compilation, no tests.
Week 7: Multinode Set/Get. Compile and test.

Major changes in MOOSE:
1. Refactored messaging and eliminated queueing. Messages now use the same 
high-level structure, but at runtime they do not dump 'send' calls and
arguments to a queue. Instead the messages are compiled into function-object
pairs and use these to directly call the target objects with the arguments.
This permits the use of pointers as arguments in messages.
This means that message calls are not synchronized by the queue - they occur
as called.
This means that the stack can be traversed to find what called what.
This is likely to be much faster, haven't yet benchmarked.
2 Eliminated threading.  As a necessary precondition for the above, threading
was eliminated. In addition to obviating the need for queues, this greatly 
simplified the process loop and scheculing.
3. Used integral scheduling. Earlier the clock emitted ticks which could have
arbitrary doubles for the timestep. Now there is a base clock dt, and all
tick events are integral multiples of this base. There is no ambiguity about
ordering nor any issue with rounding. Much simpler too.
4. Indexing of data is simplified. There are three integers to look up data:
	Id -> Identifies the Element which is a wrapper for data and messages.
	DataId or DataIndex -> Identifies a data entry in the Element. A simple
		one-dimensional array, can have a single entry.
	FieldIndex -> Identifies a FieldObject on a data entry. The Field Object
		must be associated with a FieldElement. Most objects do not
		have any field objects. Cases which do include synapses, which
		are subsidiary objects sitting on each receptor.
	A corollary of this is that there is no more multidimensional indexing
	of data entries. No more field masks.
5. MsgIds are no more. Instead Msgs are identified by ObjIds just like any other
	object.

6. The Set commands now directly call functions on the target object to assign
	values. But see parallel specialization.
7. The Get commands now directly call functions on the target object to obtain.
	values. But see parallel specialization.

8. Any DataEntry can now be the parent of an Element. Earlier this was a bit
	ambiguous and many parts of the code required instead that only Elements
	could be parents.
9. Synapses now use a ring buffer instead of a sorted queue to handle spike 
	arrival times.

In addition to these changes which are evident in serial code, there are many
changes to enable parallelization.
10. There are multiple Element classes, all derived from Element. Each has its
own rules for node decomposition. This is automatic: upon creation the 
appropriate data entries are put on the appropriate nodes. Note that every
node has the full Element tree, but only subsets of the DataEntries are 
put on different nodes.
11. Parallel messaging is implemented by using special flags during the 
'compilation' of messages. This intercepts off-node messages and puts the
message request and arguments into a buffer such that any given node only
gets a single request for the message, even if there are a hundred targets for
the message on the remote node. The buffer for each node is sent off at the
end of each clock tick in a non-blocking manner. Arriving buffers are digested
and the messages sent out to all targets on the local node.
12. Parallel field assignment and once-off function calls (set) is implemented
by detecting if target is off-node, and if so, putting into a different buffer.
This is dispatched in a non-blocking manner, but only after clearing all 
pending incoming field assignment calls.
13. Shell simulation control operations are implemented through 'set' calls.
14. Get calls work in a similar manner to 'set', except that data also gets
	sent back.

===========================================================================

25 Nov 2013: Starting to compile with the Set calls included. But this is a
bad way to develop. Should instead do multinode messaging separately,
and then do the MPI-based real test.
Compiled, clears single-node unit tests. Checkin 4858.
Now to separate out the MPI-based tests.


Setting up cross-node Shell::doCreate by going through the newly
implemented SetGet functions.
Need to unit check the argument passing and fitting into buffers,
by HopFunc.

Compiled, too sleepy to tackle furter.
===========================================================================
26 Nov 2013.
Finally tracked down the problem with off-node Set calls. The
order of evaluation of the Conv functions in OpFunc6Base::opBuffer()
is backwards. The last argument, arg6, gets evaluated first, and so on.
Presumably backwards in all cases. Need to redo conversion syntax.
Turns out C++ explicitly does NOT specify an evaluation order.

So did the less elegant but simple code to extract arguments. Now seems to
work for creating objects, but the unit tests don't seem to go all the way 
through.
===========================================================================
27 Nov 2013.
MPI is being difficult about reporting where it croaks. By stepping through
it looks like it is testShell.cpp:1718, testTreeTraversal()
From what I can see it looks like an innocuous call to Shell::doCreate 
of a Neutral on line 50.
Turns out i was being careless with the indexing of the MPI_Request array,
which is continguous rather than indexed by node number. Fixed. 

Next bug is with path traversal. This croaks on both nodes. I think 
what is happening is:

Node 0: The path traversal does not handle the rawIndex/DataIndex properly
Node 1: The 'get' call is sent across as the baseclass 'set' and this 
causes problems. (This possibility is somewhat of a guess).

So I need to tackle both the 'get' call and the path traversal logic.
But it would be good to have the 'set' call properly unit tested.

Worked on the path traversal. Should now be able to do it without having to
inquire across nodes.
Clears the path unit test. Now fails in testShell.cpp::testCopyFieldElement
which needs to assign the vector of sizes just to set up the system.
All very well, but at this point I don't have the capability to copy except
from Globals.

Some pending cross-node things:
Proper unit test for Set across nodes. I think it works.
Get. If I get this to work I can check the 'set' call too.
SetVec + unit test. Try the testCopyFieldElement with the objects created as
	locals.
GetVec
The whole list of Shell commands.


===========================================================================
30 Nov 2013
Subha suggests having elements which put contents on specific nodes.
Perhaps a better thing to do for automation is to have Elements which keep
the entire contents on one node. I would prefer to be able to do models without
reference to size of computer.

Now working on Get. In Postmaster.cpp.
===========================================================================
1 Dec 2013.
Some input on handling random numbers.
Hi,
  Got some answers to the second problem: (1) if the number of random
numbers to be created for each node is known beforehand "jump ahead"
will work (just mailed Upi about it)
(http://www.math.sci.hiroshima-u.ac.jp/~m-mat/MT/JUMP/index.html)
  (2) And for the online case, there seem to be methods for this
(http://software.intel.com/en-us/forums/topic/283349): MT2203 gives up
to 6024 independent streams and Charles Leiserson (the one who wrote
"Introduction to Algorithms" with Cormen, and Rivest) published a
paper on another system for this:
http://supertech.csail.mit.edu/papers/dprng.pdf

I believe Pritish will have more insight into what is best suited for
our purpose.

Best,
  Subha

On Sat, Nov 30, 2013 at 4:14 PM, Upinder S. Bhalla <bhalla@ncbs.res.in> wrote:
> Hi, Pritish,
>     Good to hear from you. It is good timing and we could use your inputs.
>
> Based in good part on the discussions we had during your visit, I've redone
> the MOOSE basecode to be single-threaded and done lots of other pending
> fixes along the way. I'm desiging at present for a simple MPI interface,
> easily adapted to Charm++.
>
> Couple of key design questions have come up, again motivated by our earlier
> discussions, and I don't remember the suggestions you had on them.
>
> 1. The python/scripting interface to MOOSE. With the current design, we need
> to have node zero talk to the Python script on the one hand, and to all the
> other nodes via MPI. The other nodes do not need to know Python. As I
> recall, you had suggested an alternative architecture where there was some
> kind of socket communication to one node. In principle we could have the
> Python-aware node outside the cluster, but then I'm not sure how one would
> tunnel MPI through to the cluster nodes. Do advise.
>
> 2. Parallel random numbers. Suppose synaptic weights need to be assigned by
> a random seed across all nodes. Each node knows what to do with its own
> subset of numbers, but the problem is that we may have to grind through a
> few million numbers in the random sequence before we get there. Even worse
> is how to handle random numbers emitted at runtime for node-specific
> calculations. How does one ensure that the outcome is the same independent
> of number of nodes? I'm sure this is a classic problem in parallel
> simulations.
>
> Best,
>     Upi

Now compiles but doesn't even start to clear unit tests.
===========================================================================
2 Dec 2013
Now clears serial unit tests. Fails on target node when called from 
remoteGet in parallel unit tests. Indexing issue.
Oddly, the other node croaked in clearPendingSet.

===========================================================================
3 Dec 2013.
Understood bug, now to fix. I was trying to use 'get' for the path of
an object that was off-node. The good news is that it does indeed
correctly do most of the work of going off-node to get the object.
The bad news is that it loses the fieldIndex somewhere along the way.

Put in a bit of a hack so that the TgtInfo puts the fieldIndex into
the 'size' field for Sets and Gets, which do not use the size field.

Now carries on to Shell::testCopyFieldElement.
Lots of errors on Node01, for inability to create objects. Line 401
in Shell::testCopyFieldElement.
Fixed the cpoy at that point, but a few lines later it goes bad.

Problems start in testShell.cpp::testChildren.
Seems that I need to fix up Shell:::doDelete.
Did that. Now there is a problem in testShell.cpp:450, all I'm doing
is trying a 'get' on numField.
===========================================================================
4 Dec 2013.
Incrementally going through unit tests. 
Currently a bit stuck in testShell.cpp::copyFieldElement, not because
of real errors but because SetVec and GetVec are not yet implemented.

Next error is due to trying low-level functions to look up values, in 
testShell::testObjidToAndFromPath()
Easy to fix one part of it in line 511, but would be good to
have a general solution.
===========================================================================
5 Dec 2013. Checkin 4878.
Now it is a heisenbug. I'm trying to do a simple set/get on an offnode field,
works when I step through it using gdb, fails when run without stepping.
In testShell.cpp:517.

Possibly the set is not complete when I ask for the 'get'.

Redid SetGet a bit so that both use the same MPI tag. As hoped, this fixes
the sequencing problem of set and then get.

Next problem in testShellSetGet is another SetVec issue. Postpone.
On now to the message tests. This requires further fixing of the
Shell commands.

Man Shell commands now fixed. The program now goes on till 
testShell.cpp:912, where again it runs into the SetVec problem.
Checkin 4880.

Working on SetVec. In SetGet.h and HopFunc.h: 60 ish.
Pending:
- What to do with globals
- Propagate changes through to the dispatchBuffers call and the postmaster
- Handle stuff on remote nodes.

===========================================================================
6 Dec 2013.
Working in FieldElement.cpp:85 to put in the skeleton for getNumOnNode.
The serious version of this function sould look up remote nodes to get
the values.
===========================================================================
7 Dec 2013.

Compiling the changes in for the SetVec.

Discussions with the MOOSE team, seems like there is a feeling that the
path semantics need to change for the FieldElements. Specifically, use only 
the fieldIndex part to index the FieldElement, since they are always children
of an Element whose DataIndex applies both to the base element and to the
FieldElement value.
pare
the FieldElements as always a child of a regular Element 

Silly bug in unit tests, the messages maintain their own indexing for
lookup. and this is incremented when a new message is created. But the
unit tests on the master node make and delete several messages, leaving
their indices different from those on the slave nodes.

split the unit tests into nonmpi and mpi parts,
Provide a global index when creating messages.
Clear out the indices after each block of unit tests.

Messy thing comes up with msgs. The DataId for look up of the msg objects
can get misaligned on different nodes, already does due to unit tests on
node 0. I've implemented a hack to use the DataId for the msgs as generated
by the master node. Unfortunately all the Msg::copy functions and likely 
many others need to make msgs on the fly, they are not able to use something
generated centrally. Current messy solution is to let the copy operations use
automatic incrementing.

===========================================================================
8 Dec.
Accumulating issues
'send' from a global: the MsgDigest needs to only do stuff to local node.
'send' to all: Need to use ALLDATA for targets on local node, and then 
send single calls to remote ones.

Major cleanup done on the MsgId generation, not perfect but the key commands
issue a direct specification of the mid on all nodes and the rest of the time
it does an increment which should happen identically on all nodes.

Checkin 4890.
In the meantime, on with the unit tests to testShell.cpp:942 at which point
we need getVec.

Bypassed that using a loop of 'get' calls. Now it starts messaging, which
of course crashes.
===========================================================================
9 Dec
Spent a lot of time adding a unit test for filterOffNodeTargets. Which works,
but doesn't fix the problem with the messages not going out.

Two bugs now: 
- in testShell.cpp::testCopyFieldElement:460. Number of copies
is wrong for 4 nodes, but OK for 2.
- in testShell.cpp::testShellAddMsg:1001. Basically the messages don't go.

Seems that the Clock does not have any messages to go out.


===========================================================================
10 Dec
Sorted outgoing messages of clock. The ALLDATA flag in the targets had 
confused it regarding which node to send to.

Now it seems that although there is a message from a1 to a2, the msgDigest
doesn't show any outgoing node.

Nasty. Tracked down to error in how SingleMsg was handling sources() and
targets().

More subtleties needed fixing in how the off-node targets were identified.

Now it is down to getting the postmaster to schedule message sending.
First problem is that the reinit call isn't propagating to the off-node
clocks.

Much struggling later, turned out to be just that I needed to make clean
and recompile. But now it is hanging in a confusing way, in a section 
before the 'start'. Also it thinks reinit has been called twice on the
worker node.
===========================================================================
11 Dec.
Fixed an issue with the clearPending() command so that it can safely be
called recursively. This helps.

Now it is trying to send a message on node 1 but the list of targets includes
off-node entries.

Much painful debugging of the filterOffNodeTargets later, now it clears
the message sending. Hooray! But it promptly fails in the next unit test,
where I copy messages. Checkin 4900.

Next place of failure seems to be when the system tries to send messages
to nonexistent objects, deleted when the testShell::testShellAddMsg
function ended, but accessed by the next unit test testCopyMsgOps
in Shell::doReinit. Somewhere in clearPendingSend.
===========================================================================
12 Dec.
Turns out we're accessing objects a1 and b1: these were the msg sources of
messages on the respective nodes. So it seems likely that the postmaster is
reprising the old message calls. Similar to what happened with SetGet.

Step 1: Put Postmaster on Clock 9, guaranteed to be the last called. Doesn't
fix it, but this was a necessary cleanup anyway.
Question is, do the deleted objects somehow send out stuff on the originating 
node, or is it something left over in the recvBuf on the target node?

Since the thing croaks on doReinit, which does NOT trigger a send call on the
Arith object, it is most likely something in the recvBuf. But possibly
the send buffer isn't being cleared either.

Fixed it up. The send buffer was not being cleared and also there was no
check that all sends had completed, before going on to the next cycle.
With this we advance further till the next instance of getVec.
Actually the problem is with the copy command.

===========================================================================
