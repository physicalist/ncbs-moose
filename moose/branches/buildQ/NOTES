

I have been playing around with a new messaging system, one that uses an
intermediate buffer. The big advantage of this sytem is that it should
simplify messaging in multithread and multinode systems. 

Sync messages begin to look a bit like the kinetics optimizations in 
GENESIS. 
Differing: The msg source uses 'send' to place the data in a safe buffer.
Similar: The msg dest scans list of ptrs to places in this buffer.
Similar: Will need an 'ACTION'-like mechanism for calling dest funcs from
clocks.
Possibly this will work a bit faster than existing MOOSE messaging.

Async messages are harder. Need to assemble all outgoing data on any given
thread into an expanding buffer for that thread. Data packets include src id.
These packets are transferred (issues here about selecting for targets).
On target thread, src id used to look up whatever part of the ConnTainer info
specifies the dest elements. Then the packet is delivered. Best if done through
a scan of dests, since that retains similarity with Sync messages. 

Need to introduce a mechanism for calling 'ACTIONS' directly.

=============================================================================
26 Jan 2009

send -> buffer
process -> lookup buffer
trigger -> call func. Could restrict to fixed set, or provide a func lookup 
	index. If fixed set could use virtual funcs.

Or, eliminate trigger and provide only process, proc2, and reinit.
	Would like to be able to call arb funcs.

SendBack, SendTo become harder.


Process op: well defined, clock ticked.
Proc2: ditto
Reinit: ditto.

Then: generic arrival op: Scan for op request. The memory location has
both the operation identifier and the arguments.

Or: build up scan list through messages... sounds like GENESIS.

Or: Alerting mechanism. 'SendTo' or 'SendBack' puts target id on queue

High traffic messages are scheduled.
	Synaptic input is scheduled even though it is sporadic. Total
	traffic is expected to be big.
Low traffic messages are polled by queue. When called, these are added
	to a queue for the target object.
Buffer info includes only data.
	For regular input, like conc and Vm, data only is the conc/Vm.
	For synaptic input the data includes source object info.
	For low traffic messages the data includes complete conn info, plus args
	For sendback messages like channelGates: It can be scheduled, so
		the data includes return info and the op must use this to place
		the response in the right location. Would be nice to do
		efficiently in array form.
	For field assignment: Regular low traffic
	For field readout: Data includes Id for field access object.
		This is a temporary from the command line
		It is a regular object for plots etc.


Design requirements:
- Thread safe
- Buffered data delivery for threading and for multinode operations.
- Very fast for scheduled operations, whether threaded or not.
- Connections remain fully traversible, 
- Connections remain usable bidirectionally by multiple messages.

Design desiderata
- Completely deterministic for single-thread case (consider real-time ops)

=============================================================================
30 Jan 2009
There is a problem to be sorted for any queued buffer messaging: Ensuring that
things go into the buffer without stepping on each others' toes. For
example, sending spikes. If we allocate a separate buffer per thread for
each target object, things get costly and messy. But that may be better 
than mutexes for writing into the buffer.

We expect single 'synapse' objects to manage many input axons, each connecting
with a distinct weight. Suppose 100 of these, so the convergence is up to 300.
Assume a 16-way system, we don't want to manage 16 buffers for each synapse.

Per-clock buffers: An extra step to unsort them. More info to put in to 
identify dest.

Thread-safe queue for writing: There is an extra overhead in mutex juggling
for every 'send', though the subsequent reads can be clean as they are done
on the object owning the queue. If we have per-object queues should be 
manageable.

=============================================================================
31 Jan 2009
A problem with the messaging concept: Can't put the message buf on the
target element, because the data may go to multiple targets. Instead
need the target element to manage ptr to the msg src(s) and read them.
Either that, or have src element push data into multiple target bufs.
Latter makes more sense, given that we may need to push data also into
an MPI buffer or so.
But by the same token data comes into the MPI Recv buffer and needs to
be dispersed. 
So, one way or another, the postmaster must be an active participant
in getting data in or out of the buffer.

Cost:
Pushing data:
Src needs addr for each buffer, N addrs.      Iterate N times.
Each Dest needs buffer, N * datasize.         N dests each 1 lookup.
Multhreading: No change, no need for any locks.
MPI: Push right into MPI buffer. At remote node need to do further push.
Sporadic Msgs: Push into queues of every target. No redundancy.
			: Push into a single queue, later push into target?
				No particular advantage.
Variant on Moose 1.1 approach: Target object guarantees thread-safe
	handling of incoming data... tricky. Need to put each incoming 
	arg into separate location indexed by msg src itself.

Pulling data:
Src needs single buffer, no iteration. 1 addr. No iteration.
Each dest needs addr of buffer: N* ptrsize    N dests each 1 lookup.
Multhreading: No change, no need for any locks.
MPI: Postmasters on src need to pull in data. At remote node usual pull.
Sporadic msgs: Push into a single queue, managed by msg. This
	subsequently needs to do multiple pushes anyway, unless there
	is a very high chance that each entry will be of interest to each
	receiving synapse.
	: Push into multiple queues, basically into the target object.

Seems like pulling data works better for scheduled messages.
Something like pushing needed for sporadic msgs. 
Like old GENESIS.

=============================================================================
1 Feb 2009
Now the location of the buffers.
sched data buffer
- On source object:
	+ No extra storage or management
	. Need to redirect pointers if objects are deleted or moved
	. Need to redirect pointers for zombies. But redirection needed anyway
	- Mixes message transfer with object representation.
- In a separate buffer managed by the Msg:
	+ Management relatively straightforward, set up at msg creation time.
	+ Deleting and moving objects managed along with messages.
	. Zombies could do a hack and reuse the same msg space.
	+ Separates message transfer from object representation.

async data buffers: Synaptic input.
- On dest objects:
	+ Clean synapse management.
	- Mixes message transfer with object representation.
	. Sender must scan through all dests.
	- Extra outgoing buffers.
- In multiple separate buffers managed by each of the Msgs:
	+ Again, management straightforward, set up at msg creation time.
	+ Separates message transfer from object representation.
	- Issue of additional data: weight, release prob, history, etc.
	. Many-to-many msgs have a possible economy of assignment.
	- All targets must scan through all potential sources
- Input Q on Msg, synapse-local Q on objects
	- Initial op: Get data from axon to Msg.
	- Option 1: push onto Msg Q, accumulate on Msg.
		- Thread locking needed.
		- Msg subsequently called on tick to clean input Q.
		- This could be a rare call if syn delay is large.
		- Now it shuffles data into synapse-specific Q.
			- Depending on update rate and convergence onto syn,
				this could be a single entry Q.
			- Multi-thread op here too, but very local.
		- This synapse-specific Q could also be the sorted event Q
		- Object scans base of event Q each timestep.
		- Note that we cannot do event queueing on the incoming
			APs, because of different delays to target.
	- Option 2: Immediately sort onto target synapse Qs.
		- Thread locking needed.
		- This op has to immediately do the shuffling into 
			synapse-specific Q, since otherwise full scan needed.
		- This synapse-specific Q could also be the sorted event Q
		- Object scans base of event Q each timestep.
	- When object pops event Q, it has to locate syn wt, prob, history, etc.
		- Option 1: Msg carries this info.
			+ These are features of projection pattern
			- Need to template whole Syn Msg structure
			+ Projection could compute on fly.
		- Option 2: Target obj carries this info
			+ Local calculations easier. Postsyn compt history too.
			+ No templating needed.
			- Need index in msg for target obj to look up info.

async data buffers: Sporadic input.
- On Dest objects:
	+ Clean data management
	- Need to send out to all targets (But # likely small )
	- Mixes message transfer with object representation
- On Src objects
	+ Single point of assignment
	- All targets must scan through all potential sources
	- Possible economy of assignment.
- On buffers managed by each Msg:
	- All targets must scan through all potential sources
	- Possible economy of assignment.

Seems like the best bet is to have Msgs manage the sync buffers.
The clinching point is that about separation of message data transfer
from objects.
(Added 15 Feb: Another issue overrides this, for async messages: data flow
should be unidirectional, that is, messages cannot be changed by their targets.
This would happen if the spikeQ was on the message rather than the target.)

Implemented a first pass test simulation using reacs and mols. Hard coded
in buffer info. Works. Helps set up requirements for messaging.

In this variant, the Element manages a vector< double* > that points to
the data buffers, and this in turn is referenced by a vector< unsigned int >
which converts the slots into the correct buffer location. The Element
provides some helper functions for doing individual msg data (double) lookup,
and others for taking sums and products of sequences: such ops are common.


Next:
	* Get svn working for this.
	+ Implement sporadic messages for field access
	+ Implement something that uses synaptic messages
	- Implement message setup framework
	- Implement field set/get
	- Array elements and messages
	- Scheduling
	- Multithreading
	- MPI
	- Implement distribution of elements (entries) on threads and nodes.
	- Benchmarking

=============================================================================
2 Feb 2009
Svn now working, the path is
https://moose.svn.sourceforge.net/svnroot/moose/moose/branches/Msg

For sporadic messages:
- Use indexing equivalent to Finfo definitions.
- Use a template for an adaptor function from char* args to the class-specific
	func. The adaptor func can also do the typecasting for the class itself.

Need also a queue for synaptic input. Scan on sched, but variable # of entries.
=============================================================================
8 Feb 2009
For async messages, no point in defining a specific class for the 
data packets. There will always be a FuncId but after that no telling 
what args to take.

Implemented a first simple pass at async messages for field access.
Checked in as revision 1009.

Trying to template it out. No luck.
=============================================================================
10 Feb 2009
After some more template contortions, it works. I'm not sure if this beast
will compile on other systems, though.
Checked in as revision 1010.

Siji tested it on Windows. Somewhat to my surprise, it works there too. Mac
is OK too.

=============================================================================
12 Feb 2009

First pass design for synaptic messaging, based on above description 
dated Feb 1 (though it has been updated since).

Upon tick at src:
src -> Msg -> scan through list of targets -> (threadlock) target-specific Q
Msg contains all the synaptic info, including weight, delay etc.
Msg also manages a Q for each target synapse object.

Upon tick at target synapse: Query Msg Q. Collect all data if event arrived.

=============================================================================
15 Feb 2009

Question: Should the Msg be const?
	- Gets messy with bidirectional data flow in plastic synapses.
		If we keep Msg const, then there are separate synaptic state
		variables needed on the target.
		If we allow it to vary, then the target has to write to Msg.
	- The synapse Q itself involves bidirectional data flow. Even if the
		Msg manages the pushing internally, the object has to tell it
		to pop. Not good.
	- These are issues with bidirectional data flow. However, it does
		not mean that Msgs have to be constant objects. For example,
		we could still have a projection as an object with regular
		Msg and other inputs, which could change during the
		simulation. But it would also adhere to the rule that it
		gets input from msgs, but does not affect the msgs.
Summary: 
	- Q cannot be on Msg.
	- A Msg is const from viewpoint of target: Unidirectional data flow.
	- A Msg can however be a normal variable element with a clock tick,
		and other incoming msgs.

Accessing Msg info:
	- A Msg is an edge between individual src and dest entries in arrays.
		Either src or dest entry can access through independent indices.
	- A Msg is also an edge between the array containers. Likewise.
	- A Msg may (should?) be an accessible object with fields. Name
		could be msg field name.
		setfield /foo/axon/projection[] delay 1e-3
		setfield /bar/GluR/incoming[23] weight 5
		showfield /bar/GluR/incoming[]/src
		showfield /foo/axon/projection[][dest=/bar/#/incoming] weight



Data structures for synapses:

* Fix up tests so they use assertions, not printouts
* Check in. Currently at revision 1012.
- Start setting up synapses.
	I have a skeleton in place now, in the uncompiled files
	Send.cpp, Msg.h. There are still-to-be-fixed changes to
	Element.h and Data.h to let us access Data::pushQ and
	to have indices into multiple data entries within Element.
	The threading stuff has to be dealt with at the Element.cpp
	level to lock out attempts to push onto the same Data.


A preliminary design for Elements and messages:
- All elements are array elements. 
	- The Data* d_ becomes a vector.
	- procBuf remains as is, but its range is looked up using
		indexing for element index.
- Conns are all many2many or equivalent all-to-all variants.
	- They all connect one element (the whole array) to another.
	- Typical sparse matrix implementation
	- Usually traversed only for async (spike) messages, so 
		bidrectional traversal is rare.
	- Element manages vector of vector of Conn*. 
		- First index for slot
		- Second index for various other targets.

=============================================================================
16 Feb 2009
Implemented a first pass at the framework for a lot of this stuff.
Compiles. Not in the test. 
Checked in. Currently at revision 1018.

=============================================================================
17 Feb 2009
First test for IntFire: directly insert synaptic events into its queue.
Confirmed that it responds correctly. Generates another time-series output,
need to set up to handle assertions.
Checked in as revision 1019.

Set up above test to use assertions rather than printout.
Checked in as revision 1020.

Implemented a recurrent spiking loop using messaging. Seems to work fine.
Checked in as revision 1021.

Next:
Attach sync messages also to the Msg data structure. Key issue is who
owns and manages the buffer. This has been discussed above on 1 Feb.
- Msg: Favoured because it separates data transfer from object,
	However it mixes data transfer with connectivity specification.
- (originating) Element: This also separates data transfer, easier to manage.
- Object: This has been ruled out.

Managing connection info in messages:
	- Can extract ptr info from the finfo + message, use to fill ptr bufs
		for sync messages
	- Message slots predefined only for small set invoked by Process or
		initPhase. Rest go via a map to look up entry in MsgVec.

Field assignment
	- Set inserts a request into the queue.
	- Get either needs formation of a temporary message...
		or: Inserts a request with additional info to tell where to
		send data back to?
=============================================================================
18 Feb 2009
How do we set up the locations to use in the data buffer?
The object knows ahead of time exactly what it has to place during
process and other calls. This is built into the Finfo for the msg
source. So at element creation time we can build the send buffer.
The only time this will change is if the element is resized or deleted.

=============================================================================
19 Feb 2009

Reconfigured Element to use vector of Data ptrs.
Some patching later, it compiles and runs again.

Next: Begin to set up the benchmarks for sync and async messaging.
	Look at memory and speed scaling as we do so.
	- run reac system with 1 to 1e8 Data entries
		- Ordered messages
		- Sparse matrix messages
		- Many individual elements.
	- run spiking system ditto.

Gear up with this for testing with multi-threading.

=============================================================================
20 Feb 2009

Checked in as revision 1023

Setting up a main.cpp::testSyncArray( unsigned int size ) function to
build a reaction model with a big test array for a reaction scheme

=============================================================================
21 Feb 2009
Compiled testSyncArray: OK. Ran it: Failed.

=============================================================================
22 Feb 2009.
Checked in as revision 1032.

Got the testSyncArray stuff to work. Did a little profiling. Spends
about 25% of its time in Element::sumBuf. 
Here are the timings of a single A <===> B reaction on this 1.4Ghz machine
(Lenovo X301), using O3:
syncArray10     ops=100000      time=0.057695
syncArray100    ops=1e+06       time=0.313191
syncArray1000   ops=1e+07       time=3.13012
syncArray10000  ops=1e+08       time=31.7042

Takes about 4 times longer with debugging and no optimization:
syncArray10     ops=100000      time=0.170935
syncArray100    ops=1e+06       time=1.18491
syncArray1000   ops=1e+07       time=11.4804
syncArray10000  ops=1e+08       time=115.368

For reference, genesis does:
completed 1000001 steps in 39.670479 cpu seconds 
(for spine_v59.g, which is a model with 104 mols, 55 reacs and 76 enz).
This is about 2e8 ops, so genesis is almost 2x faster. Amazing. This new 
messaging was supposed to go much faster.

Checked in as revision 1033.
Decide whether further optimization comes first, or the threading.

Did some minor tweaking to use const iterators. This gives about 
3% improvement, useful, but not too exciting.
Checked in as revision 1034.

Implemented skeleton code for threads: creates and joins threads only.
Compiles, runs.
Checked in as revision 1040.

For threading:
- simplest approach, probably not practical:
	Launch a thread for each processor on each clock tick.
	Thread computes process for subset of entries.
	Join threads after clock tick.
	Problem is that thread launches and joins are not cheap.
- Condition approach, trickier but maybe faster:
	Launch threads for each processor when calculations starts.
	Thread runs independent scheduling.
	At end of each tick:
	mutex to increment count # of threads completed.
		If all threads done
			signals to master thread/all other threads
		else
			cond_wait for count
	close mutex
	go on to next tick.
- pthread_barriers
	This is probably the cleanest. 
	Launch threads for each processor when calculations starts.
	Thread runs independent scheduling.
	At end of each tick, put a barrier.
	Problem is that pthread barriers are reported to be very slow.
	Let's see.
- busy-loop barriers
	Same as above, only don't use pthreads barriers. Instead do a
	mutex-protected increment of # of threads completed,
	close mutex
	and do a busy-loop barrier on the # of threads completed.
	
Working on pthread_barrier based implementation.

Well, it looks like it works. The thread advantage isn't huge on my
2-core laptop:
syncArray10     ops=100000      time=0.056823
syncArray100    ops=1000000     time=0.311645
syncArray1000   ops=10000000    time=3.16343
syncArray10000  ops=100000000   time=31.2464
Main: creating 2 threads
syncArray10     ops=100000      time=0.25066
syncArray100    ops=1000000     time=0.524708
syncArray1000   ops=10000000    time=2.19136
syncArray10000  ops=100000000   time=19.7902
Main: creating 4 threads
syncArray10     ops=100000      time=0.755361
syncArray100    ops=1000000     time=0.970983
syncArray1000   ops=10000000    time=2.61468
syncArray10000  ops=100000000   time=22.7247

However, I need to do a lot more testing, including confirming that it
gives the right answers. Do the current calculations go across threads?

Also I need to check if the 4-core opteron nodes do better.
Checked in as revision 1048.

Did check that calculations give the right answers.
Checked in as revision 1049.

=============================================================================
24 Feb 2009
Ran on 4-CPU opteron (2 chips x 2 cores each). 
Original( 1 thread ):
syncArray10     ops=100000      time=0.02312
syncArray100    ops=1000000     time=0.308771
syncArray1000   ops=10000000    time=3.13899
syncArray10000  ops=100000000   time=32.1735

Main: creating 2 threads
syncArray10     ops=100000      time=0.111654
syncArray100    ops=1000000     time=0.26688
syncArray1000   ops=10000000    time=1.77432
syncArray10000  ops=100000000   time=17.8221

Main: creating 4 threads
syncArray10     ops=100000      time=0.363325
syncArray100    ops=1000000     time=0.522495
syncArray1000   ops=10000000    time=2.02002
syncArray10000  ops=100000000   time=9.49299

Well, that is interesting. it goes 1.8x faster on 2 cores, and 3.4x faster on 
4 cores. Not linear scaling, but not bad either. But this is only effective
for large arrays. The barrier overhead looks pretty bad.
Successive runs on the same node cause marked improvements in single-node
performance, but not as steep for threading. For example, three runs later
we have:

1 thread:
syncArray10     ops=100000      time=0.023302
syncArray100    ops=1000000     time=0.274423
syncArray1000   ops=10000000    time=2.51467
syncArray10000  ops=100000000   time=27.6373

Main: creating 2 threads
syncArray10     ops=100000      time=0.141666
syncArray100    ops=1000000     time=0.263635
syncArray1000   ops=10000000    time=1.71346
syncArray10000  ops=100000000   time=17.0478

Main: creating 4 threads
syncArray10     ops=100000      time=0.315527
syncArray100    ops=1000000     time=0.420413
syncArray1000   ops=10000000    time=1.25926
syncArray10000  ops=100000000   time=9.42896


Trying now my own implementation of barriers. I would have liked to try it
on gj, but time to go and still debugging. 
Here is the status:
c0 thread=0, cycle = 0counter = 1
c1 Main: waiting for threads
thread=1, cycle = 1counter = 0
thread=1, cycle = 1counter = 1

Implies that we've gone through the barrier withough letting thread 0 do so.

Here is more info:
c0 thread=0, cycle = 0, counter = 1, tc[0] = 0, tc[1] = 0
c1 thread=1, cycle = 1, counter = 0, tc[0] = 0, tc[1] = 0
thread=1, cycle = 1, counter = 1, tc[0] = 0, tc[1] = 1

I fixed this by making the 'cycle' variable a volatile. But, at least on my
laptop, there is not much improvement:

1 thread:
syncArray10     ops=100000     		time=0.055594	0.0575	0.056	0.057
syncArray100    ops=1000000    		time=0.311743	0.3118	0.303	0.322
syncArray1000   ops=10000000   		time=3.07243	3.147	3.09	3.11
syncArray10000  ops=100000000  		time=31.0668	31.4	30.8	30.9
Main: creating 2 threads, pthreads barrier
syncArray10 	ops=100000      	time=0.241881	0.8	0.82	0.505
syncArray100        ops=1000000     	time=0.606399	0.57	0.78	0.66
syncArray1000       ops=10000000    	time=2.12019	2.03	2.51	2.06
syncArray10000      ops=100000000   	time=18.5136	18.2	19.5	18.1

Main: creating 2 threads, my barrier:
syncArray10 	ops=100000     		time=0.053082	0.032	0.07	0.08
syncArray100        ops=1000000     	time=0.294042	0.18	0.21	0.23
syncArray1000       ops=10000000    	time=1.85085	1.77	2.0	1.76
syncArray10000      ops=100000000   	time=18.137	17.6	19.5	17.5

more seriously, it fails if # threads > # processors. Why?
Tried making counter also volatile. Doesn't help.

One good thing is that MyBarrier seems to have much less overhead: its
speedup is respectable and consistent even for 100 entries in the array.
Based on the single-thread timings for 10 entries, I estimate it costs
around 0.05 sec / 10K ~ 5 usec per invocation on my laptop. The 
pthreads barrier is around 0.8 sec / 10K = 80 usec.

Let's see how it scales on the cluster nodes.

Well, that was entertaining. Two things to try:
- multithreading on the main MOOSE kinetic solver
	Looked at it. Should work reasonably well for bigger models with
	>100 molecules. But I'll have to write my own RK5 solver for multi
	threading because the GSL one has a monolithic driver for the 
	calculations that could be split between threads.
- contine with implementation for the synaptic input queue code.
	Looked at it. A major issue turned up: the 'process' call
	manipulates both target and source spike queues. The 'sendSpike'
	function after much indirection pushes the spike info onto all 
	target queues. The 'process' function examines and pops the local
	queue. I need to modify this so that the push and test/pop are on
	different clock ticks. This may well happen in more realistic
	neuron models. Here I can separate the harvesting of the spikes
	onto a different tick than the testing and sending spike msgs.
	With this fixed, I need to protect the spike msg handling.
	Mutexes are a blunt instrument here, because they protect code
	rather than individual data structures. Ideally I want only to
	protect the buffer(s) I am working on.
	I've suggested an approach to this in the Element::addSpike 
	function:
		// mutex lock
		// Check if index is busy: bool vector
		// Flag index as busy
		// release mutex
		// do stuff
		// ?unflag index
		// Carry merrily on.
	but this has many loose ends.


=============================================================================
25 Feb 2009
Checking it in so that I can run the tests on gj.
Checked in as revision 1055.
Oops, forgot to add MyBarrier.h
Checked in as revision 1056.
Now to run on gj.

One thread
syncArray10     ops=100000      time=0.023068	0.023	0.023	0.023
syncArray100    ops=1000000     time=0.235919	0.236	0.236	0.301
syncArray1000   ops=10000000    time=2.75227	2.46	2.61	2.52
syncArray10000  ops=100000000   time=32.2201	30.52	32.6	31.7
syncArray10000  ops=1e9		  				278.7

Main: creating 2 threads, pthreads barrier
syncArray10 ops=100000      time=0.123591	0.115	0.107	0.134
syncArray100    ops=1000000     time=0.330623	0.285	0.304	0.272
syncArray1000   ops=10000000    time=1.55576	1.74	1.727	1.76
syncArray10000  ops=100000000   time=15.2927	16.74	15.6	15.8
syncArray10000  ops=1e9	 					163.4

Main: creating 4 threads, pthreads barrier
syncArray10     ops=100000      time=0.309986	0.357	0.336	0.34
syncArray100    ops=1000000     time=0.521644	0.508	0.486	0.48
syncArray1000   ops=10000000    time=0.985141	1.842	0.989	1.38
syncArray10000  ops=100000000   time=8.70518	10.48	11.6	7.64
syncArray10000  ops=1e9						117

Main: creating 2 threads: MyBarrier
syncArray10 ops=100000      time=0.040132	0.043	0.040	0.042
syncArray100    ops=1000000     time=0.178236	0.179	0.178	0.194
syncArray1000   ops=10000000    time=1.63492	1.611	1.623	1.68
syncArray10000  ops=100000000   time=15.7017	16.7	16.6	16.4
syncArray10000  ops=1e9 					179

Main: creating 4 threads: MyBarrier
syncArray10     ops=100000      time=0.089063	0.249	0.076	0.132
syncArray100    ops=1000000     time=0.125938	0.130	0.139	0.161
syncArray1000   ops=10000000    time=0.835574	0.86	0.848	0.900
syncArray10000  ops=100000000   time=7.48444	8.17	8.33	8.31
syncArray10000  ops=1e9						114

Summary:
- MyBarrier works for 4 threads works on gj.
- The speedup is reasonable except for 10 entries in the array. 
- We're over 3.9 fold speedup on average, with MyBarrier on 4 nodes for
	10K entries. But for 1K entries and less the speedup is much smaller,
	possibly here we have cache considerations. This is confirmed by the
	last run with 100K entries. It goes very slowly here, less than 
	3x speedup, possibly because of cache contention? Perhaps it would
	work better to interleave the calculations of different threads,
	rather than to do them in separate blocks.

=============================================================================
19-20 Sep 2009
Dimensions of problem:
	Class management
		Initialization
	Element management
		Elements handle data struct management.
		Erefs do the ops that need index info.
		Elements in arrays: Already by default. Distribute over nodes
			Nested arrays: multiple child elements vs. clever lookup
		Field management: Extended fields? Child data as fields?
	Message management
		Definition at static init: sync for buf, async if hard-coded
		Creation, Deletion: Op on Msg, but sync needs extra work.
		Shared messages: Incorporated into piggybacking: below.
		Traversal
			Field access via messages
			Piggybacking onto messages
				Msg and func are just arguments to send().
			Using messages to scan for targets
			Open-ended messages? Floating messages: not on elms?
			Wildcards as message lists? On floating messages?
				+++Concept merging for connection maps.
				Must store original wildcard path in Msg.
			Iterating through messages and their targets
				Piggyback with returns instead of explicit iter
				Do we have to give return func an id? Nodes?
	Message use
		Process: Centrally called for computation. Also does ClearQ.
		ClearQ: Centrally called all the time.
		Sync: process->src->Data->buffer; process->dest->scan buffers.
		Async: send( Msg, func, args)->buffer; ClearQ->Elm->scan buffer.
		Do we separate spikes from other async (queued) msgs? No.
			Currently Data manages spike Q. Element will clear in
			Process/ClearQ.
		Passing arrays and other masses of data: Ptrs transform to mass
		Return messages?: Temp Msg made from Id of src.
		Functions and their ids: Sync across nodes. Expand dynamic? Bad.
		Would like to access Msg fields, specially dest list, like a
			wildcard. So make msg accessible like an object?
	Parallel stuff
		Threads: Element or index splits? Very sim dependent.
		MPI: Again, need sim manager to decide how to split.
		Object Id management: Master node hands out.
		Moving objects between nodes: Field-based expansion.
		Splitting Elements between nodes: Field-based expansion
	Simulation management
		Scheduling: Msgs from ticks, Special traverse func to call Proc.
		Solvers and objects: Take over Process. Element remaps FuncIds
			during clearQ. Replace Data with forwarding zombie obj.
		Solvers to each other: Messaging.
		Relation of objects to shell: As now.
		Relation of shell to parser: As now.
	Project management
		Unit tests: cxxunit or boost? Develop basecode using UTs
		Expectations for assertions:
		Expectations for documentation: Doxygen.

.............................................................................

I think I have a picture now of most of this framework. Now to design an
implmentation and testing process. Options:
1. Replace current Msg implementation with updated version.
	+ Will get rapidly to test key parts
	+ Smaller, more testable system.
	- Hands tied on heavier testing
2. Go into main MOOSE and begin replacement.
	- Horrible mess. Need to replace basecode part anyway.
	- Too tied into older forms.
	+ Get started on production use
3. Rebuild entire basecode with this design, plan to bolt old MOOSE computation
	modules on top
	+ Good idea for eventual upgrade.
	- Too much stuff to set up before serious testing on parallel stuff
	can begin.
	
Will go with option 1. the current revision is 1056.
Stages
	- Set up standalone Send for async.
		- Fix up Msg data structs to include original wildcard info.

=============================================================================

21 Sep
After some muddling, seems like the place to start is field access
(set/get) using messages. For context, assume that the operation is being
done between nodes. This forces the operation to be done in a general way.

set: Doesn't care about source, so current arrangement is fine. Here all the
	buffer needs to store is the field identity (given by funcId) and value.
get: Needs to specify source. Rather than use Id to do so, let's identify
	it by a message, since there may already be one used for repeat access.
	So we pass in the request to use the message in the access itself.
	This implies that even a transient message needs to register with the
	target Element. Should not be too hard now that we don't require
	argument checking. Messages will need to carry an Id identifying them
	on the src/dest. Otherwise we would have to put the entire Message info
	into the buffer.

	get(Id)->create temp object->locate target node->create temp message
	through to target Id->send data on temp message->target object gets
	message-> puts data onto message->back through postmaster->to 
	originating object->used in script->destroy temp object->destroy msg.

	Almost identical for single node

	This would work with little change for wildcards onto multiple nodes.
	Alternate approach is to ask the Shells to do this locally, and then
	transmit their data back to the originating shell.
	If I did not have to do the messaging, this would take somewhat less
	effort. However, if the messaging is standardized it would take less
	coding.
	
Things to do to get this to work:
Phase 1: Single node messaging
	- Code up MsgSet
	- Implement add and drop for msg.
	- Implement Cinfo that knows # and role of slots : predefined Msgs.
	- Implement scheduling and clearQ
	- Implement Finfo stuff to handle set/get functions.
	- Implement some set/get functions
	- Test above, and valgrind
	- Implement with wildcards: multiple targets.
	- Implement for tables, with predefined Msgs.
	- Implement delete of one of the msg ends.

Phase 2: Multinode messaging
	- Implement cross-node data transfer with hardcoded messages.
	- Implement cross-node message setup. Add, drop, valgrind.
	- Test single field access
	- Test massive tangled data flow.
	

Working from the middle out. Trying to implement a set/get function using
this supposedly clean approach. Current issue: Suppose we have a many->one
projection, e.g., to a channel lookup table. The get function needs to go
back to the originating object.
=============================================================================
22 Sep 2009
Finally some implementation. Goal is to figure out how to specify a single
object for a return message. Need to do the regular messaging first.
The current idea is to have the target Element itself do the final iteration
among target indices. This means that each buffer entry has the function,
its arguments, and a range of indices to which these are applied.
Efficient because the function and arguments are generated only once, saving
both space and time. The key thing is that the range of target object indices
is supplied for now by the Msg.

As far as value return is concerned, we want to tell the system to ignore
what the Msg says, and return the value only to the originating Element index.

=============================================================================
23 Sep 2009
Slowly taking shape. I need to factor in the presence of proxy elements. These
will be pretty real in the case of elements whose objects are distributed.
They may be virtual for elements that only represent their off-node actual
data. I don't know about even having proxy elements for ones that have neither
data nor any messages to the local node.

Looking into using streams to put values into the buffer, rather than the
ugly typecasts currently in use. Two things to do here:
- See how to attach streams to existing char buffers.
- Benchmark it to see if it is as fast as custom typecasting

Tried to do the stream stuff. Total pain.  Forget it for now.

Looking at messages coming to MultiNodeElement and MultiThreadElement
(the combination of the two will be worse).

Unless we precalculate the remapping, things will be hideous. We need to
work out how the range of the message splits up onto different threads or
nodes. So we need to go through all message entries (one range) and 
partition according to the local and remote ranges. At the message level,
this precalculation should result in formation of distinct sets of Ranges,
one per target node or thread.

=============================================================================
24 Sep 2009

Analysis continues. Hamstrung by lack of experience with prior implementations.
Here we have a design decision about when to expand the Range:
- During the final iteration on the target Element. The queue just stores an
	identifier for this message, and the index of the originating object.
- When we put the message on the queue. This means that the queue doesn't
	know about the message: a problem with returns. It will also put a
	lot of stuff repeatedly on the queue.

Seems clear we expand the Range only during final iteration. This means we
need to refer to the Msg on the target element by some Id.

So, another decision: Organizing Msgs. We have a 2-level organization: one is
for individual Msgs, which are maps between entries on single Elements. The
other is for conceptual connections, which are groups of Msgs and also store
the original constructor/wildcard strings for higher-level analysis. Options:
- Make a Connection class for the concepts, which manage Msgs internally.
	- Problem with identifying Msgs in the queue. We would like to do a
	single index lookup to find the Msg, rather than have to look up
	connection then Msg.
	- Problem is that the Conn index may change during setup, if there
	ever are any message deletes.

- Have a vector of Msgs, and a separate vector of Connections. Each Connection
	points to a group of Msgs.
	- Problem is that the Msg index may change during setup, if there
	ever are any message deletes.

After some pen/paper scribbling, it looks like I actually should set up a
3-layer structure:

Msg: Lowest, most specific. From single src Element to single dest Element,
	manages arbitrary indices. Implement as virtual base class with many
	subclasses for different cases of connection patterns. Stored as
	pointers. Shared between src and dest. 

Connection: Next level up. All targets from a single src Element, including
	a vector of Msgs. Equivalently, all srcs into a single Dest.
	Called by Send operation. Includes Func information.
	Also used in Wildcards. Can store original
	Wildcard info. Overlapping but not equivalent on src and dest.

Map: Highest, most general. All the conceptually related interconnections
	between groups of elements. What the GENESIS createmap command would
	build. Stores arrays of src and dest connections. Also stores the
	creation command and parameters in high-level form. Is itself an
	Element, and in due course will have
	capabilities to manipulate the Connections that are its fields.

=============================================================================
25 Sep 2009
Worked through control flow for different aspects of messaging by writing
function sequences. Mostly there, but need to figure out two related things:
- How to deal with specific info lookup for synapses
	This is simple enough. Use the target indices. Have a separate
	target index for each synapse, so that there is only one input coming
	into each. Local info solved.
- How to return data to caller, i.e., SendTo a specific target.
	The s->sendTo function does it by going straight to Element::addToQ.

=============================================================================
27 Sep 2009.
Starting re-implementation with Element. Did svn update to revision 1345.
=============================================================================
28 Sep 2009.
Compiled first pass skeleton version. Many things to do:
- Sort out FuncId stuff and how Slots are to set them up when messages are 
	added.
* Form of OpFunc: I think the size of args should be passed in, not worked
	out in OpFunc. Needed, for example, for off-node stuff.
	Fixed with creation of Qinfo.

- Handle creation of Elements and associated data
- Set up unit tests.

=============================================================================
29 Sep 2009.
FuncId stuff.
Option 1: Unique FuncId for every func. 
	- Simple and consistent.
Option 2: FuncId is semi-unique. Derived classes share FuncIds with their
	parent classes, for overridden functions.
	- We will have to maintain two sets of records: One for each unique
		OpFunc, and another for OpFuncs grouped by class FuncIds.
	- This helps with many inherited functions, such as reinit
	- This helps with zombie functions, which have to transparently replace
		the originals.
	- Helps with Slot/Conn design, since we lessen need to do separate
		Slots for multiple targets.
	Further options:
		- Use function name to determine overlap. Don't bother about
		inheritance, but do carry out RTTI checks during setup to
		ensure that the arguments match. Do not permit overlap without
		this.
			- This effectively sets up global functions.
		- Use function name plus inheritance to determine overlap.
		Do RTTI checks.
			- No globals, but will need to be careful about
			ensuring inheritance where we need it. E.g., reinit.


Trying to compile. Relevant file is Cinfo.h
=============================================================================
30 Sep 2009
Working on Ftype2.h. Also need to fix up OpFuncs and Ftype.h
Created a Dinfo class, to handle information and utility functions for the
Data type.
Struggling with Cinfo::create function. look in Neutral.h

Got it together, compiled.  Checked in as revision 1352

Next: 
	create a neutral *
	do some 'send' tests.
		Began with explict call to stuff data into buffer.

=============================================================================
2 Oct 2009
Working on most basic tests.
I'm now at the point where func requests have gone into the buffer, and the
element is doing clearQ. The execFunc fails because at this point I have no
Msgs on the Element, which is needed to farm out the function calls.
Examining how to handle sendTo calls here. Needed for table lookup like calls.
We need a way of specifying one target Data index in the Element.
Options:
	- Encode index also in Qinfo. Could make FuncId and MsgId shorts,
	and have a full uint for Data index.
		- Wasteful, as use of sendTo is very limited.
			- Table lookups
			- Solver controlling slaves.
		- Will need special messages too, since we need to encode
			the special request into the Qinfo.
	- Encode it as a flag on one bit of the msgid, using the rest of the 
		msgid as the data index. Use Msg # 0 as a
		special one which looks up this one index for its exec call.
		- Really ugly.
	- Ignore for now.
	- Encode SendBacks as an Element-level consolidation either way
		into vectors.
	- Encode SendBacks as individual Msgs.
	- Define special SendBack Msgs that read extra info in args.
	- Use sync msgs for this sort of thing.
The conclusion of all this is that if I need a SendBack capable message, I
have to set up a special msg.

There is a good bit of cleanup needed for the buffers, both for safety
and efficiency.
Safety:
	- Should do as a vector, with expansion as needed
Efficiency:
	- Align on ints or doubles?
	- Use sensible sizes for fields in Qinfo.
	- Map data directly to lookup structures like Qinfo, rather than
		doing a copy.
	
Got it together, compiled.  Checked in as revision 1353

One clarification: We have to pass in the SrcIndex for every call: many of the 
Msgs use it. For example, the OneToOne and the SparseMsg both use it to look
up targets. So Qinfo needs another field.

Also it is desirable that we should be able to use SendTo with regular
messages. Perhaps the added field for the SrcIndex in Qinfo can be used for
this?
Instead I think the data buffer should be used for the return index. Let's
have it so that if needed one can always write an OpFunc that can see the
src MsgId and src index (which together let us figure out the source Element).
These are there in the args, it is just a case of using the additional 
fields in Element::execFunc, or passing in a reference to Qinfo.
What remains is a way to tell the Msgs to use the extra index in the
args to determine the target of SendTo. Best to use the regular OpFuncs,
so this means that the Msgs also juggle the args.

Implemented it, tested the second part of it by stuffing the queue directly
and then calling clearQ. That part works.
Checked in as revision 1356

Next: 
- Get the 'send' part to work
- Do a valgrind cleanup. 
	- Figure out how to delete the data part cleanly.
	- Msg deletion cleanup.
	- Serious tests for memory leaks, lots of deletes and creates.
- Sort out message setup including type safety 
- Utility function (using messages) to 'get' field values.
- Heavy traffic tests
- Sync messages
- Start to play with nodes and threads.
- Optimization: buffer alignment, clean up management, use in-place rather than
	copy.

=============================================================================
3 Oct 2009
Before going into the above, doing a cleanup of memory allocation. 
Valgrind helped track things down.
Checked in as revision 1360: Major stuff cleaned.

Another round of cleaning up, this time mostly stuff from class initialization.
Now I have it so Valgrind is completely happy.
Checked in as revision 1361.

Implemented the 'send' operation using a hand-crafted Slot object. 
Ran through it with valgrind, good first pass.
Checked in as revision 1362.

=============================================================================
4 Oct 2009.
Look at message setup.
I've had a big simplification by separating the functions from the Msg.
For the existing MOOSE messaging concept, though, the idea is that in Process
or in response to a function call, an object should send data on a Message
specifically set up for the purpose. Using this message, the object calls
specific target functions whose type is predefined, and whose identity 
is set up at the same time as the messages themselves.

There is no room for slop: precisely the right # and type of target functions
must be known.

In the present design, the Slot does the job of tying Msg to function.
At compile time we know what the Slot type is for all slots we can use.
At Element creation time we can create predefined Slots for fast function calls.
	Has to be then, since we want to hardcode Slot identity into
	e.g. process functions.
At Message setup time we can attach a ConnId and a FuncId to these slots.

At compile time we know the ConnId for all precompiled Msgs.

Instead of a separate Slot class, let's use SrcFinfos for the slots. 
All SrcFinfos sharing a common Conn (Shared messages) just store the
index to this Conn.
The SrcFinfos/Slots do NOT store the FuncId, since they are static and the 
FuncId is defined at setup time or even runtime. Instead they index a vector of
FuncIds somewhere. This will be a fixed (statically defined) index, so we
know the starting size of the vector. Options:
	- On the Conn:
		- Should keep Conn just for the connection info
		- Clean association of Funcs with appropriate Conn
	- As a separate vector on the Element
		- A bit more economical, as it is a single vector for all the funcs.

Note that run-time defined Slots could either directly hold the FuncId, or
do this index stuff.
Note that in this design we do not need to have anything special for the
destinations in shared messages. The correct types have to exist, is all.

In many cases the shared messages had MsgSrc on both sides. Often these 
were for send-back situations. Also for bidirectional data such as 
reaction-Molecule data transfer. Options:
- Ignore this. Go back to GENESIS style where two messages had to be set up.
- Implement as a higher level op, that results in two distinct messages
	being formed, one each way, 
	- Can we put both on the same Msg?
		- This has some restrictions on there being matching conns.
		- Really only a minor matter for the msg creation func to handle
		- As at present, it could be symmetric or asymmetric.
- Try to do as a single conceptual Msg.
		- Don't see how it would work.

=============================================================================
8 Oct 2009
Target elements should decide which func to use for a given message?

Issue is having multiple possible target funcs from same SrcFinfo
e.g., mol conc going to plot, to MM enzyme and to channel.
Possible implementations:
Category 1: SrcFinfo has static-init defined conn Id and FuncIndex.
	Option 1:
		- connId looks up entry on Element::vector<Conn>
		- FuncIndex looks up entry on Element::vector<FuncId>
		- Conn marches through Msgs, using same FuncId for all.
		- Target finds OpFunc from FuncId using Cinfo::funcs_[FuncId]
		Issues:
		- Can't handle multiple kinds of target funcs, only derived.
	Option 2:
		- connId looks up entry on Element::vector<Conn>
		- FuncIndex looks up entry on Conn::vector<FuncId>
		- Conn marches through Msgs, using same FuncId for all.
		- Conn is a linked list, and next conn has next FuncId.
		- Target finds OpFunc from FuncId using Cinfo::funcs_[FuncId]
		Issues:
		- Ugly. Need to manage linked list, but only occasionally.
		- Puts Func info on Conn.
		- Costly. Each Conn manages a vector of FuncIds.
	Option 3:
		- connId looks up entry on Element::vector<Conn>
		- FuncIndex looks up entry on Element::vector<FuncLookup>
		- Conn marches through Msgs, using same FuncId for all.
		- Target finds OpFunc from FuncId using Target::funcs_[FuncId]
		Issues:
		- Need to set up vector of funcs_ on each target Element
		- Passing around and setting up a dynamic FuncLookup. Too cute.

Categorey 2: SrcFinfo has static-init defined conn Id and SrcFinfoId
	Option 4:
		- connId looks up entry on Element::vector< Conn >
		- SrcFinfoId plus specific, per msg index, to specify tgt.
		- Conn marches through Msgs, using same FuncId for all.
		- Target Cinfo has relatively small list of possible targets
			for a given SrcFinfoId, based on type matches.
			specific index pins it down.
		Issues:
		- Still problem with multiple kinds of target funcs on Conn.
Category 3: SrcFinfo has static-init defined MsgId, using Msg link lists.
	Option 5:
		- MsgId looks up entry on Element::vector< Msg* >
		- March through linked list of Msg* (all on vector)
		- Each Msg has a matching entry in vector funcs_< FuncId >.
		- Target Cinfo has relatively small list of possible targets
			for a given SrcFinfoId, based on type matches.
			specific index pins it down.
		Issues:
		- Still problem with multiple kinds of target funcs on Conn.
Category 4: Exception handling. Assume that a single FuncId will normally
	work, and treat other cases as rarely-used exceptions.
	Option 6:
		- connId looks up entry on Element::vector<Conn>
		- FuncIndex looks up entry on Element::vector<FuncId>
		Normal case: FuncId in range:
			- Conn marches through Msgs, using same FuncId for all.
		Exception: The FuncIndex is out of range
			- FuncIndex identifies vector of FuncIds
			- Conn marches through Msgs, using different FuncId
				for each, from vector of FuncIds.
		- Target finds OpFunc from FuncId using Cinfo::funcs_[FuncId]
=============================================================================
10 Oct 2009
Compiled implementation. runs up to part way, then croaks on uninitialized
variables.
=============================================================================
11 Oct 2009
Fixed croaking problem: I had redefined some private fields in SrcFinfo. 

Remaining issue with setting up the FuncIndex. This is something to be done
by Cinfo::init().

Fixed testSendMsg issue with setting up FuncIndex. 

Converted both to doing almost silent unit tests.

Implemented and cleared testCreateMsg, cleared valgrind. Need to make it silent.

Also it is unable to find the 'set' field for the field assignment part,
which is fine as I haven't yet defined it.

For doing 'set':
- Create Element explicitly, not using Create function
	Pass in a ptr to the field to be set/get? not needed.
- Create Msg using regular msg func.
- Call asend directly



Need to revisit all opfuncs to pass in Qinfo and Eref.
Done.

Need to revisit Conv<A> to return a reference or something similar, to avoid
alloc etc.

=============================================================================
12 Oct 2009.
Put the Qinfo into the OpFunc. Runs, clears.

I have two variants of the OpFunc, perhaps premature optimization. One of them
ignores all the additional info available, such as Qinfo and target eref.
The other is also derived from OpFunc, but its func takes the reference to
the Eref and the ptr to the Qinfo as additional args.

Still to set up.

Checked in as revision 1368.
=============================================================================
13 Oct 2009.
Implemented EpFunc, which is a variant of OpFunc that passes in the
Eref and Qinfo along with other args. This is needed whenever we have a 
function that manipulates the Element, or needs additional data about the
incoming message.

Checked in as revision 1370.

=============================================================================
14 Oct 2009.
Implemented testSet unit test. Seems OK, but need to clean up memory leaks.
Checked in as revision 1372.

Working on memory leak. Unexpected major problem with whole approach: the
allocated message m is needed during the clearQ, after the set command
has returned. So we have a dangling message.
Related problem: We will have clearQ after Set has returned. So script
command sequencing cannot be guaranteed, unless we tell the script execution
to hold off till clearQ. This gets worse with 'get'.

Working on 'get'. I have the skeleton in place.
=============================================================================
15 Oct 2009
More implementation on 'get'. 
- Msg leak issue could be handled if the msg is permanently stationed on the
	SetGet object (due to be the Shell). Its far end can dangle, and be
	connected to targets as needed.
- Would want to refine this to deal with wildcards, so want a Conn, not just a
	single Msg.
- Would want to do cleanup and continuation of script function on the
	SetGet::handleGet. This function is triggered only when the 'get'
	call returns. A bit fragile, will want a timeout.
- If we have multiple targets for 'get', we will need an index to go out to 
	each target, and come back with the data, so that it can be organized
	into a vector. The recipient function will then have to keep track of
	how many have returned.
- Do we need multiple 'get' buffers and funcIds? If the effective utilization
	is serial, should be OK to have just one.

After a day of implementation and debugging, seems to work.
Checked in as revision 1375.

This is leaking memory copiously. Next step is to organize set and get
through the SetGet object/shell, of which there should be just one instance.
In the current test run there were 100. This should allow us to reuse the
Msg from the SetGet object, and avoid the memory leaks.

Next Steps:
- Clean up Set/Get
- Heavy traffic tests
- Sync messages
- Start to play with nodes and threads.
- Optimization: buffer alignment, clean up management, use in-place rather than
	copy.
- Incorporate unit tests into cinfo
- Provide message tree diagnostics.

=============================================================================
17-18 Oct 2009.
Replaced SetGet with Shell. Set up automatic creation of Shell as /root,
during runtime init. Still leaks memory.
Checked in as revision 1384.

Working on handling Msgs from Shell. Need still to clear out old msgs during
Set.

Although it works now, valgrind picks up a nasty situation. When a dest
Element is deleted, Msgs on it need to be deleted too. Msgs know how to 
remove themselves from the src element, but not from Conns, which also
have pointers to them. Options:
1. Do a grungy search for Msg ptrs on all Conns. Deletes are rare so should
	be OK.
2. Store an extra index or two in each Msg for identifying parent Conn(s)
3. Conns do not store Msg pointers, but lookup indices for them on the Element.

Let's do #1. 
Done. This completely fixes the memory leaks that afflicted the 'set' function.
At some point I'll have to benchmark to figure out how much
of an impact the message deleting has on the overall performance.

=============================================================================
20 Oct 2009.
Also moved the 'get' function to use the Shell element.
Next I need to generalize both set and get to handle arbitrary types.

Trying to find a suitable place to do this. In the process I found that
Send.h and Async.h are no longer used. Removed them.
Checked in as revision 1388.
=============================================================================
26 Oct 2009.
Implemented Set and Get operations in a new templated SetGet class. Better
typechecking. Compiles but it doesn't yet work.
Checked in as revision 1404.

=============================================================================
31 Oct 2009.
Finally got to do some debugging. Fixed problem, now works, clears
valgrind. 
Checked in as revision 1422

* Need to test with doubles and other types. Done. Did a partial 
	re-implementation of IntFire, and did set/get on one of its 
	fields of type double.

Checked in as revision 1424

There are several issues with a full implementation of IntFire, most notably
that the design now requires there to be a distinct target Synapse object for
each incoming message. In the earlier version we had some extra info 
figured out by the system to identify the target SynInfo. here I just want
to use the index of the traget Eref. This is good, but now the destination
IntFire needs to juggle some subset of the target Synapses, which are
independent objects presumably on a child element.
For efficiency, the IntFire would like to have target Synapses as internal
fields.
For indexing synapses, we want each to be an individual array entry in a
big array of Synapses.

Assume we'll handle allocation etc within the parent IntFire.

How to set up indexing? 
	- Give each IntSyn the same #, which is the biggest # of syns on 
	any one. 
		- This needs us to be able to allocate an array of Data with
		holes in it. Easy to do if we have an indirection step on
		the Element, but as built the Element won't do it.

	- Set up precisely as many IntSyns as are needed.
		- Indexing and relating IntSyns to parent are both hard.

	- Explicitly make it look like a 2-D array, with variable length
		sub-indices.
	
	- Make it look like an array of Elements each with an array of
		Synapses.
=============================================================================
1 Nov 2009
Synaptic info options.
1. Separate SynInfo or Synapse objects. Each receives input from only
	one axon. The whole lot are in the same Element.
	Spike operations:
		- Spike arrives on a Synapse. 
		- Synapse sends Msg to parent Element, with delay etc info.
			It needs efficient orgn of the messaging to parent.
			Even with optimal organisation, this is costly, 
			going through entire msging again.
		- Parent Element updates its pendingEvents queue
	Process operations:
		- Check if queue is due. If so, handle event and pop queue
		- Check if Vm exceeds thresh. If so, send spike, and reset Vm.
			Otherwise do exp decay calculations on Vm. 

	This has an unpleasant extra messaging step from Synapse to parent.
	However, there may be efficiencies in the first msg from axon to
	Synapse as we guarantee a single input.

2. Messages are directed to Synapses but are processed directly by
	IntFire.
	Can't do this without some juggling of target index. See next.

3. Messages are directed to parents of Synapses. Munge the indexing of the
	target Element so that part is used for indexing it, 
	and part to index the correct Synapse.
	This could be a special case of an ability to index 2D arrays.
	But where does one stop?
	Or, the extra info could just be something that messages can generally
	do.
	This arrangement deals with the threading.
	It also eliminates the issue of passing info down to parent.

4. Give the Synapse a pointer to its parent Element or IntFire.
	Issues with threading.
5. Special OpFunc to munge index.
	- Create Element that deals with individual synapses, but points
		to the parent IntFire (or ReceptorChannel) Element.
	- OpFunc munges destIndex in some efficient manner. Bitmap may be
		best, using top n bytes for specific synapse index.
		For IntFire, we may need 2 bytes for synapse, leaving only
		2 for target IntFire. Insufficient.
	- OpFunc is class-local, so we can set up some reasonable subdivision.
	- Pass in synapse index as additional arg to the func encapsulated
		by the OpFunc.
	- Can generalize to other fields of Synapse
	- Can generalize to arbitrary arrays using templated 
		ArrayOpFunc with dimensions?


To test scalability, let's do this:
- Make a speed reference using a sparse matrix and customized code to replace
	messaging, as a recurrent network with N neurons and P probability
	of connections. Goal is to do N >= 1e5, P ~0.1.
- Make the equivalent network in MOOSE
- Profile, look at bottlenecks.
- Repeat with multithreading
- Repeat with multinodes


Let comp load of IntFire processing be I.
Let comp load of Synapse processing be S.
Let spiking occur every T timesteps.
N and P are defined above.
Let # of synapses per neuron = #

If we ignore communcations cost,
total load per timestep = I * N + N * # * S / T
Some numbers
N	#	I	S	T	Load	Notes
1e5	1e4	1	2	100	2e7	zero messaging cost
1e9	1e4	1	2	100	2e11	zero messaging cost
1e9	1e4	1	10	100	1e12	medium messaging cost
1e9	1e4	1	100	100	1e13	high messaging cost

Now we go down 10x smaller timesteps.

1e9	1e4	1000	2	1000	1e12	Realistic neuronal models,
						zero messaging cost
1e9	1e4	1000	10	1000	1.1e12	Some messaging cost.
1e9	1e4	1000	100	1000	2e12	High messaging cost.
					
Main points:
- in IntFire networks synaptic comput and messaging are overwhelming
- in realistic neuronal model networks, the costs are comparable.

Overall, efficiency does matter for spike messages.

we may need 13 digits x3.3 = 43 bits. Too much for even an int.

=============================================================================
2 Nov 2009
Working out implementations for accessing array fields, such as synapases 
on an IntFire.
See ArrayOpFunc.h::ArrayOpFunc0

The array of synapses should act just like any other array in terms of 
field access, adding messages, and so on.
	This means that the Eref::data() function has to behave the same way
	as for any other Element, and look up the correct Synapse without
	further effort.
When there is a need to do operations through the parent Data, then we use 
	Eref::data1() to return the parent data.
	Eref::index() has all information needed to look up synapse from data1.
To handle these cases, we have a separate set of OpFuncs that operate on
	data1 and pass in the index. This is the UpFuncs.
To do these seamlessly we need to make DataVec a virtual base class, and
	have 1D, 2D, 1D+field and similar options. The DataVec handles 
	deleting of data too, so if we need to have more than one Element
	refer to the same data, then suitable DataVecs have to be defined for
	each element, and only one may delete the data.
	Alternatively, we should have the Element itself be a virtual base 
	class.

Stages:
* Put in DataId for all Eref indices.
* Replace Data with char* in Element::data
* Make Element a virtual base class.

Checked in as revision 1425

Next:
* Derive off Element for Synapses on IntFunc. Checkin 1427.
* Do Synapse and SynElement implementation. Checkin 1428.
* Come up with special element creation options to set up this element. 1429
* Check that field access works with it	Checkin 1431.
+ Send spike messages to it
* Sort out how to handle its clearQ.
* Fix up sub-fields within DataId.
* Clean out const A& argument in OpFuncs. Should just pass argument, as most
	of them will be doubles. Will need separate classes for strings and
	things. Checked in as 1429, except for string stuff.

Sending spike Msg seems OK, need more tests.
=============================================================================
4-5 Nov 2009
working on testSendSpike. Runs but doesn't seem to make sense. After some
debugging got it to work, sends the spike info. Much of the problem is due
to the ugly handling of DataIds. Checkin 1433.

Need to fix that next. Fixed. Checkin 1434.
Also valgrind is very unhappy with the allocations. Need to fix that too.
	Fixed, it was a simple matter of deleting the IntFire element. 1435.

Next: 
- Implement a sparse matrix Msg type and use it to build a serious
	feedback network of IntFires.
- Clean up scheduling a bit so that we can see the data flow over multiple
	cycles.


Working on sparse matrix. In order to fill the synapse indices, I need
to fill the transpose of the matrix to start, and then transpose it.
Transposition of the sparse matrix:

Start with 

[	1	0	0	0	2	]
[	3	4	5	0	0	]
[	0	0	0	6	7	]

N_ =  1234567
Col = 0401234
rowStart = 0257

Transpose is:
[	1	3	0	]
[	0	4	0	]
[	0	5	0	]
[	0	0	6	]
[	2	0	7	]

N_ =  1345627
Col = 0111202
rowStart = 023457

To transpose.
Step 1. Sort N_ by col to get N_T
Step 2. Convert rowStart to row# for each entry, so, 
	0257 becomes
	0011122
	Sort this by col = 0401234 to get new set of cols:
	0111202
	Note that this sort needs to retain original order of otherwise equal
		numbers. So the first 4 comes before the last one.
Step 3. Sort the col itself to get the new sequence for row#s:
	0401234 becomes 0012344
	Then put row starts on each, whenever the value increments:
	02345
	and wrap it up with a 7.

Implemented. Compiles, not quite there with the unit tests.
=============================================================================
6 Nov: Fixed up, now does correct transposition.
Checked in as Revision 1436.

Next step is to do tests with messaging.
Working on it. An issue comes up with randomConnect: In the function I
set up the messaging to synapses, but the target object has not yet allocated
the synapses. Good, we can do this correctly after setting up. 
Bad, because we don't have a general way to tell objects that a specific
field needs to be assigned. It is numSynapses in IntFire, but could be
one or more different other fields.

=============================================================================
7 Nov.
Approach taken to allocate synapse memory, which is generalizable to other
kinds of array fields:

- We will usually have to access these other fields as part of the setup
	command. For example, setting synaptic weights.
- The messaging command itself passes input to a named field. More to the
	point, the SynElement is on a specific array. Should be able to
	provide info to it generally to define values in this array.
	- The UpFuncs serve this task in the Cinfo. However, we need this
	feature in the SynElement type classes quite generally. So it has
	to be something that the compiler enforces.
		- UpFuncs in the SynElement constructor?

- Remember that messages were to be the equivalent of wildcards. We should
	use the created message itself to assign fields, including setting
	up the weights and the allocation of synapses.
	- Setting weights: Implement a
		setrandom<double>( const Msg* m, const string& field, 
			double lo, double hi, bool isLog );
		function.


OK, hacked it in for now as hard-coded access functions within the SynElement.
Compiled stuff and cleaned up old printf testing, now uses assertions as
part of unit tests. Checkin 1438.

Successfully created a 10000 X 10000 sparse matrix with 10% connectivity.
So about 1e7 entries. Expect memory use to be about 1e7 * 8 bytes. The
transposition would have used about 1e7 * 12 bytes more.
Oddly it used over 1.5 G, perhaps would be less if I
reserved the space rather than fill it with push_back calls. 
For unit tests I'll use 1000X1000 as it is much faster.

Valgrind is not amused: an error somewhere. 
The size of rowStart() is 1 smaller thn it should be.
This was quite nasty. I put in assertions that should have caught it but
did not. I checked the web for odd interactions between assertions and
templates. Finally I realized that SparseMsg.o did not depend on SparseMatrix.h
in the makefile. So the SparseMsg was not seeing any of the updated code.
Fixed, compiled, reran, clears valgrind. Checking 1440.

=============================================================================
8 Nov 2009
Would like to implement a vector 'set' operation.
For now stay focussed on the sparse messaging.

I had a difficult bug in field assignment that only materialized after a very
large number of assignments. After a lot of meandering, turned out that the 
problem was that I was not clearing old messages out. In the absence of this
garbage collection, the system was correctly assigning new msgids as it went
along. In due course the system overflowed the 'short' field range.
Solution, of course, is to fix up the garbage collection of old messages,
or rather, the slots allocated to them.
Implemented it. Works. Valgrind takes several minutes to chew on it, but
eventually it too passes.  Checked in as revision 1441.

This is as thorough a test of set/get as any i
I've done so far. The vector 'set' operation would help.

Now working on synaptic delivery. The system is taking up over a gig of
RAM to store the pending synaptic events for just one timestep.

# of synapses = 1e3 * 100 = 1e5.
Should not happen even if every single synapse fires.

I wonder if the buffer keeps getting extended as the process call is done..
No, should have a cleanly separate eventq.

Tracked it down, the SparseMsg dispatcher was sending out stuff to all
targets regardless of the originating Eref. Fixed. Lots of tinkering later,
we have a reasonable IntFire network. It goes into saturation rather
quickly above a certain threshold of connectivity, otherwise decays.
Also it scales pretty well in terms of speed and memory. 
Need to do benchmarking.
Checked in as revision 1442.

Tried to do a profile. Failes outright in the optimization compile mode,
with or without profiling. Clears valgrind in debug mode. So I am confused.
Managed to track it down to very first unit test, was an innocuous array 
allocation. Fixed and now works.

Profiling shows that the most time is spent doing the field allocation.
Silly. Let's set up vector assignment.
Done, checked in as 1443.

Now did the profiling with a long (1000 timestep) run of the IntFire
network. The results are gratifying: By far the largest time is spent in the
heap operations for the synapse. All told, under 10% of the time is spent in
messaging.

  %   cumulative   self              self     total           
 time   seconds   seconds    calls   s/call   s/call  name    
 56.83     18.49    18.49 102317402     0.00     0.00  void std::__adjust_heap<__gnu_cxx::__normal_iterator<Synapse*, std::vector<Synapse, std::allocator<Synapse> > >, long, Synapse, std::less<Synapse> >(__gnu_cxx::__normal_iterator<Synapse*, std::vector<Synapse, std::allocator<Synapse> > >, long, long, Synapse, std::less<Synapse>)
 17.09     24.05     5.56 103414264     0.00     0.00  IntFire::addSpike(DataId, double const&)
  7.84     26.60     2.55 103414264     0.00     0.00  Synapse::Synapse(Synapse const&, double)
  4.95     28.21     1.61  1012617     0.00     0.00  SparseMsg::exec(Element*, char const*) const
  3.78     29.44     1.23  1024002     0.00     0.00  IntFire::process(ProcInfo const*, Eref const&)
  2.06     30.11     0.67 103414264     0.00     0.00  UpFunc1<IntFire, double>::op(Eref, char const*) const
  1.20     30.50     0.39 103414264     0.00     0.00  Eref::data1()
  0.86     30.78     0.28 102317402     0.00     0.00  Synapse::getWeight() const
  0.71     31.01     0.23 103340378     0.00     0.00  Synapse::getDelay() const
  0.65     31.22     0.21 104896327     0.00     0.00  Eref::Eref(Element*, DataId)
  0.61     31.42     0.20                             GetOpFunc<Synapse, double>::op(Eref, char const*) const


Checked in as 1445.
Now to change it so it is more like a unit test.

Calculations: 1024 * 102 synapses ~1e5
Towards the end, it was saturated: always firing. So rate = refractory
period = 2 timesteps.
# of timesteps = 1000.
So 5e7 synaptic events were transmitted, in about 30 sec. ~1.3 million/sec.

Back on 1 Nov, these were the planned steps:
To test scalability, let's do this:
- Make a speed reference using a sparse matrix and customized code to replace
	messaging, as a recurrent network with N neurons and P probability
	of connections. Goal is to do N >= 1e5, P ~0.1.
* Make the equivalent network in MOOSE
* Profile, look at bottlenecks.
---> Implement scheduling
- Repeat with multithreading
- Repeat with multinodes

I've done a couple of these. The next step is to do the standalone version
without messaging, to get a tighter estimate of the messaging overhead.
Then I need to insert a stage where I implement scheduling, before going
on to the multithread stuff.

=============================================================================
9 Nov 2009.
Made another branch, based on moose/Msg. In:

/home/bhalla/moose/IntFireNetworkNoMsg

Munged the IntFire and related code
in it to directly call functions instead of message passing.

=============================================================================
10 Nov 2009
After some debugging, managed to get the code to work. There is very little
difference with the profiling: 30.2 sec for the message-less version, as
compared to 32.54 sec for the messaging version.

Ran using 'time' a few times with optimization but no profiling.
messaging	non-messaging
37.3		33.7
37.1		33.5
37.1		33.7

So the difference is about 3.5 sec, or a bit over 10%. This is outstandingly
good.
Checked in the IntFireNetworkNoMsg stuff as the end of its branch. This is
revision 1448.

Then deleted the branch from my local machine. Still sits on SourceForge.

Now on to scheduling and threading.

The scheduler has to send messages for process and clearQ.
During runtime:
	process and clearQ alternate strictly. Many threads to coordinate.
	Shell has to be blocked, but with an option to halt runtime thread.
		Ideally this could be done reversibly.
	Graphics has to continue on yet another thread.
During setup:
	clearQ must run in coordination with the thread of the Shell.
	- Cannot be on the same thread, since we may need to hold up the shell
	while completing queued calls.
Graphics and the other threads
	- Graphics sits on a separate thread.
	- I need a separate channel for data to go from process to graphics.
		This is both for the OpenGL graphics, and for Qt.
		Looks like a thread-safe queue here. 
		Will need graphics first and computation first policy options.
	- Qt and OpenGl events will probably be handled by Python.

I'll use the recently implemented stuff for array fields to do the clock ticks.
Also I'll use priority queue to manage it, rather than the customized version.

=============================================================================

Need to call Erefs by value in EpFunc.h
Issue with having ticks as ArrayFields: they need to be sorted. If the
sort order is changed, then the messaging will have to be remapped accordingly.
This is do-able but involves a possibly messy extension of messaging functions.
The alternative is to have them as separate Elements, which is messy in other
ways.

=============================================================================
13 Nov
Working on Clock.cpp.
=============================================================================
14 Nov.
Put in skeleton of Clock and Tick, compiles.
=============================================================================
15 Nov.
Setting up unit tests. Need to define calling framework.
- Creation of ticks
	- Could create a certain # explicitly, like I do with Synapses.
	- Could have an 'addTick' function on the Clock. Would need dropTick.
		- the addClock function works better with this.
		- Messiness if I drop a tick in between the defined set.
	- Could create say 10 clocks by default, but manange only the
		set in the TickPtr vector.
	- Could get rid of 'stage' field by considering index on the Tick 
		vector. But there is no rule about ordering clocks by dt
		(though it is implicit somehwat in GENESIS for clock 0)

Anyway, now that I am back online, checked in a large backlog of changes
as revision 1455.

Went through the unit tests, converted the massive printouts into assertions.
Did a little cleaning using valgrind. Now OK. Checkin 1456

Starting up with a template for the TickElement. yet to compile.
=============================================================================
16 Nov
Now trying to compile.
=============================================================================
22 Nov.
Resuming work after IMMMI. Compilation works for the FieldElement template
to handle arrays of fields. Checked in as 1457.

Now worked through replacing SynElement with the FieldElement template.
Works, clears unit tests. Checked in as 1458.

Setting up clocks and ticks. Issue: How will ticks be added? Seems like
the safe thing to do is that any change at all in any of the tick
parameters (dt, stage, or # of ticks) should invoke a rebuild.

Implemented much of the Tick scheduling stuff. Checked in as 1459

Will need an UpValueFinfo: assignment of fields get diverted to parent.
Will need to sort out calling of Process and clearQ. Consider Reinit too.

=============================================================================
23 Nov.
Ways to approach the Tick field assignment stuff:
Pedantic: 
	The tick field access functions themselves ensure updates of the Clock.
	- Can do as a special case by making an UpValueFunc which calls
	the parent clock to do the field assignment, and handle updates.
	This is clean enough, a little tedious and ugly for the field funcs.
	- Can do as a general case by making all array field assignment calls
	into calls to the parent. Ugh.
	- Can do as a general case by providing extra args so that the function
	can work out who the parent is. Can ignore this stuff if not needed.
	Also somewhat ugh.

Pragmatic:
	Tick field access just updates fields locally. We need another call
	to the Clock to rebuild the schedule.
	The wrapping Shell functions for handling clock ticks does this.
	- This would allow calls to change ticks without having an effect
	on scheduling. Could be surprising.
	- There may yet be other cases which need to do similar things.

Hacks:
	- Provide ValueFinfo with an auxiliary SetFunc
	- Provide ValueFinfo with an auxiliary trigger func for whatever
		other operation is needed on parent when a field changes.


I'll use the UpValueFunc, which is what I had originally planned.
Checked in as 1460.
Implemented, tested for one field. Works OK. Valgrind also happy. 1461.
Implemented for second field as well. 1462.

Set up unit tests for setupTicks(). Looks good. 1463.
Called 'start'. Hangs, looks like infinite loop.

Implement getVec

=============================================================================
24 Nov.
Working on scheduling.
Algorithm:
Current time = 0
While (currrent time < end time)
	Sort all tickPtrs in order of nextt and stage.
	Execute the first in sequence.
		Current time becomes the nextt of the just executed tick.
		nextt is incremented.

Minor fix to this, since we want each tick to be called just as the system time
advances. So the first call on tick0 (with dt = 1) is at t = 1.
Checked in as 1464.

Next: handle process and clearQ alternately. This is a job for the Ticks.
To call all process then all ticks:
	No, the idea of the different stages is that a complete calculation
	can be sent on during the same overall timestep, comprising several
	ticks with the same dt but different stages.
So, assume we call process and clearQ alternately.

What to call first:
	Process:
	- ClearQ will have been called ahead of time by the system. For example,
	reinit will already have been called, and we have values ready to use.
	- If there were earler stages within this overall timestep, then 
		we will not have a way to access data passed in.
	ClearQ: 
	- This will allow a given tick to handle incoming data and deal with
	it, and pass it on in Process.
So, call ClearQ first.

Do we call ClearQ strictly alternating, or should stuff be cleared more often?
	- More often clearing adds compute cost
	- More often clearing might lessen queue length.
	- Never need stuff till Process.

So: Better to strictly alternate with Process.

Do we have multiple Process calls?
	- Several GENESIS type calculations need a separate 'init' stage then
	a 'process' stage. For example, the compartment uses
		- init: previous_state = Vm
		- process: traverse messages, do integration.
		If two compartments A and B exchange Vm, then they need to 
		exchange previous_state in order to avoid asymmetry.
	- In MOOSE, with the clearQ arrangement, this would not be an issue.
	The data exchanged will always be previous-state, due to the sequencing
	of clearQ and Process.
	Seems like I never use it in other contexts.
So, don't need multiple Process calls.

Almost there with the implementation, stuck because the 'advance' call
needs the correct Tick Eref as an argument.

=============================================================================
25 Nov.
One possible hack is to dual-derive Ticks from Msg. I don't like it.
Another is to store the Element for the Ticks in the Clock. Might be OK
if it is a child. Better if it is found from a Msg.
Separate from this, is how to rapidly access the list of Msgs from the Ticks.
	- have a distinct Conn for each Tick. They could have the same index. 
=============================================================================
26 Nov. Avoided hacks, got the TickElement from a message.
Set up a distinct Conn for each Tick, using the same index as the Tick's
own index.

Many changes needed to get all the pieces to work together. Compiles, 
yet to get it to work.

OK, now works. Cleaned out the scheduling test so it isn't verbose anymore.
Also valgrind is happy. Checked in as 1466.

Back on 1 and then 8 Nov, these were the planned steps:
To test scalability, let's do this:
- Make a speed reference using a sparse matrix and customized code to replace
	messaging, as a recurrent network with N neurons and P probability
	of connections. Goal is to do N >= 1e5, P ~0.1.
* Make the equivalent network in MOOSE
* Profile, look at bottlenecks.
* Implement scheduling
- Repeat with multithreading
- Repeat with multinodes
Now we add:
- Redo messaging with sync messages
- Make a good test case, say the signaling network again?
- Profile, look at bottlenecks
- Do multithreading
- Do multinodes.

=============================================================================
28 Nov.

Multithreading.
- Need to guarantee that clearQ is element local, and all outgoing
	messages emerge only at Process.
	- Otherwise we might add stuff to queues at the same time
	as we read from them.
	- Alternatively, have to maintain a separate queue for clearQ input
	vs output.
	- Or we could do a mutex to grab control of the target queue for the
	time we need to put data into it.
- Use a separate queue on each Element for each thread.
	- Alternatively use thread-safe queues. Less memory, more mutexes.
- Barrier after clearQ and after Process.
- Process is easy to multithread as it is data object-local.
	- separate sub-parts of data among threads.
- clearQ is hard to multithread as each msg in the queue could refer to many
	data objects, and there can be many msgs. 
- How to multithread the clearQ

Scaling up to huge numbers of threads: This should use the same decomposition
as scaling up to a similar huge number of nodes.

Suppose I have 1024 threads in my simple network example.
1. Single or multiple schedulers?
	- Do I want them to run out of sync? I'll need barrier synchs
		in either case.
2. Explicit Element Data splitting or implicit (and even dynamic) as per
	thread indexing?
	- Explicit would simplify the queueing model: It would still be the
	same as single-threaded, as each thread would manage one queue.
	- How to access stuff with explicit data splitting? Reference Element
		will have to keep track and if needed, redirect. But this
		can't be done for all messages: they will need to reshuffle.
	- Dynamic would do good things for load balancing.
3. Message structuring.
	- Explicit Element data splitting will put a separate queue request
	in for each Element proxy on each thread. So the clearQ can become
	single threaded, cleanly, but filling the queue requires thread 
	juggling.
	Note though that the queue is typically much fewer calls than the # of 
	calls needed when clearing it, since the target Msg distributes queue
	calls to many Data destinations.
	- At send time put messages through a crossbar that knows which
	threads need to receive each msg. So originating thread A has
	holding queues for threads A-Z. After barrier, we now access the queues
	by destination thread.
	- Simple implementation would be that each originating message just
	goes to each target thread holding queue. Clearly need to structure up
	this to only deliver a given message to the queues that really need it.
	Note that this is exactly what I would have had to do for multinode 
	stuff. 
		- This looks like a SparseMsg.
		- The target thread gets one big fat queue, rather than one
		per Element. Not hard, just need to add dest Id to Qinfo.
		- Alternative to one fat queue is for the message delivery
		crossbar to direct messages already into the correct Element.
	- another approach: give each Message a unique id for the entire
	simulation. When sending info, we don't now need src or dest Element,
	as this is all in the Msg. So a single fat queue is possible for
	each thread.
		- Qinfo already holds a MsgId, but it will have to become
			an unsigned int rather than short.
		- Will need a boolean to indicate msg direction.
		- Will need to expand srcIndex to be a full DataId.
		- funcId, and size remain.
		- Would like a way to cleanly handle tgtIndex for SendTo.
=============================================================================
29 Nov.
Current design looks like this:
- Single queue per thread, rather than one per Element. To be more precise,
  each thread has one incoming queue for each thread (including itself) and
  one outgoing queue for each thread (including itself).
  	- We could in principle replace this with a single pair of in and out
	queues per node, if they were individually thread-safe. Tradeoff in
	memory and speed. Probably dreadful for many threads.
		- Suppose a given thread wants to put data out at a fraction
		F of total time, and there are T threads. Then the fraction of
		time any thread is blocked by this thread is F/T.
		So the total fraction of time that any given thread is blocked
		is F again.
		- Suppose we provide a further number Q of queues per thread,
		to subdivide the total set T. Now the fraction of time
		any other thread is blocked is F/( Q * T ), so the total
		blockage fraction is F/Q. This may help, but gets messy.
		- Can we do the same for internode calls? Yes, there is an
		mpi_any_source option in recv. One can narrow things down
		by using tags for groups of messages.
	- Some object operations (like 'get calls' ) put stuff right back
	onto the queue. This will cause problems especially when we have a
	single queue per thread. Dual queues?
	- Also consider sched interleaving, where we do one set of objects 
	first so as to get their data ready for transfer. Again, dual queues
	needed. These could be the input and output queues for MPI.
	- In some cases would want a broadcast queue. If the # of target
	threads is more than half the total, for example, may as well send
	data once to all. Huge savings in memory too.
- Messages now have a unique system-wide Id. Qinfo refers to this, so queue
	knows (through Msg) which is target Element.
- Msg::addToQ is the function which puts packets onto the Queues. This
	is now expanded to decide which threads should receive each packet.
	For example, a large SparseMsg might use one SparseMatrix for the queue
	loading, and another on each thread, for clearing. The setup of this
	large SparseMsg will be the key step in thread/node decomposition.
- What happens to different clocks?
	- Some kinds of data transfer do not have to occur each dt: spike msgs.
	Want to accumulate them.
	- Sometimes we simply have two very different clocks going. Should we
	simply send data with the first available?
	- Sometimes we have small (fast) dt clocks within a thread, and can
	get by with slower dt clocks between threads. How to do?
		- Just check queue size for inter-thread data?

I think the next step is an implementation. Too many unknowns here.
- Rebuild current stuff as just a single queue. Benchmark.
- Set up simple implementation: one queue on each thread, for each target
	thread. So a 4-thread system would have 16 queues. Then there are the
	outgoing queues too.
	- Benchmark
- Set up more economical implementation: Each thread manages a thread-safe
	input queue, and is its only consumer. 
	Need 4x2 queues for a 4 thread system.
	- Benchmark.
- convert from threads to MPI.
=============================================================================
30 Nov.
Just for reference, the starting version is 1466.
We need to put a queue on each thread. Where should it reside?
- Clock. This definitely knows about the queues.
	But it requires that the scheduling be set up before even simple
	messages can be handled.
- Shell. This will be replicated on each node, but not necessarily for each
	thread.
	I already have a Shell dependency for set/get ops using msgs.
- ProcInfo.
- Static field of Element

call stack for addToQ:
Eref::asend: Creates Qinfo. calls Conn::asend.
Conn::asend: iterates through Msgs. Each calls addToQ. Qinfo is passed in.
Msg::addToQ: Checks which is src and which dest Element. Calls addToQ on 
	non-calling Element. Qinfo is passed in.
Element::addToQ: calls Qinfo::addToQ on the passed in qinfo. Passes in queue.
	This actualy does the queue juggling.

So perhaps the Queue should be on Qinfo as a static. if so, the call sequence
would be:
Eref::asend: Creates Qinfo. calls Conn::asend.
Conn::asend: iterates through Msgs. Each calls addToQ. Qinfo is passed in.
Msg::addToQ: Checks which is src and which dest Element. Fills in direction
	flag on Qinfo. Figures out the target Queue. Calls Qinfo::addToQ with
	the chosen queue index.
Qinfo::addToQ: does the queue work.

So it looks like want to put the queues on Qinfo. This seems like a sensible
place.

=============================================================================
5 Dec.
Need to work out relationship between global Msg vector, and managing msgs on
individual Elements.

Currently Element manages a vector of Msg ptrs. Element also manages
a vector of Conns. The Conns too manage a vector of Msg ptrs. To top it all,
each Msg keeps track of its index in the Element::Msg vector.

We need the fast lookup only for Conns, and there too it is a very tiny part
of the comp load. May as well always use MsgIds.
This may let us separate them... use MsgIds only for incoming, Conn only
for outgoing. Problem with bidirectional msgs.

Now we have to separate the MsgId as the index of the Msg on the Element,
from the MsgId as the universal Id for the Msg.
Note that we never seem to use either Msg::mid1() or Msg::mid2(). 
Likewise, the only time we ever use Element::getMsg is in clearQ.

Working on Qinfo::addToQ

Old call stack for clearQ:
Tick::advance: calls its conn::clearQ
Conn::clearQ: goes through all Msgs, calls 
Msg::clearQ: calls e2->clearQ. Why e2? because e1 is the Tick.
Element::clearQ: goes through its own queue, calls execFunc for each entry.
Element::ExecFunc: This figures out if it is a sendTo or regular msg.
	sendTo: figures out target, Qinfo works out op func, calls op.
	regular: calls Msg::exec on buf
	Msg::exec: figures out direction, traverses all targets calling op.

New version:
Tick::advance: calls Qinfo::clearQ with the appropriate Qid.
Qinfo::clearQ: goes through its own queue. Looks up for each entry, calls
Msg::exec( figures out direction, traverses targets calling op.

Working on Msgs.
Need to modify Elements so that they maintain a vector of Mids rather
than of pointers to Msgs. Likewise Conns.
Need to fix Element::dropMsg


=============================================================================

8 Dec
Still trying to compile, but enough done that a checkin is needed. 1476.
Compiled. Crashes. Checkin as 1477.

Currently stuck in testSendSpike around line 385. The message has been
added successfully, but in the IntFire::process when we try to send data,
there are no Msgs in the conn.

OK, turns out that the msg is deleted on the 'set' call.

=============================================================================

9 Dec. 
Got unit tests working up to testSendSpike. Checkin as 1479.

Ran into an issue where it seems like every time I do a set/get call,
it clears out all pre-existing messages. Happens at testAsync.cpp:567.

Fixed. Now it runs through the unit tests, but it does not like the old
values for Q size in testSparseMsg. I'm not sure if this is an issue with
ordering of the random number usage, or if it is fundamental. The output
looks reasonable.

Valgrind is happy with it first pass.

OK, checked out the old 1466 and tested for unit tests. Works fine, so
nothing has changed with the random number generator.

OK, put in a cout to check order of values. Order is correct and
matches with the old version.

Compared the printout of activation: turns out they were identical all along.
With this sorted it is easy to see that the difference in Q size was just 
because Qinfo is now 32 rather than 24 bytes. With this fixed, the unit
tests all clear. Valgrind is happy too. Checkin as 1481.

A bit of benchmarking: Ran using 'time' after enabling the unit tests in
main.cpp, and setting #runsteps to 1000 from 5.
36.22, 36.19, 36.17 sec are the times. Marginally faster than earlier even
though I have a bigger Qinfo. Good. Checked in as 1482 for reference.

Now I can go on to serious stuff with threads and the like.

Putting data onto Queues:
- Data goes to queue for local thread: 
	- Easy to send data
	- Emptying queue is messy.
- Data goes to separate queue for each dest thread
	- Need thread safety for filling queues.
	- Can pre-balance the incoming stuff on each thread.
	- Msg needs initial stage for dispersal of data.
	- Works well with multinode systems.
		- Want to have outgoing queue per node, not per thread on
		node. But this is just a matter of subdividing at target.
Emptying Queues:
- Each thread has its own incoming queue (option 2 above)
	- Msg pre-separates the sparse matrix per thread basis. Could be costly,
		requiring substantial duplication of msg per thread.
	- Need some care to handle msgs put back on queue as it is being
		cleared.
- Shared queue between threads
	- Need to ensure that any given object is handled by only one
		thread at a time.
	- Could do dynamic load balancing: just hand out targets as threads
		request them.
	- Could also partition based on object DataId. OK only if there
		are many target objects.
	- Something like this needed for async msgs arriving on a node.

Just implement a PsparseMsg and see what happens.
=============================================================================
10 Dec 2009.
Parallel sparse Msg and other Msgs.

Potential output queues: 
	- global: to go to all threads and nodes.
	- local: to go to specific threads and nodes
	- group: to go to groups of threads and nodes.

For a typical neural network with 10K randomly assigned targets per synapse:
	- If # nodes serving network is < 1K, we will usually want to
	send spike events to all the nodes/threads handling the network,
	as there will usually be one or more target neurons on each node.
	- Here we want each of the process threads to dump msgs into its own
	output queue, and later merge all the queues for processing.
	- Each process thread will have its own sparse matrix to handle the
	correct subset of synapses.

For a more 'clumpy' network where data goes to a small subset of threads,
	typically from within the small subset:
	- We want comms to be global within the subset. See above.
	- All other comms should be node specific. See below.

For a particularly sparse network, where any given Msg goes to a small random
	set of other threads:
	- Have thread-safe queues for each dest thread
	- Msgs know which target thread to go for.
	- Each 'send' call is directed by the Msg to the appropriate queue
	- When done, each local thread scans its input queue.
	- Msgs are set up with suitable subset of targets for the thread.

SUMMARY:
If we merge the 'global' with the 'group' cases, we need to maintain only
as many queues as there are threads + groups.
	- Threads within a group:
		- each thread has its own 'output' queue for data to go 
			within the group.
			- No special threading stuff for adding to queue.
		- Each group as a whole maintains an 'input' queue for
			stuff coming into the group from other groups.
			- This queue has to be thread-safe.
	- Threads outside any group:
		- Maintain only the thread-safe 'input' queue as above.

Any given Msg is between two Elements. We will assume that this pair is 
always in only one of the categories above: within a group or outside a group.

Data transfer of 'group' queues, from perspective of each thread.
	- During process, put off-group stuff into off-group queues.
		- on-node other threads; and off-node data each have queues.
	- During process, put in-group data into own 'output' queue.
	- When Process is done, consolidate all in-group 'output' queues.
	- Send consolidated in-group queue to all nodes in group
	- off-group, on-node queues are handled by their owner threads.
	- Send off-group, off-node queues to target nodes.
	- Receive consolidated queues from on-group nodes.
		[further consolidate?]
	- Receive mythread input queue from off-group, on-node threads
	- Recieve anythread input queues from off-group off-node
		[Consolidate input queues ?]
	- Iterate through consolidated queue for in-group, on-node.
	- Iterate through consolidated queue for in-group, off-node.
	- Iterate through input queue for off-group, on-node
	- Iterate through input queue for off-group, off-node.
		- Each thread will have to pick subset of entries to handle.

Data transfer of 'non-group' queues, from perspective of each thread:
	- During process, put off-group stuff into off-group queues.
		- on-node other threads; and off-node data each have queues.
	- During process, put own stuff into own input queue.
	- off-group, on-node queues are handled by their owner threads.
	- Send off-group, off-node queues to target nodes.
	- Receive mythread input queue from off-group, on-node threads
	- Recieve anythread input queues from off-group off-node
		[Consolidate input queues ?]
	- Iterate through input queue for off-group, on-node
	- Iterate through input queue for off-group, off-node.
		- Each thread will have to pick subset of entries to handle.

This is surprisingly messy. NEURON assumes everthing is in same group,
and broadcasts everything. Single group is probably sensible also for any
simulation run on a multicore single node system. However:
	- graphics threads will typically be off-group
	- Parser, systems setup and control threads will be off-group.
	- Could treat each solo thread as a one-thread group, thus reducing
		the problem.
	- On a single node, the overhead with consolidating everything into
		a single queue and scanning through that on all threads is
		not too bad. Main issue is skipping uninteresting msgs.

From the Msg viewpoint within a group:
	- addToQueue: Dump all outgoing stuff into the local thread queue.
	- Then by magic all queues are consolidated
	- Qinfo::clearQ: goes to Msg,
		- Msg decides what it should do on current thread.


- If Msg has < 100 targets, just do it on any given thread (depends on weight
	of each target func)
- If Msg has lots of targets, split by # of threads.

=============================================================================
12 Dec 2009.
Thread management. How does each msg know which thread it is on?

Clearing Q:
Clock, therefore Tick will know threadId.
Qinfo::clearQ could take an argument for the threadId
Msg::exec currently only takes buf as an argument, could also take threadId.
	Do we want to insert thread id into Qinfo? No, the buffer is shared.
	Options:
		- single Msg for all the targets, and it picks targets by thread
		- multiple Msgs, one per thread
	We need to be able to rebalance the targets. Also need a single point
	of reference for properties of msg. So use single Msg, pass in threadId.

Sending msgs:
Tick::advance( ProcInfo* )
Conn::process( ProcInfo* )
Msg::process( ProcInfo* )
Element::process( ProcInfo* )
Data::process( ProcInfo*, Eref )
SrcFinfo::send
Eref::asend, Eref::tsend
Conn::asend
Msg::addToQ
	While we have ProcInfo, we can put threadid into it.
	At this point the send functions do not handle thread info in any way.

Do we want to pass in threadId or qId?
	- threadId combines with internal Msg info to work out queue.
	- qId would be more explicit but would need some logic that depends
	on internal Msg info anyway.

Do we keep track of threadId within the node or for the entire system?
	- Related: is there a global we can access for the node Id?
	Well, we can pass ProcInfo around everywhere and it can keep track of
	thread as well as node id. Assume all is known.

Where do we assign and create threads?
	- Clock::start creates threads that last till end of run. 
		- Calling thread (from Shell) goes to sleep till run done.
		- each Tick advances through the sim schedule taking the 
			threadId.
		- We need to manage barriers or conditions to ensure thread 
			sync, within each Tick::advance.

OK, went through and implemented passing in of ProcInfo ptr into all 'send'
functions. Now we have thread info everywhere. If ProcInfo also holds node
info that too is present. Lots of changes. Would like to check in here but
am off net.

Valgrind: is happy.

Next step: start off simulation with optional # of cores. There really should
be a system call to find # of CPUs. We would typically use 1 thread on 
Shell, assorted threads for GUI, and then as many threads as there are 
cores for the simulations.

Minor addition to command line so moose can start up on specified # of cores.
moose -cores <n>
Default, of course, is 1. Would like to autodetect.

Implementation.

Shell should have a 'start' function which sets off the clocks.
This should be on thread 0, and it should block. 
Issue 1: We currently do not allow multiple shells. If we do, we will 
be in trouble because of assumptions about the thread with the shell,
and the use of static globals on it.  Defer for now.
Issue 2: Pthread_create( thread, attr, start_routine, arg)
	This is a bit awkward with C++. I will want to write a static function
	that goes to the appropriate Clock (there may be more than one) and
	identifies the appropriate thread. So my arg will have to identify
	both the clock and the thread. 
	

Need 2 barriers per tick: end of clearQ and end of Process.
Is there an efficient way to do this?
Condition variables: Each worker increments barrier count, this goes
on until it reaches # of threads available. Then send a signal to all to
proceed. This may well be what barrier does.

For now, just plug in the barriers. Later we can try for efficiency.

Do we make one clock+ticks set for each thread?
	- Barriers means that any calculations on them are redundant.
	- With one set, how would we call Tick::advanceThread?
	- We could have just thread0 manage the Clock juggling of threads
	- All the other threads do the 'advanceThread' call, but with
		a distinct procInfo.

=============================================================================
17 Dec 2009
Various options for handling threads and scheduling.
- Use current format, have the FIRSTWORKER thread advance and sort the 
	ticks. The other worker threads go through the same loops but don't
	alter any ticks. 
	The call sequence remains the same. 
	- Starting simulations: This is a bit tricky. 
		- clearQ calls will block, leaving entries dangling.
		- Other threads may want to addToQ or clearQ.
		- Note that this issue also needs handling for single-thread
		 	processing. Will also come up when rebuilding messaging.
			Need to shunt all such calls off to a local queue,
			but how do we clear it? Probably in Process.

- Make separate clocks and ticks for each thread.
	Easier call sequence, but tricky managing all the objects.

Let thread 0 be the parser thread. 
	- In single thread mode, parser has a non-preemptive event loop.
	This calls the GUI, the TTY and then the clearQ and Process
	for the shell, directly. Has to call clearQ because we may have info
	going to different programs, and for symmetry with multithread mode. 
		- The shell calls start on the clock, and does so directly:
		Not through the clearQ mechanism.
		- The shell knows the disposition of threads, and decides if it
		should call the regular start or just set off new threads.
	- In multithread mode, we run GUI, TTY and parser on separate threads.
	The parser is thread 0 and does a similar event loop with clearQ and
	process, except that it does not have to handle the GUI and TTY.

Likely threads: Parser, Python, TTY, GUI, then workers etc.

=============================================================================
21 Dec 2009.
Checked in 1491 with initial arguments to main() to select threading options. 

Starting sims: Where to create threads?
	- On Shell:
		- It knows all about the threading/node structure
		- Needs to call a Clock function directly: unpleasant
		- It has a clean way to separate 'start' calls away from clearQ
	- On Clock:
		- It has to ask Shell about threading.
		- It can deal with the clock functions internally.
		- No clean way to separate 'start' from clearQ.
		- Conceptually, the separate of clock calls is a scheduling
			not a shell function.

Looks like the Shell is the way to go.

Checked in as 1492.


Need to Fix up Ids to only handle Elements. Let Erefs deal with indices.
=============================================================================
22 Dec 2009.
Set up Shell::start which calls single thread or multithread versions of the
scheduling.

Setup seems to work, I create threads and harvest them. Now to actually get
them to do some work. Checked in as 1494.

Implemented threads through to TickPtrs.
Working on compilation.

Compiled, OK. Working on testing threaded clock tick sequencing. A little
tangent to implement setClock in Shell. Stuck in checkGet/checkSet,
which for some reason wants to prepend set_ onto the field name. This should
only be done in ValueFinfo, not in any of the others.
I see what happens: SetGet is quite independent of Finfos. 
I should separate out a SetField and GetField templated function from
the basic SetGet1< type >.

Done. Now we have Field<type>::set and Field<type>::get for doing fields,
which puts in the set_ and get_ prefixes. The others all use the regular
names of the field.
After some more work this now goes some way into the barriers before crashing.


=============================================================================
27 Dec 2009

Incremental progress on getting threaded scheduling to work. The usual
pthread pains.
At this point I have the system sometimes giving the correct sequence with
2 or 3 threads. 
checked in as 1498.
Rather reliable hang at 4 threads.
Seems like thread 0 has gotten one step ahead of the rest, and is hence
clearing the barrier instead of thread 2.

=============================================================================
28 Dec 2009
Turns out that this same problem (thread 0 getting one step ahead) can also
happen even with 2 or 3 threads, only less often than with 4 threads.

Sprinkled lots of barriers around in Clock::tStart. This fixed it. Works
for as many as 32 threads. Now to clean up. Checked in as 1501.

Trying to set up using mutexes. But how to set up the flag for the mutex?
=============================================================================
29 Dec 2009
Wrote a much tighter version of the Clock::tStart function that uses a
mutex and a counter variable to manage sorting and looping for all threads.
Compiled. Clears with 32 threads, but I'm a bit dubious about the 
stepping. Look at the sequence of output here:
Advance at 7 on thread 0
Advance at 7 on thread 0
Advance at 8 on thread 1
Advance at 8 on thread 0
Advance at 9 on thread 1
Advance at TickPtr::advance: post barrier2 at time = 9 on thread 1
Advance at 9 on thread 0
Advance at 9 on thread 1

=============================================================================
30 Dec 2009.
Analyzing sequence
1. Not reaching final tick. Stops at t=9.
2. The tick sequence is OK except that the barrier needs to be in the
	ticks, not the tickPtr.
3. The tStart routine goes through its while loop sometimes without
	movement on the TickPtr::advance. This seems to be due to the 
	nextTime_ field not really advancing, and hence a dummy attempt to
	get TickPtr to advance. Need to check. Confirmed. Turns out it happens
	even with the single-thread scheduling. Inefficient but the end 
	sequence is still OK. Let's fix. After analysis, looks OK.
4. Does too many steps as reported in "Advance at <time> on thread <t>"
5. Can we eliminate the second barrier on TickPtr::advance?
	- It is safe to assign nextTime_ when the first thread emerges, since
		all threads will have crossed the only reading of nextTime_.
	- When the first thread emerges, it should be safe to sort the 
		TickPtrs. Probably should use separate mutex.

	
There is a nastier problem with the threading case, in TickPtr::advance.
The issue is that I use thread 0 to advance nextTime. If thread 1
happens to emerge first nextTime is not incremented, so the thread is
able to go around the loop till the barrier again. Need to protect
nextTime using a barrier, or to have one independently for each thread.

Brute force implementation using a barrier works. But there is still oddness
in the sequencing of 'Advance'. Checked in as 1506.

Some cleanup by way of eliminating barriers in TickPtr::advance. More to go.
Checked in as 1507.

Figured out why the "Advance at <time> on thread <thread>"
sometimes does an extra round for one thread or another. The cout is
called after the function emerges from TickPtr::advance. If the second
thread to emerge does so after the first thread has done the sortTickPtrs,
then nextTime_ will already have been incremented. So the second
emerger reports a different time. No functional impact.
Item 4 above has the same issue, and can also be ignored.


Next steps:
- Implement item 5.
* Move barrier into Tick.
* Implement thread sched test as unit test.
- Set up real calculation, check values
- Benchmark
- Implement condition_wait instead of barrier.
- Benchmark with condition_wait instead of barrier.

=============================================================================
31 Dec 2009.
Moved barrier into Tick. Compiles, seems to work. Need to convert the printf
debugging of the threaded scheduling into a unit test.
=============================================================================
1 Jan 2010
Some cleanup of printf debugging. Checked in as 1515. 
I want to try to eliminate the remaining 'barrier' call in TickPtr::advance,
and replace it with a simple mutex. This did not work. I need to come back to 
it.

Implemented thread sched test as a unit test.
Done. Works. Checked in as 1516. Valgrind isn't happy though. Perhaps I need
to free some more barriers and things. Did so. Still isn't happy.
Checked in as 1517.
Turns out this is a known bug/feature of pthreads on Linux. See 
https://bugzilla.redhat.com/show_bug.cgi?id=483821
The discussion indicates that pthread_exit doesn't quite clean up, and
that it isn't likely to be fixed. So I'll leave it at this. It is only 56 bytes.

Onward now to setting up a real calculation. First phase is to totally
mess up the queue handling system. For now I've gone for a double-buffer
arrangment, but perhaps this is a false economy. For multinode stuff I'll need
to memcpy data onto the MPI outgoing buffer, anyway.

=============================================================================
3 Jan 2010
Is memcpy thread-safe? That is, if I have two adjacent segments of memory
into which I copy data, can I do this on different threads and be sure that
they will not step on each others' memory at the boundary?

Need a bit more general handling of mergeQ/readQ/clearQ/addToQ.

mergeQ is supposed to take writable per-thread Qs, combine them into another
	Q, and clear the per-thread Qs.
readQ is supposed to march through a Q performing operations as it specifies,
	perhaps by many threads at once. The Msg::exec function does the
	operation. For example, Tick::advance calls readQ on many threads.
	Each Msg::exec selects some range of Msg targets to scan through with
	the operation, set by the combination of threadId and Msg target list.
clearQ is supposed to zero out the Q contents.
addToQ puts data into the specified queue. We need to separate this function
	into the thread-unsafe version, which is called by Msgs operating
	within a group, and the thread-safe version, which is called when
	Msgs send data outside their own group.

Should I separate out the input and output queues, that readQ and addToQ 
	work on, respectively?
	- It will make it OK to allow immediate operations that affect Qs,
		during readQ. For example, the 'get' function needs to put
		data into a queue.
	- Still does not eliminate issue of thread safety in cases where
		many threads must write to same Q.

Group: has one inQ, and as many outQs as there are threads. These outQs
	do not need to be thread-safe, they are protected by the scheduler so
	they only get input from their own thread.
	The inQ is thread-safe. Data typically comes into it only during
	'process'. During 'read' and 'clear' it is blocked for write access.
	During 'process' it uses mutexes to protect data coming in. The
	Tick also does a mergeQ which takes data from a set of outQs and
	combines them into the inQ. This step looks like it has to be serial,
	which is unfortunate.

	- When doing MPI: Each group wants to 'send' all local info 
	to all other nodes working for the group. So it will want to do a 
	'send' on the inQ. This can happen on one thread while the worker
	threads clear the inQ on their own. However the MPI man page suggests
	that one should not access the contents of the send buffer till the
	send completes. See below for how to put these together.

Structural options with threads vs MPI:
	1. Always use MPI, never use threads. Suggested by a couple of people,
		also compatible with BlueGene machines.
	2. Always use threads, and use MPI only in virtual shared-memory mode.
		Never heard this option, sounds dubious.
	3. Use threads on local node whenever multicore, use MPI between nodes
		whenever multinode.
	4. Use threads on local node whenever multicore but no MPI, otherwise
		use MPI both for local cores and for other nodes.

For now, and based on the MPI analysis below, it looks like #1 would
be terribly slow. Worth benchmarking though.

For pure MPI, there are a couple of options:
	1. blocking vs non-blocking calls. Blocking is slower and prone to
		locks. Non-blocking requires extra buffers
	2. Async vs sync. Much better comm/comp balance if async. If we provide
		a guarantee that the async does not slip too far, we're OK.

For now, assume non-blocking and async.

For pure MPI, this is how data would go:
	1. Local node has a single outQ and one inQ. It has separate extQs,
		one for each group in the simulation.
	2. During Process, it adds data into outQ, and into each of the 
		extQs as per message targeting.
	3. At end of Process, each node sends a special message with currTime
		to node 0. Node 0 also sends info about the most laggard
		currTime.
	 	On all nodes it then copies outQs onto inQ. InQ goes into
		a 'send' buffer. Each mpiQ goes into its respective buffer.
		All these buffers go out using iSend calls.
	4. System always keeps an irecV posted for 'world'. 
		Whenever a 'test' clears and data has arrived, it grabs 
		data and pushes it into inQ, and posts another irecV.
		Could use memcpy or double buffering.
	5. At regular intervals throughout clearQ, it checks for arrival of
		data, and does 4 if so. Since the inQ is circular, it could
		clear up data as it goes along.
	6. On node 0, when the special Msg with the currTime arrives, it
		analyzes that and decides about load balancing. At the least
		it would send out a message to the way-ahead nodes to throttle
		back. If throttling, any given node, 
		it continues in the clearQ loop from 4 till node 0 sends to say
		to resume.  This won't be needed if we have a
		step lock for sync data.
	7. At end of clearQ, it zeros out the queue.
	8. At regular intervals throughout process, it checks for arrival of
		data, and does 4. if so. Note it does not do clearQ, just 
		accumulates data.

From the viewpoint of Queues, what we take from this is that if each node
is multithreaded, the queues needed are:
	One thread-safe inQ for each group on the node
	One non-safe outQ for each thread (i.e. all within-group threads)
	One thread-safe extQ for each non-self group in the simulation, 
		including on and off-node.
Buffers needed:
	One for catenation of all outQs
	one for each extQ

So the Queue has to also have info about groups. 
	Each group is known by: 
		# of local threads
		Group start index to index outQs
		list of external nodes or
		COM of external nodes?
	
So the indexing of queues is:
InQ: group #
outQ: groupStart + outQ #

So the inQ and the extQs are in the same set of indices, and the groupStart
begins at #groups for group 0 and goes up accordingly.

Actually this doesn't work, since it would involve renumbering queues when new
groups are added. So the current structure is:
InQ: first available #, in other words, groupStart.
outQs: Next set of numThreads #s.

When we do a mergeQ, we identify the group# and the rest happens internally
based on the Queue grouping.

Implemented, compiled, doesn't run. But many things have changed so I'll 
check it in. 1522.

=============================================================================
5 Jan 2010
Added a line in the unit test for Ticks : Qinfo::addSimGroup. This fixes up
the system, it clears the unit tests for many threads. But now there is a
problem that the unit tests will mess up subsequent scheduling. So I need
to put in a function to clear out old sim groups too. This will be tricky,
as all messages have to be recomputed. Should be responsibility of the 
Shell.

Implemented a stub function for Shell::loadBalance. It is called at init, and
can be called later. This replaces the deprecated Qinfo::setNumQs
Runs, clears unit tests with up to 16 threads. Valgrind is also OK with it
with the same caveat about 56 bytes.
Checked in as 1524.

=============================================================================
22 Jan 2010
Finally back to work after a bad spell of meetings and other duties.

Minor cleanup of headers. 
Did explicit seeding for the random number generator for the matrix unit tests.
Checked in as 1541.

Working on framework for a proper test. Given the current structure, the
remaining step is to assign workloads to the appropriate thread. 
Tick::advance( e, ProcInfo* info)
	if ( info->isFirstThread() )
		Qinfo::mergeQ( info->groupId );
	Qinfo::readQ( ProcInfo info ); // threadId also identifies outQ index.
		extract the Qinfo from the outQ buffer using info->threadId,
			which also identifies outQ index.
		sometimes: do hackForSendTo. This will require care about outQ
		Normally: extract msg from Qinfo
			m->exec( buf, info )
			Msg::exec needs two kinds of info from the ProcInfo.
				- a way to pass threadId to the op,
					in case it does output
				- A way to decide which subset of events to 
					operate on, based on the thread#
					within the group. Note that this cannot
					be the threadId, as the threadId may
					be reassigned.

This is OK for within group messaging. Need to work out how to pass
messages to other groups: they will have their own InQs which will typically
be busy during clearQ. 
	Options:
		- Do not allow messages to go out during clearQ
			- But we already have a return message in get()
				which is messy as it uses the Shell ProcInfo.
		- Have a thread-safe inQ buffer for each group, for messages
			coming in from outside.
	Do this later, after testing the within-group messaging.


Did the first phase from Tick::advance till Msg::exec. Lots of cleaning up
of code, went well. Compiles but doesn't clear unit tests.

=============================================================================
23 Jan 2010
Fixed, clears unit tests. Checkin 1543.

Setting up the ProcInfo to pass the right values in. Croaks. Turns out
that we have added two SimGroups: one for the shell, and one for everything
else. However, this fails because the # of threads is just numCores, whereas
the shell is assumed to be on another thread.

Went through and fixed up the non-threaded calculations. moose -s.
Now clears unit tests with this. Checkin 1544.
Now got it to work with different numbers of threads. Checkin 1545.
Next step is to build matrix using threading to decide which messages to
send where.

Some work to do in defining what Elements/Messages belong in what group.
	This determines which threads they are on, as well as numerous other
	things. 
For now I need to get the system to work just enough to do the IntFire network
	test.

=============================================================================
24 Jan 2010
Working through getting the IntFire network to be scheduled through threads.
Current problem: the Qinfo:mergeQ( groupId) function in Tick::advance.
How do we know which groupId to use? The ProcInfo holds the thread# in the
group, but perhaps it should also hold the group#. Done.

Difficult patch. Converting the original single queue array into inQ and outQ.
Much cleanup. Compiles but crashes.
=============================================================================
25 Jan 2010
Struggling with bugs. Cleared first one in mergeQ. Checkin as 1547.

Next one is stuck with the 'get' function in testGet():207.
=============================================================================
26 Jan 2010.
A lot of struggling later, turns out that the issue was that now I have
separated inQ and outQ, and the data was still waiting in the outQ. So it
just needed another clearQ call.

Having cleared that test, now on to the next one: testSetGet().
Same issue here: I need a double clearQ. With that in place, it clears all
the unit tests except the last one, using the -s option.
Checkin as 1548.

Fixed a bug in setting up number of threads in each queue. With that done, the
system clears all but the last unit test for various # of threads.
Checkin as 1549.

Narrowed it down to the SingleMsg between the clock tick and the 
IntFire element.

Replaced the ad-hoc message creation with SingleMsg::add. This took a bit
of work on setting up the message stub in IntFire, but it worked for single-
thread mode as well as up to 4 threads. Doesn't yet do partitioning of the
workload, but it clears unit tests.
Checkin as 1550.

=============================================================================
27 Jan 2010.
Further progress, got it to call the IntFire::process, which seems to be
happening in the correct sequence over all threads. Still crashes at the
end of it, and not clear yet if it is doing the right calculations.
Also I can now see a problem with the scheduling in moose -s (single-thread)
mode. 
Checkin as 1551.

The crash at the end of the run is due to left-over data in outQ_[0].
However, outQ_[0] should be reserved for the SimGroup0, which is used by the
Shell, and not touched by the runtime calculations.

Bypassing for now, working on PsparseMsg and its compilation.
Checkin as 1553.

=============================================================================
28 Jan 2010
Put in a unit test for the load balancing function in PsparseMsg. Works.
Tried out PsparseMsg in the testThreadIntFireNetwork. Still doesn't do what
it should.
Checkin as 1558.

Fixed up thread partitioning in Element::process. Currently hard-coded
in, doesn't look pretty, but clears tests till the final queueing segv.
Still need to sort out the threaded message inputs.
Checkin as 1559.

Minor hack to patch the final queueing segv by clearing the queue manually.
Still to properly track down, but at least it now clears unit tests.
Fix to the Element::process subdividing target Element indices.
Checkin as 1560.

The potential in IntFire does not cross threshold as we init it, so the 
network does nothing.
Looks like the initialization of IntFire isn't right.
=============================================================================
29 Jn 2010
Fixed a problem with the Qinfo::addToQ, which was putting everything into
outQ_[0]. Now there are some events coming through to the IntFire in the
thread test, but not as many as there should be.

With more debugging it gets messier. Most (usually all) of the first round
of spike messages go into just one thread. No subsequent messages emerge.

Lowered threshold so that all neurons will always fire.
Now all outQs hold the identical # of requests, all steps.

Raised threshold a bit to 0.1. Earlier was 0.2. Now we have
variable firing, but firing happens on all outQs. Except first step,
	where again almost all output is on a single queue.

Crashes occasionally, more often with more threads.

Checked Vm. That was it. It is only being set on one of the threads, the
first one.

Closer check. Turns out Vm is set correctly on all objects, right up to the
line before Shell::start in testThreadIntFireNetwork(). Something else
happens to zero it out on all but one thread.

Is there an issue with field access by indexing? Something odd about the
addresses of the IntFires.

Tracked it down to a silly pointer assignment error in Element::process.
With that fixed, it looks like the whole things works, including the 
numerical values of the Vm on different IntFires.
Also I'm not seeing crashes.

Checkin as 1562.

Some benchmarks. Increased runsteps to 1000 in testThreadIntFireNetwork(),
compiled with O3.

Previous	./moose		./moose -c 2	-c 4		-c 8
36.22		35.8		42.6/21.9	41.2/21.9	40.2/21.2
36.19		35.7		42.8/22.1	41.3/21.9	40.1/21.2
36.17		35.8		42.7/22.0	41.3/21.8	40.1/21.3

Surprising scaling with more threads. I really need to do profiling on this
to see where the loss in efficiency comes. 
I suspect in memcpy, as more threads don't seem to matter.

Made profile files:
profile.1thread
profile.2thread

I don't really see any difference. Not sure how to handle gprof with multiple
threads.
=============================================================================

2 Feb
Trying to set up unit tests for IntFire in scheduling, independent of #
of threads. Uses Vm as the calculated quantity. Currently doesn't even
come out.

TickPtr is a mess. Should just have the pointer to a Tick, as the TickPtr
gets sorted and hence shuffled all around. Instead it has a vector of 
pointers, and other nasty stuff.

Also the sort function should get rid of Ticks that lack targets.

Also the selected Conn should be specific for the tick, not a generic one.

What has happened in this bug is that there are two Ticks with small enough
dts to be called: tick0 (set to dt = 0.2) and tick1 (legacy set to 1.0)

The message to the targets was hard-coded to be on slot 10.

The Tick::advance doesn't care which tick it is, it just calls the Conn on
slot 10. 

Clearly need to do some designing here.

Step 1: Setting up messages from Tick to target Elements.
Alternatives:
	- Original idea had been to have a separate conn for each tick,
	indexed by the Tick index. Traversal efficient.
	. Issue is that the regular Msg::add function assumes the ConnId from
	the SrcFinfo, which is hard-coded. 
	. I can fudge this using a special Msg::add variant, but then the 
	SrcFinfo information does not agree with what is on the Msg. This
	may confuse traversal functions.
	. I can fudge this too using a series of 'process' SrcFinfos, one
	for each tick. This is easy but a hack.
	- Alternative: use regular messaging. 
	. Issue is that we will need to serially traverse the msgs in the Conn
	till we find the one(s) that have the target Elements for this Tick.
	All the targets will sit on the same Conn.
	- Alternative: Separate Elements for each Tick.
	. Issue is the use of pointers to other Elements within the Clock.
	# This really comes back to the bigger pending issue of multilevel
	object indexing.
Decision: Use the simple option of having a series of 'process' SrcFinfos

Step 2: Ticks and TickPtrs. 
	- Original idea: Ticks are separate array entries in the Clock.
	TickPtrs were supposed to have managed these in a lightweight manner so
	that sorting of TickPtrs is cheap. Furthermore, we should not have to
	manage unused Ticks at all.
	. Issue: TickPtrs are now heavy, with vectors in them. Bad to sort.
	. Issue: Unused Ticks are currently still managed. Can solve this
	using info about which ticks are connected to targets.
Decision: I've removed a couple of fields from TickPtr. I think the rest,
	including the vector< Tick* >, are OK for now.

Step 3: SetClock, UseClock, Resched, rebuild: High-level control functions.
	- Original idea: Genesis BC options, would like not to have to do this
	at all.
	. Issue: need automatic scheduling.
	. Issue: We need to use SetClock anyway as display needs it.


Step 4: Automatic scheduling of elements.
Alternatives:
	- Predefine certain ticks for each kind of object. For example,
	t0, t1 should be for all neuronal modeling objects.
	. Issue: Solvers. Do we have a separate tick for each solver type?
		Worse, if we have two identical nonuniform dt solvers, do we
		need two ticks for them? Or do the ticks only define external
		update times?
	. Issue: Different solution methods. Suppose we do some mols using
		Gillespie, and others using RK5.
	- Assume we will always have a SimManager of some kind. This does
		load balancing, scheduling, and so on. The default one
		knows about solvers for neuronal and signaling models.
		This doesn't deal with any of the higher-level funcs, and
		instead sets up suitable messages directly.
	. Issue: Where do we go to set plot dts? Separate, unlocked thread?


Checkin as 1566 prior to starting work on these.

Implemented step 1. This already fixes up the problem with 
testThreadIntFireNetwork() in testScheduling.cpp.
Checkin as 1567.
Step 2 has also already been done.

Cleanup of unit tests. Clears up to 17 threads (more gets too big for the
SparseMatrix limit).
Checkin as 1568.
Valgrind is happy, up to the last 56 bytes.

Fixed the bug with the simulation duration in -s mode. May need to revisit.
Checkin as 1569.

Ran the benchmarks again on a 4-node machine (gj: AMD Opteron 2.4 Ghz)

./moose		./moose -c 2	-c 4		-c 8
41.1		36.7/19.1	39.8/11.2	43.8/12.4
41.5		43.8/24.1	40.8/11.1	44.0/12.4
41.3		39.2/20.5	42.1/11.5	43.4/12.4

So it looks like the scaling on these machines is far better than on the
laptop. Almost linear speedup with # of threads. Unclear why it is so much
better scaling than the laptop was.
Laptop: Intel(R) Core(TM)2 Duo CPU     U9400
GJ node: Dual Core AMD Opteron(tm) Processor 280
Anyway, it is reassuring because I had been wondering how to do benchmarking
to find out why the laptop wasn't scaling well.

Next steps:
- Test on 8-core machine.

- Set up MPI-based parallel calculations
	- Shell::create and 'add'
	- MPI picks up and sends out appropriate inQs.
	- Polling iRecv for incoming stuff
	- Figure out which queue to put the incoming stuff.
	- One thread broadcasting Shell commands to all nodes.
	- Hard-code setup of IntFire test.
	- Parallel Element creation
	- Parallel Message creation
	- Deferred object instantiation at time of Shell::loadBalance
		- may not work, because typical scripts do field assignment
		right away.

=============================================================================
2 Feb continued.

Ran the benchmarks again on an 8-node machine
(ghevar:  Intel(R) Xeon(R) CPU           E5520  @ 2.27GHz)

./moose		./moose -c 2	-c 4		-c 8		-c 16
25.0		22.9/11.9	21.6/5.9	22.2/3.2	31.4/3.2
28.0		22.9/12.5	22.0/6.0	23.5/3.8	30.1/2.9
28.1		22.8/12.4	21.6/6.1	23.6/3.8	31.5/3.1

Also moose -s seems to go quite a bit faster: ~23.5 sec 
Unfortunately the -c segvs with some of the higher core numbers: 15, 17.
Overall, looks pretty linear. The hyperthreading is occasionally helpful,
in some other runs I've gotten it as low as 2.4 sec on 16 'cores', which means
hyperthreading on all 8 cores. There is a lot of variability.

=============================================================================
5 Feb 2010
Put in hooks for Shell::create and Shell::add functions. Involved making
4-argument OpFunc, SetGet and SrcFinfo.
Checkin as 1583

=============================================================================
14 Feb 2010
Working on Shell::create. Compiles, crashes on test.
Turns out to be a fairly fundamental problem with argument conversion in
messaging, specifically for strings.
I need to extend the Conv<T> class to return a pointer to the desired 
object, which will normally be a direct cast. For strings, I need to be
clever to handle the conversion. Perhaps the way to go is to create an
instance of the Conv<T> object, which typically is just the desired ptr.
But for strings it does a local allocation etc.
Will need to do benchmarking after all this to ensure we don't slow things
down massively.


Some design thoughts regarding interaction between parser and shell.
3 options:

Option 1:

	Node 0				Node 1
	Parser
	  ^ |
Regular	  | |
funcs	  | V
	Shell::interface funcs
	(Handles sync)
	  ^
Msg	  |-------------------------------|
funcs	  |                               |
	  V                               V
	Shell::msg funcs		Shell::msg funcs

- Needs ptr to shell in parser, or static interface funcs.
- Shell::interface funcs are blocking so that the msg funcs return
- Needs 2 layers of funcs in Shell (interface vs msg)
	OR
  Needs separate interface class.


Option 2:

	Node 0				Node 1
	Parser
	  ^
Msg	  |-------------------------------|
funcs	  |                               |
	  V                               V
	Shell::msg funcs <------------>	Shell::msg funcs
			  Sync and merge

- Needs parser to talk to the msgs, as it goes out to all nodes.
- Sync and merge looks messy.



Option 3:

	Node 0				Node 1
	Parser
	  ^ 
Msg	  |
funcs	  V
	Shell::interface funcs
	(Handles sync)
	  ^
msg	  |-------------------------------|
funcs	  |                               |
	  V                               V
	Shell::msg funcs		Shell::msg funcs

- Needs either ptr or msg interface on parser. But both of them are
	awkward, since parser funcs would like to call a func and get a
	return value. The Msgs don't do that.
- Needs 2 layers of funcs, only both are msg type.
- Which level is blocking?  Presumably the first level of Msg funcs.

.......................................................................

Option 2 is silly. Option 3 is awkward and will munge the current parser 
framework. Option 1 it is then.

Some use cases:
create
addmsg
start
init


=============================================================================
15 Feb 2010
Working on cleanup of conversions. This has cascaded down to OpFunc (done)
and SetGet. SetGet is a bit confusing and I suspect it may be ripe for
cleanup. There is a regular 'set' command which does appropriate type
checking and type conversion. Then there is an iSet command, used only in
the iStrSet. iStrSet itself is never used. The iSet does a lot of things
redundantly with the simple 'set' command.

Commented all the body of the iSet and iStrSet out. 
Got the whole thing to compile.
Surprisingly, works all through and clears unit tests. The new test for
Shell::create bails out but doesn't crash. Need to tighten.
Checkin as 1602.

=============================================================================
20 Feb 2010
Not sure why previous compile worked, perhaps I didn't do a make clean.
Anyway, I had to fix up a EpFunc and OpFunc to also use the new Conv syntax.
And it still fails in Shell::create.

Much struggling later, realized that the apparent bad passing of arguments
through OpFunc into Shell::create was just a gdb bug/limitation. Valgrind
was OK with it. The real problem was that new Cinfos were not being put
into the cinfoMap.  With that fixed, it clears unit tests and valgrind.
Checkin as 1609.
Oops, there is a small new leak.

That is probably because I don't delete the test Element.
Working on delete. Now I run into the framework issues I had deferred
- I need to have a parserCreate function that is blocking and runs on the
	parser thread.
- I need to pass the Id of the new Element to all nodes
- I may need to set up arbitrary array dimensions.

I could do this in one messaging call if I expand to 5 arguments, and put the
dimensions of the array in a vector< unsigned int > argument. Or I could
have a separate redimensioning call and do that separately.

Struggling a little now with formalising SrcFinfos and corresponding slots.
Idea is that slots should identify a communications channel. Each SrcFinfo
has a funcIndex, which looks up the Element::targetFunc_ array for the
funcId of the specified Msg.

Items still to formalise:
- Fix up SrcFinfos to use Conv.
- How is a shared message set up?
	- All the funcsIds get assigned. The size of the targetFunc_ array has
		to handle all the possible SrcFinfos even if they are 
		never used.
- How do we handle the typechecking?
	- Need a SharedFinfo much like before. It does all the individual
		typechecks during the 'add' function, which will be moved
		into Shell. See below.

- How do we handle cases where the same 'send' deals with different
		SharedFinfo message targets?
	See below. The SrcFinfo provides a distinct FuncId for each Msg in
	its Conn.
- For that matter, even with non-sharedFinfo targets, what do we do when  a
	single SrcFinfo has distinct FuncIds to call on different targets?
	- Note that the SrcFinfo has a single funcIndex to look up the
		targetFunc.
	- Currently, the target Conn is part of the SrcFinfo.
	: Put the extra info in a relay or hub if needed.
	: Put it back in the Conn, where it was in the earlier MOOSE.
	 	- Drawback: 
			- Conn ceases to be only a wire, now it has func info.
				It has dependencies on what goes over it.
			- If I want to do a piggyback msg I need to add a hack,
				such as making a new temporary msg.
			- For each entry in the MsgId vector in Conn, I need
				a whole vector of FuncIds, one for each
				dest entry in the sharedFinfo target.
			- Multiple SrcFinfos may point to the same Conn. So it
				should be the SrcFinfos who manage funcs, not
				Conns.
		- Positive: Conn now carries all msg info, and can be examined
			like old GENESIS.
	: Do the extra ones on the fly. Then where do I store the dest funcids?
	: Put an array of targets for each entry in targetFunc_.
		- How do I figure out which entry to look up?
	: Make duplicate dummy SrcFinfos. Nah. How would I call, manage, etc?
	: Linked list of SrcFinfos. But SrcFinfos are readonly statics.
	: in Eref::asend, pass funcIndex rather than targetFunc Id into Qinfo.
		- This goes to Conn::asend
		- This goes to Msg::addToQ
		- This goes to Qinfo::addToQ, but at this point the identity
			of the callerElement is lost.
		: I think Conn::asend is the one place where I know that there
			are multiple msg targets. I could successively look up
			Element::targetFunc for each of these targets. The
			Qinfo::FuncId could be set here.
	: Turn Conns into a virtual base class, having one or more Msgs.
		- This is only to make the scanning go a little faster, doesn't
		help with the function lookup stuff.
	: Turn targetFunc entries into virtual base classes, allowing 
		iteration for successive Msgs. Again, only an optimization hack
		and it will be fine to have the targetFunc_ entries just be
		vectors of FuncIds.
	SUMMARY: Conns unchanged. Element::targetFunc_ entries are now arrays
		of FuncIds. Pass not FuncId but SrcFinfo::funcIndex into
		send functions. Resolve FuncIds at Conn::asend, so when we scan
		through each Msg, we lookup Element::targetfunc_ and
		scan through target FuncIds.

- Is the backward direction from one target of the one-to-all Msg a one-to-one?
	Yes.

- How do we specify which type of message we want?
	- Should have an argument (string?) to specify type
		- Set up a static init for each msg to fill name-type mapping.
	Yes.
	- 'add' function needs to be incorporated in the Shell
	Yes.
	- Needs to be taken out of Message.h and Message.cpp
	Yes.
	- Create Message classes

=============================================================================
22 Feb 2010
Clearing the decks before the major rewrite for shared messages.
Compiles, but doesn't clear unit tests.
Checked in as 1610.
=============================================================================
23 Feb 2010
Fixing up the unit tests: turns out that there were issues with testGet
because of implicit assumptions both on the slot used for gets
(require requestGetSlot = 0) and also on the sequence of initialization
of SrcFinfos (require requestGet to be first, so that it has a ConnId of 0).
A bit more cleanup and it now clears unit tests.
Checked in as 1611.

=============================================================================
24 Feb 2010
Implemented the array of FuncIds for each funcIndex. This makes it messier
to delete messages. We need to:
- Scan through all Conns to find which holds doomed Msg. Get Msg index.
- Scan through all SrcFinfos that talk to the affected Conn.
	- erase/remove the FuncId at the affected index.

=============================================================================
25 Feb 2010
Thinking about this. We have a problem with funcIds, because the ability to
completely recreate a message is now compromised. In other words, the mapping
from message creation call to the simulation is now dispersed among
Conn, Msg, SrcFinfo, and the vector< vector< FuncId > > Element::targetFunc_
Points:
- Every SrcFinfo in a SharedFinfo points to the same Conn. Can we create the
	Msg with a subset of the SharedFinfos, even one SrcFinfo?
- Is every Conn accessed by exactly one SharedFinfo at most?
- A complex regionConnect call might decompose into multiple msgs on a Conn,
	but not all of them. For example, we have a projection to all subtypes
	of mitral cell from ORN, but another projection to all subtypes of PG
	cell. This would be fiendish to reconstruct from the outcome. If we
	do store setup info on the Conn, how do we store multiple such 
	regionConnect calls?
- If we have a separate entity storing the original messaging call,
	how do we relate back and forth?
- For economy, we may not want there to be permanent ConnIds or FuncIds
- For speed we do want to have permanent Conn and FuncIds.

All ugly. Deleting Msgs has also become ugly. I want to get back to single
funcId.

Try this:
- Eliminate Conn, Element::c_, Element::targetFunc_
- class SrcFinfo { unsigned short index };
	- If index < msgInfo.size() follow the msg.
- class MsgInfo { MsgId mid_; FuncId fid_; ushort next_ };
	- The next_ field points to a separate vector than the original msgInfo
		Or even pointers.
		Or even make MsgInfo a vector and eliminate next_
- SrcFinfo::send combines ops of Eref::asend and conn::asend, and
	iterates directly through the series of MsgInfos to put stuff in Qs.
- To delete a Msg we now need to just find the Msg on the MsgInfos.
- We may end up having multiple MsgInfos point to the same MsgId. Need to
	do reference counting so we kill it only when all ptrs go.
	Actually it is usually the other way around: msgs themselves are deleted
- MsgSpec lives in a separate indexed vector.
	- Each MsgSpec refers to its subordinate Msgs and SrcFinfo.
	- Each Msg (MsgInfo?) refers to a given MsgSpec ?
- The index off SrcFinfo also points to an entry in MsgSpec?
	- No, since we may have multiple MsgSpecs using the same SrcFinfo.
- Deletes are done at the MsgSpec level, not the low level of messaging.
- Multiple MsgSpecs may refer to the same Msg, for example, in a SparseMsg.
- Multiple Msgs may be referred to by the same MsgSpecs.
- SrcFinfo[0] is the parent->child msg
- SrcFinfo[1] is the Element->msgspec msg

=============================================================================
28 Feb
Suppose we have a SharedMsg with bidirectional func calls. Then both sides
have individual SrcFinfos indexing into their respective msgBinding vectors.
Nothing special needs to be done for the arriving msgs.

After much fiddling with the call sequence, I am now working on compilation.
Lots of templates have to be redone. I have eliminated a lot of
intermediate functions and the system just goes from SrcFinfo::send to
Element::asend to q::addToQ.
In SrcFinfo::send calls there is some work to be done to optimize and to 
perhaps use Conv rather than direct conversions. But there is an extra
data copy involved with the current form.
Need to implement BackSrcFinfo1 etc.

Now working on the SrcFinfo1::sendTo. Issue is that there are multiple
possible target Elements or MsgIds.
=============================================================================
2,3 Mar 2010
SendTo is used when sending data back to src, and when sending data to a 
completely specified target. In both cases the target elm is known.

Cleaned up Id to just have an index to the elm.

SendTo is currently a  problem. In the general case it has to provide the target
elm. So it now requires pretty much everything that the message creation 
itself requires, It may as well do what Set does: create a
temporary message and send data along it. Differences:
- If message already exists then verification not needed.
- May need to scan through target Msgs on msgBinding_ vec to find one that 
	matches. Usually only a few.

Cost of making new message probably bigger, but anyway, this is awkward.

Putting in a new Qinfo::addSendToToQ to put the data in the right place 
=============================================================================
4 Mar 2010
Now grinding through the compilation and cleanup process. Since so much
has been done, time to do a checkin. V1617.

=============================================================================
5 Mar 2010
Compilation grind. Currently fixing up OpFunc.h:GetOpFunc return values.

=============================================================================
6 Mar 2010

Still grinding through compilation. Now into Shell.cpp
=============================================================================
9 Mar 2010.
Compiled through basecode directory. Others pending. Checked in revision 1622.
Compiled entire thing. Doesn't clear unit tests. Checkin 1623.

Fixed minor bug. To my utter astonishment the thing now does clear unit tests.
Checkin 1624.
Also clears valgrind, with the usual caveat about pthreads.

Now to step back and look at the status with respect to multinode testing
* Redo benchmarking.
- Get Shell commands to work for basic functions
+ Get Shared Messages to work
	- Redesign ValueFinfos to be more like an automatic SharedMessage.
- Get Shell Messages to work for basic functions


Benchmarking on gj (opteron 2.4 GHz)
./moose		./moose -c 2	-c 4		-c 8
45		36.7/19.3	43.4/12		44.5/13.0
This is marginally slower than earlier, under 10%.

Minor shuffling to put Shell stuff in a separate directory. Will be handy
as the # of functions handled by Shell increases.

=============================================================================
11 Mar 2010
Two immediate things to sort out:
- Making a SharedFinfo for shared msgs
- Implement node-directed messages for shell operations.
	- Master to all nodes including self
	- Slave back only to master
	Also need harvester routine to check that all slaves have reported back.

Need to work out whether regular addMsg commands exist at all, or if they
are hidden under the MsgSpec ops.

Possible separate steps:
validateMsg: Checks that the src and destfields are compatible. 
	Only needs to go to master node.
createMsg: Makes Msg of specified type and specified MsgId, between 
	src and dest Elements
	Goes to all nodes, master waits till all ack.
	At this point this is a bare Msg, no functions associated.
assignMsgParms:
	Separate step where the Msg-type specific functions, e.g., 
	randomconnect or add_one_target, get called. Node-specific 
	setup stuff may happen here.
bindFuncToMsg: Binds the functions to the Msg, uses Element::addMsgAndFunc.
	May need to deal with a vector of FuncIds for shared Msgs.
	Goes to all nodes, master waits till all ack.
	Once this is done the Msg is live.
instantiateMsg: Fills in values. may want to have this as a wrap-up step
	so that the data allocation, node stuff and RNGs get done here.

These are all elementary functions. They will be coordinated by the function
that sets up the MsgSpecs. Shall that be the only one that the Shell exposes?
OK, let's try it that way.

Id Shell::addMsg( FullId src, string srcField, FullId dest, string destField, 
	);

Id Shell::addMsg( IdList src, string srcField, IdList dest, string destField );
Id Shell::addMsg( IdRule src, string srcField, IdRule dest, string destField );
Id Shell::addMsg( File mapping, string srcField, string destField );


Working on SharedFinfo, and how to validate messages with it.
=============================================================================
12 March 2010
Working on compilation of SharedFinfo. The current stuck point is 
message validation. Existing approach is to use the name of the FuncId.
This is a hack, done because ValueFinfos define two FuncIds: 
	set_<fieldname> and get_<fieldname>
May work out if I instead treat ValueFinfos as a shorthand for entering
two DestFinfos. Maybe SharedFinfos can set up all their contents? In
which case ValueFinfos are just a special case of SharedFinfo.
However, I don't want to exclude the possibility of connecting to just one
of the components of a SharedFinfo.  

Implemented the SharedFinfos and also a new scheme for validating match
for the purposes of setting up message.

Working on the create command. 
- The new elements Id:
	- In theory each node should keep track of everything and this should be
	unambiguous if we just increment the Id each time an Element is created.
		- Assumes fixed sequence of messaging - should avoid.
	- We could get the id on the master node and send it around to all.
		- Safer, let's do this.

=============================================================================
13,14 March 2010.
Compiled, runs, but valgrind not happy. Trying to clean up. Fixed. 
Checkin 1632.

Next steps:
	* Shared Msg between Shells should be set up. Checkin 1633, 1634.
		Issues again with message direction. If we have a reciprocal
		msg, the direction info is only from the calling Elm.
		Added isForward = 1 default arg to SrcFinfo<>::send calls.
		Checkin 1635.
		- Need to change Msg types between Shells so it isn't
		reciprocal: that is just too messy. It is a fairly obvious
		command/ack pair.
		Checkin 1638.
	* Unit test for SharedMsg. Things got too messy with the Shells.
		Checkin as 1636, still to clear tests.
		- We have a mess with registerOpFuncs for SharedFinfos.
			Fixed, now it registers each dest in the SharedFinfo.
		- We have a mess with registerBindIndex across the board.
			Fixed.
		- We still have a mess with isForward. Should not need user
			input at all, should figure out from elms.
		Clears tests, but isForward still a problem. Checkin as 1637.
	- The InterNodeMsg type should be defined, specially so that Shell
		can send stuff to itself and get back acks to master node.
	* Shell::doCreate should do a 'send' on an internode Msg to all Shells.
		It should wait till ack from all nodes.
		- Should the ack return success?
			- If it fails, what do we do to handle?
	- Shell::doDelete likewise
	- Test on MPI.
	- Set up Shell::warning, Shell::error as a stream.
	- Cleanup: go through and eliminate Message.cpp and its standalone 
		messaging functions. The Shell now handles it.

=============================================================================
15 March 2010
Working on msg-based doCreate. Goes into infinite loop. Hard to debug.
Working on implementing a showMsg. This regresses back to Cinfo::init
handling SharedMsgs to init all the entries.
Put in printf debugging fron tests. Compiles, but still stuck in loop.
=============================================================================
16 March 2010
Some sloppiness in handling Finfo registration has caught up with me, and
it is a spiral of problems. So I have to go right back to fixing this.
It will break a lot of the unit tests. Sigh.

Cinfo::init 
	- Copies out parent class Finfos.
		To what extent must we retain base Finfo and func indices?
		- How do we access them? If through name, no problem.
		- Can we always plop a derived class in? We do not have the
			concept of FinfoId. So should be safe.
		- But if we want to replace a class by its derived
			version, what happens? Need to rebuild linkages,
		-> will have to do so by name.
		- Can use a hint in the Finfo to say if it should be 
			registered with a predefined BindIndex. This is
			only to keep the preallocated bind_ vector small,
			useful when we have lots of SrcFinfos. But then why
			would we have a SrcFinfo if it isn't to be preallocated?
			The fields are what will proliferate, and those are
			only destFinfos.
	- To clone or to use original ptr?
		- Clone seems safer
		- Clone requires me to put in an op in all templated SrcFinfos
			as well as OpFuncs. Not on.
		- To use original ptr I'll have to use static creation of
			all Finfos, rather that 'new' allocation in each object.
		- Cinfo will no longer have to clean up Finfos when it is
			deleted.
		- Cannot now overwrite BindIndex and other Finfo fields.
		- Decide if to reuse old Finfo BindIndex. For now, do not.

	- Adds in current class Finfos. If there is a duplicate, free old one
		and plug in new one.
	- scans through Finfo map, registering each one.

Begun implementation. Compiles through to the testAsync.cpp.
Checkin 1640.

Compiles through all basecode files. 
Checkin 1642.

Compiles through Shell and IntFire. In sched there is an old hack come back
to haunt me: in Tick::advance, it uses the tick index_ to look up 
the msgBinding. With the more structured and automated scheme now in place,
this has to be done differently.
	- Set up an array of process SrcFinfos
	- Use the Tick::index_ to look up this array
	- Find the bindIndex from the identified SrcFinfo.
	- Use this bindIndex for calling process.
Done.

Need to deprecate Message.cpp, Message.h, SetGet.cpp, SetGet.h

Now compiles the whole project. Doesn't run yet. Checkin 1643.

Deeper into the murk. I was unable to debug the first unit test, insertIntoQ,
so I wrote a new one to just print out Element Finfos. That compiles but
also crashes.

Fixed issue with Finfo registration. Now clears first two tests. Checkin 1645

Now clears all unit tests except the last one, where I was earlier: using
messaging to request the create call. The problem is that the messaging is
trying to create two elements. Checkin 1646.

Clears all tests. The double create was because I had set up two identical
messages.
Valgrind is still not happy. Tracked most of it down to the Finfo definition of 
Synapse, which was still using heap rather than static Finfos. Fixed this,
also found another leak in testAsync.cpp. Now valgrind is happy. Checkin 1647.

Next steps: pending from 13 March.
	* Shell::doDelete also set up through messaging.
	- Test on MPI.
		- Start up MPI
		- Create Shell and basic Elms on both nodes.
		- Set up default Msg between shells
		- Figure out scheduling
	- Set up basic set of commands as do<Command> with messaging.
	- Set up numerical test with IntFire network.
		- Benchmark
	- Set up Shell::warning, Shell::error as a stream.
	- Cleanup: go through and eliminate Message.cpp and its standalone 
		messaging functions. The Shell now handles it.


=============================================================================
18 March. Put in wrapper stuff for parallel running using MPI. Checkin 1654.
Oops, added in the parallel directory and files. Checkin 1655.
=============================================================================
21 March.
MPI and MOOSE.
First, which parts of the unit tests do what:
- testAsync: Works fine, does more unit tests than the rest put together.
- testScheduling: Croaks badly with segv
- testShell: Doesn't croak, but never terminates either.

testShell:testCreateDelete: OK
testShell::testParserCreateDelete: hangs. This is now clear: it wants 
	as many acks as there are nodes. Currently we don't get acks off-node.
	Hence the hang.
testScheduling.cpp:setupTicks(): OK.
testScheduling.cpp:testThreads(): Croaks.
testScheduling.cpp:testThreadIntFireNetwork(): Croaks.

Fixed the issue in testScheduling: I was making a SimGroup for every node,
but actually each node makes its own. So I commented out the loop and the
testScheduling worked. testShell still hangs. Checkin 1658.

Now again looking at queue handling with MPI. See 3 Jan 2010.
Since threading is always (except for the BlueGene arch) going to be part
of such systems, lets do as follows:
- We maintain a single recv buffer. 
	- Use MPI_irecv to scan for incoming stuff, on the MPI thread. This
		goes on all the time. May need another thread than the 
		Shell thread, if Shell ops are likely to block.
	- Put GroupId in the MPI_tag. It is an unsigned short.
	- Append the received msg into the mpiQ of the specified Group.
- We also have to figure out when we can send the messages. The two options
	are to interleave with MPI_irecv, or to use the the master thread of
	each group. Problem 
	- If we use isends on thread0 within simGroup, we will need to do an 
	MPI_wait on this thread 
	before using the buffer again, to ensure the data is gone. This is
	not so bad, because we can do the iSend as soon as we have filled up
	the inQ, and then do the MPI_wait when the inQ processing is complete.
	Issue of thread safety.
	- If we use iSends on a separate MPI-thread, it will have to be told
	which SimGroups are ready to send stuff, and those simGroups have to
	block from returning (and hence touching the inQ_) till the iSend
	is done and also till the corresponding MPI_wait says the buffer is
	clear. At first we will just have two simGroups: the shell, and the
	processing group.
		- The MPI thread keeps in a loop 
			- test for iRecv, if it has come, assign it to mpiQ
			of appropriate SimGroup defined by tag.
				- A combined mpiQ for all external data is not
				practical for huge numbers of nodes.
				- Use a separate mpiQ for each of the nodes
				contributing to the SimGroup. Test for receipt.
				- As each extnodempiQ comes in, set a 
				mutexed flag to inform owner process thread.
			- check for flags indicating that a SimGroup is
			ready to send its inQ. Not sure if flag needs to be
			mutexed, perhaps volatile will do.
			- sends stuff if ready.
			- check for any sends having completed. 
				- Reports to SimGroup.
			- Check for flags from process threads to call iRecv
			for any completed mpiQs.
		------ repeat loop.------------
			- If a SimGroup is ready, the MPI thread does an
			iSend of its inQ. 
				- It puts in a flag to indicate that the
					process thread has to wait till the
					inQ is sent.
				- It adds the MPI_request to its list,
				and continues on its loop.
			- The MPI thread calls the MPI_test command or its
			variants to check on one or more requests at a time.
			- As soon as the MPI_request is cleared, the thread
			reports back to the process thread of the SimGroup that
			owns the inQ, and liberates it from a condition_wait. 
		- in Tick::advance, after Qinfo::mergeQ, the simGroup sets
		a Mutexed flag to tell the MPIthread to send the inQ. 
		The Tick continues on to process the local-node stuff.
			- After local-node stuff is done, the Tick checks
			if any off-node mpiQs have been received. It goes
			through them.
			- When each off-node mpiQ is done, a mutexed flag is
			set to tell the MPI thread to put them back on iRecv.
			- When all off-node mpiQs are serviced, use a 
			condition_wait to check if the inQ was sent yet.
			- continue with Tick process.
		

		

Unfortunate amount of data juggling here. Would be nice to have a list
	of buffers that we can rotate where needed. Later optimization.
Seems like the way to go is to have a separate set of mpiQs, indexed by
groupId.
Can we direct MPI messages to specific groups? Yes, there is the MPI-tag
which is currently an unsigned short, which will do.

Is MPI thread-safe? This appears to be implementation-dependent. OpenMPI
claims it is, but it is slightly ambiguous about whether this is here or a
goal still. MPI-LAM is not thread safe. I suspect we cannot rely on it for 
now.

=============================================================================
23 March
Working on implementation of above. Instead of regular MPI-sends/recvs
(of which I'd have to do one per node, on each node, I could do an 
MPI-broadcast on each node, but then I'd have to do one per node to receive
the broadast data. Or even more compactly, MPI_all_to_all and its variants
send info everywhere in one step. The bare MPI_all_to_all expects uniform
size buffers, but since I don't know ahead of time what the sizes are
going to be, and since they change each timestep anyway, I may as well
use it with the max likely size.

If we use the MPI_allToAll, then the loop becomes simpler:
mpiThreadFunc:
	Check that inQ is ready: condition_wait perhaps, or maybe barrier.
	AllToAll transmission.
	clear condition_wait for the Tick::advance. Perhaps barrier is better,
		as there are multiple threads to permit.
	repeat loop
Tick::advance:
	mergeQ // This gets inQ all set
	barrier// This is for processing threads, but could add mpiThread too?
	clear the condition_wait for the mpiThreadFunc.
	Start the local processing on inQ. Keep busy while the AllToAll happens.
	condition_wait for mpiThreadFunc to finish. Again, perhaps a 
		barrier is better.

Barriers:
	- We need a separate barrier for each SimGroup
	- We do NOT want to go into a SimGroup if it is not being Processed.
		- The Shell SimGroup is always being processed - or we have
			shell in both sim groups.
		- Can we put the Shell in both its own SimGroup (0) and also
		the numerical SimGroups? 
			- This would halve the need for AllToAll calls, also
			halve the barriers.
			- Will need to create the barriers at the same time
			as we create the SimGroups. So they may as well sit
			in the SimGroups.
			- But we really want the Shell comms to be out-of-band
			wrt the computational ones, so comput can start/stop
			whenever.
=============================================================================
24 March.
Is it good to have the Tick directly coordinate the mpiThreadfunc? Or
even have the mpi steps handled directly by Tick::advance?

If we have a rigid blocking call in the data exchange, then it is simpler
to put the mpi steps in Tick::advance. But we will need another barrier to
get the MPI_alltoall handling thread to complete.
The problem with doing this is that the blocking call should really sit and
wait on one thread while the local inQ is being cleared on other threads.
So we still want it on another thread.

Also another issue emerges with doing the alltoall: If the # of nodes is
large, then this takes a long time and brings in lots of data. This
means that the nodes do only communication during alltoall, and only 
computation once the data comes in. Will need to benchmark to test.


=============================================================================
25 March
Stages:
	- Get mpiThreadFunc to synchronize with Tick::advance
	- Figure out how to coordinate with shell::process.
	- Try out flaggable implementations for alltoall, broadcast, isend.
	- Benchmark.

Got the whole mess to compile again, begun process of implementing
Shell::mpiThreadFunc. Checkin 1661
Added in the mpiThread. It promptly stalled. I need to have a loop of the
correct number of cycles through the barriers. Perhaps better to be informed
about how long the calculations continue, rather than do the ugly stuff that
the ticks do.

Here we run into one of the issues with barriers. They are really designed for
lockstep calculations, and don't allow one to keep going around even if the
computation has stopped.

Hacked in a flag in Shell to indicate that the simulation is still running.
When the Clock finishes, it sets this flag to 0 as well. Seems to work.

Now on to extending the mpiThreadFunc with another barrier to clear up
the mpiQ. This compiles but fails unit test.

Passes unit test, but doesn't engage second or any further threads. They do
seem to form, though.
After a lot of messing around it transpires that all was OK, I just didn't run
the multithread calculation for long enough to see the other thread in
action. Just changed the runsteps from 5 to 50 in testScheduling.cpp, and
it is now visible when I run moose -c 2

Now the mpiThreadFunc does not kick in when I run 
mpirun -np 2 ./moose. Perhaps shell->isRunning_ is false?

No, it was just that I handn't compiled with the BUILD=mpi flag.
Now it hangs very satisfactorily, but without printing out lots of stuff.

This turned out to be an old (known) issue with the shells trying to
talk to each other without the mpi running. Commented it out. Now it
crashes in testScheduling.cpp on an assertion.

This is in testThreadSchedElement::process
=============================================================================
26 March
Confirmed that this happens only on node 0, at least over 10 trials.
Confirmed that it does not happen for mpirun -np 1
Confirmed that it does happen, on node 0, for mpirun -np 4

=============================================================================
27 March
Added USE_NODES flag. Turns out that the mpi problem happens even when the
mpiThread isn't running.

Fixed by commenting out the section in Tick::advance() where we go through
process again. This must be it: I should only call process once on each Element,
after all the incoming messages are dealt with. 
	Yes, it now clears the tests, and the
mpiThreadFunc is called as expected. But it now hangs with no CPU load 
after the job is done. Suspect imbalance in clearing barriers.

Try this: Before each barrier, the Tick sets a stage flag
After each barrier the mpiThreadFunc looks at it.
Problem is that it is possible for mpiThreadFunc to look
at stage flag before Tick has set it. We actually have to
wrap the flag assignment within barriers to be thread-safe.
Then we need to de-assign the flag within the other pair
of barriers.

B---S---B-------B---S---B
B-------B---U---B-------B

Unfortunately this isn't enough for MPI traffic. Here
we have an extra state.

B---S1--B---S2--B---S3---B  loop TickPtrBarrier
B---U3--B---U1--B---U2---B  loop TPB
Could also do with just S1 and S2 and use logical ops to work out
if we are in S3.
Problem is the MPI thread doesn't know when we're done with
the loop. Also I am myself not sure what happens with the 
end of a tickPtr cycle when there are multiple ticks.

Alternatively, perhaps cleaner, have three barriers plus the
termination barrier.

B1----B2---B3 loop ---B4
B1----B2---B3 loop ---B4

The other approach is to set off the mpiThread as just another Tick
thread. The problem with this is to guarantee that only one thread
ever calls MPI functions. I think this is safe. Let's try it.
We currently have a huge mess of things passing threading
info into the Clock::tStart. Some go through the ThreadInfo,
others are extracted from the GroupId, others directly set into Clock.

Tried to proceed anyway. Things are so messy it isn't doable.
Cleanup:
SimGroups: These contain the primary job decomposition info.
ProcInfo: Keeps track of stuff that current thread is doing.
	GroupId
	threadIndexInGroup
	nodeIndexInGroup? or current node#
	whether current thread is for mpi.
	clocke: can get globally.
	barrier
	sortMutex

Perhaps instead of passing in a ThreadInfo structure I should
pass in ProcInfos.

=============================================================================

28 March 2010: Stuck in silly assertion. Fixed up, now we
should have decent check for when we are in the mpiThread.
This uses the ugly old ThreadInfo structure for now.
But the assertion in testThreadSchedElement::process still fails.
OK, finally cleared it. I was testing numThreads, rather than 
numThreadsInGroup. The former also counts the extra mpiThread.
So the problem was indeed with the messiness of keeping track of threads.
Now hangs, but no crash.

Hangs in coming out of testThreads. When I comment out testThreads,
there is no problem.
Checkin as 1665.

Got rid of the hang. Checkin as 1666.
This is good. But I think I need to get the extra thread
running also for the Shell msg.
Then I can think about implementing it.

Setting up stuff for the Shell basic thread, in ShellThreads.cpp:
passThroughMsgQs
This framework is now OK: compiles and clears preliminary unit tests.
Checkin as 1669
=============================================================================
30 March.
Qinfo::sendAllToAll
Should be renamed, Qinfo::syncShells

The sendAllToAll will probably have to be implemented as a series
of broadcasts, which will give a chance to interleave comm and comp.

* Define size of data to go
* Allocate buffers.
	
Got a dummy data transfer to go via MPI. Checkin 1670.
Came across a surprising note, confirmed on the web: MPI2.2 deprecates
the C++ bindings. Given this situation, I need to go back at this
early stage and recode the MPI calls to C.
Done. Also put in a check for thread support, which turns out to
be missing on the current MPI implementation on my machine. Anyway,
my implementation is serialized so that I protect the MPI from 
simultaneous calls on different threads. Checkin 1672.

Added in info on inQ buffer size in the beginning of the buffer, as
an unsigned int. Surprisingly clean, all within Qinfo. Clears
unit tests. Checkin 1673.

Now beginning attempts to do actual data transfer. It does look like it
would be good to put the queue transfer stuff onto a separate thread.

Struggling. Segvs. Valgrind suggests that there is a problem when broadcasting
the inQ.
=============================================================================
31 March.
A clue: If I put in an MPI_barrier, the function hangs. So something is 
wrong with the sequencing. Trying to track this down.
=============================================================================
1 April
Looks like I've found the cause of the segvs. I had swapped
sendbuf and recvbuf in the MPI_Gather command. Now the program
can go on indefinitely without error, but I've cut it short.
Checking in 1678

Seems now to work through Create and Delete tests, but doesn't quit cleanly.
Part implementation of handleQuit etc. Need to poll on node0 for returns,
so that we can wrap up all processes.

=============================================================================
2 April
Looking at quit. Tricky because of lack of sync.

doQuit sends out msg
	It goes out to all nodes
	Whenever nodes get round to reading their Bcast, they get it.
		Nodes exit their loop after one more cycle of Bcast.
	Master gets Msg along with others.
So in theory all should exit at the same time.

Tracked it down: the msgLoop had a pthread_exit, which was bad as there
wasn't a thread going on for the main loop. Now for the first time I
have a little test which creates an element on multiple nodes, deletes it,
and quits cleanly.  Checkin as 1679
Works with any number of nodes (up to 17) and also in combination with
multiple threads (up to 4nodes x 4 threads).

Now need to set up 'doStart' to see if I can work with multiple nodes doing
computation.
Put it in, yet to check.
Next steps:
	- Confirm that clock scheduling is OK
	- Run test simulation with IntFire neurons.
	- Test on different architectures.


=============================================================================
5 April.
Trying to get the system to handle the 'start' command.
Control flow is now in shell hands, .cpp:357

=============================================================================
6 April.
Got start command to work very minimally in unit test. Issue was just that
the ackStart was not being called so there was no info about the job finishing.
Now it runs and stops nicely on 1, 2 nodes with 1 and 2 threads.

Incorporated testSchedThreads to keep track of clock ticks. Tried
out multinode. Two issues.
	1. Doesn't seem to be synchronizing between nodes
	2. Nodes seem to set off one more thread than I want.

Other than that it seems to work. Valgrind is OK too. Checkin 1682.

Now I understand both points. Item 2 was actually due to the
mpiThread being reported along with the worker threads.
Item 1 was because mpiThread doesn't currently do anything.

Some cleanup, followed by an hour of debugging because I couldn't figure out
why it didn't work any more. Now fixed. Checkin 1683.

Trying out mpi_Allgather in Qinfo to transfer data during compute time.
Seems to be doing the right thing, even for mpirun -np 2 ./moose -c 2
ie, 2 nodes and 2 threads on each.  Checkin 1684.

Now considering how to explicitly test internode message passing. Will have
to create objects with known node decomposition, and have the MsgSpecs
figure out how to direct data.
=============================================================================
7 April 


Set up a Sparse Msg that maps from A[] to B[] such that
A and B have one entry on each node.

Some test cases:
V1:
A[] has as many entries as there are nodes. One per node.
A[0] = 1
A[1] = 1
A[n] = A[n-1] + A[n-2]
In other words, a Fibonacci series.
The series will propagate in the appropriate # of timesteps.

V2:
Same, except now A[] has 100 entries, and we distribute among n nodes.
=============================================================================
8 April
Trying to get going again. Turns out that I hadn't gone back to fix up the
other unit tests. Lots of bugs. Fixed most, but now the system doesn't terminate
with mpirun -np 2.
Checkin 1685.

Figured out termination problem. The multinode Shell unit tests execute
from node 0, and send other nodes ahead to a loop to poll the queues.
This doesn't work when the next step is not the loop, but the sched
tests. Fixed this part by putting the Shell tests last. Now it fails
with an assertion in the IntFire tests in testThreadIntFireNetwork.

This problem is probably because the test is set up to run on one node
but the 'start' command checks # of nodes and tries to run it on multiple
nodes. For now I'll bypass the testScheduling. Now it clears unit
tests with multiple threads and multiple nodes, by not doing some of
the tests.

Implemented Shell::connectMasterMsg and set it up to execute at
MOOSE initialization. This connects up all shells on all nodes to
each other so that the do<Function> calls can be sent out on messages.
Checkin 1686

Implementing 5-argument MOOSE functions to handle data transfer for
Shell::doCreate. May have to do 6 argument to handle message creation.
Checkin 1687.

In a bit of a tangent, implemented a specialization for
Conv< vector< T > >.
This will be useful in passing arbitrary vectors around, but for
now I need it to pass the dimensions of the newly created Element.
Checkin 1688.

Working on implementing the 5 argument function for the Shell::create
command.
Also lots of cleanup of printf debugging.
Checkin 1689.

- Decide how to record partitioning info
	- Completely implicit. Based on node#, numNodes, and numData, derive
		local indices by a formula
		- lets one find desired node from index.
		- Could put in decomposition mode in order to get more variety.
- Decide how to instantiate Elements
	- Do so on creation, which is when the size should be known. 
		- Nice because then there is not just-in-time juggling when
		  fields etc are assigned.
		- Bad if there are different alternate decompositions to try.
	- Do so on 'start'
	- Do so whenever fields are assigned.
	- Do so on creation, and be willing and easy to redo at any point.
		- This would be helped if there is a terse specification of
		the current decomposition
- Should Messages be converted to creation with predefined MsgId?
	- If all is done in perfect sequence, no need. Same applies to Elements.
- How to spec Messages in a high-level way
	- Current set are simple enough that the spec can be extracted
	from the message
	- Exception is the SparseMsg which will need a high-level front-end.
	- Means that the message creation function needs some thought.
- When to set up the low-level messages
	- Wait till 'start' or 'reset'
	- Assuming Elements are done right away, do it right away.
- How to partition stuff in low-level messages
	- Look up partitioning rule on Element
	- Most low-levels follow right on.
	- SparseMsg needs some calculation.


Working on updated Element instantiation and creation. Lots of unit tests
in testAsync to be redone.
=============================================================================
9 April
Cleaned up compile after redoing Element creation code. Clears single node
tests, not MPI.
Checkin 1690

Ran valgrind to see if it picked up the problem with MPI. It picked up
something else, fixed. MPI still a problem.
Checkin 1691

MPI issue is the familiar one of having to have the worker nodes go in
a loop to clear Q entries for one test (testShellParserCreateDelete())
before we go on to other unit tests. It usually works on 2 nodes if I
put them in for 3 cycles, but this hack does not scale to 17 nodes.

Put in a relatively clean way to ensure that exactly the right # of
clearQ passes occur: test for creation of the Element. This works with
1, 2, 4, 17 nodes.
The partitioning of data entries among elements on different nodes seems
to be OK.
Checkin 1692.
Turns out it doesn't work when the 'doStart' begins right after it: hangs.
Fixed, just needed one more passThroughMsgQs on the worker nodes to 
send out the ack after creating elements.
Works for 2, 4 and 17 nodes.
Checkin 1693

Now on to runtime messaging.
- Set up messages with a predefined Id on the master node.
	- Is this the same as an Eid?
	- Do we have a single Message child on each Elm and using
		indexing for Message Spec data?
	- Do we redo MsgId to behave more like Id? 
		Or should we downgrade the Id to be like a typedefed int?

Begun process by putting in skeleton code for doCreateMsg,
and the various fields that implement it.
Checkin 1694.
Getting there. Need to make a test case of Shell::doAddMsg 
=============================================================================
10 April
Working on test case. Implemented 'builtins' directory. Implemented
Arith class there, currently it just adds its two inputs. Implemented a
very superficial test case for it.

Next: Implement a "DiagonalMsg". Should provide diagonal messaging with
a specifiable slope, width and offset. 
Then: Make DiagonalMsg node-aware, thread-aware.
Then: Try it out with the Arith class.
Checkin 1695.

Implemented DiagonalMsg. It sets up messages between arrays, where each
array entry connects to a matching entry in the target. The difference between
indices of the src and dest entries are the stride = destEntry - srcEntry.
To handle multiple such mappings, use multiple DiagonalMsgs.

Implemented the Fibonacci series as a good test for DiagonalMsg as well as
Arith. There is some mess with scheduling in that it retains legacy
clock ticks even though they are not used.
Checkin 1698

Something unpleasant happening with the moose processes. Lots of bad
interactions between the various unit tests, specially in mpi mode.

Put them all in the simplest way, and the situation is that the whole set
run fine except in mpi mode.

Bodged together a workaround: test for useMPI and skip the problem functions
if MPI is running. Now the thing clears all combos of thread and node tried.
Checking 1699
=============================================================================
11 April
Minor cleanup: separated the Msg.h and Msg.cpp file into separate files for
the individual msg types.
Two nasty subversion timeouts later, I've finally been able to check in this
minor update: 1700.

Putting in place the framework for Shell::doAddMsg.
Checkin 1701

Now need to tighten up the data decomposition, and what is stored on each
Element.

Perhaps a Decomposition/indexing class that does:
- Take the DataId and look up the appropriate Data entry.
	- DataId should be indivisible. Only the data() functions should
		query its parts?
- Reports whether a given Data Id is there
- Provide whatever info is needed by Messages to build their tables.
- Deal with array dimensions in lookup.
- Returns field, parent data, and other levels of nesting.
	- Currently the FieldElement class is templated on Field and Parent,
	and uses a lookup function provided by the Parent to find an 
	indexed Field.
- Deal with different decompositions.

=============================================================================
12 April.
Data Id uses an unsigned int to index data, and another to index field.
Actually should be an unsigned int and whatever else is needed to figure out
how to partition the dimensions.

I would like to also be able to take, say, a cell model and array-ize it.
And the whole thing has to work on N nodes.

Here are some of the use cases.
	- synapse[ IntFire# ][ syn# ]: Current deal. Can look up either
		parent IntFire, or any of a variable # of synapses.
		Summary: FieldElement class template.
	- compartment[ neuron# ][ compt# ]: This would work fine, even with
		different # of compts per neuron. But differs from 
		FieldElement in that the compts are not special fields, but
		a different index. Here the parent Element or the DataHandler
		should either do a vector of vectors, or some other 2-D array.
		Summary: Variant on Element or DataHandler class.
	- channel[ neuron# ][ compt# ]: Keep one Element per channel type.
		But not all compts have the channel. Can use an extra lookup
		table to go to the channel entry in the array, from the compt#.
		Summary: Another variant on Element or DataHandler.
	- compt[ glom ][ neuron ][ compt# ] Probably best done as a set of
		individual gloms.
	- molecule[ neuron# ][ compt # ][ mol# ]: similar to compartment or
		channel above, but will have to split one of the indices to
		look up mol#.
		Summary: Variant on Element/DataHandler, further split of
		DataId index.
	- Solver has taken over a neuron. Has an Element tree for the cell.
		Array of these solvers has the same Element tree, but each is an
		array. But indexing is backwards. 
		/model/cell[342]/dend23/KA
		The messaging to children would do the right thing.
	Access to indices:
	Msg::exec seems to call indices directly.
		DiagonalMsg: scans through data part, incrementing by 'stride'
		SparseMsg is worse: explicitly constructs 
			DataId( colIndex[j], fieldIndex[j] ).
		OneToAll: Checks for dim. If 1, scans through dim1. 
			If 2, scans through each of dim1 (data) then dim2(field)
		Here we want to create a DataId and tell _it_ to do the
		incrementing.

Now how to split among nodes. 
	Element->data( dataId ) should always be able to find the data, or
	report it off-node. 
		Likewise Element->data1( dataId )
	So, dataId does not know where data is located. That is Element's job.
	Element->findNode( dataId ) should be able to tell where to go.
		Or, could do with a DataHandler using a single Element class.
	Element->data( dataId, mynode ): possible? Should I require every
		attempt to acees the data to have myNoe available.

Use variants on Element, or delegate to DataHandler?
	- Element currently does too many things, notably handling messages,
		fields, and Cinfo.
	- DataHandler would have to be virtual and provide an interface to 
		

Q: Should Data itself do the lookup, ie., provide virtual lookup and dimension
funcs? This works well for nesting of arrays of X in Y. Doesn't help for
2-D arrays of X. Doesn't help if we want to nest two arrays, X1 and X2, in Y.
The FieldElement kind of arrangement also helps if we just want to have a
child Element that is fully contained but is seen as its own Element.
Ans: No, Data should not do any lookups. Leave it to the DataHandler.
	Data Handler will also take over the kind of role that FieldElement used to
	play.

DataId: Currently structured explicitly to look up data and field. Should be
more general. Should know what its dimensions are.



If we want the DataHandler and the Msg::Exec to both agree on how to do ops
on DataId, then we need an open and explicit definition for DataId as
a series of ints one for each dimension.

If we want a safer representation, we need to make DataId opaque, which 
is OK for DataHandler but tricky for Msg::Exec. But perhaps a good thing.
Will end up with more explicit specification of what the msgs are projecting to,
in some cases. Certain Msg types may need to be extended to do somewhat
different things for different kinds of projection. For example, SparseMsg
currently assumes a single target synapse on each target neuron. However,
if the first index is neuron, the second is compartment, and the third is 
synapse, we could have multiple synapses on different compartments but on 
the same neuron

This comes back to asking, how do we handle multiple levels of nesting?
We don't want to overdo this. The original explicit single Elements were
inefficient but simple.
Array Elements as here are efficient at one level, but add complexity.
Array fields in Array Elements are very efficient, but add further complexity.
2-D arrays would have a similar level of complexity.
We could stop here.

Summary:
1. Keep DataIds the way they are for now. Higher order indexing should be done
with caution. Perhaps add info about dimensionality of the DataId.
	1a. May need to put in some checking code for the messages.
2. Make a DataHandler virtual base class, templated for funny lookups that 
	FieldElement used to do. This deals with all the data access.
	2a. DataHandler requires node info as well as DataId to find data. This is
	so pervasive it should be a global static field of the Shell.
	2b. DataHandler requires info about decomposition of this specific Element.
	This is local info. Rather than weigh it down with options and ifs, the
	decomposition info is coded into derived classes of the DataHandler.
	2c. Each DataHandler knows how big the whole data set is to be. But it
	doesn't have to allocate it right away.
	2d. Can merge in Dinfo?
3. Element is now a single class, and its job is handling messages and
	field info. Has a DataHandler ptr to deal with the data.

=============================================================================
13 April.
Starting on implementation. Made DataHandler and two derived classes
ZeroDimensionData and OneDimensionData.
Checkin 1704.

Added skeleton code for FieldDataHandler.h.
Checkin 1705.

- Need to gear FieldDataHandler.h up for handling multiple nodes
	In progress.
* Need to rename ZeroDimensionData to ZeroDimHandler.h, etc.
* Need to do some unit tests for them.
* Need to do major overhaul so Element does not use its own data access fields.
* Did some cleanup of functions of DataHandler.
- Need to figure out how to do prototypes and other globals (all node) elements.
	If this is easy to flag, then all the initial setup should be in
	global mode (without allocation) and then the load balancing should
	take place. For now, proceed with immediate allocation so as to test
	the messaging.

=============================================================================
14 April.
Did renaming of ZeroDimensionData etc.
Checkin 1707.

Working on Element conversion. Pending points:
OneToAllMsg::exec needs to handle nodes. Should ideally pass the iteration
	into the DataHandlers.
Likewise SparseMsg and PsparseMsg.

FieldElement is turning messy. Ideally should be created automagically
as soon as the parent element is made. Problem crops up now with
Id allocation.
Working on automatic creation of FieldElement. This is done by having a
FieldElementFinfo with all the required access func info.
It will use the registerFinfo
to register a PostCreationFunc with Cinfo, and this will be used when the
parent Element is created, to make the FieldElement

Checkin 1708, 1709 as an intermediate step while I work on this.
Got the whole mess to compile. Doesn't clear tests. Checkin as 1711.

Starting to clear it up. Practical question: Do we allocate
data on the 'new Element' command or wait?

Starting to get some unit tests to clear. Currently stuck with instantiation
of a synapse on IntFire.
=============================================================================
15 April.
Lots of headaches with the FieldDataHandler. I think it needs to embed the
parent element's data handler. 
Intermediate commit 1712.
Done the embedding. Some more cleanups and progress into unit tests.
Intermediate commit 1715.

More fixes, now clears unit tests.  Checkin 1716.
Valgrind reports some leaks. Happens when deleting a FieldElement.
Hard to track down. For now I'll go on to send some internode messages.

Problem 1. Ticks are being split across nodes. Need a way to create objects
globally, with all entries present on all nodes.
Implemented this. 
Problem 2. Now I need to have FieldDataHandlers iterate through the parent
	node decomposition. Implemented skeleton using begin, end and
	iterator++ to do this.
Problem 3. Many unit tests fail with the regular construction
commands because creations are on only one node. Modest progress in
fixing. Checkin 1717

=============================================================================
16 April
Now unit tests work. Put in global Element construction for all testAsync tests,
and it seemed to come together painlessly. Checkin 1718.
Minor cleanup of printf debugging. Checkin 1719.

Converted the Fibonacci test in builtins to run multinode. It works!!!
Tried different # of nodes, automatically does it right. Checkin 1720.

Working on full IntFire network matix.
=============================================================================
17 April.
I've run into a lot of problems because older, low-level unit tests,
don't play well with MPI.
Conversely, I need to do the MPI unit tests in a manner which is completely
high-level, using the properly operating MPI infrastructure.

Setting up a framework. The non-mpi tests will just do their usual thing.
The MPI tests will have to work with the existing infrastruture for handling
multinode messaging.

=============================================================================
18 April.
Framework begun. The non-mpi test are fine, but the mpi tests have yet to
take shape. But it seems to work smoothly.
Checkin 1721.

We will need to set up Shell::doSetClock and doClearClocks.
For now, let's just eliminate these and directly assign to the 
Clock object. So what we really need is Shell::doSet and Shell::doGet,
and their vector equivalents.
Use Shell::doAddMsg to do the actual scheduling for now. Eliminate the 
useclock function as a legacy.

Before going much further, let me clean up the node-acknowledge system.
Most of the cases just need to return an ack to the specific request.
Given that the requests are issued in serial, at this point there is no
need to identify who the request came from either.

Should I ever permit multiple shells, this would have to change.
Updated the ack system to have a single ack, and pass back node # and status.
Checkin 1722.

Working on Set and Get. Later will need a close look at the
affected functions that rely on get especially. many in testAsync.

I have put the design for Set/Get in the DesignDocument. One pending issue is
serializing the Set calls. Options:
	DestFinfos send back acks:
		No, because the DestFinfos are normally called by 
		time-critical functions.
	Magic wrapper for all DestFinfos to be called by set
		Ugly
	Shell sends in a separate test function on the same temporary message,
		which returns an ack.
		Should be OK. The messages are executed in sequence and for 
		Shell calls we should operate always on thread 0.

First, let's sort out the single-node operations.
Need to cleanly attach msg to a given MsgId, so that we can use MsgIds
across nodes.
	- Provide a Msg::Msg function that specifies MsgId.
		- Use a special Msg class for the set/get msg that knows how to
		use this constructor.
	- Make a msg attached to the Shell at the start, with one end 
		attached to the shell and the other end dangling. The free
		end gets attached to whichever Element is being assigned.
		- Need to modify Msg base class
		- Need to make a special msg class.
I prefer the first option.

In the process of implementing this. Put in framework, currently haven't
tied it to the single-node set/get functions, let alone the multinode. 
Checkin 1723

=============================================================================
19 April 2010

Some longer-term perspectives. 

We need these things to work before merging in the mainline MOOSE
	+ Set/Get functionality: GetVec yet to be done.
	- Multinode message setup
	- Unit test for IntFire system
	- Benchmarks and optimization
	- Continuous messages
	- Element tree, wildcards
	- Move and copy of elements
	- Node balancing, even if it is done offline
	- Autosched
	
We need to work into these things as we bring in mainline MOOSE
	- Solvers
	- Parsers and threading
	- Graphics and threading

The conversion of mainline MOOSE will be tedious
	- object conversions
	- Unit tests as we go along.
	- Back conversion of MUSIC and related stuff.
	- Fix SBML, specially units

Release management has issues
	- Library dependency bloat
	- GUI cleanup

Finally, we get to interesting stuff
	- SBML and NeuroML
	- nkit, kkit, chankit, etc.
	- Tutorials and demos
	- A complete illustrated course
	- SigNeur

Back to the basic issues of set/get
Q: Does the command execute on worker nodes or on the master?
	We probably need to provide for all options. Start with strictly
	local version.
	Options:
	- Local node set/get for stuff it knows is local.
	- Local node calls all nodes for set/get
	- Master node calls all nodes for set/get
	Danger with the local node stuff is that the master node might want
	to also do a Set. Who is going to control the Msg if it is to persist
	for long enough for a return msg?
Conclusion: Only do master node control. If finer level set/get is needed then
we should be making an object with its own messages.

Done much of the implementing. Doesn't clear unit tests.
Checkin 1725.
=============================================================================
20 April 2010
Stuck in infinite loop doing the testSetGet unit test.
Valgrind showed problem.
Cleared up a problem with PrepackedBuffer.h and its converter Conv.
Added unit test for this. 
Valgrind now happy.
Still stuck in infinite loop doing the testSetGet unit test.
Checkin 1726.

Possibly the Master->worker message is not set up. No, it is.
Do we ever use SrcFinfo::addMsg? If not, get rid ofit.. Yes. we do.
Does the Msg::add function do the right thing with SharedMsgs?
	Usually not. Need to fix.

Another point is that the basic unit tests should not get stuck on this
high-level issue. Should I recode the tests to bypass it?

For now still struggling to get it to work. I have put in a function
Qinfo::reportQ(), which examines inQ[0]. Shows that the
FuncId going to the target element for field assignment, is
incorrect. The actual FuncId is 14, which is for handleSet.
Should be 0, and oddly seems to be 0 earlier in the stack.

Just noticed that the MsgId was always 1. This MsgId is reserved for
Shell connecting to Elements for SetGet. I think that it has been
overwritten for the ones going to the other Shells. Yes.
Fixed it, now clears a good set of unit tests before failing in testSparseMsg.
Checkin 1729

=============================================================================
21 April 2010
Another bizarre bug in testAsync. The Field::setVec command at line 754 
is changing the e2.e_->msgBinding_[0].size() from 1 to 0. 
In other words, messages are being deleted or something. 
Valgrind doesn't see anything odd.
The SetGet1::setVec does nasty things with the shell msgBind indices.
Should be fixed, perhaps that is part of the problem.
Fixed one issue with SetGet: Replaced all instances of iSetInner
with the current Shell::dispatchSet. Still fails at the same place.
Checkin 1731.
Found problem. Another nasty issue with the MsgIds. Turns out that
the deleted assignment Msg was being saved in the garbageMsg_, and
then the MsgId of 1 was being reused. 
Now it croaks somewhat further along.

Now it fails on an assertion in testScheduling.cpp:271, which is in the
function  "testThreadIntFireNetwork()". 

=============================================================================
22 April.
Checked messaging from Tick. Seems OK.
Seems the setclock function never actually delivers the command to
Clock::setupTick.

Struggling with this. Perhaps first let's fix two pending items:
- the Get function
- the setVec function, which works but is miserably inefficient.

Massive changes to implement the updated Get function. compiles, does not
clear unit tests.
Checkin 1733.
=============================================================================
23 April.
Got a first round of tests on Get to work. Checkin 1734.
Need to fix GetUpFunc. Fixed.
Also confirmed that one of the slow operations is SetVec. 
Will get to that soon.
Now it fails the unit tests where it did before I began working on the 
Set/Get stuff: in the testScheduling.cpp:testThreadIntFireNetwork.
Checkin 1737.

One option for SetVec: Put in a special flag for all of the data
or field indices. For example, ~0. 
	- We can use the entire same 'set' sequence with little or no
	change in the functions.
	- OpFunc::op will have to change in all cases, to handle
		single or a range of Ids.
	- This takes away the functionality of OneToAll type messages,
		and forces the OneToOne types to deal with both.
	- We could achieve the same result by adding a flag argument to
		the Set functions.
=============================================================================
24 April.
Good progress on SetVec. Nearly there except for 2 things:
- Need to put in a cleaner way to give the opfunc the Qinfo it expects. 
	This is in the AssignVecMsg.cpp file.
- Need to handle passing in data bigger than BLOCKSIZE. We're already 
	at that limit.

- Problem with handling 2-D vectors through a single vector assignment.
	On child nodes we won't be able to iterate through the whole lot, so
	won't know where to start.
- If we split up the msg, need to ensure that the ack part is last.

Checkin 1739

Bypassed this for now, simply made BLOCKSIZE bigger.
Now unit tests go on properly till the same old problem with testScheduling.
Seems that the IntFire::process is just not being called.
Tracked it down. There was a hangover of stuff in the queue from the 
testScheduling.cpp:testThreads. Inserting a Qinfo::mergeQ fixed it.
Checkin 1740.

Valgrind indicates lots of cleanup. The faster SetVec makes it much easier
to run.
One nasty set identified, not too happy with the fix but it helps: There were
two lowLevel functions for Set and Get respectively, each with a
different bindIndex. This bindIndex was used to clear the temporary msg
after use. However, as they were using different bindIndices, every time I
switched from Set to Get or vice versa, one msg was left uncleared.
Hacked around by using lowLevelGet for both.
Still lots of junk to clean up in valgrind, seems to have to do
with FieldElementFinfo.
Checkin 1742.

=============================================================================
25 April
Moving on to message setup. 
- Treat messages like other objects, except that instead of one parent they
	have two.
	- Provide a Cinfo and the usual field and setup stuff for messages
	- Regular field assignment stuff is OK.
		- This means we can send messages to messsages. Hm.
	- Merge in concept of Msgid with that of Id.
- Higher-level messageSpecs can manage a group of messages.

Put down lots of points for this in the paper notes.
Could try the following:
- Provide a Cinfo for each Msg, 
	- look up in the usual way to create msgs
	- Accessed through a virtual function that looks up the appropriate
		MsgClass::initCinfo.
- Provide a common Element for all Msgs
	- Element does an end-run to get Cinfo appropriate for the selected Msg
		How do we look up the right one?
		Many cases where we do a lookup Eref.element()->cinfo()->stuff
	- Element only allows Get-type messages.
	- Special DataHandler looks up message based on DataId( MsgId, 0 ). 
- Create Element on the fly whenever inspecting a given Msg.
	- Likely to cause problems with its life-cycle.
- Have a permanent Element, but change its Cinfo on the fly
	- Sure to cause problems with wrong Cinfo.
- Provide a separate Element for each of the Msg types.
	- Cinfo is set just once, on creation
	- Need to add and delete msgs as they are cycled.
	- Msg has to be able to identify either the Cinfo or Element that 
		it is associated with.

For now:
1. Set up a multinode doAddMsg.
	- Added infrastructure, yet to test. Checkin 1743.
2. Test out fibonacci
3. Test out IntFire

Implementing a big unit test for doAddMsg. This revealed an issue with
setRepeat, so I made a new unit test for that as well. Moved the setRepeat
out of the doAddMsg.
The message unit tests revealed a huge problem with SingleMsg and 
OneToAll msg. These have a single DataId as a starting point, and there
is no test for that DataId. I have hacked the two of them to do the test
at the exec stage, after the data is in the queue, but this is obviously
inefficient. Need to exclude the data before it gets sent. Anyway,
this clears the unit tests.
Checkin 1744.

Cleaning up the send operation with a test at the Element::asend and
Element::tsend. The latter turned out to have another bug, which I have
fixed, but which may well break a unit test. Let's see.
OK, clears tests. I do need to make a unit test specifically for sendTo.
For now, move on. Checkin 1745.

Next: make a SparseMatrix. Then go to multinode tests.
Also need to get rid of the legacy cruft in Message.cpp and the various
Msg<subtype>::add functions.

Made SparseMatrix, included it in unit tests. But this won't work on 
multiple nodes because I manipulate the matrix directly rather than through
a Shell command.  Checkin 1746.

Now trying out multinode stuff. In addition to the SparseMsg setup,
we have to get all the scheduling (setclock etc) done in parallel before
we can test this.
Struggling a bit here. The old setclock used simple 'set' commands to
do the assignment. How would these work across nodes?

=============================================================================
27 April 2010

Need to separate queues for Shell messages to local node and other nodes. By
default, all Shell messages should be to local node unless they are to 
the Shells on other nodes. Shell messages will typically be for Set/Get
kinds of operations.

Responses to Shell requests are either from 'get' functions or are acks.
In due course higher-order objects may generate Shell requests to orchestrate
complex ops such as gradient descent calculations. All these messages need
to be directed to the master node.

inQ[0] on master node gets input from queues on all other nodes.
	All messages to shell on any node must end up at master node.
inQ[0] on worker node gets input only from master node.
master node shell needs a special queue sending stuff to other shells
all node shells need a special queue for sending stuff to local node objects.

Also any messages to a global should stay on the local node and not
clutter up MPI. All our Shells and clocks are globals.

Currently we have two sets of input Qs: inQ and mpiQ. We have a single set of
outQs.

If we follow through on this, we will have to add 
	- localQ: for incoming data that is on local node only.
	- localOutQ: for dumping output that is to go to local node only.
	- Message-specific stuff for directing queue assignments in
		Element::asend.
An alternative is to have the msg on the target node decide if it should
	bother with the msg. It would have to ask if the srcIndex of the 
	src is on the current node.
	- Can be done for specific msg types, in the exec call itself.
	- Will add to internode traffic. Consider global calls to tables.
Both these look expensive. The additional queues will be a pain to manage,
	but have the advantage of not being bound by BLOCKSIZE.

While we're at it, let's consider queue block overflow.

Wrote out the possible Q arrangements on pen/paper. It proliferates 
unpleasantly, specially considering the multiplier effect of multiple threads.
Worst case would have, for each thread, one localQ, one outQ, and as many extQs
as there are SimGroups.

Option 1: Do the mergeQ in parallel, multithreaded. Each thread merges its own
outQ into the inQ, the localQ, and the respective extQs destined for other
COMMs. To do this, each thread scans through the outQ and fills up a vector
with start and end blocks for each destination, and simultaneously tallies
size. The threads provide each size in respective slots in a vector. Then
they hit a barrier or condition wait. The master (or zero) thread then 
tallies up sizes, allocates upper and lower bounds for each thread, and then
releases each thread to fill up its section of the queue.

Option 2: Actually this scanning could happen while the queues are being 
filled in the first place. This would be Element::asend or Element::tsend.

Decision: Go with option 2.
-----------------------------------------------------------------------------
Dealing with BLOCKSIZE. Most likely case is large setup message, copy or
other once-off. If the sustained messages have blocksize problems then it
will have to be increased.

Option 1:
	Locally, let inQ expand. Its first word contains bufsize, so we know
how big it will be. Send this out, up to the limit of Blocksize. The target
nodes now must scan all recieved msgs, and if any Blocksizes are too big
they (all) have to post another MPI_allgather. 
	- Still need to work out how to set up buffers to manage ths.
		- Could implement an overflowQ which fills up from the problem
		nodes. 
			- Need to have as many overflowQs as there are problem
			nodes. But this is dynamic.
- Could in principle do a different message passing, one which uses a specific
MPI call like MPI_bcast, with the known entire message size. This would get
the info over in one step, providing that we are still within the MPI size
limits. This is cleaner and no buffer juggling needed to assemble the final
message.

Option 2:
	While doing the classification of outQ entries, each thread also
keeps track of buffer size. Unfortunately the final tally will have to be
done on the single thread. Here we know if we're out of bounds for BLOCKSIZE.
Send along a size flag along with the block so all nodes know right away they
have to keep gathering data.
Split up Q entries to keep things down. Have an auxiliary Q for the overflow.
	- Doesn't help for a single giant assignment, like lots of syn wts.
	- Still doesn't help with setting up buffers to manage big input.
This will in any case reduce to the previous option for giant single messages.

Decision: I'll go with option 1.
Would like to have the rest of the mpiQ processed while all this extra
stuff is being transmitted. But that is a later refinement.

-----------------------------------------------------------------------------
Since I'm recording major design decisions, here is how I'll handle the
access to Msg fields.

1. Have a separate Element for each Msg class. 
	The cinfo for the Element handles field access for the Msgs.
	The Element can initially just manage MsgIds in the DataHandler array .
	Eventually it may help to have them actually allocated on the 
	DataHandler.

2. The Msg has a virtual function returning the Id of the managing Element.
	This Element has the appropriate Cinfo for this class of Msgs. 

3. The mapping from MsgId to specific Data entry happens through a map. I
	don't anticipate huge traffic in this direction. 
		Other options:
			- subdivide the MsgId address space to do this faster. 
			- have an indirection array to convert MsgIds into 
				the array index on their specific Element.
			- Allocate MsgIds based on Msg type.
	Summary: MsgId -> DataId( ArrayIndex, MsgId )
	If MsgId is badMsg, figure it out from ArrayIndex.
	If MsgId is reasonable, always use the MsgId to figure it out.

4. Path access to Msgs: MsgSrc_Element_path/SrcFinfo[index]. Should
	convert directly into a MsgId.

5. Projections (higher order Msgs) may manage several Msgs, including 
	ones between different Elements. Can do so through Msgs to the Elements
	that wrap the Msgs.

6. Field assignment: Regular SetGet calls. The Eref defines the holding Element
	and through the DataId we specify the Msg.

7. Message handling into MsgHandlerElements: same as usual.

-----------------------------------------------------------------------------
27 Apr 2010 
Back to debugging.
Implemented a busy loop option for starting moose, so I can get gdb to
examine each of the nodes.
Turns out that the data getting to node 1 is correct for setclock:
the arguments are correct and so is the function.
The problem is that for some reason the old message is left intact on 
MsgId 1, still going to the 'arith' element e1, through an AssignMsgVec.
Instead it should be redone to go to the Clock through a regular
AssignmentMsg.
Perhaps should check flushing of Qs.
=============================================================================
28 Apr 2010
Nasty debugging, issue of queue sequencing.

To help, here is the list of all funcIds.

Shell.set_name: 0
Shell.get_name: 1
Shell.set_quit: 2
Shell.get_quit: 3
Shell.completeGet: 4
Shell.start: 5
Shell.setclock: 6
Shell.loadBalance: 7
Shell.handleAck: 8
Shell.create: 9
Shell.delete: 10
Shell.handleQuit: 11
Shell.start: 12
Shell.handleAddMsg: 13
Shell.handleSet: 14
Shell.handleGet: 15
Arith.set_function: 0
Arith.get_function: 1
Arith.set_outputValue: 2
Arith.get_outputValue: 3
Arith.arg1: 4
Arith.arg2: 5
Arith.process: 6
Tick.set_dt: 0
Tick.get_dt: 1
Tick.set_localdt: 2
Tick.get_localdt: 3
Tick.set_stage: 4
Tick.get_stage: 5
Tick.set_path: 6
Tick.get_path: 7
Tick.parent: 8
Clock.set_runTime: 0
Clock.get_runTime: 1
Clock.get_currentTime: 2
Clock.set_nsteps: 3
Clock.get_nsteps: 4
Clock.set_numTicks: 5
Clock.get_numTicks: 6
Clock.set_numPendingThreads: 7
Clock.get_numPendingThreads: 8
Clock.set_numThreads: 9
Clock.get_numThreads: 10
Clock.get_currentStep: 11
Clock.start: 12
Clock.step: 13
Clock.stop: 14
Clock.setupTick: 15
Clock.reinit: 16
Clock.set_num_tick: 17
Clock.get_num_tick: 18

Here is an inspection of the queue as we wrap up the message Adds.

147                     readQ( proc );
(gdb) call Qinfo::reportQ()
0:      inQ: [0]=120    [1]=0   outQ: [0]=0     [1]=0
Reporting inQ[0]
Q::MsgId = 2, FuncId = 8, srcIndex = 0:0, size = 8, src = root, dest = root
Q::MsgId = 2, FuncId = 13, srcIndex = 0:0, size = 64, src = root, dest = root
(gdb) n
149             inQ_[ proc->groupId ].resize( BLOCKSIZE );
(gdb) call Qinfo::reportQ()
0:      inQ: [0]=0      [1]=0   outQ: [0]=32    [1]=0
Reporting outQ[0]
Q::MsgId = 2, FuncId = 8, srcIndex = 0:0, size = 8, src = root, dest = root

After the handleAddMsg is done, not surprisingly another handleAck is generated.

So we have a handleAck in the queue just _before_ the handleAddMsg.
That should never be. We should not have reached handleAddMsg while a 
handleAck was pending. I need to work back through the tests to see
where the ack may have been first generated.
Sequentially reinstated tests after commenting them out. Turns out that I
had put testShellAddMsg both in the testShell and testMpiShell. I'm not
sure why this matters. I tried having it done two times in a row and it
was fine. Perhaps testCreateDelete is the problem.
testCreateDelete should not be a single-node test. It uses Set/Get
functions which are now multinode.
OK, I see it. The mpi tests are done only on multinode systems. Duh.
Now confirmed that the testShallAddMsg has the seeds of its own destruction.
But a Qinfo::mergQ( 0 ) before it stops it from crashing.
Checkin for now since I've been making a lot of little changes and fixes.
Checkin 1751.
Ack call is indeed pending as you come in to testShellAddMsg.
First appears after testScheduling.
There is also a huge amount of stuff in the queue after testBuiltins.
Remains after testShell, which is an empty function.

Tracked down the issue to testScheduling:testThreads. The call to 
Shell::start sends out an ack msg, which is not required in this case.
Fixed, now seems to do all unit tests on single node. Fails on 2.
Checkin 1754.

Now trying to get things to work on 2 nodes. Nothing does.

=============================================================================
29 Apr 2010

Step back a bit. Implementing a multinode version of testShellSetGet. This
fails on 2 or more nodes.

After some tedious multinode MPI/gdb debugging it looks like the problem is
with the queue request sent out to the local message on node0, which gets
sent over to node 1 as well.
The msg referred to in this queue is present only on node0 at this time,
hence it dies. But a worse bug would be when an existing message from an
earlier assignment sits on the Shell. Then we would send data on msgId 1 to
the wrong target.

Options:
- use a localQ. This avoids sending the data at all.
- have node-specific MsgId rather than all at setMsgId==1
- encode node info somewhere in msg. Other nodes can ignore.
- Clear out temp msg quickly, so we can check for 0 msg.

I think I'll have to set up the localQ. Other options are hacks, and the
need will come up again. Prior to that, checkin 1755.

Now it clears the testAsync unit tests. Extremely slow for Set/Get,
specificaly in testSetGetSynapse. Not clear why. Fails at 
testScheduling:testThreadIntFireNetwork.
This is significant progress. Checkin 1756.

Put in localQ and some additional queue management code. Some debugging later,
clears unit tests with single thread. Checkin 1757.

Fails right away with 2 nodes. Looks like the system sees that the Shell->Shell
msg is going to a global, and puts the data on the local node.
Yes, fixing this helps.
Stuck now in 'Get'. Here we currently have the object send the data right back
to the master node. Now we'll have to relay through the local Shell.

Implemented this extra stage. Clears the single-node unit tests, croaks 
on 2 nodes.

Next: Get multinode tests to work
	Apply to IntFire network
	Implement MessageElements for field acces
	Implement big data packet sending.
	Clean up old messsage.cpp and like cruft.

=============================================================================
30 April.
Clears the SetGet on multiple nodes. Now stuck in doStart. The queues
are bad, and both nodes fail in readBuf at Qinfo.cpp:185 trying to access a
MsgId of 0.

testShellAddMsg: about to doStart
1:      inQ: [0]=388    [1]=0   outQ: [0]=0     [1]=352 mpiQ: [0]=4000000      [1]=4000000      localQ: 4
1: Reporting inQ[0]
Q::MsgId = 2, FuncId = 8, srcIndex = 0:0, size = 8, src = root, dest = root
Q::MsgId = 0 (points to bad Msg), FuncId = 0, srcIndex = 2:8, size = 0
Q::MsgId = 8, FuncId = 1, srcIndex = 4294967295:0, size = 2, src = tick, dest = a1
Q::MsgId = 0 (points to bad Msg), FuncId = 524288, srcIndex = 65536:4294901760, size = 65535
1: Reporting mpiQ[0]
Q::MsgId = 2, FuncId = 8, srcIndex = 0:0, size = 8, src = root, dest = root
Q::MsgId = 6, FuncId = 2, srcIndex = 0:0, size = 1668248176, src = d1, dest = d2


0:      inQ: [0]=0      [1]=4   outQ: [0]=0     [1]=192 mpiQ: [0]=4000000      [1]=4000000      localQ: 4
0: Reporting mpiQ[0]
Q::MsgId = 2, FuncId = 8, srcIndex = 0:0, size = 8, src = root, dest = root
Q::MsgId = 6, FuncId = 2, srcIndex = 0:0, size = 1668248176, src = d1, dest = d2

Went through one round of stepping through the system. Didn't help.
I think one issue is that we initiate the runtime state while still within
Qinfo::readBuf::m->exec. This leaves the existing buffer dangling and other
bad things. Want instead to change over gracefully.

Did that. That uncovers lots of problems with how we do 'process'.
currently the testThreadSchedElement::process is bad.
=============================================================================
1 May.
OK, the problem is that the Element class is no longer virtual. So the
'process' in testThreadSchedElement doesn't get called, and instead the
original Element::process does.

Put in a proper TestSched class, derived from Data, with all the usual
initCinfo and so on. Now it works for one thread but not for two.
Checkin 1758.

Seems like the system goes through ticks, in setupTicks, but doesn't call
target.
=============================================================================
2 May.
Solved the setupTicks problem, it was a bit of a waste. I was using absolute
lookup to find the Tick Element in Clock::start. However in the setupTicks
test, I was creating both the Clock and the Ticks locally. Hence I was
not seeing any of the targets of the test Tick.
Checkin 1759

Now the system clears single-thread tests. The 2-thread tests run into
problems because I now explicitly check threads before executing process on
any Data entry. The old test design assumed not. Need to change the 
TestSched class to be a proper multi-dimension type.
Checkin 1760.

Tightened up test for TestSched. Now compares time alignment between threads.
Checkin 1761

Little fix done for OneDimHandler::process. Now it often clears the tests
for 2 threads, but for 3 or more it either hangs or fails the assertion on
the output. Checkin 1762.

In order to tighten up the Tick::advance logic, I have introduced a second
barrier. Earlier there was just one barrier, being used twice. This doesn't
seem to make any difference to the running, but perhaps debugging will be
cleaner.

Fixed a bug that came up because the Clock was not initialized to zero 
properly. But the earlier symptoms still persist: usually clears
2-thread tests, sometimes fails. Always fails with more threads.
Valgrind doesn't find anything wrong, other than about 2K of memory leak.
Checkin 1763.

Checked output. The values are usually pretty close but not right on,
sometimes one of the values is right, sometimes it is quite far off.
Nothing pending in the queues.
This looks to me like possible race conditions: data slopping between threads.

The thread balancing is as follows:
Process: Thread X out of N threads access data[ X + Y*N ] where Y is a 
non-negative integer.
PsparseMsg: Thread X out of N access row[ X + origRow * N ]
targetThread = ( colIndex * numThreads ) / ncols.
So successive targets are in the same thread:

Array index	Array size	# threads	Thread #	Low	High
colIndex	ncols		numThreads
0-1023		1024		1		0		0	1024
0-511		1024		2		0		0	512
512-1023	1024		2		1		512	1024
0-341		1024		3		0		0	341*
342-682		1024		3		1		341*	682
683-1023	1024		3		2		682*	1024
etc.
So my Process load balancing was wrong. Not too much to fix, though:

Msg type	Handles threads?
Single		No
OneToAll	No
OneToOne	No
Single		No
Diagonal	No
Sparse		No
Psparse		Yes.

Now the inverse calculation: What is the range of Array indices for a
given array size, # threads, and thread #:
lowIndex = ( thread# * array size ) / numThreads
highIndex = ( ( thread# + 1 ) * arraySize ) / numThreads
	For high index we use C convention of one more than the last one used.
This will give:

Array index	Array 	#thrds	Thrd#	Low	High	lowV2	HighV2
colIndex	size	
0-1023		1024	1	0	0	1024	0	1024
0-511		1024	2	0	0	512	0	512
512-1023	1024	2	1	512	1024	512	1024
0-341		1024	3	0	0	341*	0	342
342-682		1024	3	1	341*	682	342	683
683-1023	1024	3	2	682*	1024	683	1024

This isn't right. Low and high indices are rounded down where they should not
be.

Try version 2: See columns lowV2 and highV2
lowIndex = ( thread# * arraySize + numThreads -1 ) / numThreads
highIndex = ( (thread# + 1)* arraySize + numthreads - 1 ) / numThreads
This looks OK. But it is messy to index. Let's try it to see if the 
threads work. No, they still don't. The error looks much the same as before.
The are either very close, or one is close and the other is off.
This time the 4-thread version seems to work OK at least half the time.
3-threads works occasionally. Suggests that something else is also wrong.
Checkin 1764.

Another option is to change how PsparseMatrix decomposes things. 

=============================================================================
3 May
Went through another round of checks for the thread decomposition. See
may03_2010_thread_decomp_test.xls
The formulas above seem OK.

Did a printout of each tick for 1 to 5 threads.
This shows that the problems set in quite early, and seem to be confined to 
thread# 2, which starts out by not having any pending events, and
this trickles through to other problems.
Oddly enough even with 4 threads, #2 seems to be a problem
For 5 threads, #2 and #4 are problems.
Pending events are the job of the message. From the tables it is clear
that the process operations are all being called.
I should flag to indicate the event (outgoing msgs) and their threads to
see if the alignment is good.

=============================================================================
4 May
A table of where the zero msgs come in.
#threads	Blank threads
1		-
2		-
3		2
4		2
5		2,4 or 4
7		0,1,3,4 or 1,2,5,6 or 6

If it were a fixed set of blank threads I would suspect the PSparseMatrix
decomposition. As it is, I need to look for some thread/race issue that
systematically blanks out an entire block of messages, and once done, the
config continues across multiple timesteps. Suggests a setup issue.

There were numerous problems with multiple threads accessing and assigning
fields in shared memory. Tried to fix this but it still causes problems.
The frequency of failures is now lower, but the nature is the same:
blocks of messages don't communicate. 

Put in a printf onthe PsparseMsg::exec function too. Turns out that
the offending threads simply do not call the exec function.

OK, I think I got one problem: we are not really treating the queue as 
readonly. Instead in the readQ (or readLocalQ) call itself, we zero out the
bufsize. Need to separate out the zeroing and protect it.

OK, seems I can put it in mergeQ, which is also protected.
Note that mergeQ is protected in an awkward way using two barriers.
This may go much faster as follows:

in mutex:
	increment counter for barrier
	Check for completion
		If complete:
			mergeQ
			clear condition.
	condition_wait
everybody emerges at the right time, only a single barrier-type
function.

Anyway, that is for the future. For now, I put the zeroing out of the queues
into mergeQ, right where it resizes the same queues, and it now works.
I have run a series of threads up to 19, 4 repeats each, in a little foreach
loop, and moose never complains. Above 19 it goes over the sparse matrix 
size limit. Major step forward. Checkin 1766.

Cleaned up memory leaks: all were due to post-creation of 
FieldElements. Valgrind now happy, barring the pthread leak. Checkin 1767.

============================================================================
5 May
Now on to tackling multinode runs. It fails because of a mid in Qinfo being
zero.

I think one good approach is similar to
what I did for the threads: Just put out enough debug information to
find where problems happen.

Going into mpiClearQ the queues look OK.
Then things go wrong.
Looks like the contents of QueueBlock are bad.
OK, I think I've tracked it down. QueueBlock is OK but part of the data is
in outQ[0] and part in outQ[1]. Presumably there were some setup messages
still pending, so they got put in the other queue. Yet more thought needed
to straightening out queueing system.

We currently have a set of groups, each of which can occupy some threads
and some nodes. There is one inQ for each group. The inQ has whatever
data needs to go to other nodes in the group.

We also have outQs, one per thread. Each outQ is matched by a QueueBlock.
Need to combine. There is some confusion here about how groupIds look them up.

We have the localQ, which is just one queue handling all strictly local
messages. But this doesn't take care of many possible local calls in
different groups. But it doesn't matter. They can all be done together.

We also have mpiQs, one for each node or SimGroup sending info to current node.
Somewhat poorly defined for other SimGroups.

In a nutshell, the SimGroup organization is quite messy here. This turns out
to cause problems when trying to fix up the QueueBlock system.
A hacked together version finally there, checkin 1768.

OK, clears multithread tests.
And also finds a new place to crash in the mpi run. So some progress.

Got through the message execution in testShellAddMsg. The outputs don't match
though. Checkin 1769.
============================================================================
6 May 2010

Big progress: When I change the runtime from 1 to 2 in testShell:355
(testShellAddMsg) then all the outputs match up, except the already known
problem with the SparseMsg. We're in business.
Checkin 1770.
- Why does the system require two clock steps? One worked on a single node.
	Seems like the first step doesn't actually pass any of the
	process time message info.
	OK, this is what seems to happen:
		- The message passing happens before the 'process' in each tick.
		- The 'process' call is needed to dump the msg data into queues.
		- On one node, the subsequent scanning for the field values
			clears the queues.
		- On multiple nodes, the field value scan uses special MPI
			calls between shells and does not clear the queues.
	So, a second timestep is needed to complete the message passing 
	between nodes.

- Make valgrind happy.
* Set up the Msg field stuff.
	- Implemented SingleMsgWrapper. Compiles, clears tests, yet to 
		exercise it. Checkin 1772.
	- Went through with the implementation. Lots of cleanups. 
		Clears tests even though it isn't complete. Check 1773.
	- Cleaned up destructors. Checkin 1774.
	- Made a unit test (on SingleMsg) to check the fields it digs up,
		and to reassign targets of the message. Works. Checkin 1774.
* Confirm the functioning of the current tests
	- Got the testShellAddMsg to work with 1 thread, 1 node, using the
		new field access to the PsparseMsg.
	- Got it to work with 2 threads, 1 node. Had to add several additional
		utility functions to the Element interface to PsparseMsg 
		including loadBalance
	- Got it to work with 1 thread, 2 nodes. Simple fix on PsparseMsg::exec,
		and now that the assignment of the matrix is good it went
		cleanly.
	- Not yet working with 2 threads, 2 nodes.
- Set up a multinode IntFire network, and test.
	- Doing setup. Compiles but croak in unit tests.
- Set up multinode/multithread IntFire network.
- Benchmarks and optimization?

============================================================================
7 May 2010
Checked in working version from yesterday. Network had failed. 1776.
Working on setting up IntFire network using message fields. This still isn't
clearing.

Got setup of IntFire to work, using message fields and Shell-based commands
only. This works with 1 thread. Good progress there, but 2 threads crashes,
and 2 nodes stalls and I can't tell if it is in an infinite loop or just 
extremely slow. Checkin 1777.

Fixed issue with node stall: it was just that I had 'quit' the simulation and
as far as the other nodes were concerned, they were wainting on MPI_finalize.
With this the IntFire network simulation runs and Vm100 is Ok
But Vm900 differs. 
============================================================================
8 May 2010.
Looking at IntFire circuit and the PSparseMsg setup.
Very peculiar.
Even on 1 node, it isn't displaying rows 100 and 600.
Even on 1 node, it is only showing up at t = 0.4.

On 2 nodes, it seems to miss rows 0, 100 and 600. Seems to do row 500 twice.

1. Why it only shows up at t = 0.4: at other times only  a few axons are 
firing. So none of the selected (%100) set is hit.
2. Actually it does all rows that the single-node version does. The #
of input msgs does match up between 1 and 2 nodes.

Looks like there is a problem with the setup of the sparse matrix
on node 1.

Checking the sizes array. There is a stark contradiction in the printfs in
PsparseMsg::exec. Need to do a printf to see how the node 1 array is set up.
Confirmed problem. All colindices for node 1 are 1. Fieldindices are big
numbers, unlike the fieldindices for node 0 which are usually small single 
digits.
============================================================================
9 May 2010.
Walking through the code with gdb.
Matrix looks good on both nodes before transposition.
The various lookup operations for resizing the SynIndex look OK.
Matrix transpostion fails. Astonishing, I thought I had dealt with this
and had unit tests for it. rowStart looks OK but colIndex on node 1 is all 1.

Confirmed problems with transpose. I can cause the matrix code to crash with
a test matrix that starts with lots of empty spaces.
Some ugly, careful indexing and housekeeping later, I think I've fixed the
matrix transposition code. It was so broken that it is amazing it worked
for the earlier, smaller unit tests.

But it _still_ doesn't work with the multinode IntFire test.
After fighting with GNOMEs keyring for quite a while, finally managed to
subdue it, and was able to checkin as 1778

Looks now like the weight assignment was completely off. Need to figure out
how to get the correct # of synapses across all nodes, in order to do this
assignment. Then the check becomes interesting too.

Design issue: Suppose there is a field like NumSynapses, which has a different
value on each node. How do we get individual NumSynapses? How do we get the
total?
We could set up a vector of the values... But that assumes we know how the
nodes are decomposed.
We should know all this at the time that the synapses are set up. If it is done
using a global function like the SparseMsg randomConnect, then each
node will know where it starts from. Need to get this info into the 
appropriate DataHandler.

AssignMsgVec is also messed up for 2-D data assignment.

============================================================================
10 May 2010
For fields like numSynapses, which have input from all nodes, we need to
set up a distinct Finfo which may need a distinct OpFunc to run it.
This can do the gather and summation of data from all over the place.

Also need to merge the variants of the SparseMsg to give a single one.
============================================================================
11 May 2010

There seems to be a memory overwrite bug, causing things to crash. For once
valgrind isn't much help as it reports a different problem in a different place
than gdb. But it seems likely that one of the Element or DataId,
contents is being overwritten.

Here is some tighter debugging info. Looks like parentDataHandler ptr is also
funny:
(gdb) p se.e_->dataHandler_->begin()
$4 = 0
(gdb) p se.e_->dataHandler_->end()
$5 = 0
(gdb) p temp->dataHandler_->end()
$6 = 100
(gdb) p temp->dataHandler_->begin()
$7 = 0

So this is something nasty about the FieldDataHandler class and how it has
inherited from the DataHandler.

Changed the DataHandler to use a pure virtual function for the 
startDim2index. Now it works. Don't ask why.
Had to clear up a number of unit tests involving the random connected sparse
matrix. This follows from the fixes to the SparseMatrix::transpose() function.
Clears unit tests on 1 thread, 1 node. Other cases fail.
Went to regular make (as opposed to the MPI version). Did a valgrind test.
No bad accesses, but lots of leaks. These turn out to be due to the 
MsgManagers. When they are cleared out valgrind is happy.

This version still doesn't work with multiple threads. Fails when trying to
doStart in testScheduling.cpp:testMultiNodeIntFireNetwork.
Seems I need to do a loadbalance.... Done, now it works for 2 threads.

Trying to set up for multinode. Need to hard-code in checks for weights/delays.
There is something funny happening with the start_: should be 0, comes out 10.
This turned out to be a nasty little bug in randomConnect. Fixed, and now the
sparse matrix and IntFire solutions are back to what they used to be.

Implemented some tests for synaptic weights and their assignments. 
1 thread clears it. 2 nodes fails even before it gets there.

============================================================================
12 May 2010
Checkin of all the above pending stuff, 1781.

Fixed yet another bug in the randomConnect. Now the connectivity is set up
OK but it fails in the tests for the weights, on 2 nodes.

Checked, found that the startSynapse number was not set correctly for worker
node. Some more minor fixes in the randomConnect. Now it works. IntFire network
seems to be set up and compute correctly across nodes. But it still fails
for 2 nodes each with 2 threads. Checkin 1782.
Also works for 3 and 4 nodes.

Cleanup: Folded the old SparseMsg code into PsparseMsg, and renamed the 
PsparseMsg class to SparseMsg.  Checkin 1783.

Cleanup: Renamed the PsparseMsg files too. Checkin 1784.

Cleanup: printf debugging. Checkin 1785.

Cleanup: Got rid of Message.cpp:add. Used only in testAsync. Checkin 1786.

Cleanup: Got rid of SingleMsg::add. Checkin 1787.

Cleanup: Getting rid of all dependencies on Message.cpp. Checkin 1788

Cleanup: Getting rid of Message.cpp and Message.h. Checkin 1789.

Cleanup: Make a separate directory for Msgs. Checkin 1790.

New stuff: Implemented check for buffer size, followed by special send of
	expanded data. This works for 1 and 2 nodes but not 3 or 4.
	Makes a huge speed difference on multinodes.
============================================================================
13 May 2010
Trying to figure out why it fails for multinodes with the expanding buffer code.
This happens just after the big data broadcast.
Nothing obvious at this point. Using printf debugging I can see it handles the 
expanded broadcast just fine, and the subsequent 'gather' seems to be OK
as well.
What makes it odder is that it fails for > 2 nodes.
Tried valgrind with mpirun -np 3 valgrind ./moose.
Doesn't report anything amiss. The system fails an assert.

Looks like there are some leftover/spurious message packets in queue 1, 
when the system sends the big messages which are on node 0.
Tried to add in an mpiClearQ after the last cycle of Process. Doesn't seem
to work. Queue 1 still has lots of stuff.
============================================================================
14 May 2010
Tried clearing out Queue1 hard coded at the end of clock::tstart. Doesn't
work.

Found another problem with the handling of readMpiQ. Basically, on worker nodes
I don't really want it to inspect any nodes other than node 0. 
For now, at least. Also implemented a function Qinfo::emptyAllQs to clear
up stuff after a processing run. Together these make it work on n = 2, 3, 4
nodes.
Checkin 1792.
Recompiled with optimization, threads, MPI. Timings
Try	Machine		1thread	2thread	2 nodes	4thread
1	laptop		37.7	23.4	23.17
2	laptop		37.9	23.4	23.01

This is pretty wild. The unoptimized MPI stuff is actually faster than
the threading?! Need to check on a serious machine.
Checkin 1794.

I needed to redo the IntFire network to get it to generate some usefully
diverse Vm values at 1000 steps. Managed by adding -ve weights. Put in a
test - not assertions, but an if statement. Confirmed that things work for
1 core, 2 nodes and 2 threads respectively. Timings have now changed:

Try	Machine		1thread	2thread	2nodes	4thread	4nodes
1	laptop		29.8	18.7	18.4	17.7	32.1
2	laptop		29.6	18.2	18.5	17.7	32.1

Importantly, it handles the higher node/thread cases correctly, even if slower
for more nodes.

Tested mpirun -np 2 ./moose -c 2: Incorrect output.
Checkin 1795.

============================================================================
16 May 2010
From 19 April: Some longer-term perspectives.  
	+ Set/Get functionality: GetVec yet to be done.
	* Multinode message setup
	* Unit test for IntFire system
	+ Benchmarks and optimization
	- Continuous messages
	- Element tree, wildcards
	+ Move and copy of elements
	- Node balancing, even if it is done offline
	- Autosched
	- Documentation
	
We need to work into these things as we bring in mainline MOOSE
	- Solvers
		- Replace Cinfo and DataHandler on Elements that are solved.
		- The Finfos in Cinfo all get replaced.
		- The DataHandler points right to the solver.
		- The messages and other stuff remain untouched.
		- The Fids are all local to the Cinfo, so they now just refer
			to their new functions.
	- Parsers and threading
	- Graphics and threading
	- Extended fields
		- coordinates, generated rule-based on DataId
		- arbitrary new fields, one per entry
		- Element-globals: fields that are one per Element.

Got rid of leftover stuff from FieldElement. Checkin 1796

Decide how to place the subdirectory for Element heirarchy: Neutral, 
the wildcard stuff, copy, and possibly the various data handlers.

I think this should be in its own subdirectory. Call it tree.

Wildcards: Vector of FullIds. For compactness, have special cases
	to indicate that all entries are included.
	
Operations for wildcard traversal:
	- Implement a wildcard Filter object
		This will be needed in due course also for complex messaging.
	- Do local node traversal for Elements
		Filter manages field-dependent operations, such as name/value
		lookups.

Copy: Too many permutations, need to tighten:
	Local/Global src, Local/Global dest, src single/array,
	dest x1/xn.  Msgs within/outside tree.

Move: Change parent is one, but may have to convert singles to arrays? Later.
	Invisible move between nodes

Load balance: Move data between nodes, while retaining original tree

Serialize: Used for dumping data. Copy may use. 

Read: Formal ML based load of model structure/parameters/state

Write: Formal ML based dump of model structure/parameters/state.

============================================================================
17 May 2010

Practicalities: Moved Neutral into the shell directory to begin the process.
Set up the Parent/child msg to pass an int.

Checkin 1797
============================================================================
18 May 2010
Working on benchmarking. Got it to compile on an 8-core Nehalem.

Try	Machine		1thread	2thread	2nodes	4thread	4nodes	7thread	7node
1	laptop		29.8	18.7	18.4	17.7	32.1	
2	laptop		29.6	18.2	18.5	17.7	32.1
3	Nehalem		20.1	9.5	9.3	5.3	Failed	3.3	3.8
4	Nehalem		20.8	10.2	10.6	4.9	Failed	3.1	3.7
5	Opteron1node	33.6	16.1	17.8	9.7	10.5	9.7	13.6
6	Opteron1node	33.7	16.1	18.3	9.8	10.5	9.4	11.8

	1 job/node	1thr	2node	4node	8node	16node	32node
7	Opteron clus	30.9	16.8	9.3	5.7	5.5	5.7
8	Opteron clus	30.9	16.1	9.3	5.0	5.5	5.1

	2 job/node	1thr	2node	4node	8node	16node	32node
9	Opteron clus	30.9	17.8	10.1	6.4	4.8	6.8
10	Opteron clus	30.9	17.9	9.6	6.4	4.9	6.9

Note that the Nehalem was running something else 100% on one of the cores.
So the 7thread/7node cases are actually faster than if I use all 8 cores.

This problem at least does not scale too well on the cluster past 8 nodes. 
But it runs!

Made the problem 2x bigger: 2x as many IntFire neurons, 4x as many connections.
This was done by setting 'size' to 2048 instead of 1024.
We have connectionProbability = 0.2, so the # of synapses ~ 0.84 million
There are 1000 timesteps and 7/20 neurons have fired in the previous timestep,
so about 2048 * 1000 * 7/20 = 0.72 million APs have fired.
With 0.2 connectivity, this comes to 2048 * 0.2 * 0.72e6 = 294e6 
synaptic events.

Try	Machine		1thread	2thread	2nodes	4thread	4nodes	6thread	6node
1	Nehalem		93.9	45.8	failed	23.0	23.2	19.7	15.8
2	Nehalem		93.9	45.8	failed	25.3	25.5	16.2	15.9

	1 job/node	1thr	2thr	2node	4node	8node	16node	32node
3	Opteron clus	142	69.1	failed	31.3	16.2	11.1	11.0
4	Opteron clus	136	72.7	failed	31.2	16.1	10.8	11.2

	2 job/node	1thr	2thr	2node	4node	8node	16node	32node
5	Opteron clus			failed	38.5	19.5	14.2	17.4
6	Opteron clus			failed	38.8	20.3	11.4	17.2

This is interesting. The cluster scaling beyond 8nodes is pretty poor. 
No benefit at all in going to 32 nodes. This is using an infiniband network.
Would be interesting to see scaling with more detailed neurons.
I wonder how much of this is setup time... Let's rerun with 2000 sec 
runtime.

Also there is a puzzling crash that seems to happen at a certain
number of nodes, and across machines. Need to track down.

Checkin 1798

Redoing for 2000 sec runtime to estimate setup time.

Try	Machine		1thread	2thread	2nodes	4thread	4nodes	6thread	6node
1	Nehalem		187.0		failed	55.6	49.9
2	Nehalem		171.8			50.3	48.6

	1 job/node	1thr	2thr	2node	4node	8node	16node	32node
3	Opteron clus	271	124.7	fail	61.2	31.5	19.7	17.4
4	Opteron clus	275.8	134.9	fail	61.0	30.9	18.5	19.7


10000 neuron model, 200 steps
Try	Machine		1thread	2thread	2nodes	4thread	4nodes	6thread	6node
1	Nehalem		556.8	308		189.8	217.2	127.2
2	Nehalem		680	336		207.6	185.1	129.9

		1thr	2thr	4node	8node	16node	32node	48node	64node
3	GJclus	1367	603.7	236.5	90.6	42.9	27.1	26.0	26.8
4	GJclus	1408	580.3	236.3	90.7	42.7	26.9	25.9	26.6
	no overlap					25.0	23.3	22.5
							24.5	22.8	21.3

Firing rate in 10000x200 model is 22 out of 81 per timestep. 

============================================================================
19 May 2010
Working on Neutral.cpp. Trying to implement Field access to name 
(which is stored on Element)

============================================================================
20 May 2010
Continuing with implementation. Removing the 'name' field from the Neutral and
putting it into the parent Element has led to all sorts of updates needed
in the unit tests, about halfway through. Checkin 1800.

Went through the remaining tests, now clears all. Checkin 1801.
Ran valgrind. It is happy.

Now setting up Neutral as the base class for all Elements. Odd bug pops up in
testScheduling.cpp:69, where the timings don't match.
Turns out to happen when Tick is derived from Neutral. Will deal with it later.
Checkin 1802.

Implemented creation-time message from parent to child. Surprisingly it 
Just Works. Yet to do full testing on it, but it clears old unit tests.
============================================================================
21 May 2010
First "simple" function in doing msg traversing turns out to be messy.
I want to get the FullId of the parent of any Eref. Looks like I have to:
- scan through Element::m_ array of all destMsgs. 
	Likely that it is entry 0.
- Find the srcElement from e1()
- go to SrcElement, scan through Element::msgBinding_ till we find one that
	mathches the MsgFuncBinding, that is, both the mid and the fid are OK.
	Check if this 
	

============================================================================
22 May 2010
Msg::findSrcEref: Find the full Eref of the src, given the dest. In most 
Msg cases this is the src Element. Sometimes, like the SparseMsg, we will 
need to look up msg internals to find src from dest and its DataId part.

============================================================================
23 May 2010
Implementing Neutral::getParent. This requires some message traversal 
functions, so lots of implementation in the different Msg classes. 
Checkin 1804.

Partial implementation of unit tests for traversal.  Checkin 1805.
Implemented all unit tests for Msg::findOtherEnd. Checkin 1806.

Added tests for Element::findCaller. Checkin 1807.

Added tests for Neutral::parent (to look up parent id). Checkin 1808.

Added 'children' field to return a vector of child Ids. Tested OK. Checkin 1809.

Added 'path' field, added 'me' field. Tested OK. Checkin 1810.

Added 'getChild' function. Need a LookupFieldFinfo to handle such cases,
where the field uses and extra argument. In array type lookups I could use
the index field of the dataId, but this isn't general.

============================================================================
24 May 2010
Put in a unit test for raw getChild, without a Finfo wrapper. Works.
Checkin 1811.

Trying to set up LookupValueFinfo and associated funcs. 
Working on LookupEpFunc.

============================================================================
25 May 2010.
Tracked through SetGet and Field operations for doing a 'get' call. 
It goes through Shell::innerDispatchGet. There
is no easy way to pass in an argument to the Get function. So the 
LookupEpFunc isn't much use.

============================================================================
27 May 2010.
Starting move and copy operations, which are handled by the Shell. Checkin 1813.

Implemented move operation, and its unit tests. Works. Checkin 1814.

============================================================================
28 May 2010
Working on 'copy' func. Trying to compile.

============================================================================
29 May 2010
compiles but I don't have any data in the new Element tree. Need to 
implement pure virtual function DataHandler::copy( n ).
Implemented most of the stuff for Shell::doCopy, compiles, does not
clear unit tests. Checkin 1823.

============================================================================
30 May 2010
Fixed up Shell::doCopy. Clears unit tests, but they do not yet stress
the data handling of the copy operation. Checkin 1824.
Big things still to fix:
- delete operation to clear all children recursively
- Copy operation to copy all messages.

Implemented Neutral::isDescendant, and unit test. Clears. Checkin 1825.
Separated out Copy operations into ShellCopy.cpp. Checkin 1826.

Need to implement Msg::copy( Id orig, FuncId fid, Id newElement, Id newTgt, 
	unsigned int n );

============================================================================
31 May 2010

Message copying into array turns out to be messy.
		What happens to:
Msg type	OrigSrc		OrigDest	NewMsg type
SingleMsg	ZeroDim		ZeroDim		OneToOne
SingleMsg	ZeroDim		OneDim		OneToSlice
SingleMsg	OneDim		ZeroDim		SliceToOne
SingleMsg	OneDim		OneDim		SliceToSlice

OneToOne	Zero or One	Zero or One	OneToOne I think.

OneToAll	Zero or One	Zero or One	SliceToAll: parents
						SliceToFatSlice: comput
						OneToAll: master-worker control

Diagonal	One		One		SliceDiagonal

Sparse		One		One		SliceSparse

The outcome of this, of course, is that n-copies won't work till I sort out
the issues of 2-D arrays and add the Slice msg types.

Did a first pass, pending implementation of these issues. Compiles but
does not clear unit tests. Checkin 1827.

Was duplicating the parent->child msgs. Now clears unit tests.
============================================================================
1 June 2010

Confirmed functioning of messages in copied tree, and also correct 
copying over of initial values. There is a big revisit of all this 
pending to deal with the issues of arrays: dimensions, start, end
of copied data on multiple nodes, and so on.
I've gone through some of this on pg 49 of the handwritten notes.
Basic conclusion is that adjacent indices should remain together.
Also it looks like the current DataId will do with a linear lookup: it is
up to the DataHandlers and possibly Msgs to keep track of dimensions.
Also think about promoting locals to globals if they are going to be
copied.
Other big pending issue is proper cleanup of children after delete of
parent.
In addition I need to rebuild the signaling solver. That is a major use case
to be converted over, and then I need to look at parallelizing it.
Also ext fields. This comes up in deleting element trees.
The stages in the earlier version were: 
Mark for deletion
Clear Msgs
Delete.
This works out because it is far faster to clear Msgs if we know that the
whole lot are to be wiped out. Otherwise there is overhead to find and
clear dangling ends. In principle I could first build the delete tree as
a set, and use that instead.

To do ext fields, we need another hook, or to replace the Cinfo. 
The kinds of ext fields we need are:
- shared. All the entries on the Element should see them. The 'name' is an
	example. 
- One per object entry. Coords are an example.
- Purely synthetic. Coords in a regular array are an example here too.

Why not do as child Elements?
	- Slow. This is primarily because of linear search by name.
	- Parsimonious. Adds nothing to the existing structure.

So Child Elements it is. But it is kind of strange to decorate an entire tree
with additional child Elements. Also assumes everything is derived from 
Neutral. If we were going to make this strong assumption, then it could have
been built into the Element class.

Turns out I already had a good indicator for deleting elements: the cinfo_
field. When set to 0 it tells the Msgs not to bother to traverse.
Coded up, clears unit tests, but the destroy op isn't actually being called.
need to decide if deleting an Id should refer to the Neutral::destroy.

To be symmetrical with the Element creation convention, let's have it so that
Shell::doDelete really does destroy the element and its children, but the
Id operations do not.
also we need to be careful to put the doDelete operations on a safe queue
separate from the Messaging queue.

The Shell::doDelete seems to be OK, but it has uncovered at least two problems:
- The previously pending issue of Synapse descent (or not) from Neutral.
* the testMove() function turns out to have problems. While the moved child
	thinks it has moved, its old parent still wants to hold on.
	Further confirmed by putting in a count for # of children.
  Fixed the Shell::doMove. It was not deleting the old msg, just disconnecting 
  	Now all the unit tests clear. Rather a lot of stuff now to checkin.
	Checkin 1831

============================================================================
2 June.
Fixed up many of the memory leaks with valgrind. Still need to tackle the
issue with the Synchan inheriting from Neutral: I think I can then clear the
rest.
============================================================================
3 June.
Nasty bug came up with the scheduling: When Tick is inherited from
Neutral, it inherits the Parent SrcFinfo. This messed up the hard-coded
indexing in the tests for process messages out of the Tick.
Fixed, clears unit tests again.  Checkin 1833.
Excellent, clears valgrind too without further messing around.

From 19 April: Some longer-term perspectives.  
	+ Set/Get functionality: GetVec yet to be done.
	* Multinode message setup
	* Unit test for IntFire system
	+ Benchmarks and optimization
	- Continuous messages: Should we abandon these? 
		- Enormous simplification of system.
		- High-traffic continuous stuff should use solvers
		- High-traffic stuff is pretty efficient using queues.
		- Likely speed penalty.
	- Element tree, wildcards
	+ Move and copy of elements
	- Node balancing, even if it is done offline
	- Autosched
	- Documentation
	
We need to work into these things as we bring in mainline MOOSE
	- Solvers
		- Replace Cinfo and DataHandler on Elements that are solved.
		- The Finfos in Cinfo all get replaced.
		- The DataHandler points right to the solver.
		- The messages and other stuff remain untouched.
		- The Fids are all local to the Cinfo, so they now just refer
			to their new functions.
	- Parsers and threading
	- Graphics and threading
	* Extended fields
		- coordinates, generated rule-based on DataId
		- arbitrary new fields, one per entry
		- Element-globals: fields that are one per Element.
============================================================================
4 June.
Another thought for a sweeping change: How about eliminating the regular
messaging form ( GENESIS legacy) for signaling simulations? Instead have all
molecules go directly over to solver calculations?

I'm implementing a skeleton for the kinetics directory, which has been
present but not compiled for a long time. Seems that it would be far more
efficient to go straight to a solver-based design here.
For now, put in Mol and Reac implementations and complete compilation.
Checkin 1835.

Design for signaling calculations:
- Molecules grouped into compartments and 'groups', like GENESIS.
	- Note that compartments != groups
	- Compartments point to their spatial definition, but the compt
		itself could be inside or outside, or a surface.
		- Would it make sense to have functionless messages: just
		indicate connectivity without any implied operations?
			- Issues with some of the builtin functions, that assume
			one end is on the msgBinding_ vector.
			- How do we identify the msg if there is no associated
   			  funcId? We have gone to some lengths to ensure that
   			  Msgs do Not carry func info. Here we would be stuck.
		So for now, don't go there.
Q: Which is 'parent' for this organization, and which is handled by messaging?
	Option 1: Parent = compt, groups as msgs, compt pts to geom.
	- allows for multiple levels of grouping
	- compt natural container for reacs/mols.
	- Problem with kkit convention, needs effort to convert
	Option 2: GENESIS grouping, assign compts by Msgs.
	- Same convention as kkit: possibly cleaner conceptually?
	- Awkward messaging for volume issues, compartments get muddled.
	- Note that the solver handles volume issues
Pick option 1.
Q: Organization on solver, vs GENESIS backward compatibility
	Option 1: Solver is parent cell/compt. Compts or molecules
	are ElementFields.
		- Nasty nested ElementFinfos to handle 
		  channels/synapses etc.
		- Nasty SparseMsg to stand in for various inter-compt
		  and other msgs. If we don't expect to support this 
		  level of backward compatibility, we could eliminate.
	Option 2: Implement whole mess on solver with whatever
	data organization is good for computation. 
		- Implement entire arbitrary Element tree elsewhere,
		  with whatever data organization is good for user. 
		- If necessary make the tree structure readonly so 
	  	  that we don't get into nasty callbacks when adding 
		  or removing molecules.
		- Use smarter versions of FieldElements to access
		  the data. These can make their own heirarchy as
		  they like, but use a FieldDataHandler which takes
		  extra args to know which portion of the data set
		  it should handle.
	Option 3: Same as option 2, except:
		- Use messaging to map data entries from solver to
		  aribtrary Element tree.
		- How often do we need fast field access?
		- Explicit but messy map may be worse than implicit one.
	Issues
		- The link between data and access is implicit if it uses
		  ptrs for the Data, and messy if it uses messages.
		- When and how are the two versions of the
		  model interface synchronized?
		  - At Reset on the solver
		  - As part of the function that adds reactions to the solver
		  - At creation time using kkit or SBML reader
		- Do I rebuild the entire Element interface if I implement a
		  solver with a different data structure?
		- Which is the reference model/data struct?
			- The full mol element tree is expensive esp in arrays
			- The 

Most fundamentally: how essential is it to preserve backward compatibility?
Easy: drop it.
Medium: Preserve field and Element browsing compatibility, and any scripting
	that uses this. Field assignments but only partial Element creation.
Medium-ugly: Preserve message browsing compatibility too, specially message
	scanning. But all readonly. A few special messages, such as for I/O
	could still be possible to create/remove in old form.
Hard: Preserve full messaging and Element creation compatibility.

I think that though the centre of gravity should shift to a Python-optimized
view of the model, the BC is important enough to deploy people to get the
medium and medium-ugly aspects to continue to work.

If we want this to happen, we need something on the order of option 2 or 3,
where an entire apparent Element tree is visible.

Some of this comes to the question: can we map a large number of distinct
Elements with individual names, such as molecules or electrical compartments,
onto an array? If this is done then various issues can be sorted. 
Key features:
	- Names managed separately
	- Data through any of the >0 dimension DataHandlers, including
		FieldDataHandlers
	- Element traversal knows how to deal with this. That is really a
		Shell issue.
	- Wildcard and allchild traversal also knows how to deal with it.
	- Provision for unique children for individual entries.
	- Separate out node-specific subsets.
Key issues addressed:
	- Scalability
	- Backward-compatible interface
	- Browsable interface.

Much work. We need a really good reason to do this. Scalability is the only 
one that existing Element structures do not handle. Here too it might not
be such a problem:
	- # of individual Elements is usually finite and small, even in things
		like a complex neuronal model
	- Scaling usually involves making huge arrays of the originals. This
		can be done efficiently in MOOSE.
In the existing MOOSE, Raamesh had implemented a transform for array indexing.
It went like this: If we make an array of a prototype tree, then each Element
in the new tree is an array. However, to the old Genesis it looked like the
parent of the tree was an array and it had individual children.
Options:
	- We don't need to replicate this in MOOSE.
	- We could replicate this only in the GENESIS interface.

Related issue: predefined indices for Element::m_ vector of MsgIds.
Index 0 should be parent.
Index 1 could be the explicit link to the solver.

		-----------------------------------
Some decisions, from the need to maintain a uniform interface.
1. Will maintain medium to medium-ugly levels of backward compatibility for
	purposes of browsing, both within SLI and Python.
2. Will maintain regular Element tree for things like cells and reac networks.
2a. Existing array functionality will handle scaling issues.
2b. Special msgs keep track of grouping, other system-specific conventions.
	Src is on group organizer, dest on components of group.
3.? When loading models use the minimal interface class.
4. Minimal interface class will include dummy messages for connectivity info
4a. Doesn't hurt to make the Msgs functional, but then the 'process'
	has to be turned off.
4b. Need to provide simple and complete API for traversal e.g., 
	vector< FullId >targets( string msgName ); // Looks up outputs of MsgSrc
	vector< FullId >sources( string msgName ); // Looks up inputs of MsgDest
5. Solver classes must be able to build their model definitions from the
	interface classes.
6. For runtime updates, solver classes must 'take over' interface classes
	in situ by replacing cinfos and data handlers.

These are essentially the same decisions as in the earlier MOOSE.
		-----------------------------------

Steps:
- Implement standalone kkit parser to build models.
	- Move out SUMTOTALS, buffered mols and other oddities into special
	entities like arbitrary math op, buffered subclass of mol, etc.
	- Build it on compartment heirarchy, with grouping defined by 
		additional msgs.
	

Implementing OpFuncDummy for use in grouping. Done.

============================================================================
6 June.
Implementing ChemCompt. Some points.
- It would be nice to have a piggyback msg so that child Molecules could easily
	look up their 'size' so as to report conc. The context of piggyback 
	use is when there is already a message connecting two objects but we
	would like an additional function.
- It would be nice to have a fast lookup across Elements, avoiding the 
	messaging. Again, consider conc calculations. Perhaps this is
	legal only in FieldElements.

Setting up parsing using the CellParser code from moose_g3.
============================================================================
7 June 2010
Checking in the kinetics code changes. Checkin 1842.

============================================================================
8 June 2010
Extended fields revisited.
If we can automatically promote child Elements to fields, we're in business.
This is like the converse of FieldElements.
Does it have to be hard-coded, or only in the Shell functions?

One option:
	- Make Element->findFinfo function to replace current lookup
		through cinfo.
	- If it fails to find finfo, tries for child object name
		matching the field.
		- Perhaps a flag to indicate that the child is to be
			used as a field?
		- The name of the base object is just ""
		- Shell::dispatchSet will have to do stuff.

	- Alternatively, Shell::innerSet could go after the child
		Element, except that all we have here to find the
		correct child is the FuncId. 
		- This would be a good place if we could manage it
		because it handles the incoming Msgs.

============================================================================
10 June 2010
The SetGet::checkSet function has all the info needed to decide if the
field is actually a child Element. Supposing we define fid == 0 as a
predefined fid for setting value of entire object. fid == 1 would have to
do the 'get' function on it.

What could go wrong:
	- mis-name a field, and as a result try to 'set' a child Element.
		This is unlikely because of type safety rules.
	- Waste of a couple of good FuncIds: for set and get resp.
	- It is a bit of a hack.
	- Hard coding this is ugly.
	- Will need to redo stuff here for adding msgs to these 'fields'
	- How to handle SetVec etc.
	- Type of same fid changes with Element class: this is OK
	- Figure out how to do 'get'

At init, Cinfo could automagically adapt Dinfo to assign the base set/get
Finfos.
This is messy. Simpler to have Neutral define set_this and get_this as
dummy funcs, and then override these in classes which are allowed to be
used as ext fields. Might also be useful in data transfer across nodes
and other kinds of serialization.

============================================================================
11 June 2010

Working on ext field elements. Implemented a 'Real' class. I would be
happier if I could figure out how to do the operations
double foo = Real(1.234);

Also I am unsure how far I can go if Real is not derived from Data.

With this in place I can now start to munge the field assignment code to
look for child Elements to act as ext fields.
============================================================================
12 June 2010
One impediment in doing this is that we still derive all MOOSE classes from
Data. This derivation is only used in DataHandler<subclass>::process, 
which typecasts the data to a Data*, and then calls its Process.
We could do better by using a Process call in Dinfo. This would be typesafe
and would also allow for classes that do not support Process.
Later.

To do child Element field assignment:
- Add Element return to SetGet::checkSet
- Fix Shell::dispatchGet to look for the finfo on the child element if needed
- Later: Fix Shell::doAddMsg to handle set/get driven by message.

Checkin 1852.
Now need to put in unit test
Put in test for 'set' command. To my astonishment, it works. Checkin 1853.
Actually the unit test was buggy. Fixed. Still works. Also handles SetVec
correctly. Checkin 1854.

Added in functionality for ExtField 'get'. Unit tests happy. Checkin 1855.
Fixed memory leak found by valgrind. Checkin 1856.

Renamed Real to Mdouble. The string class will then be Mstring, etc. 
Checkin 1857.

For tomorrow:
	- Get rid of Data class.
	- kkit parser
	- Solver infrastructure.
Then I can go back to parallel coding.

============================================================================
13 June 2010
Starting on ReadKkit. Skeleton coded and it runs. Checkin 1858.
Incremental progress on framework for argument conversion. Checkin 1859.
Further progress and checks on ReadKkit argument conversion. Checkin 1860.

Now I need to implement the tricky part of this process, the grouping and
the compartment assignment.
Planned design for kkit successor is that the compartments form primary
groups. Within these, there can be organizational groups that do not have
any compartmental implications, but are just for placement. 
Orthogonal to this, we can have other organizational groups that may span
compartments. For example, synapses.

Conversion has some frequent issues. For example, PKC exchanges between
the cytosolic and membrane-bound forms. I often do not even have a volume
difference between them. These are legacy models, so I may not be able to
do the right thing automatically in all cases.
In the synapse, AMPAR cycles between PSD and bulk. kkit tries to assign
a compartment (geometry and geometry[1]) to them. May as well use this, but
sometimes these have to be merged if the volumes are the same. 
Problem is that the grouping is orthogonal. For now make sub-groups within
the geometry. Avoid third layer of grouping.

So the heuristic is like this:
- Build up the kkit tree the original way
- Check for compartments as the model is loaded. Build these on /kinetics.
	At this point I can't deal with nesting. Maybe never should.
- Groups that are intact in one compartment: no problem, they just move into
	the compartment.
- Groups that have components in multiple compartments: duplicate group names
	in the sub-compartments.

Merged compilation into main tree. Checkin 1861.

Added Shell/Neutral calls for tree traversal.
Like May 9, had to fight with GNOMEs keyring for quite a while, 
finally managed to subdue it, by adding a line in  .subversion/config:
password-stores =
and was able to checkin as 1862.

Added functions for field access to Shell::cwe_. 
Clears tests for Shell::doFind. Checkin 1863.

ReadKkit now creates real live Groups and Molecules. Checkin 1864.
ReadKkit now creates Reacs. Checkin 1865.
ReadKkit now creates messages between mols and reacs. Checkin 1866.

============================================================================
14 June 2010
Implemented Enz, incorporated into ReadKkit. Msgs pending. Checkin 1867.
Put in most of the Enz msgs too. Checkin 1868.
Still to do the MMenz case.

============================================================================
15 June 2010
First pass MMEnz implementation. Compiles. Checkin 1869.
Second pass: Sets up the MMenz messaging. Checkin 1870.
Added in the test file dend_v26.g that I used. Checkin 1871.

Minor cleanups for unused variables vol and slave_enable. For now I've
just put in hooks for their future use.
Also implemented Pool::diffConst field. Checkin 1874.

- Define compartment objects and some default geometries.
- Make /kinetics a compartment with the default volume
- The assignCompartment function maintains a list of compartments
	with different volumes. Every time a new volume appears,
	it creates a new compartment. For now assume that all
	objects with the same volume are in the same compartment.
	Returns compartment id.
- After build is done, then we scan all groups to see if all contents
	are in same compartment. We need to build up a map of
	all mol ids, which refer to their compartment.
	Are all mols in same compartment?			(A)
	- Yes: If same compt as parent of group, retain.
	- Yes: If differs from parent of group:
		Has new compt been instantiated?
		- Yes: move group to new compt
		- No: Make new compt. Move group into it.
	- No: Split group into same/diff compartment parts.
		With each part of group, goto (A). 

- Groups are invisible to a compartment. Any molecule looking for its
	volume or geometry goes ancestorwise till it finds a compt.
- 


compartment has a collection of boundaries
	- Each is a geom.
	- Simple compartments are just enclosed. Single boundary.
	- Complex compartments have different diffusion rates on
		each boundary. E.g., cylindrical compartment part
		of a long cylinder.
	- At each boundary there is a possible adjacent compartment.
		- Can we have both the surface compt and also the
			next one out?
		- Adjacent compartments may be dimension shifted.
			(e.g., cytoplasm to membrane)
	- All compartments have a size.

	- A membrane compartment may have 
		membranes on others. Multiple boundaries.
	- Compartment as a whole has a volume

	- How do we square with 'outside' concept of current SBML?

Worked out ChemCompt and Boundary to set up compartmentalization and 
interfaces between compartments. Will also need a geometry class for these
to connect to. Checkin 1875.

I could scan through and find all volumes.
I could scan through and put all Ids in a list by volume
I could create compartments on the fly as soon as I find a new volume.
	It could be on the current group
	It could be on /kinetics
		But if the compt
I want /kinetics to have the biggest volume, others inside it.

First scan: find all volumes
Assign /kinetics to biggest, put all compts right on /kinetics
Second scan: find groups that have a single volume. Put on appropriate
	compartment. May lose nesting?
	Groups that have multiple volumes: split up into volume-defined groups.
	Put onto appropriate compartment.

Massive struggle to get the compartmentalizing done. For now I'll skip it
to test that the model gets loaded and can run. Next:
* Implement tables to test output
- Implement enough scheduling to run model and test it.
- Implement the solver. This is what this exercise was really about.
- Then I can come back to the compartmentalization.
- Look at the sbml reader.

============================================================================
16 June 2010

Starting on tables. Implemented them from scratch, not bothering with too
much backward compatibility in this round. The table entries are 
handled as FieldElements. Checkin 1878.

Put in first pass unit test for Table acting as a buffer for incoming
data. Checkin 1879.

Put in a manual check that the table can dump data into xplot form. 
Commented out after checking.
Coded in the details for ReadKkit to talk to tables as plot substitutes.
Checkin 1880.

Now to implement Shell::doUseclock, the hook for which is now implemented in
Shell.cpp.
============================================================================
17 June 2010
Thoughts on generic wildcarder, one that can do network messaging too.
- Object
	- Portion that does traversal. Follows wildcard tree.
	- Portion(s) that handle constraints and return a double. For example,
		distance from source, membrane potential, or combination.
	- Portion(s) that operate on constraints. For example, create a
		message, set a synaptic strength, connect up a clock tick.
	
Converted in previous moose wildcard code, threw out the indexing stuff.
Lots of Set/Get fixes needed to do string set/gets without prior knowledge
of types. Checkin 1886.

Compiling in the unit tests for wildcard, again based on earlier version.
Still huge number of compiler errors.

============================================================================
19 June 2010
Wildcard.cpp finally compiles. Fixing up unit tests.
Turns out that StrSet/StrGet are not working. Set up unit tests for 
them.

Still struggling to get them to work.

Now past the StrSet/StrGet unit tests, back to the wildcard tests.
Slow but steady progress through wildcard tests. Currently at 
Wildcard.cpp:545. Checkin 1891.

============================================================================
20 June 2010
Cleared next round of unit tests for wildcards. This was painful. Checkin 1892.
Minor issue: The wildcard tests for fields issue a warning. Don't want it.
Valgrind is not at all happy.
Cleared up two big issues, the deletion of the test trees for wildcards and
for /kinetics. Remaining stuff is a lot of uncleared pointers from getSetGet.
I think this can be replaced with a call to the function that uses getSetGet,
(innerStrGet), rather than returning the pointer.
This will require lots of changes. Checking in the fixes so far. Checkin 1893.
removed all getSetGet calls, replaced with memory-safe Finfo::strSet and 
Finfo::strGet calls in Finfo and all derived classes. Valgrind reports
_still_ more leaks. Checkin 1894.
Further fixes to ReadKkit, improved, still leaks. Checkin 1895.
Yet more cleanup. Finally leaks gone. Checkin 1896.

Now to get back to testing chem kinetics. For starters, 
* run model, dump data, and compare with genesis.
* Bring in solver to scan model tree and zombify it.
* Test out solver too
Then in no particular order:
- Eliminate Data class
- Implement arbitrary math handler
- Fix up compartmentalization
- Implement readSBML
- Parallelize

Set up code to run model. At this point it just stalls.
Fixed the stall, now it crashes. It is calling process on a mol,
even though the src of the message is a reac and the arguments do
not seem to be right.
One subtle possible issue that SharedMsgs can be added either way,
but the code requires that the src and dest have a specific
direction. This could get messy if we wanted to use one of the
messages in a standalone way at the same time. For example, consider
Mol::nOut. It could be the src message for several things, so it
should always be on e1. Suppose its target thought the same in 
a shared message. There would be confusion.
I had earlier handled this simply by checking whether the src was
e1. Replicated this approach. Lots of changes in SrcFinfo, Qinfo and so on.
Compiles but crashes. Checkin 1898.
Progress. Now clears all unit tests and even the ReadKkit run.
Answers are wrong, though. Checkin 1899.

============================================================================
21 June 2010
Enzyme model test working now. Checkin 1900.

Thoughts on eliminating Data class and handling process and reinit:
- The Data class is used only to provide 'process' calls to Tick. 
- We actually also need to provide a reinit.
- Earlier idea had been to use Dinfo to provide process and reinit.
- Alternative: 
	- use the regular message framework for setting up the 
		msgBinding_ entry on the Tick.
	- Process becomes a shared msg with reinit.
	- Tick goes explicitly to this entry and scans through it directly,
		eschewing use of the queuing system.
	- This keeps the infrastructure to the minimum, and also bypasses the
		issues of putting process calls in the queue.
	- This also makes it easy to handle reinit, init, and other arbitrary
		target functions from process.
	- When an Element is zombified its process msgs should be eliminated.
	

In the meantime, lots of cleanup to get simple test programs for enz, reac,
mmenz to work. Now clears the Kholodenko.g model. Checkin 1901.

Had a look at KineticHub. Almost all of its contents will vanish, since the
DataHandler will be shared directly with the zombies and the zombie access
functions will do all the necessary lookups. No messages will change either,
except the Process ones.

- It would be nice to have a notification (another message?) when a message
	is added or removed. This will let me change the structure of a model
	while keeping the solver informed.
- I need a clean interface to the DataHandlers.


Adding solver code. Created subdir ksolve. Checkin 1902.
============================================================================
22 June 2010
A little benchmark: ran the kholodenko model for 60K sec at dt = 0.1.
Identical run in GENESIS: 2.4 sec. MOOSE: 12 sec. Pretty heavy costs for
the queued messaging. May be good to profile.

Did so. Here is the top of the profile:

Each sample counts as 0.01 seconds.
  %   cumulative   self              self     total           
 time   seconds   seconds    calls   s/call   s/call  name    
  9.78      0.88     0.88 22200390     0.00     0.00  Element::asend(Qinfo&, unsigned short, ProcInfo const*, char const*)
  8.60      1.65     0.77 37200000     0.00     0.00  SingleMsg::exec(char const*, ProcInfo const*) const
  7.54      2.32     0.68 37200390     0.00     0.00  Qinfo::addToQ(unsigned int, MsgFuncBinding, char const*)
  6.59      2.91     0.59 37200390     0.00     0.00  Qinfo::assignQblock(Msg const*, ProcInfo const*)
  6.20      3.47     0.56 37854727     0.00     0.00  std::vector<char, std::allocator<char> >::_M_fill_insert(__gnu_cxx::__normal_iterator<char*, std::vector<char, std::allocator<char> > >, unsigned long, char const&)
  5.42      3.95     0.49 90001576     0.00     0.00  Msg::getMsg(unsigned int)
  5.36      4.43     0.48  1236668     0.00     0.00  readBuf(char const*, ProcInfo const*)

This is somewhat difficult to work on, as there isn't any single huge 
bottleneck. Messed around with it a bit, but the improvement is negligible.
Now MOOSE takes 11.5 sec. Anyway, checked it in for reference. Checkin 1904.
Cleaned out benchmarking stuff. Checkin 1905.

Starting out on the ZombieMol. This turns out to be easy and clean to
implement if it is subclassed from Stoich.  Checkin 1907.
Pending issues:
- Swapping out contents of Element for zombie
- Stoich should handle arbitrary arrays, or explicitly take care of sims
	where the molecules are going to turn into reac-diff vectors.
	The latter is easier and far more powerful. Only the molecules
	need to add dimensions: reacs remain fixed through the volume.
	I need to then add the diffusive calculations.


Added in ZombieReac implementation. Checkin 1908.
Added in ZombieEnz implementation. Checkin 1909.
Added in ZombieMMenz implementation. Checkin 1910.

All this is good baseline stuff. Now the crux of the matter: swapping
Elements. Simple approach is a Element::zombify function,
to be called by the functions that scan the existing tree and put in the
new one.

Working on this. Mostly OK, except how do we decide what kind of DataHandler
to build when unzombifying an Element? The preliminary code is in 
ZombieMol.cpp and Element.cpp. Got the zombify stuff set up. Checkin 1912.

Put in zombify stuff in Reac, Enz and MMenz. Checkin 1913.

Other part of zombification: Scan path and figure out stoichiometry matrix
from messages.
============================================================================
23 June 2010
Stoich::setPath begins to handle traversal. Yet to deal with messages.


============================================================================
24 June 2010
Still working on traversal. Checkin 1918.
Traversal coming along. Major updates including eliminating the Element
calls relating to sync messaging. Not yet there, but checkin 1919.
Putting in the RateTerm creation stuff for MMEnz. Checkin 1920.
Putting in the RateTerm creation stuff for Reac. Checkin 1921.

Put in general functions in Element for getInputs and getOutputs for
specified Finfos. Replaced these in ZombieReac, seems OK. Yet
to carry through on other Zombies. Checkin 1922.
Next: Clean up zombies. Build the stoich matrix. Run. More unit tests.
Cleaning up ZombieReac and ZombieMMenz. Checkin 1924.
Cleaning up ZombieMol. Checkin 1925.
Cleaning up ZombieEnz. Checkin 1926.

Putting in the stoich matrix stuff. Printed it out, seems OK.
Added in stoich matrix stuff for enzyme too. Printed out 
barebones enzyme, seems OK. Handles the bigger dend_v26.g model,
but I won't try to decipher the stoich matrix. Checkin 1927.

Now on to the solver proper. Added in the Stoich funcs to handle it.
Need a clean way to exchange the data structures.


============================================================================
25 June 2010
First pass at GslIntegrator. Compiles but doesn't do anything as I haven't
set the USE_GSL flag. Checkin 1929.

Fixed up USE_GSL flag. Compiles. Checkin 1930.

Trying to set up message from Stoich to GSL. Like the plot messages, I want
to harness the 'Get' command capabilities to do this. Examples:
				Initiate		Handle return
- Script/shell 'get' request:	Shell			Shell
- Plot 'get' request:		Plot			Plot
- GSL stoich request:		Shell or Manager	GslIntegrator.

Defer for a bit. Focus on the solver. Seems to be getting close. But it dies.
Turned out there was a sign flip for the Stoich matrix terms for Reac.
Fixed. Runs. Does Kholodenko oscillatory model correctly. Checkin 1932.

Implemeinting hooks for Table to be able to request a datum,
typically used when trying to plot values on distinct timesteps.
Trying to compile...
Compiled, clears tests without hanging, but need to test the new functions.

============================================================================
26 June 2010
Implemented unit test for table to request a datum. A bit of a hack, but the
whole 'get' system is a bit of a hack. Checkin 1936.

Some cleanup of diagnostic messages during unit tests. Checkin 1937.
Checking memory leaks using valgrind. Many fixes. Checkin 1939

From 20 June, we had this list, which I have extended with dependencies
* Eliminate Data class			<- full reinit/proc msgs
- Implement arbitrary math handler
- Fix up compartmentalization
- Array and diffusive kinetics		<- Multidim arrays
- Implement readSBML
- Parallelize				<- Fix scheduling
- Fix scheduling 			<- full reinit/proc msgs
- Try replacing y_ with S_		<- Benchmark solver 
- complete ReadKkit			<- Buffers, Sumtotals

============================================================================
27 June 2010
Lets start with the Data class.
Q: Should I extend the process( Eref, Qinfo*, ProcPtr ) to have an extra
argument to decide between reinit, process, and other stages?
A: No. It is tempting but the other stage argument would need to be tracked
	in the Tick. Messy.
Q: Can I extend the SharedMsg with arbitrary extra msgs on the same line?
	In other words, piggyback?
A: Should be possible, but probably irrelevant, as different Ticks will
	be doing other stages.

There was a hiccup in the implementation because I had not fully worked
out how to get the DataHandlers to call 'process' for all objects. Did an
implementation, this is how it could go:
- Create a ProcOpFunc<T>: public OpFunc
- It does the usual OpFunc stuff, and has an extra function 
	proc( char* obj, Eref e, ProcPtr p )
	which does the type casting into T and then calls the T::*func
	for the process/reinit/init and other actions.
Pros and cons of this arrangement over earlier Data base class:
- Can handle arbitrary # of functions of the Proc func form: proc, reinit, etc.
	This can be hacked using Data, but not cleanly.
- No Data base class needed
- Need to maintain an entire specialized structure for process. This is true
	for both versions, but more so for the Data base class.
- Uglier. Typecasts etc during operations, though the Data form also needed
	typecasts.
- Possibly an extra operation or two. e->cinfo()->getOpFunc( fid ) as opposed to
	the Data::process virtual function.
- With the Data::process virtual function I could have passed in an extra
	argument to specify which of the ops to do. Need to trickle back to the
	calling Tick, and it would have to uniquely associate the correct func
	number to it. Not too different from passing in an fid.
Overall, items 1 and 3 are the crucial ones. Go with the conversion.

Conversion now compiles, but doesn't clear unit tests.
Checkin 1941.
Amazingly, just one more fix and the unit tests clear. Checkin 1942.

I would like to also fix the passing of Eref as the
full object. Better to pass by reference, though the compiler might
be doing that anyway.

Next, to fix scheduling, I need to standardize on a shared Finfo
for Proc/Reinit and set up the scheduler to deal with this.
- ReadKkit to default create the solver and stuff, and to
schedule it.
- Solver to remove process messages if present.
- Solver expanding into arrays
- Solver expanding into arrays with diffusion using RK
- Parallellization of all this.
- Multithreading of all this.
============================================================================
28 June.
Consolidating process and reinit into a shared message. Done for
kinetics and ksolve directories. Checkin 1948.
Done for builtins and biophysics. Checkin 1949.

Converting over to the consolidated form for Ticks. Croaks in unit tests.
============================================================================
29 June.
Tracked down bug. Unit tests now clear. Checkin 1952.

Applied the 'proc' shared message in unit tests. Croaks. This led to 
consderable fixing up of the Shell control of scheduling, specifically,
implementation of reinit, stop and terminate functions. Checkin 1956.

============================================================================
30 June.
Set most objects to use the shared Msg "proc" rather than the old "process"
message. Unit tests showed up a funny failure, finally tracked down to
failure to clean up the 'doReinit_' flag within Shell. Fixed, now clears
unit tests again. Checkin 1959.

Plans on going to array stoich solver, leading soon to stupid diffusion
calculations.
- The S_ vector becomes a temporary holder for the current compartment.
- The RateTerms and the vector V do not need to expand, and they can keep
	pointing into the S_ vector.
- updateV does not change what is there. 
	However things get unpleasant on multithreads.
- UpdateV is extended with a loop that fills in the diffusive flux. This
	will need just another table for diffusion constants.
- innerGslFunc cycles through multiple rounds of copying successive
	compartments worth of data back and forth into S_, and doing
	updateV each time.
The issue with the above set is that it won't work on multithreads.

Could consider the following bigger changes:
- Replace the S_ vector with a pointer to the current chunk of the y_ vector.
- Replace the RateTerm pointers with indices. This is slower but means
	that they can be used multithreaded for different chunks of data.

Refinements to updateV may still be relevant, but the details of how to
set up compartment updates are not, given that we need to implement a
completely different reac-diff solver. More important is to get the
data structures right.

============================================================================
6 July 2010
Went through converting function calls using Eref to using const Eref&.
Checkin 1975.

Some cleanup of ksolve tester. I had tried to use a ptr into the S_ vector 
directly for the GSL routine, but it didn't work. Checkin 1978.

Trying to put in buffered molecules, the inheritance seems to be temperamental.
============================================================================
7 July 2010
Putting in BufMols. The test for this has thrown up a lot of bugs with
scheduling, limitations of wildcarding, and so on. For now it
works but need to clean up. Checkin 1980.
* Make it so the 'ISA' flag in wildcards applies for ancestor classes too.
- Get rid of 'stage' in ticks. Their index and dt should do.
* Fix Gsl integrator to handle buffers.

Implemented Cinfo::isA function. Checkin 1981
Incorporated into Wildcard.  Checkin 1982
Added ZombieBufMol. Checkin 1984.

Fixes to Stoich and ZombieEnz, now it does buffered and regular enz reactions
OK. Checkin 1985.
I thought I was set for testing a big simulation, but ReadKkit croaks because
it doesn't know how to handle SumTotals. Need to put in the arbitrary math
expression calculator.

Brought in Raamesh's MathFunc. It seems pretty complete.
General idea:
- Have a RateTerm wrapper for the MathFunc
- Have another RateTerm table for handling updates that occur without 
	integration, but affect molecules that the reactions may use.
	e.g., sumtotals and tables
Looked at the RateTerms. Don't really fit well with the MathFunc. I
need a tighter form, say MathTerm, that handles the reactants and the
function string. I should set up a dummy version to do the SumTotals first.

Converted MathFunc over to current code base. Set up its unit test, it clears. 
Checkin 1988.

============================================================================
8 July 2010
Function handling taking shape. On the solver side, there will be a vector
of FuncTerms to handle all functions. FuncTerms are the base class and have
an operator()( double t ) so that they can accommodate functions of time
as well as molecule levels.

On the MOOSE object side, we need a base class FuncBase. This will basically
encapsulate a FuncTerm of the appropriate type, and provide hooks between
the messaging and the FuncTerm. Also will need to provide Zombification 
functionality.

Implemented framework on solver side. Compiles, clear tests, but doesn't
exercise any of the new code. Checkin 1990.

Implemented SumFunc as a standalone object. Not sure that the framework is
suitable... let's see how it looks as I scale up. Checkin 1992.

Implemented FuncMol as a derived class of Mol, designed to relay its input
to the molecule count. Updated ReadKkit to deal with it. Correctly loads
and runs model in ee mode, but the solver is yet to handle it. Checkin 1995.

At tail end of compiling ZombieSumFunc and colleagues, needed to get the
data into the solver.
============================================================================
9 July 2010
Completed compilation, solver now sees it and seems to work. Checkin 1999.
Tried with the bigger production model. ReadKkit still fails.

Fixed ReadKkit, but the simulation resuts for dend_v26.g model are wrong.
Checkin 2000.
Looks like PKC-active has something funny going on. It is the target of
the sumtoal. It is flat in the new MOOSE but rises steeply with the old one.

============================================================================
11 July
The GSL version fails on the PKC model from DOQCS, acc48.g...

		Genesis		GSL
PKC-active	0.0719806	0
PKC-cytosolic	0.825743	0.878
PKC-basal*	0.0165155	0.0176
PKC-Ca-memb*	0.0286744	0.0305
PKC-Ca-AA*	0.00385561	0.00415
PKC-DAG-memb*	0.00624439	3e-5
PKC-AA*		0.00432258	0.0046
PKC-DAG-AA*	0.0123681	6e-5
Ca		0.08		0.08
DAG		11		0.0569
AA		5		5

So, DAG is doing something odd.
The EE calculation in MOOSE looks OK.


A bit side-tracked: I did a valgrind to see if I could track this down.
Valgrind reveals a huge mess in MathFunc. Memory leaks galore. Need to fix,
but will have to defer.  Checkin 2013.

Fixed a minor leak in Stoich, commented out MathFunc test. Valgrind now
clean. This also means that the problem with the model above is not 
memory-leak related. Checkin 2014.

Tracked problem down to Stoich.cpp. I had used the wrong offset for 
updating the FuncMols. Now works with pkc model. Checkin 2018.

Checked again: dend model _still_ does not match up.
============================================================================
12 July
The old moose does handle it OK. Going through mol by mol:
Ca: OK
DAG: OK
AA: Goes down to zero in new code, initial part is OK
CaN: OK
AC2: OK
cAMP: Way too low. About 3 orders of mag.
CaM.Ca4: OK
GTP-Ras: 10x too low.
Raf-GTP-Ras: 50x too low.
PKC-active: Starts OK, but fails to build up.
PP1-active: OK
tot-auton_CaMKII: OK
tot-CaM-CaMKII: OK
MAPK*: zero. 

Tested the obvious possible problem from this list, of MMenz with a 
buffered substrate. GSL is OK.

Stripped off other stuff to make a model of AC, AC.g.
This also shows a discrepancy but the other way:
AC2*-Gs:	OK
AC1-Gs:		OK
AC1-CaM:	A little low
AC2*:		OK
cAMP:		3x too high.

Removed the PDE stuff, to make model AC_noPDE.g. This matches up. Is it the
PDE stuff on its own?

Removed the Gs stuff, to make model AC_no_Gs.g. This does not match.
cAMP:		3x too low (in gsl)
AC2*:		OK
AC1-CaM		OK, a bit of what looks like numerical error at ~5%.

Removed AC1 also, to make model AC_no_GS_no_AC1.g. This is way off.
cAMP:		None at all in gsl
AC2*:		OK.

Funny, the system doesnt seem to call the MMenz zombification routine.
============================================================================
13 July
Compared the AC_no_GS_no_AC1.g output for ee and ksolve modes. The cAMP
differs, and is wrong in both.
Fixed difference between ee and gsl methods. Now cAMP is zero in both. 
	Checkin 2035.
	- Can't be just MMenz. The Kholodenko oscillatory model works, and
		is full of MMenz.
	- Enz on its own and MMenz on its own work.

Reduced problem to just one enzyme, in AConly.g

Tracked it down. Unexpected: it was bad comment handling in ReadKkit.cpp,
which was discarding lines with the string "*/". AC2*/kenz was therefore
always discarded. Fixed this. The AConly.g file now matches, and so does the
original dend_v26.g. Checkin 2038.

Fixed problem with missing first data point in plots. Added operation to
request data at Table::reinint. Checkin 2040.

Quick benchmark on dend_v26.g run for 2000 sec:
current MOOSE: 97.5 sec, 
old optiMOOSE: 87.5 sec 

Unsure why there is this difference. All the work should be done by the
gsl solver, which is the same.

Next major cleanup is for volume handling and compartments.

One fundamental issue here is that if I refer to a different Element for
the volume in order to calculate conc, I do NOT want to use messaging to
access the data. The queueing data transfer would cause scheduling issues.

Another issue: explicit vs spatial indexing. I should really ask the system 
what the conc is at a given point in space, rather than specify a compartment
through indexing.

Things to ask of a molecule:
- n throughout the entire compartment
- n in a subvolume based on the grid indexing
- n in a specified region of space
- conc likewise

============================================================================
15 July.
For some of the more complex cases, implement an 'inspector' object.
For starters, just obtain local conc. Assume mol as a whole has access to
vol info. Would like a object-wide field in the data handler, one that isn't
duplicated when it becomes an array. At reinit this field is updated by the
compartment. I could implement this by having a derived class with the global
field. A temporary of this is created when the data is looked up, and the
global field is filled in. This would involve some nasty memory management.

For now, put in the extra double for the volume. Wasteful but the alternatives
are not much better. Note that we have other similar things in the pipeline,
like diffusion constant. Note also that all these go away when the solver
gets rid of individual entries. Checkin 2052.
============================================================================
16 July.
Setting up scan for volumes for generation of compartments. Vol categorization
code puts the test model dend_v26.g into 3 compartments, clearly incompatible:
# cases		Vol		Notes
129		2.767e-27	Likely the molecules as there were 129 of them
				But the value is wrong. Should be 1e-15 m^3
49		600000		Likely enzymes.
5		1		?

Fixed various issues here: the vol scaling is corrected, and the enzyme complex
vols are set by that of the parent enzyme. Checkin 2053.

Now return to the issue of organizing compartments and groups.
- Should 'containment' be represented by group hierarchy?
- Should 'neighbour' be represented by peers?
- Should compartmentalization take precedence over grouping?

We have three kinds of tree structure to deal with:
- Grouping
- Containment
- Neighbours.
Can we merge the containment and neighbour tree structure? 
	Should we treat a compartment as a single entity with an array of
	parts that have diffusive coupling?
	- It may have structure (spines, etc) that need to be positioned off
	individual compt entries.

No, I think we keep each of these distinct. There can be separate messages
to keep track of each of these relationships.

Options:
1. Use current grouping as parent/child relationship.
2. Use Containment as parent/child relationship
3. Use above merged containment/neighbour tree as parent/child.
4. Use tree hierarchy in whatever grouping seems sensible, let 
	Containment/neighbour messages deal with themselves. By default
	put containers in flat hierarchy like in neuron model defaults, but
	this is not mandatory. 'Sensible' in this case also implies that
	the grouping should be easy to array-ify.

1. is simple for kkit use, but at odds with desired formalizing of containment.
2. is good for SBML and other containment forms, but is not informative about
	multiple array entries each containing something. 
3. is probably not good to mix concepts. We want very clear-cut info from
	the messaging.
4. is free-form and puts the burden of organization on separate message-linked
	trees.

Use cases: 
	Dendrite with linear diffusion and lots of spines
	Presynaptic terminal with lots of vesicles
	Neuron model paralleled to electrical model

Seems like all of these can be done with option 2. Other levels of organization
are all easy, and all that we have done is to state that the 'containment'
concept is the primary one.

For kkit conversions: 
Largest compartment is not always container. Consider PSD and diffusive models.
Thus, /kinetics should not be assumed to be a compartment. This, though, will
make it extremely hard to handle backward compatibility in scripts.

Option 4 it is then. The next thing is to standardize the location and
organization of the grouping, container, and geometry objects.
ReadKkit puts stuff under a specified modelname, which can be under any
parent. 
/pa/modelname/kinetics
is where stuff now goes.
So all these things should go under /pa/modelname:
/pa/modelname/compartments		: Msg to all child Elements.
/pa/modelname/geometry			: Msg from/to compartments
/pa/modelname/grouping			: Msg to relevant child Elements.
/pa/modelname/kinetics			: Neutral based groups and kinetic model

============================================================================
17 July.
Put in first step at implementing the compartmental stuff. Molecules
are adopted by their compartments. Seems OK.
Should I also put reactions into compartments? See what SBML does. 
Checkin 2054.

From 28 June we had:
- ReadKkit to default create the solver and stuff, and to
schedule it.
- Solver to remove process messages if present.
- Solver expanding into arrays
- Solver expanding into arrays with diffusion using RK
- Parallellization of all this.
- Multithreading of all this.

1. I need to fix the naming. ChemCompt refers to a cellular compartment,
	which may be spatially extended. I should have a separate naming
	with the spatial compartment, which is internally uniform and is used
	for numerical calculations. Let's call it voxel.
2. Different cellular compartments are very likely to discretize differently.
	They are even likely to use different solvers. So solvers should be
	compartment-specific.
3. If solvers are compartment-specific, then we need to set up a clean way
	to interface between them. Brute force is forward Euler to estimate
	fluxes. I've done a little calculation to work out a Crank-Nicolson
	scheme with an intermediate transfer 'compartment' that doesn't
	react, but I haven't figured out how to deal with the mol concs in it.


Consider voxels A-1, A, A-1 where 
	A is the inserted transfer voxel,
	A-1 is the last voxel on the first compartment
	A+1 is the first voxel on the second compartment.

Explicit calculation:
A(t+1) = A(t) + dt*( A-1(t) + A+1(t) - 2A(t) )

Implicit calculation:
A(t+1) = A(t) + dt*( A-1(t+1) + A+1(t+1) - 2A(t+1) )

To make it Crank-Nicolson, we take the average of the above two. 

For now, just use forward Euler for flux. Possibly exp Euler even.
Each timestep, get 

Flux = A-1(t)*D/dx - A+1(t)*D/dx

vtot(A-1) = v - Flux
vtot(A+1) = v + Flux

Another complication: Stoich will need a different flux for each boundary.
So boundaries have to be ElementFields. As an example, a branching dendrite
will need 3 flux entries: one for parent, and two more for the two branches.

Stoichs will have to match up the molecules that diffuse, and send out
only those. This looks like a set of extra 'v' terms.

Now that this is defined, I need to go back a couple of steps to set up
array stoichs within a compartment. This is needed before I can figure out 
how to deal with the end-terms for flux.

============================================================================
18 July 2010
Starting on array-ification of Stoich. 
- For GslIntegrator, we now need to build a y_ vector big enough for entire
	model.
- For Stoich, we need to keep assiging S_ and passing in the correct
	part of y_
- For zombies, we need to access y_ rather than S_. 
- Should we move the y_ vector into the Stoich?


============================================================================
19 July
Did a first pass implementation. This further underlines the desirability
of using indexing rather than pointers in RateTerms. Specifically, the
loop begs to be parallelized, but can't be as long as the native data
structure of the Stoich is being used for all calculations. This could
be avoided with indexing, as we then only need to base the indices off
different points in the mol conc vector. The remaining 
difficulty with this idea is that in diffusive models, we need to look
up buffer and func molecules in a different way from the variable 
molecules. Offsets in mol Conc need to go to different parts of the 
overall varMol vector, whereas the buffers and func mols can be shared.
Options
	- fold buffers/funcMols into the RateTerms?
	- Have distinct kinds of RateTerms for buffered mols.
	- Do memcopy into thread-unique buffers
	- Put the buffer/func mols at the end of the entire varMol set.
First two options will need multiple distinct kinds of RateTerms.
Fourth option will work and will also deal with individual access to all fields.

Anyway, get the first pass working first. This needs changes to GslIntegrator
to move the y_ vector out of it into the Stoich.
Did this, but more updates needed. The gsl output isn't coming because I
had changed the 'Mol' class. Fixed that, but now I get all zeroes because the
plot is trying to look up concentration, and the Stoich system doesn't
currently keep track of volumes. So all the Zombies return 0.
Checkin 2060.

============================================================================
20 July

Will need 3 modes of operation at least:
1. general Stoich class, picks up entire simulation for GSL or Gillespie
	calculation applied across whole model. This one will need to refer
	to all of the compartments.
2. Compartmental Stoich class. Fills in a single compartment and can
	discretize it in various ways. May even take over from the Compartment
	class. This one will have to provide flux terms for interfaces,
	as well as do something sensible about reactions spanning boundaries.
3. Compartmental nonStoich solvers, such as Smoldyn. Fills in a single
	compartment and does whatever it does internally. May take over from
	Compartment class. Provides flux terms.

For starters we need to implement 1, to be able to load and run generic
models. 
Then we need to convert this and RateTerms to use array indices rather than
pointers.
Then we derive the compartmental Stoich class from it and set up to do
1-D arrays with branching etc.

Did item 1 which was to pick up all compartments and deal with the volume
and conc stuff. This required several fixes in ReadKkit. Checkin 2076.

Fails on regression test with psd12.g, which has 2 volumes.

There is a problem with the kinetics code that cascades over the later test
of the shell messaging. Unclear why, but there are lots of memory leaks.

============================================================================
25 July
Turns out that the problem is in other models as well, but surely with the
kinetics code. Also the valgrind problem is probably just because we exit
from MOOSE with an assertion, leaving lots of stuff lying around. So thing
to focus on is why the kinetics stuff causes problems with later messaging.
Note that the ksolve is not the problem, it also happens with the regular
kinetic calculations.
Looks like there is a problem with Shell::setclock, or at least setting
clock 1. Clock 0 seems OK to set.
============================================================================
27 July
Still struggling with the setclock issue. Some random cleanups in the vicinity
of the problem, but they haven't helped. Checkin 2093.

Eliminated the Tick::stage field. Went through and cleaned up all the effects
of this change, with the idea that a global clean of the tick handling code
might fix the setclock bug. The scheduling unit tests clear with the new
code, but the setclock issue with ReadKkit persists. Checkin 2094.

The unit test clears if we run the test for 2 timesteps rather than 1. Seems
that the key thing here is to clean up the execution of all ops in each
timestep. This is still pending.

With this, ran psd12.g. Now the gsl version works but the original one 
doesn't. Valgrind is clean.

============================================================================
29 July
2 minor pending things before going to change the structure of the ksolver.
- Find why only 4 graphs are being plotted. The ones on moregraphs are not.
	I checked, and the plotpath is OK, and all 7 graphs are made.
	There are issues with setting up lookup for PlotId
	Fixed. Also fixed plotpath in dumpPlots. Oddly it dumps now for
	EE but not GSL. Tracked down to another plotPath issue in testKsolve.
- Find why the EE method fails in MOOSE
	- works for enz.g
	- Works for a reduced version of psd with only one compartment.
	- Works for a reduced version of psd with one mol added in bigger compt.
	- Works for a reduced version of psd with two mol added in bigger compt.
	Started over with psd12.g
	- Fails when the exocytosis step is removed.
	- Fails when the PKA conc is set to zero.
	- Fails when PKA is deleted.
	- Fails when both exo and endo reactions are deleted. Now we have
		no exchange between compts.
	- Fails when dangling Ca and PKA are also deleted.
	- Rebuilt plots. Now there is naming confusion in plots, and I need to
		fix it. However, the underlying problem persists.
		Call this one twocompt.g for reference.
	- Removed M of the big compartment. Problem remains.
	- Removed all reactions in big compartment, leaving just RP and M*
		pools. Now output is all zero, not informative.
	- Set init conc of M in small compartment to 1. Outputs still match.
	- Set init conc of M* in big compartment to 1. Outputs still match.
	- Set init conc of M in small compartment to 0. Outputs still match.
	- Made a reac between Rp and M* in big compartment. Outputs still match.
	- Set init conc of M* in big compartment to 0. Outputs still match.
	- Replaced the reac between Rp and M* with an enz. Outputs now 
		differ, but only by a factor of 2. Note that this means
		that M* exceeds total init conc in the EE case.
	- The enz above was in compartment B. Moved it to /kinetics.
		No change, outputs as before.
		differ, but only by a factor of 2.

This gets tedious. Let me fix up the graphs for now.
Fixed. But the problems with the EE calculations remain.

Let's start removing the small compartment objects.
	- Removed A/M*. Problem persists.
	- Removed phosphatase. Problem persists.
	- Removed kinase. Problem persists.
	- Removed RP. Problem persists. Now we just have A/M. 
	Save this reduced problem version as minimal.g
	- Removed M as well. Now the A compartment is empty. Problem persists.
	- Removed the A compartment. Problem persists. Copy to minimal.g
	- Shuffled the order of definition of addmsgs to match those in enz.g
		Problem persists.
	- Shuffled the order of definition of pools to match those in enz.g.
		Specifically, the kpool handling the enzyme is now first. Was
		last. Problem persists.
	- Plotted all molecules. Bizarre stuff happening with enz pool and
		substrate, looks like jump at TRANSIENT_TIME. Are we just
		running into numerical issues? The rates are not big.
	
	Reduced dts. The EE method now works, when there is no variable DT.
	With same dts, where finedt == simdt, ran EE with variable DT. 
	Still OK.
	With simdt == 2x finedt, ran EE with variable DT. Still OK.
	Still OK.
	Ran in GENESIS with suggested dt of 0.01. Works fine. Fails in MOOSE.
	So I have an error somewhere in the EE calculations. Ugh.


During reinit:
	Mol::reinit: n=ninit, A_ = B_ = 0, nout.send.
	Enz::reinit: r1 = k1
During messaging: 
	Mol:: A and B increment on incoming A,B. Which doesn't happen.
	Enz:: r1, r2 and r3 update on incoming n. This happens.
Mol::proc1:  
	Mol::proc: integrate then send. First integ is pointless.
	Enz::proc: send, then reset r1. First send finally has data.
During messaging: 
	Mol:: A and B increment on incoming A,B
	Enz:: r1, r2 and r3 update on incoming n

To change: The Enz is on a later tick. It is executed only after
all data from t0 has arrived. So we may as well start calculations
right away.
This doesn't work. Reverted.
I checked if there is something bad happening with volumes. Doesn't seem to be.

Derived another test case from the generic enz.g, simply by scaling
volumes up by 1000x.

Check what enz does on creation with its cplx molecule.
Showed fields. Nothing obvious.
Checked what the timing of the 'tick' is like. Seems OK.
============================================================================
01 Aug 2010

Going to try again, this time just printing out values at each place each
timestep. 
In the meantime, checked in the updates as 2104.

Printf debugging to the rescue. The problem seems to be that the reinit
call sends out messages from all mols, and then the process call
also sends out the same messages. The 'sub' function is therefore called
twice for each molecule, doing a multiplication into the internal variable
r1_ each time. During normal process operations r1 gets reinitialized, but
not during this initial timestep problem. That is odd.

OK, this is what the scheduling does:
During reinit, it just calls reinit on all objects, in scheduled order.
During process, it first deals with all the pending queue stuff, then 
	calls process on all objects.

So there is no clearing of pending stuff after reinit. If there are multiple
sends during this phase, they all just pile up till the first 'process'.
Should rethink and clean up.

Can I keep all 'sends' within a timestep? No, the 'process' comes second
and there will usually be spillover to the next timestep.

in that case I may as well set up the rule that the only 'sends' that reinit
is allowed to do are 'sends' that span timesteps.

In reactions, the reacs and enzymes cannot send useful data till
they have done their 'process'. So the sequence should be:

REINIT:
Molecule reinits and sends out data.

PROCESS:
Stage 0:
	Reacs/Enz handle msgs
	Reacs/Enz send update
Stage 1:
	Molecules handle msgs
	Molecules send update.

This also has implications for resume and cleanup of processing.
It is cleaner to have a separate 'resume' or 'restart'
function called when a sim is beginning or resuming. If so,
reinit just sets boundary conditions.
restart sends out the preliminary data to start the cycle.
Should decide if reinit should fold in restart.
Restart always needed when resuming simulation midstream.

Another way to put it, is that at the end of any timestep, the
system should be in a state where all pending data can be flushed,
and where the state variables of the system can easily regenerate
the messages needed to continue calculations where they were left
off.

OK, worked. This took a long and weary time to fix, but I've now
set policy for how to handle multi-stage process calculations.
Checkin 2105.

OK, now change gears to look at implications of indexing the stoich
calculations, rather than direct pointer refs.
Phase 1: Benchmarks of existing version.
Binary		File		Runtime		Plotdt		User time
moose_msg	Kholodenko.g	6e6		5		19.1
moose_msg new	Kholodenko.g	6e6		5		19.9
moose_g3	Kholodenko.g	6e6		5		28.6
moose_msg	Dend_v26.g	200		1		9.7
moose_msg new	Dend_v26.g	200		1		14.2
moose_g3	Dend_v26.g	200		1		8.4

Interesting divergence here. The smaller model runs way faster on current code,
bigger model a bit slower.
Checkin 2106.

Now go through and reimplement Stoich and RateTerms to use indexing.
Done. Looks like a pretty severe speed penalty specially with the bigger
model. Checkin 2107. Let's see if I can track down the speed issue.

Did profiling. Nothing obvious.

============================================================================
2 Aug
Try more careful benchmarking. I started out with a really big model (acc81.g),
except this gave me still more bizarre results, with the new MOOSE being
astonishingly fast on it. Then another mid-size model.

Binary: moose_msg new (with array indexing). On: Mishti: (Lenovo X301)
	U9400@1.4GHz.

File		#Mol	Runtime	Plotdt	t1	t2	t3	Comments
Kholodenko.g	15	6e6	5	18.8	18.8	18.8	2.25 on 500s dt
spine_v61c.g	104	500	10	18.5	18.6	18.5
Dend_v26.g	129	200	1	14.0	13.9	14.0	Surprising slow	
acc81.g		1050	200	1	12.6	12.6	12.6	Surprising fast

Binary: moose_msg old (with array ptr lookup). On: Mishti: (Lenovo X301)
Kholodenko.g	15	6e6	5	18.8	18.6	18.9	2.1s on 500s dt
spine_v61c.g	104	500	10	13.7	13.7	13.7
Dend_v26.g	129	200	1	9.5	9.5	9.5
acc81.g		1050	200	1	12.6	12.5	12.5

Binary: moose_g3 (Jul 21 64-bit build). On: Mishti: (Lenovo X301)
Kholodenko.g	15	6e6	5	27.9	27.9	27.8	Surprising slow
								2.2 on 500s dt
spine_v61c.g	104	500	10	11.8	11.8	11.8
Dend_v26.g	129	200	1	8.3	8.3	8.3	Eh?
acc81.g		1050	200	1	28.4	30.4	28.6	Eh??

These results are all over the place. The only consistent finding is that
the old moose_msg binaries (pointer version) work faster than the new 
array index variants. Sometimes by 50%, sometimes not at all. The old 
moose_g3 binaries are all over the place, sometimes better still, 
sometimes dreadful.  I really need to compile and run these on another machine.
Just to be sure, I also went and compared the output of the calculations
with those on Genesis. Seems OK.

============================================================================
4 Aug
Now running benchmarks on Nehalem

Binary: moose_msg new (with array indexing). On: Ghevar: (Xeon E5520@2.27GHz)

File		#Mol	Runtime	Plotdt	t1	t2	t3	Comments
Kholodenko.g	15	6e6	5	10.6	10.7	10.7	1.1s on 500s dt
spine_v61c.g	104	500	10	10.6	10.6	10.6
Dend_v26.g	129	200	1	8.7	8.7	8.7
acc81.g		1050	200	1	8.8	8.9	8.9

Binary: moose_msg old (with array ptr lookup, R2106). On: Ghevar
Kholodenko.g	15	6e6	5	10.7	10.9	10.8	1.2s if plotdt=
									500
spine_v61c.g	104	500	10	7.7	7.7	7.7
Dend_v26.g	129	200	1	5.8	5.8	5.8
acc81.g		1050	200	1	8.9	8.9	8.9

Binary: moose_g3 (Oct 14 2008 64-bit build). On: Ghevar
Kholodenko.g	15	6e6	500	1.8	1.8	1.8	Note long plotdt
spine_v61c.g	104	500	10	8.4	8.5	8.4
Dend_v26.g	129	200	1	6.3	6.2	6.4
acc81.g		1050	200	1	9.3	9.3	9.3


Conclusions
1. The array indexing does have a significant impact in medium-sized models.
	This is between 30% and 50% slowdown.
	My interpretation is that this is cache-related. Really small models
	have everything in cache, or are otherwise limited. Really big models
	are out of cache regardless.
2. The new solver compares well with the old one in most cases.
2a. The Kholodenko model initially gave odd results, but it turns out that
	these were due to file dump time. Without it, things are comparable.

Discussed with Subha, he doesn't have any additional insights other than
agreeing that array indexing should not cause such a big speed hit.

Let's do one more cycle of profiling to see if there is something else
obvious happening.

A lot of work later, turns out that the models are not producing exactly
the same output. There are issues with the Ca handling, and the # of 
calculations differ in the profiling. This may be the problem.
Specifically, it looks like CaM-Ca4 is different and so are its downstream
molecules. Possibly a high-order reaction bug.

Working through by elimination of the pathways. The critical one seems not to
be CaM, but PP2B.
A bit more tracking, and I can get the discrepancy just with the PP2B (CaN)
model.

Reduced still further, it is just the two Ca binding steps, each of
which is a reaction 2nd order in Ca. I get a steadily near linear
increasing CaNAB-Ca2.Co with time in the 2107 checkin case.

Tracked down to the single reaction 
2 Ca + CaNAB <===> CaNAB
with Kf 1000 and Kb 0.1. (kf in this vol of 1e-15 is 2.78e-9)

Found error - nasty bug where virtual function args had been changed,,
but the one for the derived class had not. So the default virtual func
was being called and giving the wrong answer.
With this fixed the timings are:

Kholodenko.g	15	6e6	5	19.4	19.4	19.8	2.4 on 500s dt
spine_v61c.g	104	500	10	13.4	13.4	13.4
Dend_v26.g	129	200	1	9.7	9.7	9.7
acc81.g		1050	200	1	12.7	12.7	12.8	

These new times are almost exactly like the old (array pointer) ones.
Also confirmed that the output now matches perfectly (md5sum) with the old
one for the Dend_v26.g model. So we are in business. No barrier to going
ahead with the indexed approach to calculate diffusion models. Checkin 2112.

============================================================================
10 Aug 2010
Instead of trying to do different geometry calculations independently, I'll set
up a data structure that keeps track of junctions. Each junction has a scaling
factor derived from cross-section area and length of the compartment, and
applies this individually to each diffusing molecule. Will need a bit more info
for motors and non-uniform diffusion.

class jn {
	double scaling;
	unsigned int vox1;
	unsigned int vox2;
};

However, many of the jn entries could be generated on the fly based on higher-
level knowledge of model shape and voxelization. # of entries is quite
large, ~# voxels. 

Other consideration is to make the representation general enough that other
solution methods than Runge-Kutta could use it.

Should I index the junctions as independent entities, or have the first 
index refer to the first voxel? Issue of multiple references.

Seems like
a) All reactions within a compartment are the same. Same Stoich. 
	- But we might use different solvers for the same compt
	- We might want to share stoichs for different identical compts: spines.
		- alternative is to treat them as diff geoms.
b) Each compartment (ChemCompt) supplies an iterator to internal junctions.
	Or a lookup function to let us get the relevant junctions within and
	at the boundaries of the compartment.
c) The compartment holds one or more geoms. 
	- Used internally to figure out junctions
	- Used externally to draw.
d) Compts talk to each other by another set of junctions, implemented as
	FieldElements. These contain info about how to exchange molecules,
		and include mappings of mol indices between compts.
e) Motors are a special case of inter-compt reactions. It is a bit premature
	to specify them in full, but the general idea would be to use the
	full SBML capabilities to define the reactions, and add in a scaling
	using XA and L of each voxel. Don't mix up with the diffusive stuff.
f) Buffered gradients. Another nasty one to specify. Tedious to
	implement: a vector or func for init values, and housekeeping to
	fit it into the numerical calculations with each voxel.

Some specific cases:
A neuron model could have four compts: the soma, the dendrites, the spines
	and the psds
	These would have special geometry refs that look at the neuronal 
		morphology spec. Need to handle multiple separated geoms for
		spines/psds.

============================================================================
10 Sep 2010
We need another layer of info; specifying which solver to use where.
Option 1: just use another compartment, even if the stoich is identical
Option 2: Use the definition of cellular compartments as domains where 
	reactions are the same (even if there are spatial gradients). Then
	I need to subdivide the cellular compartment into numerical 
	compartments.

15 Sep 2010
Note that either of these is messy. The model is being confounded with the
algorithm. Alternatively one could argue that we are just setting the model
up into domains of deterministic and stochastic behaviour.

If we use option 2, then we really want the ability to use a higher-level
spec to tell the system which algo to choose where. It could for example be 
based on dendrite diameter, or it could be a simple geometrical spec.
Big advantage is that I can swap around numerical approaches independent of
basic model definition.

I also need to build in the interface capabilities with other solvers: adaptors.

chemCompt ------/ Model
		/ Stoich
		/ metaGeom ---/	Geoms
		/ innerJunctions
		/ outerJunctions
		/ spatialReactions
		/ spatialParameters
		/ spatialAlgorithms [---/ GeomList]
		/ adaptors[]

Some details:
	- metaGeom could refer to a class that takes a path to a neuronal model
	and manages a chemical decomposition to match it. Or it could be
	a vector of regular geoms. Has to provide discretization info. 
		- Need a vector of discretization objects, which specify
		gory details of junctions as well as a lot of the spatial stuff.
		Also handle geoms. Hm.
	- innerJunctions and outerJunctions need to refer to assorted geoms
	as well as to discretization 


============================================================================
16 Sep 2010
Subha and others are having difficulties with the current MOOSE. So I need
to change gears and focus on getting the new base-code ready for adoption.

Key issues:
- Thread handling needs some cleanup
	- Need to set up a separate GUI thread and possibly another for 
		the openGL stuff. Or they could go out by MPI.
	- Generic cleanup
- Queue stuff needs cleanup
	- See about streamlining the allocation into local/offnode queues.
	- Have another look (later) at optimization. This is a big bottleneck.
- Array handling and indexing and lookup needs some work
	- need also to sort out policy for multidim arrays.
		- Could just add ops for DataId to take a vector of 
			indices and vector of sizes to compute linear index.
		- Will need to add ArbDimGlobalHandler.
		- Clean up numData assorted set of ops in DataHandler.
	- Sort out field vs element array logic.
+ Unit testing
	- Need to resurrect the whole bank of unit tests.
	+ need to create regression tests driven by C++ for system-wide stuff.
- Create and copy need work, specially for arrays
- Complete list of shell functions. This may need to provide an
	interface to the Set/Get operations through strings.
- Move may need work
- All Shell functions need to be made thread and MPI-compatible
- I need to build some unit tests at the Shell level for all operations,
	and an automatic way of running them through multithread and
	multinode and combinations thereof.
- Proper Design Document
- Fix up dimension junk in the create function: give a simpler version for
	0 and 1 dim arrays.
- Treat arrayFieldElements so that we can easily look up their dimensiosn.

============================================================================
18 Sep 2010
Finally started to code. Renamed Shell::create into the more systematic
Shell::handleCreate. Compiles. Turns out that I have an error now because
I deleted a file 'foo.g' which the ReadKkit was using to test.
Should separate out unit tests from regression tests.

============================================================================
19 Sep 2010

Setting up MOOSE regression test system
First order is to ensure that it can correctly load and run specific models.
In addition, From Wikipedia:
"Therefore, in most software development situations it is considered good 
practice  that when a bug is located and fixed, a test that exposes the bug
is recorded and regularly retested after subsequent changes to the program"

For MOOSE, I'm putting this second kind of issue into both the regression and
unit test mandates, as appropriate. The general scope of regresion tests
is entire system function. The scope of unit tests is function of
specific small subparts.

I'll implement a function in Table to read in files including single and
double-column xplot files, scanning through for a specific xplot name,
and doing interpolation in due course.

This needs a regression test.
I need a regression test now for testing ReadKkit too.
I need a regression test for testing ksolve too.
So I've commented out these operations as they utilize an external model file
	for their tests, and hence are more appropriate for regression testing.

Finally, all compiles and clears _unit_ tests. Checkin 2179.

Next: Make a table comparison function or Class. Class it is, since there
will be a range of parameters to assign for comparison. This is a bit
redundant when considering the capabilities of NumPy, but we need it for
low-level runtime calculations so it stays.

Done. Implemented as a function wthin the table to avoid class proliferation.
Does RMSdifference  and scaled RMS difference calculations. Use the latter
for measuring goodness of fit in a dimensionless manner. Checkin 2180.

With this in hand I can implement some regression tests. First to test
the testers.

I've now set up some of the framework for the regression tests. Still need
to clean up the base Makefile so they are not compiled in at all unless
a flag is set. Still the table test doesn't work. Checkin 2181.

The test doesn't work because I haven't fixed up StrSet and StrGet.
Currently they go dangling from the DestFinfo. Need to decide if I should
use the virtual forms or go direct to statics.
============================================================================
20 Sep 2010

Implemented the StrSet. Involved fixing lots of functions and OpFunc derived
classes. Checkins up to 2183.

With that, the regression tests started to give real error messages. Fixed
those. Checkin 2184.

Added a few more regression tests for the Table. This revealed yet more
bugs, including a nasty one for getting the outputValue. Fixed. Checkin 2185.

At this point I think Tables are pretty well covered, and with them, many
of the utilities needed for regression tests. On now to fix up
some of the model-based regression tests.

Need to implement a general Shell command to load in a model from a spec file.
Shell::doLoadModel( fname, destpath );

Turns out that Id::Id( const string& path, const string& separator )
	has yet to be defined. Ugh. Fixed. 
Implemented some unit tests for components of Shell::doLoadModel. Other
parts will need regression tests.

Checkin 2188.

Worked through regression tests for checking file type and then for
loading and running Kholodenko model using ReadKkit. Added a few
more unit test cases for bugs that turned up.
Checkin 2189.

Now we have the base regression test framework working.
============================================================================
23 Sep 2010
Immediate two cleanups:
- Array indexing, both elements and fields
	- Always have array as linear array. The Data Handler currently 
	does a lot of stuff to return different slices/indices of it. It
	can probably generate an indexer class that the Shell uses to
	make sense of the array index.
	I would like to be able to index arrays by text strings too, but
		not critical.
	Options:
		- Function that takes arbitrary args and generates linear index.
		- Function that takes vector of uints like the dimensions vec.
	Problems:
		- Linear lookup of something with ragged arrays. 
			Here we need to transform quickly from the linear
			index to the multidimension one.
		- Resizing arrays
		Options:
			Use rectangular dimension assumption.
- Implement Shell::doResize to change dimensions of an Element.



So the interface looks like this:
	unsigned int linearIndex( vector< unsigned int > dimIndex );
	unsigned int size( unsigned int dimension, vector< unsigned int > dimindex );
	unsigned int numDimensions();

	In addition the DataHandler needs generic functions for setting these
	things. setSize would have to pass in a vector of sizes
	Iterators.

============================================================================
30 Sep
Slowly working on AnyDimHandler which is meant as a prototypical data handler
that fully exercises the capability of DataDimensions class.
Need to incorporate node allocation in AnyDimHandler::resize();

Use case for 3-D array is obviously diffusion.


Dangling functions for the AnyDimHandler:
- copying to globals from non-global. This can be deferred. It requires
	a way for all parts of the object to be consolidated. The Shell
	may need to do an all-to-all send where each node sends out its
	own data contents.

An issue is: who sets multinode policy? Ideally should separate this out
from the DataHandlers. Comes up because the copy and allocation operations 
involve multinode decomposition. This decomposition will often need to
talk to the shell:
	Allocate: Need to decide 'start' and 'end' of the data chunk. More
		generally, there may be many ways to chop up the data.
		Should this policy be decided by different subclasses of
		DataHandler, or should the DataHandlers just do this generally
		and have the Shell tell them how data is done?
	Copy into Globals: Shell needs to arrange that all nodes contribute
		their data to the composite global
	Copy into another dimensions: Typically will want to assemble
		all entries into a block, and then duplicate this into the
		new dimension so that each node gets one or more blocks.
		But there are lots of policy issues such as the granularity
		of the blocks and the # of nodes.

Option 1: Proliferate DataHandlers to embody all this policy
Option 2: Implement DataHandlers generally enough that they can handle any of
	the policies, but leave the policy decisions to the Shell and up.

What is in a policy?
	-> Given a vector of dimensions, and N nodes with T threads each, 
	which object indices should be on a given node? 
	- Note that all threads have access to all the object indices on the
		node. So the threads are needed to figure out policy, but we do
		not subdivide objects by thread.
	- Policy also depends on the messaging structure. But that is a still
		higher-level decision, coming from the solver or model builder.
Policy operations:
	Allocation / resizing
	Duplication/copy, including adding more dimensions
	isDataHere... 
	Looking up data from multidim index

If we assume all DataHanders are n-dim arrays, then example policies are:
	- linear block along data array: Easy to look up, matches the Id.
	- cuboid block in n-dim array: good for neighbour connected models
		- columns and other low-dim versions are variants of this.
		- Spatial mapping need not match index mapping, but the
		duplication of blocks in new dims is likely to retain structure.
	- scattered entries along data array: Easy to resize. Hard to redim.
		But redim is tricky anyway.
	- Sticky dim0. This means that we try to keep objects on dim0 
		(the fastest changing dim) together, as they are likely
		highly connected. e.g., neurons in a cortical column or glom.

Policy		Allocation	Lookup		IsDataHere	Resize	add dim
Linear block	find start/end	[id - start]	start<=id<end	OK	hard
scattered	1 + size/N	[id/N]		(id+offset)%N	OK	?
cuboid block	size/N multidim	vec(id - start)	cuboid		Messy	Easy

Sticky dim0:
Linear block: as before, with start and end being calculated accordingly.
scattered: 	dim0 * (1 + size/(N*dim0) )
				dim0		((id + offset)/dim0)%N
				

Multilevel arrays:
	Bulb model case
		Compts in each cell are individual Elements, as each has
			different channel composition.
		Have a set of distinct prototype Cells, with their solvers.
		Make arrays of each in order to set up one glom.
		Have a set of distinct prototype gloms, each arrays of
			assorted mitral and PG cells.
			Say mit[0-7] in glomTypeA, mit[0-4] in glomTypeB
		Make arrays of each of these gloms for whole bulb.
			mit[0-500][0-7] in glomTypeAs, mit[0-600][0-4] in B.

	This won't work with a single base array. Need to make two base mits.

============================================================================
5 Oct 2010
AnyDimHandler taking shape, and I've begun the process of getting the newly
defined interface into the base DataHandler class.

Pending issue is to globalize or unglobalize DataHandlers, which also requires
the ability to pass contents around. This is in principle also enough to
do dynamic load balancing, but there are issues with the ordering.
Specifically, if we want to shift some objects from node A to node B,
we are likely to mess up the ordering of objects on the data handlers of both
nodes.

============================================================================
6 Oct 2010
Starting the compile process. Since there have been many incremental changes,
I'll checkin things. Checkin 2199.

More fixes. Checkin 2200.
More fixes. Checkin 2201
============================================================================
7 Oct 2010
More fixes. Checkin 2201.

More fixes, now compiles ZeroDimHandler.cpp.

Continuing with fixes. Need to figure out how setData works. It has to be able
to handle block assignments.

============================================================================
9 Oct 2010

Slowly continuing with compilation. Checkin 2207.

Redid interface for setData, now it is setDataBlock and the constraints are
set much more tightly.

Issue: What to do in setDataBlock when copying into block that is decomposed
among multiple nodes? See OneDimHandler::copyToNewDim for this situation.
	- Should each individual part of the OneDimHandler be replicated 
		newDimSize times? Actually at present this does NOT happen. 
	- Should pass in an entire data block, or only the sparsely filled
		data block on the current node? If the former, how?

Usual scenario should be that we only copyToNewDim from a global.


Some test functions:
make test object global, add dimensions till a 4-dimensional thing.
make test object local, add dimensions till a 4-dimensional thing.
make test object global, add dimensions, and last dimension is decomposed.
globalize model
unglobalize model
make 100 cells from a global prototype
Make a planar (2-D) array of cells
move some objects between nodes (part of load balance)

============================================================================
10 Oct 2010
Compilation now past the DataHandler classes, and running into the expected
issues in the rest of the code. Checkin 2208.

Now moving onto data1 as used in Eref and thence in many other places.
It is to "Returns data entry of parent object of field array"
Perhaps if I want the parent entry I should request the parent explicitly?
But it is true it is rapidly obtained from the Ids.
It is used in UpFunc.h, in cases where the parent object has to be called
to execute a function in the child. The child is usually an array entry.
Examples are syn input to an IntFire.

Key issue is this: when we have an array of fields, then the data entries
for the fields are distinct from the parental data entries, and the fields
are within a C++ array on the parental data entries.

Array of glom
	-> Array of mit cells
		-> Array of compts
			-> array of channels (but these are different elements)
				-> Array of syn inputs on receptors

Elements associate messages and Finfos with data.
	Except sometimes Elements are used to hold extended fields
	and Elements sometimes hold other fields with subparts.
	Here they still associate things with data...
Finfo handles function types for Elements using OpFuncs.
Messages associate functions with data.
DataHandlers associate Elements with data.
Dinfo handles data types for DataHandlers.
Elements are looked up by Ids
Data within an Element is looked up by DataIds.
	This isn't fully right. The whole object is looked up by the DataId,
	except that sometimes we also look up Fields with the DataId,
	when it is a FieldElement.
OpFuncs provide a common interface for arbitrary Object functions.

============================================================================
11 Oct 2010
FieldDataHandlers vs regular DataHandlers
When do we need the field data, and when the parent object data?
	- For the purposes of OpFuncs we always need the parent object rather
		than the field object, as the functions are based off the
		parent. Could get rid of UpFuncs if this were resolved.
============================================================================
12 Oct 2010.
Grind through all the files to figure out where the Eref::data() refers to the
parent and where it refers to the field itself.

Finally it compiles through the entire basecode directory. Checkin 2209.

Finally also we have a clear direction for the FieldDataHandler interface.
Here we need to ensure that it has a similar iterator behaviour to the 
regular DataHandlers. In other words, if we go from begin to end we should
see all the field entries on all the Data that happens to be on current node.
Would also like a way to jump partway into the data sequence, say the first
field entry on Object[2][3]. This comes up in AssignVecMsg.cpp.

Another thing came up: DataHandler::addOneEntry. Same as push_back. This
is only used in one place, on the MsgManager.cpp. There are issues
with doing this on non-globals, since the node allocation could get
scrambled from the default. The MsgManager is a global, so this concern
doesn't apply, but I shouldn't introduce a one-use feature.

Got into the gory details of how we manage Elements corresponding to Msgs.
Somewhat messy, let's see if there is a way to clean up. The fundamental 
problem is that there are many Msg subclasses and each has its own fields,
so one grand array of the whole lot will not work. The current approach has
independent Manager for each subclass with an array of MsgIds, and then the
Msg.h base class manages another static vector to help the msgs look up their
index on their respective Managers.
============================================================================
13 Oct 2010.

Bodged this to work by using resize and setDataBlock. This is horribly 
inefficient, involves a reallocation and copy of the entire set of existing
messages for the addition of each one. Will need to revisit.

Got it to compile through the msg directory. Now stuck with Shell.

============================================================================
14 Oct 2010
Compiled through all files, then finds lots of undefined virtual functions
in the various DataHandler subclasses. Checkin 2211.
============================================================================
15 Oct 2010

- Check that all the Handler::Handler( Handler*) constructors alloc data
- Replace assimilateData with setDataBlock

Compilation coming along, working on eliminating 
'assimilateData' and merging it into setDataBlock

setDataBlock( const char* data, unsigned int begin, unsigned int end,
	unsigned int dimNum, unsigned int dimIndex

Use cases:
To set a single value in a 0-dim dataset: 
	setDataBlock( data, 0, 1, 0, 0)

To set a single value at index 'i' in a 1-dim dataset: 
	setDataBlock( data, i, i+1, 0, 0) or
	setDataBlock( data, 0, 1, 0, i)

To set a single value at index [i][j] in a 2-dim dataset:
	setDataBlock( data, j, j+1, 1, i) or
	setDataBlock( data, 0, 1, 0, i * numDim0 + j )

I don't like this.
Here is the logic. Suppose we have a DataHandler data[x][y][z]
where sizes are 4, 5, 6.
The z dimension varies the fastest.
A zero dim slice would be [x1][y1][z1]: in other words, it fully specifies the
	entry.
A one dim slice would be [x1][y1]: It specifies x and y fully and z is the
	slice dimension. This would have size 6.
A 2-dim slice would be [x1]: It specifies a plane in y and z located at x1. 
	This would have size 5*6 = 30.
A 3-dim slice is silly, but it would be defined by a zero size slice vector.
	This has size 4*5*6 = 120.

Now within these slices, the setDataBlock function specifies a range in
	the slice, which is assumed linearized.

To set a single value in a 0-dim dataset: 
	setDataBlock( data, 0, 1, {} ) where {} is an empty slice vector.

To set a single value at z1 in a 1-dim dataset: 
	setDataBlock( data, 0, 1, {z1} ) or
	setDataBlock( data, z1, z1+1, {} )

To set a single value at y1, z1 in a 2-dim dataset: 
	setDataBlock( data, 0, 1, {y1,z1} ) or
	setDataBlock( data, z1, z1+1, {y1} ) or
	setDataBlock( data, y1*nz+z1, y1*nz+z1+1, {} )

To set a single value at x1, y1, z1 in a 3-dim dataset: 
	setDataBlock( data, 0, 1, {x1,y1,z1} )

and so on. The preferred form is the first one in each case.

To set a vector of values from z1 to z2 in a 1-dim dataset:
	setDataBlock( data, z1, z2, {} )
	setDataBlock( data, 0, z2 - z1, { z1 } ) We disallow this because it
		makes the slice lookup ambiguous.

To set a vector of values from z1 to z2 on y1 in a 2-dim dataset:
	setDataBlock( data, z1, z2, {y1} )

To set a vector of values from z1 to z2 on x1, y1 in a 3-dim dataset:
	setDataBlock( data, z1, z2, {x1, y1} )

To set a 2-d array of values from y1,z1 to y2, z2 on a 2-dim dataset:
	Cannot do.

To set a 2-d slice of values from y1,0 to y2,nz on a 2-dim dataset:
	setDataBlock( data, y1*nz, (y2+1)*nz, {} )

To set a 2-d slice of values from y1,0 to y2,nz on a 3-dim dataset at x1:
	setDataBlock( data, y1*nz, (y2+1)*nz, {x1} )
This does not really work so cleanly. 

To set the complete 3-D block on a 3-d dataset:
	setDataBlock( data, 0, nx*ny*nz, {} )

Do these forms now permit arbitrary node decompositions to be merged into
a single vector, e.g., for globalization? Not really. We'll need the global
DataHandler to ask its localDataHandler counterpart to figure out how to put
values into the correct places on a transfer vector, which is then copied
in its entirety onto the GlobalDataHandler. Which brings us back to the
assignData function.

getSlice( numDimsInSlice, sliceStartDim, vector< unsigned int > slice
	unsigned int sliceStartDim = slice.size();
	unsigned int numDimsInSlice = dims_.size() - slice.size();

This should give a slice of data including the lower (rightmost) indices.

Trying it out in AnyDimGlobalHandler.cpp

Decided against a recursive tree of DataHandlers for successive
dimensions, at least for now.
============================================================================
16 Oct 2010
Implementing simple version of setDataBlock :
setDataBlock( const char* data, unsigned int numData,
	const vector< unsigned int >& startIndex )
setDataBlock( const char* data, unsigned int numData,
	unsigned int startIndex )
which just starts the data assignment at the specified startIndex.
Lots of files changed, so I'm checking it in even though it doesn't
yet compile. Checkin 2216.

Compiled but does not link, the usual vtable issues for DataHandlers.
Checkin 2217.

Sorted the vtable issue as hinted on the discussion lists. Made a 
DataHandler.cpp to define the only non-pure-virtual functions. Now compiles.
Checkin 2218.

Finally starting to go through unit tests and clean up the DataHandler code
along the way. Checkin 2219.

============================================================================
17 Oct 2010.
Doing unit tests. Run into issue with size and indexing in FieldDataHandler.
	Consider synapses being set up. The addition of a synapse may cause
resizing of the dimensions of the FieldDataHandler, unless we hard-code in
some limits here. This affects the way we index and identify entries.

Approach 1: Don't index stuff this way. Don't ask for common size for
field entry arrays. 
	This gives the possibility of autonomous, ragged vectors for fields.
	This gives up single-integer indexing.
	This implies that all field arrays are on a single node, as it is
	otherwise very hard to keep track.  This makes coding sense anyway, 
		as fields are meant to be coded serially as parts of an
		individual object.
Approach 2: Put in callbacks for every function that resizes a field array,
	to update the current field size. 
	Note that this gets out of hand very quickly in a multinode array.

Chose option 1, which is the older version anyway. This means that the
setDataBlock index should be changed to a DataId. Done. Compiled, still 
crashes.  Checkin 2221.

in AssignVecMsg.cpp:exec around line 50: Issue is how to map
entries in PrepackedBuffer pb to DataHandler entries, especially
FieldDataHandlers. Fixed.

Many bugs squashed, but still stuck on a test for setting a
vector of synaptic fields, in testAsync.cpp:566. Incremental checkin 2222.

Further bug squashing in FieldDataHandler iterators, now it clears the
test for assigning Synaptic delay. More bugs loom. Checkin 2223.


============================================================================
18 Oct 2010
New bug turns out to be the confusion between parent data and field data
in UpFunc. 
Current approach: UpFunc uses special access functions to get at the parentData
Option: use a special DataId::fieldId to indicate that the regular
	OpFunc should return ordinary data. Otherwise OpFunc looks up field
	data. 
Decision: Too messy. Stay with current system. This means filling out
	parentData() function in all DataHandlers.

Replaced parentData() with a simpler and more general option, to return
	parentDataHandler(). The default virtual function for this returns
	self, and for the FieldDataHandler returns the parent.
	Checkin 2224.

Added DataHandler::setFieldArraySize and getFieldArraySize,
	again as functions useful for FieldDataHandlers.
	Checkin 2226.

Fixes to OneDimHandler::nodeBalance. Now clears testScheduling().
	Checkin 2228.

Now it is stuck in the actual values from the IntFire network. Seems to
be calling process and sending spikes and handling their arrival OK.

Tracked down error in unit test for IntFire network, to bad assignment of
weights and delays.  I had not used the
correct function to find how many synapses needed weights/delays set.
Now clears this unit test, more to go. 
	Checkin 2229.

============================================================================
19 Oct 2010
Fixed a problem with copies. It was always copying Elements to the next
higher dimension, even for a single copy. Still need to test the next higher
dimension copies, but I have reason to believe they will fail.
Checkin 2230

This actually clears the regression tests too. But I need to fix the regression
tests so that they find the appropriate subdir for the data files
automatically.

Now on to some additional unit tests, such as the higher-dim copy.

Implemented unit tests for many functions, including copy and higher-dim copy,
for DataHandlers. Checkin 2231.

More unit tests, now for AnyDimGlobalHandlers. A bit stuck on the
semantics of the copyExpand function. Which dimension does it expand? Or should
I skip it altogether?

============================================================================
20 Oct 2010
Always expand dimension 0. This is what the 1-dim handler does, and we'll
continue it.

So if we have a 2-D array[3][4]

01	02	03	04
05	06	07	08
09	10	11	12

and we want to expand it to [3][7], we would get:

01	02	03	04	01	02	03
05	06	07	08	05	06	07
09	10	11	12	09	10	11

and so on.

The problem here is that the internal ordering of the data is as listed in
the numbers. I can't just copy a block over.

Finally sorted this out, and did some more fixes to 
AnyDimGlobalHandler::copyToNewDim. Now it clears the unit tests and
regression tests. Checkin as 2232.

Next step is to do a valgrind.

============================================================================
21 Oct 2010.
Fun with valgrind. Most of it is not freeing up test copies of DataHandlers.
Fixed these, one baffling one remains where it is not clear where the
call originates.

Put in a useful check for current directory to tell the system not to do
regression tests if it isn't in the right place. Then used this with
valgrind to see if the puzzling leak is in regression tests. It isn't.

Dropping individual unit tests:
testAsync, testMsg, testBuiltins, testShell, testKinetics, testKsolve,
in fact all of nonMpi tests are clear.
Looks like it is in the mpiTests. Either in testMpiMsg and testMpiShell.
Not in testMpiMsg. Confirmed testMpiShell. Now subdividing that.
Not in first half. Nor First 8.
It is in the testCopyMsgOps function.
Doesn't have to do with running the copy. Doesn't have to do with
scheduling the copy. Making the copy is the problem.
Tracked it down to the OneDimHandler calling allocation constructor of its 
parent class, so that both were allocating data. Fixed. Cleaning up.
Now valgrind is happy with all the unit tests. Checkin 2233.

============================================================================
22 Oct 2010
How does MOOSE decide which Qblock to put stuff in?
	Checks if the message dest is a global
	Checks if the Msg is a setMsg
	Checks if the Msg is between shells
	Msg::isMsgHere: returns true if msg accepts input from specified DataId
These are all known when the Msg is created. So we don't need a separate
assignQblock function

How does MOOSE put stuff in Qs?
Qinfo::addToQ
Qinfo::addSpecificTargetToQ
Qinfo::assembleOntoQ

Which Queues are there?
- One outQ for each thread.
- One localQ for msging within current node.
- One inQ per group.

Currently there is a lot of copying back and forth.
We should have a double-buffering system where the in and outQs swap 
between cycles.

Also the Msgs already have enough info to put data in the correct queue.

Aim: Stuff goes into queue. Then queues swap. Q becomes readonly and stuff
is read from it.

Issues: Data may need to go to multiple Qs. Then we should have a globalQ.

============================================================================
23 Oct 2010
Added in default checking of # cores and # nodes.
Now unit tests fail as my laptop has 2 cores. Checkin 2237.

Minor cleanup in argument handling. Checkin 2238.

Tracked down error when I run with 2 cores. The sparseMatrix does ugly stuff
for load balancing in sparseMatrixBalance. This has to be eliminated. The
memory layout should be independent of # of threads. Any clever stuff for
subsets of the data should be done in other ways.
Here we want any given targetThread to access a subset of colIndex:
targetThread = ( colIndex[j] * numThreads) / ncols;
In other words, we ignore colIndex entries that do not fit this criterion:
colIndex[j] == ( targetThread * ncols ) ) / numThreads.
All the terms can be precomputed so it is a simple comparison.

targetThread	ncols	numThreads	colIndex
0		27	4		0
1		27	4		6
2		27	4		13
3		27	4		20

So we will need an upper and lower bound for each thread to decide if to use
the colIndex. Or just use the comparison above.

Used the upper/lower bound as they set a nice range within the for loop.
A few other threading dependencies of messages surfaced, dealt with.
I've now fixed up the threading dependency in SingleMsg and SparseMsg.
Diagonal and OneToAll and OneToOne all need this fix and matching tests.
Checkin 2239.
Set up threading balancing in OneToAll msgs. Seems to work. The problem
with the existing test is that even if multiple assignments are done on
different threads, they just overwrite each other, so there is no record
of redundant value assignments.
Set it up for OneToOne and Diagnonal Msgs too. I think all the Msgs have been
covered. Checkin 2241.

I already have a nice test for messaging, with the one limitation that it does
not catch cases where the same value is set more than once. With this fixed
I should look at cleaning up the queueing for multiple threads.
Also should decide about thread-groups and node-groups concepts: is it
simply too early to worry about them?
============================================================================
24 Oct 2010

Need to smoothen out Shell::start, stop, continue, reinit and reset.

First: Implemented a couple of more complete tests for oneToAll and sparse
msgs. These stress out the threading. Current implementation clears it.
Changes made in Arith.cpp and testShell.cpp.
Checkin 2242.

Setting up benchmarks in regression test directory so that we can
load in and run real models.
Fixes to Ksolver following assorted changes that were not tracked in
the earlier unit tests. 
Checkin 2243.

The benchmark throws up more undiagnosed errors. Turns out the GslIntegrator
isn't working. Also turns out that the apparently OK regression tests on 
readKkit were not catching the fact that the simulations had not run at all.

============================================================================
25 Oct 2010
Tracked down problem: The wildcard system fails with a path of the form
"./##". I think it will handle just "##". Need to fix and put in unit tests.

Put that in, also had to sort out scheduling. Now it computes, but the
values differ from the reference file. After checking with the old MOOSE,
it seems like there is still some difference, but it is small.
Checkin 2244.
- Still croaks for 2 and 3 threads, in interesting and diverse ways.

Put in a couple of assertions in Qinfo::readBuf. Size fails.

============================================================================
26 Oct 2010
After a few tests, it seems that there are various modes of failure of the
multithread calculations. One is that the simulation doesn't do enough
steps. Other is that the queue points to a null message. 
Seems to work OK on single-thread calculations.
Oddly, all this is with the solver in place so the amount of messaging is 
quite small. Should do another test with much heavier messaging to stress
the queueing.
Checkin 2245.

I need to decide if I should try to fix current code or just do the planned
queue reimplementation.

Checked all the messages, they do have the thread-specificity in exec().

A closer look at the threaded messaging. This is sketched out on pg 59 
of the paper lab notebook.

Outcomes:
1. There has to be a defined mapping of each object onto a given thread
for the Msg::exec. This has to work across messages: If there are 10 messages
coming into an object, all of them must be on the same thread or there will
be errors. Message threading has to be reworked to do this.
	- The obvious way to do this is to hash the Id and the DataId::data()
	indices together (addition would do). Use something like
	thread = (id+dataid) % numThreadsInGroup. This is simple but gives
	a messy and inefficient way to go through non-contiguous sequences 
	such as SparseMsg. We would have to check every entry. 
	It would be faster to have bounds. But probably a small cost.
		- Alternately, we could subclass off the SparseMsg to pull out
		thread-specific subsets. This is fast regardless of the 
		iteration scheme. It is similar to what I did earlier and
		shares its problems. Could be done much more cleanly.
	- The other way is to use just the dataId, assuming we know max dataId:
	start = ( thread * maxDataId ) / numThreadsInGroup
	end = ( (thread + 1 ) * maxDataId ) / numThreadsInGroup
	This is likely to overload stuff onto thread 0 because many 
	Elements are singletons.
	- For complex objects like solvers, the solver itself may want to
	do the thread partitioning. One way around it is to make a new kind of
	ZeroDimHandler that knows that 'process' should be called on
	all threads.

2. There are 2 phases in each tick, each separated by barriers:
	- Phase 1: Tick::advance->Msg::process->Element::process->
		DataHandler::process. The DataHandler does the thread splitting.
		Each message output goes as follows:
		SrcFinfo->Element::asend->Qinfo::addToQ. 
		The message knows which Queue to put stuff into (see below)
		Each Queue set has individual queues for each thread.
	- Barrier 1: This terminates the 'process' call. It also, atomically,
		swaps outQs with inQs; outG with inG. The handling of
		outMPI and inMPI is more complex, discussed below.
	- Phase 2: Qinfo::readQ->Msg::exec calls. Here the Msgs scan through
		their targets, often many of them. The Msg::exec is called
		on all Msgs from all threads, the Msgs have to partition
		which subset of target objects they will handle on each thread.
		Note that the newly swapped outQ may be used already.
	- Barrier 2: This terminates the Qinfo::readQ->Msg::exec calls.
3. We need 3 sets of Qs, each with one queue per thread. These are:
	- within-group queues: outQ and inQ
	- Between-group queues: outG and inG
		This isn't right. We need an outQ and inQ equivalent for 
		every group if it is to be a computational group. If on the 
		other hand we have a single computational group, then this
		could work. If the definition of a group is a set of highly
		interconnected objects for which we should do all-to-all
		communication, then we will need multiple computational groups
		for large models. We do want to have independent buffers
		for the all-to-all MPI transfers, but these will be merged
		with respect to threads.
	- Off-node queues with possibly slow tick rates: outMPI and inMPI
		- The outMPI accumulates data till this node comes to its turn
		to send data. Then it does an MPI_Bcast.
			- There is a tradeoff here as it will accumulate
			_everything_ in outQ and outG. Memory vs. frequency
			of clearing.
			- We can do the MPI_Bcast for an entire cycle, till
			the next barrier1, since the outQ and outG will 
			handle data accumulation. But then it must sync.
			Most likely it will do this between barrier 1 and 2,
			since this gives the inMPI time to collect info
			between barrier 2 and 1.
		- The inMPI will typically get input from other nodes every
		cycle. It can receive data between Barrier 2 and Barrier 1
	- If we have to have tighter sync between nodes, we'll have to
		replace accumulation and MPI_Bcast with MPI_Allgather. In
		the original code I also have an MPI_Barrier. 
		- Here the 'sends' will still have to go between barriers 1
		and 2, and we'll have to insert a third Barrier to deal with
		the received ext data. 
		- Much more memory efficient, as we can use inQ and inG
		directly for the 'sends', and only have to maintain suitable
		inMPI queues for the incoming data from other nodes.
	- How do we assign groups? If Elements have group info then we could
		do this. But we haven't formalized groups to this level yet.
		Clearly the whole Element would be in a group.
		Perhaps there could be a grouping Element and all children of it
		are in its group.


4. Still need to sort out comms between other threads such as the Shell
	and the GUIs. Probably just go on the outG/inG queues.

============================================================================
28 Oct 2010
Implemented mapping of all targets of Msg::exec onto specific objects
defined by threadNumber. Encapsulated this function into ProcInfo::execThread
since ProcInfo knows about threads and it is always present when doing exec.
Fixed up all Msgs to use the ProcInfo::execThread to decide which targets to
call.

When all compiled, it still clears stuff with 1 thread, but starts falling
apart in the regression tests with more than 1 thread.

============================================================================
29 Oct 2010
Checkin 2253.

While the current decomposition seems clean, it is also very inefficient 
especially when the # of threads scales up. At present the approach is to
test every putative target object to see if it is in line for execution
on the current thread. Note also that the thread assignment of an object
during message Q clearing is completely independent of the thread assignment
during 'process'.

Which is the earliest we can partition between threads?
	- The obvious place is at the Msg level, since the Msgs know which
	are the target objects. 
		- Problem is that a whole lot of queue grinding and Msg
		checking will have to be done for every msg to find which
		ones are on the current thread.
		- One level of efficency is to preselect the groups of objects 
		to be executed on each thread, within each message. This will 
		be relevant for OneToAll and Sparse Msgs. For others I don't
		see a way around the scan-and-check approach.
	- A much nicer place would be at the queue level, so we avoid 
	scanning at all if it isn't going to be executed on the current thread.
	Possibly at the time we do the Element::asend, we could check with the
	Msg to decide where to assign the queue entry. But now we have a 
	proliferation of queues.
	- An intermediate approach is to put some thread info in the Queue
	data struct (header) for the msg, so we can skip some msgs before
	going to Msg::exec and all the associated processing. Question is,
	does this become too much of a corner case? It would apply when the
	dest is local, and when it is a single Msg.
Summary and conclusion:
	The idea of messages is to postpone the divergence of data flow to 
	the Msg::exec stage. So queue grinding should not be the rate limiting
	step, even at high thread counts. For now stick with the current 
	approach of scanning into all messages.

See above. There are also concerns about the grouping design, which would
result in proliferation of outQs/inQs. 

Option 1:
	Single block of outQs (one outQ per thread) and a threaded memcpy
	phase when each msg is sorted into respective inQs, one inQ per _group_.
	This requires that we put some group/Q identification stuff into the 
	Qinfo header so we can very quickly read through and assign.
	The threaded memcpy phase will need care to avoid stepping
	on parts of memory used by other threads.

Option 2:
	Emphasize threads over groups. Sort the inQs by thread, as figured
	from target object set. This requires a bit more info about the
	target object and Msg, but again this is feasible at the stage when 
	the outQs are being filled.
	Problem here is that many Msgs, esp. OneToAll and Sparse, will have 
	multiple target objects sitting on different threads.
	Option is to subdivide these, but this could well happen within the
	Msg itself, where it would be cleaner.

Option 3:
	Have outQs subdivided both by thread and by group. There will be lots,
	but this eliminates subsequent sorting by group, and we can reuse
	the outQs as InQs without memcpy.

In all these cases, especially case 3, it would be nice to be able to put all 
thread-sections of the queue into a single contiguous memory block for MPI
transfers. To do this we want:
- A single big block of memory
- Separate pointers into this block, managed by each queue.
- A good estimate of size of queue so we avoid
	- reallocation
	- blank spaces in final data transmission.
- a dumy queue entry to fill in the blanks, to indicate how much space to 
	skip before next queue entry.

I think Option 3 is the cleanest and most susceptible to streamlining.
Here we want separate blocks of data for each group.

Steps:
	1. Implement Qvec to provide the contiguous data block.
	2. Associate groups with Elements
	3. Fix up message sending and receiving to use Qvec
	4. See if it works.
	5. Recode thread handling
	6. See if it works
	7. Clean up SendTo

Done 2. Compiles, clears unit tests. Not hard.
Implementing changes in Qinfo. Implemented Qvec.h, need to do Qvec.cpp.
Need to scan through Qinfo.cpp, replacing old code with the 
updated code using Qvecs.

============================================================================
30 Oct 2010
Checkin 2256.
Trying to compile with Qvecs.
Wrote out another scheduling/queueing/Messaging/MPI block diagram,
pg 61.
Need to work how scheduling (different ticks, different dts) will mesh with
the queues and specially if we do all queues in each tick, or not. The
key issue is that there is no point at which both the inQ and outQ are empty. 
So there will always be leftover operations from the previous Process cycle.
If we change ticks at barrier 3 (before phase 1) then the leftover ops will
execute somewhere in the subsequent phase 2.
If we want to end a simulation cleanly, we need to carry on one more cycle
but without doing the exec in phase1. Even with this it is conceivable that
we could have a Msg destination calling Send, which in turn calls another
Send and further that this could go on indefinitely in a loop. A more
concrete case: Suppose we have a plot request on tick 3, and this is the
last tick called. Then we have:
(Previous phase: Plot::send)
Phase 1, last cycle of simtime:		Phase 2, last cycle of simtime
Plot::process ->SrcFinfo::Send->	Tick::advance->ReadQ->Msg::exec
Element::asend->Qinfo::addToQ->outQ	Send request arrives at target.
destination is target.			Target puts data in outQ

Phase 1, first cycle after simtime	Phase 2, first cycle after simtime
	Process not executed.		Either Tick::advance->ReadQ->Msg::exec
					or a special wrap-up call to Phase 2
					ReadQ->Msg::exec-> puts data into plot

============================================================================
31 Oct 2010

This is one simple case. One could imagine further levels of regression, such
as a further pushing of the data from the Table into graphics.

One way out of this is to have the Phase 2 (and possibly Phase 3 ) continue 
indefinitely as part of the regular event loop. In this scenario the system
would continue throughout with all 3 phases, and trigger calls to the Clock
to initiate the TickPtr::advance operation during phase 1 only when the
simulation was happening. 
The drawback of this is that it will keep crunching heavily even when sitting
around waiting for the user to wake up. Perhaps put in
a small 'wait' during Phase1 when we are not doing Process. Anyway, like MPI,
the goal isn't to be efficient during idle, but during processing.

Note that if I do this I have to change how the Shell parser functions deal
with the event loop. Either use a a conditional wait in each such function,
or keep starting and restarting the event loop.

The other aspect of this analysis was to see how the system would cope with
different clock ticks. Seems that the key part is that the Ticks have to
insert the appropriate Phase1 into an ongoing event loop with Phase 2 and 3.
I don't think the dangling operations are critical, but we can insert a
requirement that Process-triggered operations should be one-hop if they are
to be delivered in the next timestep.

Deep into compilation, specifically working through Qinfo.
Checkin 2263.

Done with Qinfo.cpp, but we need clearQ for SetGet. Need to work through
this one.

Pending:
SetGet::completeSet() ::clearQ. What to do about this?

I need to implement the scheduling framework somewhere.
- Clock: Good place to do scheduling, but odd place for a generic func.
- Shell: Good place for interactions between parser thread and sim threads.
	Adequate place for system-wide scheduling ops. 
- Qinfo: All the queues are here, but it is an odd place to do scheduling.

Let's put it in Shell.
Going through the process. If I use clock::advance then it will be
quite inefficient to cycle up to the required tick every dt. Question is,
will this be a significant slowdown or is the flexibility worth it?
Considering the huge amount of stuff being done every dt, I don't think this
cost should be a consideration. Will benchmark and see later.

For all the clearQ legacy calls, what we really need is a call that posts
a conditional wait till the main loop clears one cycle. Or two, if we're
concerned about returns from off-node queries.

How to do this? This is related to the original problem of how to deal with
blocking on simulations and appropriate waits for completion in shell commands.
Have our inner loop check the 

We cannot have SetGet operations take place off-cycle, as they insert 
messages into the queue. They should occur safely in phase 2 but not
elsewhere.

============================================================================
1 Nov 2010
Three cases of interaction with inner loop:
- Set/Get operations.
- Shell calls that will need transmission to other nodes and possibly 
	lots of work on remote nodes. Design avoids remote nodes talking to
	each other for most setup ops.
- Processing.

Q: Does the Shell/parser thread also run through the main event loop?
- Also runs on main event loop: will limit processing and will lock 
	shell updates to sim updates. So discard this option.
- No, runs separately. Here we provide a few key functions to achieve 
	clean meshing: 
	- Set/Get: they forward stuff into Shell::dispatchSet/Get. These
		issue 'send' commands, see below.
	- Shell operations: do<Function>. These issue send commands. The 'send'
	commands therefore need to be protected such that they are
	issued during phase 1. Then we need to have a conditional wait till
	some flag of completion is returned, to ensure strict serial behaviour
	of a sequence of setup calls.
	- Process: Here the 'send' command just tells the clock that it is
	to begin advancing. Again, a flag for completion is needed.
	Current flag is from the 'isAckPending' system, which wraps a loop
	for testing responses from all nodes.

The test for completion flag works by sending in a ack message back to the
Shell. These acks are counted till all nodes have returned.
The way to change this would be to have a condition_wait on the isAckPending
flag.

Discussed with Subha. He feels that PyQt can run on a separate thread from
the scripting (blocking) thread. So I'll go ahead with the design.

Maybe set up prototype as a standalone pgm.
This is in testProcLoop. Checkin 2273.

Added the thread and node initialization stuff. 
Compiles, runs with debug messages. Checkin 2274.

Added skeleton for barriers. Checkin 2275.
============================================================================
2 Nov 2010
Design now for the data transfer on the queues.
- Needs to track where it has been
	- A listing of last few nodes and threads
	- A count of total touchdowns on any given node/thread
- Needs to track originator
- Needs a (fixed) trajectory rule.

Implemented Tracker class to do this. Compiles, not yet tested.
Checkin 2279.

Should I make multiple event looops with different sets of ops in them
or should I just have the proc and exec ops and do the rest through
checking the threadId?

While mulling on that I've gone one step further to have the
system set up to send info around using queues. Separated out the
EventLoop stuff into a separate .cpp file. 

Set up to insert entries into the queue. This works now for one node,
one thread, but doesn't do anything for more. Checkin 2302.

Now it inserts entries into queue and prints out where they are
at each tick. So far MPI messaging not deployed. Checkin 2303.

============================================================================
3 Nov 2010
Implementing a barrier that has a function to execute while the threads
are waiting, when the last thread is coming through. Checkin 2307.

Using FuncBarrier, after a bit of debugging seems OK. Checkin 2308.

Now into the MPI stuff. Looks like the MPI_Allgather will be
awkward as it requires us to have ready a big target buffer. 
Try MPI_Bcast.

Phase 2: exec inQ on all nodes.
	Send out MPI_Bcast with inQ[0] contents on all nodes
	Recv it on inMpiQ1 (mpiRecvQ swaps with mpiInQ )
	Barrier

Phase 3: exec inMPIQ1 on all nodes except node 0.
	Send out MPI_Bcast with inQ[1] contents to all nodes
	Recv it on inMPIQ2
	Barrier

Phase 3: exec inMPIQ2 on all nodes except node 1.
	Send out MPI_Bcast with inQ[2] contents to all nodes
	Recv it on inMPIQ1
	Barrier
	.
	.
	.

Phase 3: exec inMPIQ2 on all nodes except node N-2.
	Send out MPI_Bcast with inQ[N-1] contents to all nodes
	Recv it on inMPIQ1
	Barrier

	exec inMPIQ1 on all nodes except node N-1, which is last node.

	Barrier before we go around again to phase 1 (process).

============================================================================
4 Nov 2010
Begun implementation and testing of MPI event loop. This compiles and runs
without crashes, but the tracker isn't going where it should. Checkin 2309.

Fix to routine that advances tracker position. Now it seems OK for 
multiple threads, multiple nodes. Checkin 2310.

Some more cleanups. Checkin 2313
At this point it runs nicely on the cluster, with up to 100 'nodes'. 
But it is excruciatingly slow. Clearly I will have to put in a variety of
message passing formats, such as the memory intensive allGather.

Next step: put in a thread for the Shell to talk to. In the testProcLoop
this will be a function which lets the main system put in a new Tracker,
and to query the system for tracker positions.

============================================================================
5 Nov 2010

Current system:
	- Shell::doOp
		initAck
		op.send
		while isAckPending {
			clearQ
		}
	- set
		SetGet< type >
		checkSet
		type conversions into a temp buffer
		Shell::dispatchSet
		Shell::innerDispatchSet
		requestSet.send() to the target shell.
			Shell::handleSet on target shell
			Shell::innerSet
				create AssignmentMsg and tie to target
				shelle_->asend() data.

	- get
		Field< type > (derived from SetGet1< type > )
		Shell::dispatchGet
		Find field, type checks.
		innerDispatchGet
		initAck
		requestGet.send
		while isAckPending
			clearQ
		..........................................................
			Shell::handleGet
			make new AssignmentMsg and tie to target
			lowLevelGet.send
				Goes to target object, asks for data return
				GetOpFunc::op
					Gets data from object
					Converts to buffer
					fieldOp on buffer
						Puts data into prepacked buffer
						Finds incoming Msg
						Synthesizes return Qinfo
						Adds return to Q going to shell
				
			(now waiting for the return value )
			Shell::recvGet
				sets getBuf size for return value
				copy return value from PrepackedBuffer into 
				getBuf on Shell.
				handleAck: Adds another to the returned acks
		..........................................................
		Eventually all the acks are back and isAckPending clears
		innerDispatchGet returns the getBuf
		in Field< type >::get
			take ptr to returned getBuf
			Convert it 
			return converted value.

1. Inserting a request without a wait, such as a 'set' call.
	- In Shell/parser loop:
		Do all the stuff up to the point where it needs to 'send' data
		pthread_mutex_lock( &shell_set_mutex );
			Shell::op.send();
		pthread_mutex_unlock( &shell_set_mutex );
	- In shellEventLoop:
		Simply protect the barrier with a mutex. This ensures that
		the swap operation does not happen when the 'send' is done.
		pthread_mutex_lock( &shell_set_mutex );
			p->barrier1()
		pthread_mutex_unlock( &shell_set_mutex );

2. Inserting a blocking request, such as a call to create an object.
	- In Shell/parser loop:
		Do all the stuff up to the point where it needs to 'send' data
		pthread_mutex_lock( &shell_set_mutex );
			parser_block = 1;
			Shell::initAck()
			Shell::op.send();
			while ( isAckPending() )
				pthread_cond_wait( &cond_variable, &shell_op_mutex );
			parser_block = 0;
		pthread_mutex_unlock( &shell_set_mutex );

	- In shellEventLoop:
		Protect the barrier with a mutex. This ensures that
		the swap operation does not happen when the 'send' is done.
		pthread_mutex_lock( &shell_set_mutex );
			p->barrier1()
			if ( parser_block && !isAckPending() )
				pthread_cond_signal( &cond_variable );
		pthread_mutex_unlock( &shell_set_mutex );

3. Handling a 'get' call, which is a blocking request followed by a 
	transfer of data to the caller.
	Same as above only the Shell/parser loop collects the data from
	the Shell buffer when it carries on with the Field::get call.
	Note that this assumes that there will be no other data tranfers
	using the Shell buffer. This is safe provided we never use the
	'get' call within the code. 


Implemented this, but not yet the actual Shell input to its eventLoop.
Compiled and runs. Checkin 2320.

Make an implementation for the Shell input. Compiles but hangs.

============================================================================
6 Nov 2010
Checkin 2321

Some progress on Shell input. Now it handles message insertion and termination 
correctly for multiple threads, but not multiple nodes. Checkin 2324.

Turns out that the condition for quitting was quite tricky. I need to get
the info out to all nodes and be sure it is acted upon, within 1 cycle.
Sort of hacked it in here. Works for various combinations of nodes and
threads. Checkin 2325.

Added adjustable time for system to wait when testing master thread control
calls, equivalent to parser calls. Need a longish time as MPI can be slow.
Checkin 2326.
This clears tests on up to 60 nodes and assorted threads on the cluster.

So, with this I'm ready to go back and start fixing up scheduling on the
Msg branch of MOOSE.

1* Fix up Qvec, make some solid unit tests for it.
2. Change Clock scheduling so it can be activated as a single operation 
	within the thread-handling loop. The current arrangement has the
	thread-handling looop deeply nested in the clock scheduling.
	Make some new unit test for the revised scheduling.
3. Fix up main() so it calls the new threading stuff
4. Implement the new threading/MPI stuff.
5. Figure out how to retrofit the old unit tests that rely on Set/Get and
	other queue-dependent operations.


Starting on 1. Qvec has been fleshed out. Compiles. Gets up to the Qvec
test, which is still empty. Checkin 2327.

Added many tests for Qvec, many bugs squashed along the way. Next to add
test for the 'Qvec::stitch' function. Checkin 2328.

============================================================================
7 Nov 2010
Added tests for Qvec::stitch. Seems OK. Checkin 2329.

I really would like to be able to clear the old unit tests, even though they
have this nasty dependency on all the queue stuff. Let's set up a clearQ
that handles things simply but locally.

Did that with a simple variant of Qinfo::clearQ that just does the swapQ
and then readBuf. Now it clears a few more unit tests, till it
gets to SetGet. Then it hangs waiting for isAckPending, because
the mpiClearQ is currently an empty function. Checkin 2330.

Need a bit of redesign here. We have two levels of Set/Get:
	- The Shell operations which need to be able to access any field on any
		node. This needs the entire messaging loop stuff to be
		operational.
	- A local version which operates only on local node and handles its
		own updates of the queues. Used mostly for testing.
		If this is strictly local node then we don't even need the
		queues. We can go straight to the Element pointer.
		The risk with this is that it isn't thread-safe. There isn't
		even a ProcInfo to tell the function the thread context.

Stick with the current design of Set/Get, but in the unit tests don't go into
the off-node operations and instead test the Shell::innerSet.
After much bugfixing and messing around, clears this test. Checkin 2331.
One key change here is that now the system does NOT wait for individual
Set operations to clear before returning. It just queues them up. This required
changing the general 'asend' command for the message to a 'tsend' command
so that the target is uniquely specified. Oops, this won't work because the
Element for the target isn't uniquely specified. Anyway, this got me to fix
tsend. Reverted back to the immediate completion of the Set command.
Checkin 2332.

Trying to get the next unit test, for Get, to work. This has entailed a 
diversion to the Qinfo::reportQ() which is now much more informative.
The testInnerGet still doesn't work. Checkin 2333.

Fixed up sequencing in Qinfo::clearQ. Now it works. Completed the remainder 
of the testInnerGet. Checkin 2334.

Gave in to the mass of unit tests, and put in a test for single-thread mode
on the Shell dispatchSet and dispatchGet functions. Also configured the main
code so that the testAsync is called withthe Shell::isSingleThreaded set to
true. With this the system now clears several more tests. Need to put in the
single-thread test in a few more places. Checkin 2335.

============================================================================
8 Nov 2010
Struggled a bit with setVec tests. Got side-tracked into cleaning up a hack
in the AssignVecMsg::exec function, which entailed adding a new version of op()
in all the OpFunc classes. This turned out to fix the original problem too.
Checkin 2337.

Further efforts on cleaning up shared message handling. The isForward flag
wasn't being set. Checkin 2338. Now stuck with testAsync::testMsgField.

============================================================================
9 Nov 2010
Ran into a nasty problem with not checking the source when sending messages.
This crops up in cases where the message is point-to-point, so only specific
source indices are permitted to send the data. This led to some cleanups in
the Msg and queue setup code, to make it harder to ignore essential flags
when sending messages. Now it clears all the unit tests in testAsync.cpp.
Checkin 2339.

In order to clear further unit tests, we need to run ShellParser type commands
like doCreate and doAddMsg. This required cleaning up their parser blocking
functions and incorporating design for future threads.

Now it gets stuck, as expected, with the scheduling. It has cleared the
basecode unit tests, the messaging tests, and the Shell stuff. 
Time to tackle item #2 above, the scheduling.
============================================================================
10 Nov 2010
Working on scheduling. A skeleton of a function chain from Clock::advance
is now in place and compiles. This advances the simulation by one Tick, which
may be a subset of a full timestep if there are multiple ticks within a
timestep. Need to make some low-level unit tests. Checkin 2341.

Incorporated automatic assignment of parent Element to all Ticks, needed to
iterate through the messages going out from the Tick. Checkin 2342.

Next: reinit, start, unit tests; then multinode/multithread start/stop/reinit.

To do reinit etc we cannot hold any state variables in the 
Clock, TickPtr or Tick, since multiple threads use them. These variables
will have to be in the ProcInfo, or to be advanced in Phase 2.

Separated the different levels of 'advance' and 'reinit' functions into 
phase1 (spread op among many nodes ) and phase2 (assign clock/Tick values).
This now compiles. Checkin 2345.
============================================================================
11 Nov 2010
For some reason I have no messages registering when I try to run Tick::reinit,
even though the messages do turn up when I query the Element directly.

Figured it out: At creation time the Clock hard-codes the Tick::ticke_ field
to the system Tick. Had to undo this to get the testTicks() unit test set to
work. Checkin 2348.
I don't like the hard-coding. Remove that. Done.

Some progress. The hold up is now in handling of the tickerator_, which gets
set to 0x20 during TickPtr::advance.

============================================================================
12 Nov 2010.
Looked closely at why the iterator was messed up in TickPtr. Turns out
to be a symptom of a fairly big goof-up: I was sorting the entire TickPtr
data structure, and each time the entire ticks_ vector was being rebuilt.
Messy. I need to make a container that just has a pointer, which is what
TickPtr was originally meant to do.

Did it. Cleaned up. Now tests go a little further before croaking.
Checkin 2350.

Fixed a minor problem with the return value for Clock::getCurrentTime().
Now clears the setupTicks unit test.

Turns out there were multiple dependency problems when I did a make clean and
recompile. Fixed. Also split the scheduling tests so that the recent
single-thread tests are in a separate function, testSingleThreadScheduling.
Checkin 2357.

Starting big overhaul of event loop  in ProcessLoop.cpp.
============================================================================
13 Nov 2010
Begun compilation of the overhauled code. I've put it into Shell.
Checkin 2358.

Got it to compile. It goes to the same place as last time in the unit tests.
Checkin 2359.

Now I've separated out unit tests that require the process loop. It clears 
these. Checkin 2360.

This lets me have a go with valgrind... memory errors everywhere!
Cleared out two of the nastiest run-time errors. Still lots of leaks.
Checkin 2361.

Trying to get 'start' to work. I need to settle on a design for how the
acks are sent (always from shell?) and how shells connect to other shells and
clocks.

============================================================================
14 Nov 2010
Two ways to handle Shell operations that manipulate the model structure:
- Implement dummy Msgs and DataHandlers for moved or deleted msgs/objects,
	that cleanly intercept and ignore all calls for a cycle or two, 
	till the queues are flushed. Then the actual delete happens.
- Add some extra processing to the swapQ function to clear out any pending
	structural operations in the Shell. Then scan the soon-to-be InQ
	and clean out entries that would impinge on deleted objects, etc.
	This will require a way to further queue operations for the Shell to
	handle during its process.
	Or it will require that I build in a check for valid msg and dest in
	each ReadQ step. This is much easier. Anything that removes an 
	Element or Msg will end up clearing out the Msg ptr from Msg::getMsg.
	If the Element has just been moved the Msg goes with it.

I prefer the second option, using the generic check for valid Msg.

For Shell functions that talk to other objects: Set and Get have to be
relayed through the local node Shell. A permanent message to the Clock
may not be a bad thing for run/stop/reinit and so on. These can proceed
through the regular messaging.

First, need to set up some tests of the functioning of the ProcessLoop.
I would like to set up some reporter objects and inject operation requests.

Still need to sort out groups. Need new unit tests for those as well.
This simply requires that each msg puts its output into the Qvec for the
specified group.

	Group 0: Includes all nodes. All messages to and from Shells:
		the master msg to other shells, and the clock controls. Any
		display I/O messages too?
	Group 1: Any subset of Elements under group 1.

For now we'll put everthing in group 0 and just do broadcasts to all nodes.
Need to get things working.
Checkin 2362.

Fixed a couple of nasty threading bugs. One was that I had initialized one of
the barriers to the wrong number of threads. The other was that I had not
terminated an initAck with an waitForAck function in a couple of Shell 
functions. Now it unpredictably crashes with a couple of different
errors in different places, but further along. Checkin 2363.

Still inching further. It crashes in testShell:testShellAddMsg because
it requires a reinit before a start. That has been patched over but it
needs an internal automatic reinit to avoid this issue.

Added a SharedMsg between Shell and Clock as they have a lot of functions
to call. Implemented ack callbacks in Clock, but it still doesn't work for
reinit. So the test still stalls. Checkin 2364.
============================================================================
15 Nov 2010
Hangs:
main thread: Shell::waitForAck: cond_wait.
eventLoopForBcast: barrier3
eventLoopForBcast: barrier3
mpiEventLoopForBcast: barrier3
shellEventLoop: barrier3.

Turns out that the various threads are still looping. We just are not
able to get out of the cond_wait. The ack is still pending.
The handleAck never gets triggered. Looks like
the reinit function is never sending back an ack.
Need to check if Clock::reinitPhase2 ever gets to ack.send. This needed
fixing, fixed it. Checkin 2368.

Working on the process function and other operations that
take place after testShell.cpp:647 (shell->doStart( 2 )).
Confirmed that at least some of it is safely getting to the Clock::process. 

============================================================================
16 Nov 2010
Into thread debugging mess.
- System sometimes fails at copyTree test.
- System sometimes applies reinit operation in testShell.cpp:testShellAddMsg
	but sometimes does not.
- System sometimes starts up process, sometimes goes right ahead into the
	output comparison.

Need to go through this one step at a time. First fix up memory leaks and
bad accesses using valgrind.
Fixed one memory leak, turned out to be a misunderstanding of
how Id::destroy works. It does NOT clear out children. To do that
you need to use the Shell or Neutral destroy command.

This cleans up the system, it clears the basic unit tests without
any memory leaks. Checkin 2369.

Looked at the copyTree test failure. Printf debug it as it is rare. This
is the error condition:
.me = 4009628519[1076094549:1633825638]; fullid constructed= 77
.me = 4009628519[1076094549:1633825638]; fullid constructed= 77

It generates the same outlandish numbers each time it croaks. These numbers are
NOT the same as FullId::bad(), which is 
bad = 0[4294967295:4294967295]

I'm doing testTreeTraversal both before and during the running of the 
processLoop. It has this problem only during the processLoop.

Checked if all the elements themselves have been correctly created
by Shell::doCreate before this error. Seems good.

Tried to put the initAck inside a protective mutex. Doesn't help.
============================================================================
17 Nov 2010
Drew out a thread/barrier diagram, and this shows up a major problem.
I need to ensure that messages are inserted into the outQ by the parser
thread only when the worker threads are halted. This means that it has
to be done within one of the barriers, preferably during the 
Qinfo::swapQ function which inserts inside barrier1. Alternatively I could
put the parser sends into a separate outQ which gets merged in during the
swapQ. Need to integrate this thread scheduling with handling the model
modifying ops by the Shell.
See sketch on pg 66 of pen/paper notes.

For getting parser operations into the processLoop:
	- Have a separate Qvec for the shell 'send' operations.
	We could have swapQ grab a mutex and within its protection, put the
	parserQ contents into an outQ if needed. 
	- Or, Put the parser stuff into a special portion of the outQ Qvec 
	for the shell 'send' operations. The special part of the Qvec is
	indexed by the ProcInfo that belongs to the shell, so that none of
	the worker threads step on it.
	Here we will have to protect barrier 1 with a mutex as we do now, so
	that parser calls don't try to dumpt data while the swapQ happens.

	Both of these options rely on using special outQ info from the local
	ProcInfo of the Shell. Both also require a mutex.
	Seems like we
	Decision: I think the first option is better.
About executing stuff by the shell:
	- Nothing happens in phase 1.
	- Shell handles incoming msgs in its own thread, in phase 2. A few of
	these (like returns for 'get' calls) proceed right away. Others 
	are flagged for protected execution.
	Or, we could put the shell into the regular process/execute cycle.
	I prefer the latter.
	Here we use EpFuncs on the Shell, and push-back the Qinfo arguments,
	as these point to locations in memory for the start of each message
	block. So our secondary queue is a vector of Qinfo* which is
	then converted back into OpFuncs for the shell to execute. Ugly.
	- During one of the protected barriers, possibly barrier 1 which does
	lots of other stuff, the shell scans through pending jobs if any. Note
	that this will mostly happen during setup.
		- The Shell functions that handle the EpFuncs must operate in
		either of two modes: one to accumulate their contents into
		the shellQ, and the other to actually execute the operations.

See sketch on pg 67 of pen/paper notes.

Steps:
* Implement ShellQ in a special Qvec. Just put it in group 0,
	and separate out the main event loop into group 1.
* Implement (in Qinfo) a special Q for structural Shell calls.
* Go through Shell calls, convert to EpFuncs if they are structural.
	Then set up so that they dump stuff to the structuralQ unless they
	are within barrier1.
* Update SwapQ to check the structuralQ and call the funcs if needed.
- Fix silly ProcInfo names to myNode, myGroup, myThread, and so on.
- Test.

Implemented most of the steps, compiles, fails now in first pass 
unit tests. Checkin 2370.

There is some problem in how acks are generated. The code passes the
wait place (in single thread unit test mode) As far as I can tell the ack
does not emanate from the handleCreate function. 

Put a breakpoint on all 15 ack.sends. Turns out it isn't just these,
but the lowLevelGet.send reaches recvGet, which calls handleAck.
internally. 

============================================================================
18 Nov 2010
Finally tracked down this ugly bug. Turns out it was some ack events
inserted into the queues some two unit tests previously, in 
testScheduling.cpp:setupTicks. Added some clearQ calls and that fixed it.
Now it is stuck back where it used to be, in testShell.cpp:testShellAddMsg.
This is at first order just that the call isn't going out to the Clock.

Went into the execution. Turns out that there are two messages
going out from the Tick to each target.

Tracked this down to a clearQ in doAddMsg just after sending out the
request, before waitForAck. Fixed. Now it doesn't go to process at all.

Checked that there are exactly 14 process messages made, one for
each target Arith Element. This is now part of the unit test, and though
it clears this, the process is not called on the targets.

Should I try calling the Clock directly to start it?
Tried. Still doesn't work.

The Clock::handleStart isn't good to access directly as it will change the
'isRunning_' flag at any point, out of sync with the process phases.
Checkin 2371.

============================================================================
20 Nov 2010.
Added some diagnostic fields to count number of passes through various
phases of the process loop. Also added Clock::printCounts() to print them.
Checkin 2372.

This shows that the system always does one pass through phase2 before
croaking. Phase 1 is unpredictable. It is hit rarely, about 10% of the time,
and sometimes does both reinit and advance, and sometimes only advance.

Changed the runtime to 30. printCounts reports erratic numbers of passes 
through the 'advance' phases, from 1 to 29. In all cases the output stalls.
Interestingly, when I put printCount in the code itself right after 
clock->handleStart(), it always reports 0 (10/10 times).

OK, that makes sense. I'm just carrying on in this thread regardless of what
happens in the main loop. Perhaps a sleep call is needed.

Progress! Now it clears the first comparison. In the second message, onto b2,
it looks like b1[2] is getting all the input values and summing them... 
This is a OneToAll msg, let me check the execution logic...

Yup, there was an error in the OneToAll msg logic. When I fixed this it clears
all of the messaging tests in testShell.cpp:testShellAddMsg(). Note that at
this point I'm still using the 'sleep' call to ensure that the messages
get through. Checkin 2373.

I wonder if this might mess up the scheduling too, since the messages 
from the Ticks are OneToAll. Checked code. No, doesn't seem like it. 
Next: See if I can now get the system to work properly with shell::doStart().
Doesn't. Confirmed it doesn't work even if I put the 'sleep' call back in.

Went back to shell::doStart( runtime );
Now tried changing runtime. This is interesting.
For runtime of 3, it fails. Clock::printCounts() reports:
Phase 1: Reinit = 0;	advance = 1;	null = 2011
Phase 2: Reinit = 1;	advance = 2;	null = 2008

For runtime of 4, it works. Clock::printCounts() reports:
Phase 1: Reinit = 0;	advance = 2;	null = 1767
Phase 2: Reinit = 1;	advance = 3;	null = 1764

Discovered another thread problem. The start and stop from processing were
not happening in a thread-safe manner, as the handler function is processed
in parallel with lots of other threads. Set up a global flag to notify the
clock of requests to change isRunning_, and I actually do the change of this
key flag within barrier3 where it is safe from messing with anything else.

After this I'm still not quite there.
For runtime of 3, it works. Clock::printCounts() reports:
Phase 1: Reinit = 0;	advance = 2;	null = 1766
Phase 2: Reinit = 1;	advance = 2;	null = 1764

but, for runtime of 2, it fails. Clock::printCounts() reports:
Phase 1: Reinit = 0;	advance = 1;	null = 1715
Phase 2: Reinit = 1;	advance = 1;	null = 1713

Note that now the # of advance cycles matches between phase1 and phase2.
I also see it is possible to barf at reinit. Same issue.

Another issue revealed in testShell.cpp:testCopyMsgOps, which uses a similar
set of messages to test copying. It clears this only if the Shell::doReinit()
is called before Shell::doStart(). Need to have an internal check in 
Clock::handleStart to do its own reinit. 
Anyway, at this point it clears all the Shell unit tests, including runs
through the process loop. Although issues remain, this is a major step further. 
Checkin 2374.

Pending issues:
	- # of advances vs. runtime, and # of cycles needed to send msgs and
	clear tests.
	- Reinit flag needs thread protection, same as isRunning_ flag.
	- Clock::handleStart should check if it has been reinited.

Worked on all 3 above. New, much more complete structure for dealing with
Clock reinit, start, and stop requests safely. Still gets stuck.
Seems to be going through 4 Qvecs in inQ and outQ. This is wrong. Even though
it may have nothing to do with the problem, I'll see if fixing it helps.
Fixed. Doesn't help. Still gets stuck in Shell::doReinit in an infinite loop
waiting for an ack.

Figured it out. Shell sends an additional mail to itself which messes up
some of the calls. Now the code passes this reinit, but then croaks on the 
processing.

Fixed this too. It was that the new, functional reinit was clearing out all 
the initial values I had set. Same problem also came up in the similar test
on the copied element tree and messages, testShell:testCopyMsgOps().
With these cleared it goes further still in the unit tests.
Checkin 2376.

Next place it crashes is SparseMsg.cpp:randomConnect. It is simply that
multiple threads are calling the function at the same time, doing lots
of vector push_backs and messing up. This is because the AssignmentMsg
does not do a check on threads in the exec func. Fixed for AssignmentMsg
as well as AssignVecMsg. Also OneToOneMsg had a bug related to this, fixed.
Now it crashes somewhat further on, looking like the non-reinit bug again.
Checkin 2377.

Yes, it was non-reinit. Now it clears the doStart, but the resultant 
numbers differ. Obvious issue is how many steps actually happen, something
there were already issues with in the testShellAddMsg function.

I've gone back and fixed the match between # steps and runtime. Now in 
testShell.cpp:testCopyMsgOps() and testShellAddMsg()
I can get by with doStart( 2 ), which does 2 'advances:
Phase 1: Reinit = 1;	advance = 2;	null = 1844
Phase 2: Reinit = 1;	advance = 2;	null = 1844

The 2 is essential: First call to process, in phase1, sends out the 
output_field of a1, b1, c1, etc. This arrives by phase 2 of the first cycle.
However, the output_ field of a2, b2, c2 etc is not updated till the second
call to Process. 
Checkin 2378.

============================================================================
21 Nov 2010.
Rare crash: in Qvec.cpp:47

Realized I had a lot of crashes in Qvec. After some fiddling around, saw at
least one of the problems: in Qvec::push_back, I do reallocation of the
whole vector if the incoming data gets too big. This will cause problems if
another thread is doing a memcpy into the same data_ buffer.

Here I have contradictory requirements. Let's assess them:
- Thread safety: I need to be able to push_back into the Qvec on different 
	threads at the same time.
- Single buffer: For sending to another node, I need to provide a single big
	buffer with all thread data.
- Memory efficiency: If we have a big contiguous buffer, there shouldn't be
	big holes in it.

Here we have 2 basic solutions:
- Use a single contiguous memory chunk with portions for each thread.
	-- Thread safety requires mutexes for every insert.
	- Single buffer is built in, but have to 'stitch' thread sections 
		together. Bad side-effect is that we MPI send the holes too.
	- Memory efficiency is dubious, will need periodic garbage collection.
- Use a separate data vector for each thread.
	+ Thread safe.
	-- Will need memcpys for consolidating into inQ
	. Memory efficient to the extent that the C++ vectors are.

So it looks pretty clear-cut. Will need to shift to a separate data vector
for each thread, within the overal Qvec data structure.

Did complete overhaul of Qvec. Thanks to the wonders of modular design,
it worked as soon as it compiled. Cleared its own internal unit test too.
Fixes the funny memory bugs with Qvec, but now we're back to the values 
computed by the IntFire network. Checkin 2379.

Those were fixed by changing the call to Shell::doStart.

Now we're back to the problem of the Start happening without a reinit.
Tracked it down to this: the number of TickPtrs and TickMgrs on the clock is
more than actually used in the model. The old ones haven't been cleared out
when rerunning a new model. The old ones haven't properly inited the 
tickerator.

I should just have the 'start' function do an automatic scan to make sure
that no ticks etc are dangling. Doesn't need to alter the existing settings,
just clean out old ones.

Fixed things up by replacing the iterator TickMgr::tickerator_ 
with an integer index. This clears the fibonacci unit test for builtins.
Now fails at the getMsg test. 
Checkin 2380.

This had more issues with doReinit. Now it clears it, and that is the last
of this set of tests. Also clears regression tests. The system then stalls
because we don't have a way to tell the process loop to close now.
Checkin 2381.

Going systematically through the # of threads. At 5 threads I get a 
sporadic failure in testScheduling.cpp:testMultiNodeIntFireNetwork, on the
weights again. I suspect it is that the assignment of weights through the
non-blocking set call is taking a while on some process thread, and in the
meantime the parser thread moves on to use mtrand again for other things.
I've put in a test usleep to see if this diagnosis is correct.

Parenthetically I decided to fix up the 'quit' call, which easily fits in
the framework for the clock to do things with the event loop. That now works
too.
Checkin 2382.

Using the quit call I was able to set up a test loop to see how often the
system fails when a 1 sec usleep is in place after the call to set up
the randomly connected network. 0/47 failures.

13 /29 failures without the usleep. Pretty clear outcome.

Messing around with sending an ack for Set functions.  I've implemented
an ack call in AssignmentMsg (or AssignVecMsg)::exec stage. This fails because
'get' calls have their own ack, and this same exec stage is used both for set
and get. Thus we end up with extra acks floating around, causing problems.

- Need to differentiate between 'get' functions and 'set' functions at the
	AssignmentMsg (or AssignVecMsg)::exec stage. Reason is that the
	'get' functions currently return the data internally and don't want the
	extra ack.send.
- Should be possible to put a much simpler wrapper on the getfunc and convert
	its return value within this AssignmentMsg::exec, into a buffer. Then
	it could go back using the same code as sends the ack back for Set.

============================================================================
22 Nov 2010
Outline for Set/Get without using intervening message creation:
Parser (node0) -> outQ for parser -> Parser pauses in WaitForAck
-> data mutex protection -> inQ -> MPI_Bcast ->
ReadQ -> Special Qinfo option to make temp Msg that isn't attached at either
end -> temp Msg delivers call minding data location and thread -> 
	\ Get function converts data if on target
	\ Get function sends ack if off target
	\ Set function sends ack in all cases.
-> temp Msg cleared -> data or ack goes onto outQ -> swapQ -> inQ -> 
MPI_Bcast -> 
ReadQ -> Special Qinfo option to make temp Msg -> temp Msg delivers call 
only on node 0, minding thread -> Shell handles return call ->
	\ may deposit data in case of 'get'
	\ Always triggers handleAck both for set and get.
-> Exit from waitForAck.	

Another level of messiness: which thread to use to send ack back? If we
pick one, how do we know that all threads have completed? I guess that the
'send' will only take place after the threads hit a barrier, so any one
would do.

Looking at code, it would be nice to generalize to use the same templates for
the whole lot. This means to treat functions generally as
template< class R, class A1, class A2... > func( A1 arg1, A2 arg2 ... )

Things might get problematic if we have R as a void. Turns out this can be
dealt with using template specializations. Made little test program in
subdirectory testVoidReturn.

Also it might be possible to consolidate a lot of different template headers
if we set up the OpFuncs in the same classes as SetGet. What of the
different kinds of OpFunc, though? There are OpFuncs, UpFuncs and EpFuncs.
Eventually even the Dinfo might be done here.

Lots of possibilities, but for now I just want to get the system to work.

Had some trouble with unit tests because of debris in the queue following
the new acks in Set. Slowly clearing through these.

After getting very down and dirty with the queues and appropriate msg
acks, it suddenly works and clears all tests. Even with 5 and 11 threads.
In addition, I have eliminated one level of message bouncing in the 
Get function, so that the queue entry with the data goes right back to the
calling Shell, without an intermediate relay. So big progress here.

Next step is to try to check this in, over a very unreliable internet
connection. 

Then I can move onto testing on multiple nodes.
From there it is benchmarks, scaling and the like, while Subha brings in the
Python parser.

============================================================================
23 Nov 2010
Checkin 2383.
Minor cleanups to code especially Shell handlers for functions that are now
dealt directly by Clock. Got rid of a couple of test directories.
Checkin 2384.

Now setting up the MPI data transfer buffers. Some issues:
- Need to transfer either the size or the # of Qinfo entries, right at the
	start of the data block. Since I use InQ to send out data, this means
	that InQ has to begin with this info. Can be done during the 'stitch'
	command.
- Need to use a different set of functions for the swapMpiQ:
	- Instead of stitching together data, we want to keep the data_ 
	vector of buffers empty.
	- We need to ensure that the linearData_ buffer is of the right data
	transfer size.
- Need to deal with pre-assigned data transfer sizes in MPI.
	- Need to track distribution of actual inQ sizes for outgoing data
	- Need to dynamically resize buffer and transfer sizes to avoid
		fallback but not to have too much overhead.
	- Need to have a fallback mechanism for transfers bigger than 
		allocated data.
	- If there is a reasonable proportion of zero size transfers, should
		be able to skip those. More cleanly if there are simply
		zero messages between any two zones/groups.
		- If there are messages but they are rare, skipping gets
		complicated. We would need to go through the master node
		to relay info if there is a rare message, somehow stall
		everyone, notify the tgt nodes, and then send the message.
		Or we could have it done through MPI_Irecv which would normally
		pass through, but if data were pending could take action.
		Here again there are synchronization issues.
- Deal with node and group handling.
	- add yet another looping layer for the nodes in the eventLoopForBcast.
	- Or we assume that any node other than node 0 is in only one group. 
	- Or put node 0 in its own, none-processing group, and the other 
		nodes belong in processing groups.
	- May need a sparse matrix to indicate connectivity of groups of nodes.
	- In the old GENESIS this was called 'zones'
	- Cases: 
		- All in one zone, 
		- multiple zones on one node (for a big SMP system)
		- Each node is in one zone only (for a big MPI cluster)
		- The case of each node having multiple zones is strange,
			since one would like to put the dense communications
			within a node.
			This would typically only happen on node0 if we
			were very short of nodes.


============================================================================
25 Nov 2010

For starters, assume:
	Group/Zone 0 connects to all nodes.
	Group/Zone 1 connects to all nodes.

Setting up hook functions for managing size of MPI transfers including
overflow if the block size is bigger than allocated.

How to signify? We need to always transfer the actual data size in the
current buffer. If a big block we would need to also send the total block
size. The originating node # is nice to know, but it is available from
the Recv context and does not have to be in the buffer.


Need to have separate size reporting functions in Qvec: for the total
buffer size and for the data component of it.

Now it compiles, but the grouping stuff has put the allocations all in a mess.

============================================================================
27 Nov 2010
Checkin 2391.
Ground through the unit tests, now all clear again. Checkin 2392.

Ground through again, this time with an MPI build. The code now
clears the single node unit tests but not with 2 nodes.

============================================================================
28 Nov 2010.
Run into a difficult bug with threading. When I have a single
core defined, it croaks randomly. All symptoms point to a threading
error. It is OK with > 1 core. Also fails in a similar manner in
multinode mode.

After much struggle, finally tracked it down with the help of valgrind.
Turns out that I was confusing 1 core with SingleThread mode. With this
fixed, the system now works correctly with 1 core as well. However, the
single-thread mode (moose -s ) hangs in testShellAddMsg, when it calls the
Reinit function.

Some more fixes later, this is the situation:
0x00000000004cd7c7 in Qvec::isBigBlock (this=0x8a09e0) at Qvec.cpp:124
124             assert( linearData_.size() > HeaderSize );
(gdb) p linearData_.size()
$1 = 0
(gdb) p HeaderSize
$2 = 8

============================================================================
29 Nov 2010

Some more cleanup, got to work on single node again.
Checkin 2394.
It still doesn't clear all unit tests in single-thread mode. Defer.

Now working on cleaning up stuff so valgrind is happy. Mostly minor
stuff in cleaning up the pthread variables. Checkin 2395.

With this sorted, back to the MPI issues. It promptly crashes.

Some more work on MPI. Added in fix for big data packets. Now it doesn't 
crash, just hangs. For reasons I don't understand, it keeps going around the
event loop even when one of the nodes is halted using GDB. Instead it should
block at the MPI_Bcast call. Revisited this, it now seems to block
properly in the event loop. Checkin 2396.
Data comes to other node, but it doesn't get to the shell 
handleCreate function as it should.

Found it does get there, but it never clears the Qinfo::addToStructuralQ
command.
I think the issue is this: clearStructuralQ is called in barrier 1. The MPI 
Qs are all set up _and cleared_ in phase 2/3, in a little loop through 
different SimGroups. So, by the time barrier1 comes around again, 
the original MPI queue it pointed to will likely have been overwritten by
some other group. Note that this won't happen for the local node queue.
I need to make separate storage for the structural Q.
Did it. That seems to fix a lot of things. Now the code gets much further,
including lots of multinode stuff, before crashing. Checkin 2397.

============================================================================
30 Nov 2010
Tracking down problem on multinode. It is happening in a get call to find the
parent of a newly created object, in testShell::testMove.
Two bugs for starters:
- Both nodes report isDataHere is true. If the object is a ZeroDimHandler,
	only node 0 should report this.
	Seems that the object is a ZeroDimGlobalHandler on node 1, and
	a ZeroDimHandler on node 0. Odd.
- Node 1 parent not assigned, reports Id() instead.

Looking at Shell::innerCreate. Oddly, the 'adopt' function does the right thing
on both nodes and creates the appropriate one-to-all message at a low-level. 
I need to check how the 'parent' function works.
	Looking at the 'parent' function. Seems reasonable. Need to follow up
	with the message tracing part in the debugger.

Looking at Element::new Element. Oddly, this seems to set up both with the
isGlobal flag at zero and the correct ZeroDimDataHandler.

============================================================================
1 Dec 2010
One clue: On node 1, the Element that gets passed to Neutral::getParent is
not the same as the one created. Instead the Element points to the Shell.
So something got lost on the way here.

Examined the actual calls arriving in Shell::handleGet and recvGet. Turns out
that two calls come in to recvGet on node 0: one with the correct data,
and the other with a null Id. RecvGet declines a call on node 1 as well.
Also extra calls get to handleAck on both node 0 and 1.

I wonder if the issue is that both groups are sending out data at this point
to all nodes. We need to make it so that for any target node, only one 
group at a time is allowed to transmit data.

One issue: Looks like Shell::handleGet is issuing an extra ack in cases where
the data is not present. This happens because the AssignmentMsg::exec
is already returning an ack in all cases after the op which in this case
is the get or set function on the tgt Element.

Removed the extra ack.send in handleGet. Now the system hangs earlier.
This is in testShell.cpp:91, testTreeTraversal. Again, this is 
a 'get' function.
============================================================================
2 Dec 2010
After much checking, turns out that the ack in handleGet was OK. Now it
hangs in the original place, the testMove() function, in the same way,
with an (0,0) id returned instead of the actual object parent.
Checkin 2398.
OK, I think I've tracked it down. The problem comes up because the
lowLevelGet.send call goes to all nodes. It should really only be handled
on the local node. Similar issue will happen for set.

One solution, which I did earlier, is to make zone0 exclusively for node-local
messages. This leaves the question of how to send parser calls to other
threads.
Another option is to have the local node set/get call (innerSet and
handleGet respectively) only set up the AssignmentMsg, and not do the send. The
send call is the job of node 0 only. This makes sense specially when
we use vector assignment.

I think this works. I've now cleared the unit test hurdle that had me stuck
for a long time, and it now croaks in testShellSetGet.
Checkin 2399.
Very odd, it actually clears 50 of the assignments... aha, here we have
node decomposition issues. Since we've already confirmed the single entry
set/get on all nodes, it is likely to be the setVec.

============================================================================
3 Dec. 
Not much progress. The assignment seems to get messed up as expected,
because of bad handling of target object indexing in AssignVecMsg::exec.

============================================================================
4 Dec. 
Sorting out the indexing. I need to maintain parallel indices in 
DataHandler::iterator: one vecIndex, and one regular DataId index.
This entails maintaining a correct start_ and end_ integer index in
FieldDataHandler specially, but possibly also in other objects, so that 
they can quickly figure out their internal integer indices.
============================================================================
5 Dec. 
Working on the loose ends of FieldArrays. The problem is that it is a ragged
array, but I would like to be able to do vector assignments to the entire 
array. Other data handlers support this and it is a generally good idea in 
terms of model set up. The current interface is

setFieldArraySize( objectIndex, size );
unsigned int getFieldArraySize( objectIndex );

The issue is that to set data blocks I need to know start field indices on
each node. The parent DataHandler can figure out if the object is on the
current node, but it takes a full data traversal to find the start field
index. Strongly argues for each node having always current values of start
indices for fields, for linear array lookup.

Option 1:
	Change assignment data interface so that we always assign the whole 
	thing:
	setFieldArraySize( vector< unsigned int >& fieldArraySize );
	This will go to all nodes and they can update each entry as needed.
	May be overkill but in terms of data transfer we would need to 
	cascade data from one node to the next, which is worse.

Option 2:
	Provide increment in data size rather than absolute value, 
	let all nodes figure out how to alter their indexing.
	setFieldArraySize( objectIndex, int deltaSize );
	This is more compact but messy because it assumes that we know
	original size. Will need to maintain that on master node, which spoils
	symmetry across nodes.
	Will need feedback function to master node in case we run into negative
	indices. 
	Will also need another function to set sizes in the first place.

Option 3:
	Force it into a regular array framework. Use the largest dim of the
	ragged array, add a bit for expansion, and deem this the size of the
	system. When sending out the value definitions, pad out the rest with
	zeroes or come up with a padded vector data struct. Within this it is 
	the job of the individual FieldDataHandlers to pick the correct
	subset of data. Handles expandability well: will allow growth of 
	individual array dims without any change to the other nodes. Will need
	a way to feed back info to master node when we run into limits.
	Either that or maintain all sizes on master node, which is also ugly,
	and possibly impractical.
	Will need to alter a whole lot of unit tests.

Option 4:
	Bearing in mind that all calls go globally, have each node maintain
	the size of each object entry. This will require a vector of size
	numParentObjects on each node. Poor scaling.

Option 5:
	Have each node maintain its own start field index, and feed back to the
	master node so that this index remains current if there is any 
	operation that inserts fields.
	
One way out of the issue with rescaling sizes on master node is for the
relevant commands to include an explicit cycle where each node is asked for
its max dimension, and then when the data comes back, for the master node to
send out a resize command to all worker nodes.

On the same lines, we could also have the initiating command ask each node for
its # of entries, and then send back a vector with the start entry for each.

I would normally go for option 5, as being the leanest of them all. Problem
comes in when we have simulations which dynamically (and locally) add synapses.
This is way in the future though.

============================================================================
6 Dec.
Slowly starting on option 5. In FieldDataHandler::setDataBlock.
May need to change constructor for iterator in all objects.
============================================================================
7 Dec.
Looking at the setDataBlock function. This is extremely hard to handle
using Option 5. Option 3 can do it easily. May have to bite the bullet and
use that.

Mostly done here, but still need to put in handlers to set the 
FieldDataHandler::fieldDimension_. This is what the system will have to use to
ensure that the fieldDimension is larger than any field array size.

Compiles now but doesn't clear unit tests. Checkin 2402.

I think I see where to intervene. The FieldElementFinfo handles a
setNumField function. Currently this just does the assignment, but it could
also be templated around so as to call the Shell to check the size of the
Field arrays, and update if needed. Quite involved for such a seemingly
simple assignment.
The alternative is to explicitly code the request for updates into
the 'setNumField' functions every time. Not good.

The postCreationFunc creates the FieldDataHandler that we
want to use, but it isn't handy for giving the DataHandler. Better
to use a template to wrap the existing setNumField operation,
which is currently handled by the regular OpFunc1.
If we make this into an EpFunc1 we should be able to find the
Eref and from this its Ids. How to find the Eref/Id owning the
FieldData struct? This is needed to pass to
Shell::doUpdateFieldArrayDimension( Id fieldId );

Will have also to make a FieldDataHandlerBase class to provide
a common interface esp for updating the fieldDimension, and a lot
of other functions too.

============================================================================
8 Dec 2010
Implemented FieldDataHandlerBase class. Now compiles and crashes
in the same place on the unit tests. Checkin 2403.

Sequence:
Call to setNumField
	- This sets a flag in the FieldDataHandlerBase warning that this
		may need updating. I could just update it each time, but it 
		would be bad in the case of a SetVec.
	- The field is set
	- Would like, after exit from the assignmsg loop, to check if there
		is any stuff to update. 
Also I want to maintain a 'reserve' size so that I don't have to change

Immediate test is to use the already compiled low-level functions and
set up the system.

This is proceeding well. Steadily clearing many of the unit tests, the
new indexing is simple but takes an extra couple of steps over the earlier
form. Checkin 2404.
Currently debugging testScheduling.cpp around line 448
============================================================================
9 Dec 2010
Fixed FieldDataHandlerBase usage, specifically, the implementation of 
FieldDimension in such ragged arrays. Now clears unit tests. Checkin 2405.

How to tell system to send back FieldDimension request to Shell: I see a
way to do this by setting a flag in the FieldDataHandler, and checking it
in AssignVecMsg/AssignmentMsg at the time the ack would normally be sent.
Question is, can we generalize to any op and any callback?

- Could readily send back just a different ack call.
- Harder to send back, as we would like to do here, a call with specific 
	data added. Here we want to send back the updated 
	biggestFieldDimension value.
- However, the ack return contains the node and status info. We could
	replace the status info with an identifier indicating what the
	Shell should do. A somewhat ugly option, in principle the fid
	should deal with it.
- How often will we want to send some extra request back?
	- When a local function call requires global updates, like here.
		We are treating the arraySize as a field, though actually it
		might be set up at a lower level so as to hard-code the
		subsequent check for biggestFieldDimension.
	- Whenever we want to report an error cleanly, rather than bailing.
- Note that these return calls centralize the response to the situation,
	as they require the Shell to have a function to deal with it.

An altogether different approach is to have the field receipient function
do something to respond. Here we have IntFire::setNumSynapses, which
obviously knows it has to do something. The sticking point here wasn't 
adding in a message, it was that we would have huge numbers of return 
messages flooding the system following a single call to setVec. We would
like a way to 'reduce' these messages and send a single call off-node.
- Ideally this should not even involve the Shell, and should go to the
	IntFire Element on each node to give it the info that it has to work out
	to decide how to update the fieldDimension. 
	setNumSynapses-> sendLocal( synapses_.size() ); -> findBiggestSize ->
		send( biggestSize )

- It is a little odd in that the IntFire would have a mandatory msg to itself.
	It could in principle go to an Element-wide assignment call. Here
	we run into threading issues, and the question of how the final
	tally will be taken.
- If I go through the regular messaging system, the operation will no longer
	be atomic and other calls may intervene that mess up the system.
	I may need a way to tell the system not to send the ack just yet.
- Could implement a special SendToSelf function that takes the ProcInfo
	and deals with the threads using a mutex, and also
	guarantees that a second target function will be executed when the
	last thread concludes. May as well call this a threadReduce operation.
	- Generates biggestSize for the given node using threadReduce, 
		sends Msg out.
	- recipient scans through all 'biggestSize' candidates from all nodes
		and figures out how to update its local fields appropriately.
		recipient sends out deferred ack to Shell.
	
	Instead of msg to self, perhaps we could do as a 'set' assignment call.
	No, there will be a huge number of these calls coming in, one from each
	node.
	- Could implement a nodeReduce operation which creates a dummy Msg
	within an object for doing this data transfer, and then removes the
	Msg when the call is done.
	- May even wish to have a compositeReduce function which deals with
	the threads and the nodes all together. Conceptually:
		send out some data ( current object numSynapses )
		threadInputFunc(input from each thread): operate on it
		threadOutputFunc(1 per node): reduce data and send to all nodes
		nodeInputFunc(input from each node): Operate on received data
		nodeOutputFunc( 1 per node ): Reduce data, do something with it.
	Remaining issue is the ack.

	If we do the nodeReduce in a distinct call, we might be able to
	complete the whole thing within the original cycle.

Let's first test out threadReduce and nodeReduce in a standalone program.

============================================================================
10 Dec.
Wrote, compiled using
	mpicxx -g reduce.cpp

The simple approach doesn't work. Issues:
- We need to have a separate function call after each thread has done its 
	assignments, so as to be able to collate the entries. 
	- This separate call will trigger the threadReduce and later steps.
	- It has to come on each thread.
	- The AssignmentMsg::exec or AssignVecMsg::exec func should call it
	around the time the ack is called.

- We need a way to have a temporary data struct on each thread, that holds
	the running total of whatever reduction operation is in progress.
- The original dest function needs to know which thread it is on. 
	- This info is there in the exec function that calls the dest operation,
	but procInfo is not normally passed in.

Perhaps the general function we are looking for is to be able to set
	Element-wide fields like name and this size, from individual
	object functions. Has to be conditional, and has to be efficient
	so as to handle lots of objects on different threads, and has to
	deal with inter-node synchronization.
	The most we can pass back is a fid indicating what function to call.

A related issue here: How do we assign Element-wide field values, taking into
account the threads? The current design assumes that all fields are object 
fields and can be partitioned between threads. But the name and possibly 
various allocations will cause problems. ElementValueFinfo and EpFunc 
will need to change to handle this.

Note that our framework should also be usable by the SparseMsg::randomConnect
function.

Suppose we design a higher-level function to handle these cases. The components
are:
	- Set or setVec 
	- A threadReduce operation, which could be specified as an object 
		operation.
		This is tricky because we need to juggle threads to find total
	- A node reduce operation using allGather. 
		This needs to take individual data values from the threadReduce
		and pass to all nodes, followed by collating of all node input
		on each node.
	- Doing something with the result.

The randomConnect will need the latter two.

Could imagine this being used for counting molecules in voxels in a spatial
sim. Also to estimate efields at specific recording sites. Basically all 
are cases where a single Element needs to collate something across all entries.

Fixing up testReduce to do this. First, checkin old version as 2406.
Rebuilt testReduce. New version seems to work on 1 node but dies on more.
I was able to run it on 3 nodes and got different results a lot of the time.

Checkin 2407.

============================================================================
24 Jan 2011
Back to work on this. The compilation command line is mpic++ -g reduce.cpp

- Set it so that numObjects now specifies total # in entire calculation, not
	# on local node. This change also cleans up the rand # generation
	so that it is independent of the node decomposition.

This is working on 1 and 2 nodes with assorted # of threads, croaks on 3 nodes.

Fixed. It was a misinterpretation of the calling syntax for MPI_allgather.
The number specifying how many incoming values there are refers to the #
from each node, not the total #. This bug held me up for a long time.
Checkin 2427.

============================================================================
25 Jan 2011
Need to get mpd running to test the reduce function on many nodes.
Did this on GJ.
Needed to clean up zombie processes to do so. Wrote "kill_mpd" which
marches through to kill stuff.
Then to run the test, I use:
/usr/local/mvapich2/bin/mpiexec -machinefile /home/bhalla/mpd.63hosts -np 60 a.out 1000000 4
which gives max= 2147483534
on all nodes. So it seems OK.

More fundamentally: I need a function that is easy to invoke from our regular
object functions, that can deal with 'reduce' operations. Possible approach:
- Generic 'set' function is called on all nodes.
- Function calls threadReduce operation using ProcInfo argument to specify
	thread#. 
	- ThreadReduce is templated
	- ThreadReduce combines info from each thread safely
- When all threads are done (in other words, system is within the next barrier)
	the ThreadReduce queue triggers nodeReduce.
	- when nodeReduce is done with the allgather, it does a final 
	assignment to a field, which may be on many objects on many nodes.
		- This is a templated class function available as fid?
		- This is done serially.
	
Use cases:			Thread op	Node op			Notes
	Updating numSynapses. 	get max #	Update DataHandler
	Smoldyn: mols in vol	add up mols	Update each volume
	Setting Element name	Do nothing	Assign Element name	?
	Efield calculation	Sum contribs	Assign Efield		use msg

The alternative to all of this is to subsequently issue a regular messaging
call to do the updates.
In either case the field needs some special handling to deal with fallout onto
other regular fields and DataHandler fields.

An issue with the regular approach is that it is poor at handling reduce-type
operations. It would get a big vector and then serially digest it. So it is 
worth building in reduce operations as a standard feature. Since we want
transparent parallelization, this means a certain structure:

	- a function, for doing the op, whatever it is.
	- An argument specifying a field to scan over
	- Another function for doing something with the op, such as assigning
		a value in an object.

It would be nice to have the reduce function be completely independent of 
	any prior class, something like an 'algorithm'. Or to have the
	reduce function be handled by a specific instance of a class...
More generally, it would be nice to have ops that take one set of objects/fields
	as arguments for a function in another. 
Here we are getting perhaps too general. The first task is to get the connection
set up to work.
	
Working on it. Implemented Shell::reduceInt, but it isn't
used yet. Put printf debugging to pin down line at which the
code currently croaks, in testShell.cpp:706.

============================================================================
26 Jan 2011
Implemented a utility function
FieldDataHandlerBase::unsigned int syncFieldArraySize();
which gets FieldArraySize from all nodes.

============================================================================
27 Jan 2011
One option is to maintain a flag on each object to indicate whether its
fieldArraySize is valid or not. Certain functions don't care, but if a
function does need to update the value it should happen automatically.
First, check manual operation.

Zeroth: track down crash. Turns out to be a simple matter of trying
to access a field that is on another node.
This may be a good time to implement a 'getVec' function.

Currently fiddling with Shell::recvGet, trying to work out how best to 
handle the random order of incoming parts of the getVec. I think it best
to find the linear index of the source Eref, and plug in the entries in the
right order right away. There is a bit of a niggle in doing this if I want
to handle strings and other variable size returns.

Need:
*Func to get linear index from Eref
* Cleanup of DataHandler to report linearIndex
* Cleanup of DataHandler to report local and total entries.
* Cleanup of get and getVec to use the getBufVec
* Cleanup of get and getVec to empty out the getBufVec when done
* Shell fields to hold isRecvVec_,
* Replace getBuf_ with getBufVec_ everywhere.
* fix up Shell::dispatchGet

Got it to compile. Fails rather soon in unit tests. Checkin 2429.
Incrementally fixing up unit tests.

* Check if we still need lowLevelRecvGet.
* Deprecate Shell::name_
* Fill in the Handle operation to select AssignVecMsg.
* Put in some unit tests for GetVec
Go back to Set and SetVec to generalize wrt strings and subparts of vectors.

============================================================================
28 Jan 2011
Marching through testAsync.cpp cleaning up assertions, mostly to do with the
new distinction between DataHandler::totalEntries and DataHandler::localEntries.

Got it to clear testAsync, more assertions of the same kind pending elsewhere.
Checkin 2430.

Got it to clear all unit tests. But I anticipate problems in multinode.
Valgrind reports 8 bytes lost, due to delayed claearing of the getBuf.

Also I expect issues with getvec on globals: replicating input. Probably
already happens with regular 'get' call.

Implementing the getVec now.

It hangs in an apparent infinite loop. Qinfo::reportQ shows that the
appropriate Q entries are in fact present at the start of things.

Tracked things down to the point of executing the retun function on the Shell.
The function id (fid) is 0, which seems to mean delete, which seems 
erroneous. This is set in AssignVecMsg.cpp:57 from q->fid, which I need to
further track back.

============================================================================
29 Jan 2011
I think I've tracked down the problem with getVec: The AssignVecMsg expects a 
PrepackedBuffer with all the fids in it as an argument.
Arguably this is silly, as we want to use a single Fid here.
	- Could extend PrepackedBuffer to return a single value for all indices,
		as an option. Messy.
	- Could just fill up PrepackedBuffer as a vector with identical fids
		in the whole thing. Wasteful.

More generally, there is some ugliness in how the arguments are passed in
innerSet and innerSetVec, one of which uses Prepacked buffer and the other 
postpones it. Likewise HandleSet does things in an unpleasant way. This should
be cleaned up.

Get rid of lowLevelSet... I don't think it is used.

Got many changes to compile, but doesn't yet work.

Tried to check in but SourceForge has reset all passwords and I have to 
re-register.

A bit more work. Uncovered a nasty old bug with SrcFinfo1 in particular,
which was still using sizeof( T ) rather than the Conv code. Now it
clears this particular problem with Set, but more pending.

Checkin now works. Revision 2432.

Fixed another few bugs, now it clears the unit test for getVec. Need to
do another few tests of this in other contexts, specially for the syn weights.

Checkin 2433.

Valgrind still happy except for that lost buffer entry.

Cleanup of many deprecated funcs in Shell.
Checkin 2434, 2435.

Added another unit test for getVec, this time for all the delays in a ragged
array of synapses. Works first time.

Cleaned up a lot of tests for zero in testAsync.cpp, now using the
doubleEq function. Checkin 2437

Started off on mpi debugging. First problem is in testAsync.cpp:605
which tries to syncFieldArraySize. This fails because the call has to be
executed on all nodes, while this is running just on node 0.
============================================================================
30 Jan 2011

Setting up Shell::handleSync. It occurs to me that this could also be a 
Neutral function, and thus work on all Elements using the 'set' interface.
For now let's see the usage patterns.

This still doesn't run. It gets stuck in a race condition, competing between
the main MPI_broadcast used for the normal internode comms, 
and the MPI_allgather used for the sync.

May need to do the clumsy way after all.

- Functions that alter dimension fields set a flag.
	- nodebalance?
	- setNumField
	- setFieldArraySize
	- setFieldDimension
	
- Functions that rely on the fieldDimension check this flag
	- totalEntries()
	- linearIndex( const DataId& )
	- dataId( unsigned int linearIndex )
	- sizeOfDim( unsigned int dim ) may need this for field dims
	- biggestFieldArraySize
	- globalize probably doesn't

  Note that the flag can be figured out on the master node: doesn't need 
  additional communication.
  Then: Implement a Shell function that intervenes whenever one of the the 
  dependent functions tries to run in the presence of the flag.
- doSyncDimension
	- Send call to all nodes: a 'reduceGet' call.
		- reduceGet is propagated down to all threads and objects
			using a variant of AssignVecMsg
			- The usual 'get' function extracts a value, but
			instead of dumping it into the queue, it sends it
			somehow into the reduce function.
		- Reduce function operates on values from objects on each thread
		- Same reduce function operates on values from each thread.
		- The reduced value is put at a single location on the node
		- This value is propagated to master node, on the lines of
			the usual 'get' call. One value per node.
			- In principle each node could accumulate values from
			all other nodes, and avoid consolidation on master.
		- Same reduce function reduces values from different nodes.
	- The single reduced value is now propagated to all nodes in a 'set'
		command.
		- May need to have a specialization of 'set' as well to 
		deal with assignments to Element fields.


Two key concepts:
	- A way to reduce return values from many threads and objects
	- A way to direct assignments to Element fields like dimension, name.

============================================================================
31 Jan 2011
Outline of reduce operation approach, revisited.

- Data structures:
	- A reduceQ[numThreads] on each node, holding Reduce entries:
	- A ReduceFinfo that generates the type-specified Reduce class,
		as a ReduceBase ptr.
		- Takes the Get fid from the reduced object to set up the
			Reduce object.
		- Digests the completed ReduceBase ptr to update internals.
		- This is located on a distinct object from the one to be 
			reduced, and it can operate on any field of the correct
			type on any object.
	- A Reduce class with the following API:
		- Derived from Reduce base class.
		- Created using the regular 'get' operation.
		- Has to hold the typed Get func for the object.
		- Has to generically call this func internally using a
			ReduceBase ptr and an arg for the object.
		- Op to scan through vector of self entries, keeping
			internal tally of reduced value(s)
			- Or return a ptr to the reduced data, also used below
		- Pass size and data so that reduceQ can call MPI_allgather(?)
		- Generate a data transfer component?
		- Scan through MPI_allgather data for final reduce
	- A way to harvest reduced data and dispatch using msg.send call.
	- A ReduceMsg that has a special exec function. I would prefer it not
		depend on any new msg type, so we can deploy to any target.
		Otherwise make it like AssignVec.
- System initiates call through the usual 'Set/Get' mechanism
	- On each thread, in ReduceMsg::exec, a new Reduce is appended to the 
		ReduceQ.
	- The Reduce digests input from each object on its thread.
	- During or after the Barrier, on a single thread, all the Reduce
		entries are digested. One Reduce takes each of the others
		as arguments and reduces them.
	- We now have a single composite Reduce per node. If we're within
		the Barrier we could do an MPI_allgather. If not we could
		send the Reduced data to the master node, like a getVec call.
		Options:
		- Barrier/MPI_allgather: We now complete the Reduce operation by
			doing a 'set' call. This makes the whole Reduce process
			a bit like message passing.
		- Barrier/MPI_allgather: We complete the Reduce operation by
			sending data back to Shell, like a 'get' call.
			Here we would want to do something else with the data,
			typically with a script. So the Reduce op is more like
			a ReduceGet.
			- Doesn't work because data takes arbitrary types.
			Much better to dump it into an Element that can be
			interrogated through the regular interfaces.
		- Barrier/MPI_allgather: The Reduce operation is completed
			by a 'set' call on the original Element(s). If the
			Reduce operation is provided by the Element it can
			be done internally.
		- post-Barrier on thread 1: First two options above are OK.
			Third option is dubious because of threading.
		- Convert the whole Reduce operation to a Msg.
			This is great conceptually, but how do we 'get' the
			reduced value into the script for once-off operations?
			- Make the target of the Msg be the Reduce class?

Summary of user view of the process:
	- There is always a dest object for the Reduce operation. This dest
	object is a regular class and among its other attributes, defines the
	Reduce op. So we can build up a set of reduce operators and use them
	in different contexts. Efields come to mind.
	- getReduce does a once-off reduction but not into the Shell, instead
	into the Reduce object.
	- There are also message Reduce options where the operation happens
	periodically, or is initiated by the target Reduce object.
	- For the FieldDataHandler, we just have a Reduce operation within
	the Handler itself. So Reduce looks like one of the Finfo types?

============================================================================
1 Feb 2011
Skeleton code now taking shape.

Data structures:
- ReduceFinfo which stores the digestFunc from its parent class.
	The digestFunc takes the final ReduceBase object and incorporates it
	into internal fields.
	API:
	ReduceFinfo< T, F, R >( name, doc, digestFunc )
	ReduceBase* makeReduce( const OpFunc* f )
		Creates a new specific Reduce object, does typechecking with
		the OpFunc to be sure that the reduced variable(s) are OK.
	digestReduce( const ReduceBase* r, char* object)
		Executes the digestFunc.
	Creates a SrcFinfo to trigger the reduce call.
+ ReduceBase which defines the API for the specific Reduce subclasses
	primaryReduce( Element*, DataId ) // reduces from original objects
	secondaryReduce( const Reduce* other ) // Reduces from other ReduceBases
+ Reduce subclasses store all internal variables used during reduction.
	Also store a pointer to the function that extracts the original
	variable from objects.
- A GetOpFunc::reduceOp( const Eref& e ) const;
	This additional templated func in GetOpFunc and related subclasses
	returns the reduced variable when given the object.
- ReduceMsg 
	- does a special set of ops on all targets, 
	- similar iteration as AssignMsgVec
	- stores ReduceFinfoBase ptr which it needs to make the Reduce object

Call sequence:
- Set up originating class with ReduceFinfo.
- Create a ReduceMsg from originator object to array of reducees
	This ReduceMsg has the originating ReduceFinfo ptr.
- Trigger call along this Msg.
- Usual dispatch of call through messaging to all nodes and threads.
- ReduceMsg::exec, called in Phase 2 and 3 as usual.
	Creates a Reduce instance for each thread, and bungs it onto the 
		reduceQ. At this time it also puts the originator and its Finfo
	Scans through all reducees, doing the primaryReduce
- Clear Reduce queue. This is in Barrier 3
	- Scan through the queue, merging entries
	- Send out MPI_Allgather or MPI_Gather 
	- Scan through node returns, merging entries
	- Use the originator ptr and rfb to do digestReduce.

- Eliminate ReduceMsg if this was a once-off op.

Astonishingly, this all takes place within a single cycle.

Now, let's see if it works.

Implementing: ReduceBase.h. Finally compiles in and clears
unit tests. It isn't being used yet.
Checkin 2439.

Some headaches with type matching in ReduceFinfo.h
============================================================================
2 Feb 2011
Implemented a first pass of ReduceFinfo as well. The whole thing compiles
and clears the non-parallel unit tests. Checkin 2440.
Still to put in:
	* Creation of ReduceMsg
	* Connecting up to ReduceMsg
	* Setting up ReduceQ
	* Setting up stuff to do in Barrier 3 to clear ReduceQ
How to test:
	* Do a stats analysis on a table using Reduce.
	- Count synapses.

Started on creation of ReduceMsg: both the class definition and the hook from
Shell for its creation from the script. 
============================================================================
3 Feb 2011
Implemented ReduceMsg, ReduceQ, got it to mesh together and compile.
Checkin 2441.
============================================================================
4 Feb 2011
Added Qinfo::clearReduceQ to Clock::checkProcState, which is the 
function executed during Barrier3. This compiles easily and clears
unit tests, still not testing its functionality though. Checkin 2442.

Now to set up MPI operations in ReduceBase::reduceNodes.
============================================================================
5 Feb 2011

Code now in place. The MPI code isn't yet tested, will begin with doing the
reduce ops on a single node. Still to do are the unit tests with suitable
objects. Checkin 2443.

Added in Stats object to test Reduce ops. This revealed lots of bugs in 
the other Reduce code. Now compiles and clears unit tests, still to
test the Reduce ops themselves. Checkin 2444.

Built the test function in testBuiltins.cpp. This 
successfully triggers activity in ReduceMsg.cpp but clearReduceQ
is not being called.

Completed the unit test, and after some debugging it finally works on 1 node
with a selection of 1 to 11 threads. Checkin 2445.

Now to implement ReduceMaxUint for the synapse array sizes.

Working on it. Templated a ReduceMax class to do the calculation. Plugged
it into Shell. 

This led to all sorts of compilation headaches. Rather foolishly I decided to
simply remove Shell from header.h to resolve these. This required update of
nearly 50 files. Finally compiles and clears unit tests. While at it I also
split off the Set/Get code from the Shell.cpp, into a separate ShellSetGet.cpp
file. Checkin 2446.

Next to set up automatic fixing array sizes.

============================================================================
6 Feb 2011
Some cleanups in printf debugging lines. Checkin 2447.

Now into the core code for updating Data Handlers. Unit test in 
testAsync.cpp:: testSetGetSynapse. 
compiles but doesn't clear tests. Checkin 2448.

This didn't work either, since the test is before the ProcessLoop is running.
I need another test. Checkin 2449.

Implemented it in testShell. Getting close. Now need to
get the reduced value back into the Element->DataHandler from
the Shell. Note that the value has to go into 
FieldDataHandlerBase::fieldDimension_
using the call FieldDataHandlerBase::setFieldDimension.

Looking more closely at what is needed to get the value to the right place.
First: How does the synapse array size get set?
- Directly by assigning setNumSynapses, on the parent.
- Indirectly by randomConnect and other functions that operate on the
	FieldDataHandler belonging to the synapse. These functions call 
	FieldDataHandlerBase::setFieldArraySize which calls the setNumField
	function with the parentDataHandler pointer. The setNumField
	function then calls the setNumField function pointer, which in this
	case is again setNumSynapses.

- Options: 
	- Add setFieldDimension as a func passed into the FieldElementFinfo
		Will need to change call args for identifying field to
		Shell::doSyncFieldArray, to pass in FieldElementFinfo as well.
		We already need to pass in the setNumSynapse fid.
	- Add a ptr to the parentDataHandler to indicate fields.
		Bad idea, we may have many such fields.
		
The FieldElementFinfo which we refer to for setNumSynapses,
in its postCreationFunc, creates the Element and the FieldDataHandler
for the synapses. We want these to do the setFieldDimension.
In many cases the child Element id is 1 more than the parent, but not
if the parent has multiple FieldDatas.
============================================================================
8 Feb 2011
The FieldDataHandler is the one that needs to keep track of current indexing,
so we'll put the updating operations in it. 
We need to get from it: size of array
We need to put in it: Dimension of FieldDataHandler.
	The first is a user-accessible field.
	The second should be hidden.
Shell dealings with dimension are a bit unusual for the reduce operations.
Normally we'd expect the destination element to be the end of the story, but
here we need to go back and put the reduced value in target Elements.

Seems to clear this issue, but now crashes later on in an odd place.
Valgrind isn't able to diagnose.
Was stuck for a while, but gdb 'watch' function saves the day:

Breakpoint 1, init (argc=1, argv=0x7fffffffe7f8) at main.cpp:187
187             s->setShellElement( shelle );
(gdb) p s
$1 = (Shell *) 0x82da98
(gdb) p shelle
$2 = (Element *) 0x82d820
(gdb) p &(s->shelle_)
$3 = (Element **) 0x82da98
(gdb) watch *0x82da98
Hardware watchpoint 2: *0x82da98

This ran at full speed, and found the problem in the destructor for
ReduceMsg, which would have been very hard to diagnose any other way.

May be running into a bad fallback when creating message manager.
The msg manager for the ReduceMsg seems to have the same ptr as the Shell.

This has turned into a big mess, since we need the MsgManagers to set up any
messages, but the Clock sets up tick children and each wants to set their
own child ids. See grunge in main.cpp around line 170.

Finally got it to work with a really messy little hack to put the 
initMsgManagers function within the MsgManager::addMsg function, and test
for the mid coming when the Tick is connected to its parent Clock. There are
other ways to do the same thing, all unpleasant. Now clears all unit tests,
for 1 to 11 cores. Checkin 2450.

So now we're clear to go on to multinode tests. 

The base MPI compile took a bit of fixing. Then it fails to clear
unit tests.
============================================================================
9 Feb 2011
A look at the assertion failure revealed that the stats total was twice
what it should be. This suggested a simple fix: the
ReduceBase::reduceNodes() function was calling tertiary reduce on the
node 0 object as well as other nodes, thus doubling its value. Fixed
range of iteration, and it now clears single node unit tests.
Checkin 2451.

Finally onto multinode tests. Cleared the earlier crashes due to attempts
to access objects that were decomposed off the master node. In doing so
reconfirmed that the GetVec function works, and works across nodes. Now
stuck in the interesting and crucial place where I test internode messaging
using each of the message types. Checkin 2452.

Struggling through the messaging on 2 nodes, 1 thread. 
- On node 1, the target indices and function for target b2 seem OK.
- Qinfo reports sensible stuff on each node.
- The process call on Arith seem to work OK, but isn't picking up the signal.

============================================================================
10 Feb 2011.

- Did extensive printf debugging. Looks like the SingleMsg simply isn't
	calling the Arith::arg3() assignment operation.
- The a1->a2 Q entry isn't invoked on node0, but is invoked on node 1.

Looked at the calling code. No obvious issue. Got to trace.
Perhaps nodes other than node 0 are unable to send regular message stuff
	back. Note that the Get does work across nodes, but there the
	Q entry is inserted directly.
I tried monitoring data transfers between nodes. Clearly there are data
transfers happening. Why should the SingleMsg data not be there? Why should
the OneToAllMsg data get scrambled?

Will need to write a func to scan the Qs for Q entries destined
for the suspect msgs, and display contents.

I think there is a problem in Qinfo::reportQ, it isn't reporting 
the mpiQs.
============================================================================
11 Feb 2011.
Fixed bug in reportQ.

Finally can see what is going on. The bad messages originate on node 1 and
are OK in the originating inQ:
1: Reporting inQ[1]. threads=1, sizes: 17
Q::MsgId = 19, FuncId = 17, srcIndex = 3:0, size = 8, isForward = 1, e1 = a1, e2 = a2
Q::MsgId = 20, FuncId = 17, srcIndex = 2:0, size = 8, isForward = 1, e1 = b1, e2 = b2
Q::MsgId = 21, FuncId = 17, srcIndex = 2:0, size = 8, isForward = 1, e1 = c1, e2 = c2
...

But when it gets to node 0 the first two are bad:
0: Reporting mpiInQ. Size = 544
Q::MsgId = 19, FuncId = 17, srcIndex = 3:0, size = 8, isForward = 1, e1 = root, e2 = e2
Q::MsgId = 20, FuncId = 17, srcIndex = 2:0, size = 8, isForward = 1, e1 = root, e2 = b1
Q::MsgId = 21, FuncId = 17, srcIndex = 2:0, size = 8, isForward = 1, e1 = c1, e2 = c2
...
The remaining ones are OK.

Is someone altering the Msg pointed to by MsgId = 19 and 20?
Note that the other appear OK.

How are MsgIds guaranteed to be the same across different nodes?

============================================================================
12 Feb 2011

Confirmed that MsgId misalignment is the root of all bugs. Stepped through the
testShell::testShellAddMsg and found that the first two messages (to a2 and b2)
have different msgIds for node0 and node1. The rest are identical,
thanks to the garbage collection I've put in for msgIds.

Will need to go through the message assignment logic. 
- Unit tests do a lot of node0 local stuff, and scramble MsgIds.
- In Elements and Ids, I assign Ids from the master node. Should I do for MsgId?
- Alternative is to send out a msgId cleaning function after unit tests.

Decided: 
	1. Will have a msgId cleaning function called when unit tests are done.
	2. Shell::doAddMsg will generate the msgIds, just as the doCreate does.
		These msgIds will be propagated to all nodes.
	3. At present no other function is allowed to make messages.

Minor issue with this neat resolution is that the Shell::handleAddMsg already
has 5 message arguments, the max. Will need to consolidate.

Looking at the code for the add message, it is clear that a lot of the error
checking should be shifted to the top level doAddMsg function.

Stages
* Make 6-argument EpFunc, SrcFinfo, SetGet
* Place into code, compile
x Move error checking up into doAddMsg.
* do MsgId generation in doAddMsg after assorted checks.
* Make sure that the assignment Mid is set up first and correctly to 1.
* Alter handleAddMsg and innerAddMsg 
* Alter Msg::msg to take the MsgId argument.
* Alter Shell::AddMasterMsg to pass in a msgId for the new msgs.
* Alter Shell::handleUseClock to pass in a msgId for the new msgs.
* Msg id handling on other nodes will lag, so new msg creation funcs coming in 
	to worker nodes will need to expand msgids there.
- Do a valgrind to see where things are stuck.


Got 6-arg funcs in, now code compiles. Checkin 2455.

Trying to get it to run. Clears most tests but fails in 
processTests:testBuiltinsProcess:testGetMsg.
Since I have so many changes done I'll check this in. Checkin 2457.

Gdb is having trouble here. 
With some printf debugging I get:
0.0: reinit[0] binding = (0,26)
0.0: reinit[0] binding = (0,19)
0.1: reinit[0] binding = (0,26)

so we have a problem with the mids again.
Turned out that the Shell.cpp:handleUseClock had not been fixed to use
Msg::nextMsgId, and was just passing in 0.
Now it is OK and clears unit tests:
0.0: reinit[0] binding = (122,19)
0.1: reinit[0] binding = (131,26)
0.1: reinit[0] binding = (122,19)

Checkin 2458.
============================================================================
13 Feb 2011
Now running tests with MPI. As usual, using
mpirun -np 2 -c 1 
to avoid any multithreading issues.

It first complains about an apparent double deletion of an 
Element in testShell.cpp:325, which is the testCopy function.
	Warning: Id::destroy: 94 already zeroed
For this lets check the assignment of Ids.

Then it dies a few tests later.

Used printf debugging to understand the problem with 
	Warning: Id::destroy: 94 already zeroed.

Get (for 2 nodes):
Id::bindIdToElement 'f4b' = 94
Id::destroy 'f4b' = 94
Warning: Id::destroy: 94 already zeroed
Id::bindIdToElement 'f4b' = 94
Id::destroy 'f4b' = 94
Warning: Id::destroy: 94 already zeroed



Surprisingly, it is not double deletion.
Someone is stepping on it? Will have to put a 'watch' on the memory location.
Tried it, doesn't work for mpi.
Recoded to give the vector a reserve capacity. Will try a 'watch' on the
reserved location.

Did so. Worked. Showed something that I should have seen from the printf
listing above: it is indeed double deletion. The id 94 is being deleted
twice, the first time fine and the second time is caught.

Checked when the two deletions occur. Printfs on the copy command show that
the copy of id 94 is 98. Surprisingly, the two deletions of 94 occur 
immediately after each other, and have nothing to do with the deletion of
98. Does the childlist duplicate an entry?

Here is a new trace using printf debugging from the Neutral::destroy command:
	Id::bindIdToElement 'f1' = 89
	Id::bindIdToElement 'f2a' = 90
	Id::bindIdToElement 'f2b' = 91
	Id::bindIdToElement 'f3' = 92
	Id::bindIdToElement 'f4a' = 93
	Id::bindIdToElement 'f4b' = 94
	Id::bindIdToElement 'f2a' = 95
	1: Copy: orig= 90, newParent = 0, newElm = 95
	Id::bindIdToElement 'f3' = 96
	1: Copy: orig= 92, newParent = 95, newElm = 96
	Id::bindIdToElement 'f4a' = 97
	1: Copy: orig= 93, newParent = 96, newElm = 97
	Id::bindIdToElement 'f4b' = 98
	1: Copy: orig= 94, newParent = 96, newElm = 98
*	Neutral::destroy: id = 89, name = f1, numDescendants = 6
	Id::destroy 'f1' = 89
	Id::destroy 'f2a' = 90
*	Id::destroy 'f3' = 92
	Id::destroy 'f4a' = 93
	Id::destroy 'f4b' = 94
	Warning: Id::destroy: 94 already zeroed
	Neutral::destroy: id = 95, name = TheElephantsAreLoose, numDescendants = 4
	Id::destroy 'TheElephantsAreLoose' = 95
	Id::destroy 'f3' = 96
	Id::destroy 'f4a' = 97
	Id::destroy 'f4b' = 98
	
Note specially what happens at the starred lines: There are 6 descendants
(which is correct), but the descendant 91: f2b has gone missing. Why has
f2b become disconnected?

Ah, one interesting and important point: I get the same error message
and diagnostics on a single node, with 1 or 2 threads. Same symptoms. So
it isn't related to the messaging. Good, I can separate out the issues.
Let me first put in a few assertions to make sure that I catch this bug
in the future.

These assertions show that the problem is either with Neutral::buildTree or
with the Neutral::children() function itself.

Since this has turned out to be a potential error, I've made a separate unit
test to these ops: testShell:testChildren(). Now it clears this test: it turns
out the tree is done depth-first. The original bug with the warning about
the descendant remains. Checkin 2459.

Going back to the problem with the deletes. The problem does NOT crop up in
testChildren, which sets up exactly the same little tree. Some funny
scrambling seems to happen when copying.

This gets still more confusing. In testCopy, even before the copy is done,
the second child of f1 (f2b) has gone missing and is replaced by the id of
f4b. The same sequence of creations clears the testChildren.

Checked if the precise location in the elements() vector matters. Doesn't.
Checked with valgrind. Nothing happens.

Clue: When I move the same testChildren() function after testMove(), 
the problem appears in testChildren. Move may be doing bad things.
Confirmed: when testChildren is put just before testMove() it is OK. Run just
afterwards and it fails. Nasty.
Checked this in as 2460 as a reference for this bug, before I begin to
tighten up the testMove test.

Changed the testMove, but again it doesn't fail on its own, but instead causes
downstream errors in testCopy. This is another interesting error, testCopy
goes into an infinite loop, which depends on testMove being there ahead of it.

Fixed a little shortcut I had made in Shell::handleMove, where I reused the
just-deleted MsgId. Now I generate a new MsgId for the new parent->child msg.
Things now seem to work. Perhaps the reused MsgId was also put into the garbage
handler for Msgs, and then got doubly assigned.

Checkin 2461.

Cleaned up some printf debugging.  Checkin 2462. 

Now back to mpi test. Still fails, as far as I can make out it is in the
same testShell.cpp:testShellAddMsg. Seems to die in just making the
test OneToOne msg, with a assertion failure in Msg.cpp on node1.
Looked at it.  This assertion failure is because the remote-assigned 
messages are likely to be bigger than the already allocated msg vec size.
Need to provide a cleaner way to fill up.

With this fix I now have it fail a bit further along, while testing
the results of the messaging. Progress!

Further good news: The output matches for cases a through d,
representing Single, OneToAll, OneToOne and Diagonal msgs. The
Sparse msg has failed on the off-node portion.

Put in some printf debugging, result is very simple: there are no entries
in the SparseMsg table on node 1.

More debugging, turns out that the values never get set. The MsgManager::getMid
call returns a badMid value so the setEntry call never finds the associated
Msg. Need to figure out how the mid value gets set.

It is set when the message is created. For some reason we have different
entries in node0 and node1. But the value assigned seems OK.

We have a discrepancy in the data handler for the SparseMsg Element.
It claims to be global (from its isGlobal function), but has 3 entries on 
node1 and 5 on node0.

What should it do?

The data handler for the Manager Element of any message contains just the MsgId.
The different Manager Elements each hold arrays of these MsgIds.
There is a static vector of unsigned ints, Msg::lookUpDataId_, indexed by 
msgid, to look up the dataIds for each msg. So:
Msg to DataId: Msg::lookUpDataId_[msg->msgid]

object to msg: Msg::safeGetMsg( getMid() );
where the getMid function just returns the mid_ value of the MsgManager.
Then the derived MsgManager, such as SparseMsgWrapper, typecasts the Msg to
the appropriate type and does stuff with it.

For this to work, the object indexing and msg indexing must be identical
across nodes. Msg Indexing has just been fixed up after a lot of effort.
Looks like the object indexing isn't consistent. By making temporary 
SparseMsgs during unit tests in single-node mode, we would have added
extra entries. 

How does this mess things up? The DataId that looks up the MsgManager object
comes from the Msg::lookupDataId of the master node. This is misaligned if
the master node has been doing other stuff.

How to fix?
- Eliminate lookupDataId and all data in the Msgmanager, and just have them
	a zero dim case with a virtual index by mid. It converts this to the
	appropriate type.
	In other words their DataId.data is just the MsgId.
	- This fails as soon as we want to store extra information about each
	Msg, such as the random seed or the connectivity info.
	- But the info could just as well be stored in the Msg itself. So the
	typecasting trick would work.
	- Will need to make a new class of DataHandler, MsgDataHandler, to
	do the nasty lookup.
	- There is some pending oddness about how to deal with iteration.
		- Successive MsgIds may belong to different Msg types, so 
		ideally we would like to iterate only through msgs of the 
		correct type. We could go back to an auxiliary array but would
		return to issues of sync between nodes.
		- A global vector of MsgTypeId[ MsgId ] could address this.
		Still will need to know which type we are at the DataHandler 
		level.
	- Related issue is how to prevent attempts to look up an arbitrary
		DataId/MsgId index into a specific MsgDataHandler. 
		Should be able to return a dummy entry by doing the typecast.


Starting on this. Lots to do.
	+ Arrange to call Msg::initMsgManagers at the right time.
	+ Fix initMsgManagers so that the new ids are adopted by the parent.
	+ Merge in Wrapper and Msg functions for all msgs
	+ eliminate MsgManager stuff from makefile
============================================================================
14 Feb 2011

Setting up initMsgManagers. Because we have a simpler interface, 
I don't need to do the earlier acrobatics to fit the MsgManagers 
in after the Clock and Tick are defined but before any messages 
are sent.

Done the first pass coding, now to compile this mess.
Compiles through basecode directory. Time to checkin.

Now compiles through but doesn't link. Checkin 2464.

Now compiles and links but doesn't run. Checkin 2466.

The problem is that when we try to create the MsgManager Elements, there is
an automatic allocation of the Msg. This causes problems with the basic
constructors.
Let's instead create some dummy field here to keep Dinfo happy, but never use 
it.
That works. I can now go back and get rid of the silly null constructors for
Msgs. Checkin 2467.

Replaced the usual Element constructor with one that explicitly takes
DataHandler argument. With this, I can pass in a MsgDataHandler. Now the
unit tests clear. Checkin 2468.

Cleaned out the null constructors for the various Msg subclasses. 
Now it compiles, clears unit tests, and also clears the messaging tests in
MPI mode. Dies shortly afterward, apparently in testCopyMsgOps.
Checkin 2469.

The problem isn't in execution, it is the copy operation itself.
The target element isn't found in 
ShellCopy::innerCopyMsgs, which dies around line 94.

With some printf debugging it seems that the copy tree is missing one pair
on node 1. A smaller earlier copy went fine.

[We're going to have trouble with copy for Elements like enzymes which make
their own children.]

Looked at the previous function where the copy tree is built. 
ShellCopy:InnerCopyElements

An error there:
1: ice, pa = 0, pair= (129,140)
1: ice, pa = 140, pair= (130,141)
1: ice, pa = 140, pair= (131,142)
1: ice, pa = 140, pair= (132,143)
1: ice, pa = 140, pair= (133,144)
1: ice, pa = 140, pair= (134,145)
1: ice, pa = 140, pair= (139,146)
1: ice, pa = 140, pair= (136,147)
1: ice, pa = 140, pair= (137,148)
1: ice, pa = 140, pair= (138,149)
1: ice, pa = 140, pair= (139,150)

(the node0 cases are OK).
Note how the 6th one has a jump in the first entry. It is also repeated at
the last entry.
============================================================================
15 Feb 2011
Some more printf debugging. The original (source) tree of Elements itself isn't 
set up correctly on node 1.

1: handleCopy: ntree= 11
(0, 129) pa
(1, 130) a1
(2, 131) a2
(3, 132) b1
(4, 133) b2
(5, 134) c1
(6, 139) e2
(7, 136) d1
(8, 137) d2
(9, 138) e1
(10, 139) e2

Look at entries 6 and 10. Entry 6 should be c2, #135.
The c1->c2 message is a OneToOne.
All the subsequent Elements are children of pa.

Checked if this oddity is dependent on the messages set up in the test.
Yes, it is. I commented out the messages and c2 reappeared on the list.
The Copy operation still failed, though. If I comment out the SparseMsg
set up as well then the copy operation is happy. A bit more compiling and
commenting later, it seems that if any message is there at all, the Copy
operation fails. Fails at the usual place, innerCopyMsgs, when it can't
find the original Element at either end of the message being copied.
More printfs show that the message in question is not garbled, but misplaced:

1: innerCopyMsg orig = (130, a1), e1 = (0, root), e2 = (140, pa2), fid = 15, mid = 36
The fid and mid are OK, but the msg contents differ from those on node 0:
0: innerCopyMsg orig = (130, a1), e1 = (130, a1), e2 = (131, a2), fid = 15, mid = 36

The erroneous msg on node 1 is the one created by the copy command itself 
for the base of the newly-formed tree:
Id pa2 = shell->doCopy( pa, Id(), "pa2", 1, 0 );
This would use the Shell::adopt function which uses the risky Msg::nextMsgId
function on its own. What has happened seems to be that this has been done
before the Shell::doAddMsg function has reached node 1. The doAddMsg
function generates the msgId on the master node.

More printf debugging.

I think the issue isn't ordering, it is just that when a msgId is claimed
by the master node, the local node Msg garbage collector isn't informed 
and it is still able to generate the same msg entry from the garbage list. 
Maybe should abolish the garbage collection for msg ids.

To be strictly correct with regards to Ids, would have to build the entire 
copied structure on the master node, then transmit the Element and msg Ids over.
This will have its own complexities.

Got rid of MsgId garbage collection. Now it clears the unit test for 
creation of the copy, as well as the messaging in copies.

Dies a bit later though, in testScheduling.
Checkin 2470.

Cleaned out a whole lot of printf debugging messages. Checkin 2471

It goes OK to the same point for 1,2,3,4 nodes. I've incorporated the 
local count of synapses into the unit tests. Oddly for 8 nodes it fails
with the assertions on biggest field array size. 

For the smaller number of nodes it fails when checking weights. I suspect
it is just an array misalignment.

Checked, not an obvious aligmnment issue. Here is some output 
from  mpirun -np 2 ./moose -c 1
in testScheduling.cpp:testMultiNodeIntFireNetwork.

Got wt = 0.00347109, correct = 0.00347109
Got wt = 0.0106652, correct = 0
Got wt = 0.0191519, correct = 0
Got wt = 0, correct = 0
Got wt = 0, correct = 0
Got wt = 0.00509742, correct = 0

Very odd. Why is the 'correct' vector, which is something we set up
directly in the C++code, going to zero?

Now it transpires that the vector has already got the zeros. Should track.

============================================================================
16 Feb 2011
I've gotten to an unpleasant random bug point. The program crashes with 
different error messages at different points. 
Single thread: never seems to crash.
2 threads: Crashes in one of two possible places:
	- One place is triggered by getVec. The immediate problem
	is a bad value for the MsgId in hackForSendTo
	- Second place is digesting the output of the getVec, when the # of
	synapses are being counted. Printf debug shows that most of the
	returned vec is empty, though the size is right.

Checked, it has the same symptoms when recompiled without MPI.
Is it some earlier funtion leaving droppings?
	
============================================================================
17 Feb 2011
The testMultiNodeIntFireNetwork function gets called twice, once for the 
regular scheduling, and once for MPI. The program crashes with either or both
present.
If both are eliminated, the program sometimes makes it through to the end,
but on some runs it crashes.

============================================================================
18 Feb 2011
Checked whether syncFieldArraySize() is the problem. Commented it out,
reran. Still dies, both places as above.

============================================================================
20 Feb 2011
Commented out all the IntFireNetwork runs. 
Now it clears the run with two threads, usually, but croaks (usually)
with more than that.

Commented out the testSchedulingProcess and the
testMpiScheduling from the main. Now it clears all combinations...
Actually no, it still manages to fail sometimes.

============================================================================
21 Feb.
Qinfo ptr is bad in Qinfo::readQ->readBuf->hackForSendTo.
Some garbage is coming out of the queue.
Possibly even out of the Qvec argument of readBuf.

============================================================================
22 Feb
Some places to look:
- AssignmentMsg.cpp:sendAckBack. This seems to behave properly in using the
addToQbackward function using a sensible procinfo.
- Qinfo::hackForSendTo. Never did like this, maybe temporarily put the
	tgt id in the Qinfo or somehow add it to the args.

Some debug/tracing later, there seems to be a problem in the contents of the
Qvec. Did the Reduce opertion put junk there?

Checked insertion of ops into ReduceQ. That is OK, happens on separate
index.

As it turns out, at this point the Shell:;reduceInt is commented out...
in FieldDataHandlerBase::syncFieldArraySize()
was this the non-functional version?
============================================================================
23 Feb
Yes, this was the version that didn't work. The actual logic goes through
ReduceMsg::exec.

Looking in Qinfo::clearReduceQ. Things look mostly OK, exept that the
address of the two ReduceQ entries are in completely different ranges. 
Something odd happening here.

Went through trying to eliminate all calls to the Reduce functionality.
Got rid of the call to reduceStats in testBuiltins.cpp, and the call to 
shell::doSyncDataHandler in testSyncSynapseSize() in testShell.cpp.
I still have problems.

Now it looks like I may have to revert to an earlier version of MOOSE
to figure out what has broken.

============================================================================
24 Feb
Checkin 2478 as a reference commit to return to once the threading issue is
identified. The last thread-clean version seems to have been 2450.

Checked this out. Compiled it, confirmed it runs well with 1 to 11 cores.
Now to figure out what fundamental things have changed.

Process loop:
	Only shifted around one commented out line.
Shell calls: Seems reasonable. testShell.cpp has been massively changed.
	But we still get errors in other tests even if it is not called.
Set/Get messaging: Seems OK. Only changes are in Element interface.
OpFuncs and EpFuncs and SetGet.h.: These have had an extra template with 
	up to 6 args. Seems OK, but this is a particularly sensitive section.
	Are there any I'm missing? Also checked Conv.h, that is clean.
Finfos.
	SrcFinfo just has a new template for 6 args. Looks OK. Others seem
		unchanged. None of the .cpps have changed.
Handlers: No change, except the new MsgDataHandler.
Qinfo: Looks OK. Almost all changes are to reportQ
Msgs:  .hs look sensible, mostly the simple API changes.
	.cpps: Msg needs checking. Seems Ok.
		SingleMsg:106c234, 108c236: OK.
Scheduling: All changes seem to be in testScheduling.cpp
[Reduce operation]. I've looked at this a lot already.

Summary: No obvious slip-up in the new code.

Next: Systematically block out the tests from the main, to see if the problem
can be put down to a subset of them.

============================================================================
25 Feb
Changes made after 2450:
2451: Fix ReduceBase::reduceNodes() to tertiary reduce not on starting node.
2452: Fixing multinode tests, using GetVec instead of direct ptr access.
	Here I realized issue with MsgId alignment.
2455: 6 arg funcs implemented
2457: Working through unit tests
2458: Silly bugfix to use Msg::nextMsgId
2459: Various printf debugs, added a unit test testShell::testChildren
2460: Debugging sequence of calling testMove.
2461: Fixed reuse of MsgIds
2462: cleaned up printf debugging
	Here I realized issue with MsgDataHandlers, and redid using direct
	MsgId as DataId for the handlers.
2464-2466: compilation, linking
2467: Got rid of null constructor for Msg.
2468: Making MsgDataHandlers. This version clears unit tests, see if it
	handles multiple cores.
2469: Works through the first MPI unit tests
2470: Got rid of MsgId garbage collection.
2471: Got rid of some printf debugging. Clears some unit tests. This is
	where the threading bugs became obvious.
2478: Reference commit, still has the threading bugs.

Looked closely at the garbage collection. Doesn't seem like a likely problem.
Should simply go back and find where the problems start.
============================================================================
26 Feb
Revisited checkin 2468. This usually runs and is OK with as many as 11 cores,
but does fail sometimes. When it does so the symptoms are similar to those
we saw with 2478. I also saw a failure in the message test following a copy.
One difference is that in the message lookup failures, it seems like the
Qinfo isn't bad, which it was in the 2478 errors.

Some numbers: v2450 ran 12 times with 11 threads without error
v2469 ran 14 times with 11 threads, 2 errors.
v2470 ran 18 times with 11 threads. about 4 errors that look like 
	clock startup failures, and 2 that look like a message problem
Current (2478): fails every time with 11 threads. 
	About half with the NUM_TOT_SYN assertion failure, and half before that
	with the messaging failure.

Cleaned up printf debugging from 2470. 
Out of 24 trials (moose -c 11) we have:
- 1 error: testShell.cpp:795 in testShellAddMsg.
- 1 error: testShell.cpp:768 in testShellAddMsg. These are both after a getVec
- 3 error: Msg.cpp: getMsg( MsgId ). Another bad msgId.
- 1 error: Simple segmentation fault. No idea where.

I wonder if getVec is the root cause of it all. Let's redo the tests
with a single thread, moose -c 1. Out of 10 runs,
- 1 error: testScheduling.cpp:516: testThreadIntFireNetwork (after a setVec)
I don't understand this. Without threads, it should be deterministic. 

Next: work on the stuff from v2470. 
	- Put in a loop to test SetVec and GetVec with an eye to their
	reliability
	- Consider if there is a better way to do the return from Get and 
	GetVec than the hack. Maybe even expand Qinfo.
============================================================================
27 Feb
Start with a new branch using rev 2470. 
I want to have the threading stuff completely clean before going 
on to the internode stuff.

I've called this branch threadMsg. It is rev 2481. Shifting this
file and other operations over there.

Fixed up error messages. running with -c 11.
in 30 cycles these are the outcomes: 
testShellAddMsg: 			4 failures
Msg.cpp m< msg_size assertion: 		8 failure
testCopyMsgOps				2 failure
quit exit				1 failure

Checkin 2482.

Attempt 1: Put in the test for 'addToStructuralQ' into 
each of the handleSet and handleGet functions. This had cascade
effects in other places to deal with the changed arguments to the
functions. 
In 38 cycles these are the outcomes:
quit exit				1 failure
Segv					2 failure
Msg.cpp m< msg_size assertion: 		1 failure
testShellAddMsg 			2 failure
testCopyMsgOps				2 failure

So it looks like this has helped significantly with the 
	Msg.cpp m < msg_size assertion issue. Other failures look about
	the same, and may well be separate issues.

Checkin 2483.
Looked for other cases where structural calls were not thread-safe.
The innerAddMsg call is protected by the Qinfo::addToStructuralQ()
except for the Shell::connectMasterMsg function, and that is called
in a single-thread region of the code anyway.

May need to have a closer look at the getVec and setVec.

Trying first to persuade the system to crash in gdb. After 10 or
so cycles it fails with the Msg.cpp error. This is
in one of the 11 worker threads, but looks like 8 other of the worker
threads have the same problem.
The Qinfo in readBuf is garbled in its msgId, funcId and srcIndex,
and has a supposed size of zero. Oddly its booleans are OK, they
usually go bad in pointer errors.
The calling test is testShellAddMsg in testShell.cpp:771, this is
a getVec call. 
I have a funny odd-number size in the buffer. I wonder if this is
permitted. Name length is not the issue.
Did a printf debug line. It happens, but rarely. One place is in 
testShell.cpp: testCopy.
Second place is in 
testShell.cpp:testCopyMsgOps.
Both times it is precisely in the shell::doCopy command.
OK, the reason may be that one of the arguments is a bool.
Do we have an alignment issue here?

Despite the odd-numbered qinfo size, the test runs completed.

Got it to fail again. This time it was again after a copy, but the
actual error happened in a getVec call.

Let's try putting in a template specialization for 
Conv< bool >
so it uses at least 4 bytes.

This does avoid the odd-number buffers.

In 40 cycles of "./moose -c 11" these are the outcomes:
testShellAddMsg:		4 failures (all off values returned by getVec)
testCopyMsgOps:			2 failures (off value returned by getVec)
Msg.cpp m < msg_.size()		1 failure (Q entry was odd sized here).
quit exit			1 failure

Well, this is strong correlative support for the idea that the odd-sized
queue entries are associated with bad data. The number of failures hasn't
changed though. It is also strong support for continuing issues with 
getVec.
Checkin 2484.

Put in a printf to report what the returned vector is like in cases
where it fails:
In 47 runs:
void testShellAddMsg(): Assertion `ret' failed.
( 3, 3 ) ( 3, 3 ) ( 3, 0 ) ( 3, 3 ) ( 3, 0 )

void testCopyMsgOps(): Assertion `ret' failed.
( 5, 5 ) ( 4, 4 ) ( 3, 2 ) ( 2, 0 ) ( 1, 1 )

void testShellAddMsg(): Assertion `ret' failed.
( 1, 0 ) ( 2, 2 ) ( 3, 3 ) ( 4, 4 ) ( 5, 5 )

void testShellAddMsg(): Assertion `ret' failed.
( 14, 14 ) ( 13, 13 ) ( 12, 0 ) ( 11, 15 ) ( 10, 10 )

void testShellAddMsg(): Assertion `ret' failed.
( 14, 0 ) ( 13, 13 ) ( 12, 12 ) ( 11, 11 ) ( 10, 10 )

testCopyMsgOps(): Assertion `ret' failed.
( 5, 5 ) ( 4, 4 ) ( 3, 3 ) ( 2, 2 ) ( 1, 5 )

Also had a several more cases where readbuf was odd sized: it was 1
in all of these cases.

So what it looks like is that the bad getVec values can be either
just zeros, or can also be scrambled indices. Also we tend to get
just one or two bad values in any given case.
Note that we are _not_ getting outrageous values that we would
expect from a bad pointer. Looks more like misplaced good values.

In the initAck/waitForAck pair on a single node with many
threads, is it possible to return before all threads are done?
If so, we may simply need to defer the ack from the getVec till
all entries are in...
============================================================================
28 Feb
Putting in an additional test on getVec to ensure all entries come in.
I wonder if it might be good to have for the regular 'get' call.
As I suspected: it tends to hang. The problem in this test case is that
retEntries is bigger than the number that are supposed to come back.
Tracked it down. The problem is that we use dataHandler::totalEntries
to get the retEntries. However, the number of used entries is frequently
smaller than totalEntries. Additionally, totalEntries itself needs a 
complicated call to fill up. So we would like a self-correcting way of 
keeping track of the arrival of all getVec values.

Options:
	- for getVec, call another function to look up # of entries on 
	each node (localEntries) and compare that with the # that have 
	already come in.
	Strictly speaking, it should be enough to do another get/return 
	sequence to guarantee that all have come in.
	This is hard to set up because we don't know what would be a suitable 
	second fid to use to send the second message.
	- Put the # of expected entries on the ack itself.
	This is certainly doable. The Assignment vec knows how many have
	gone out and can return the total. But it would expand the API.
	Also will need to subclass AssignGetVecMsg off AssignVecMsg to
	do special things in the exec function. Tedious.
	- Alter the AssignVecMsg returns so that it returns the # on each node
	as part of the returned fields. This is messy and won't add anything
	to the previous option.
	- I can set up a dummy function for the Get and GetVec functions
	to call to wind up the threads.
	- I can wait for the ack in a single-thread (barrier) region of
	execution, so that all the threads are done. Has to be barrier3
	since we need all nodes in.

Also need to do something similar for assignment msg: we really need
to guarantee that a value has come back before carrying on.


Implemented something like the last option: I made a variant on the
waitForAck function that forces another cycle through Process.

In 32 cycles of "./moose -c 11" these are the outcomes:
testShellAddMsg:		5 failures (all off values returned by getVec)
Msg.cpp m < msg_.size()		2 failure (Q entry was odd sized here).
segv				1 failure

No improvement.

Need to verify that it is indeed doing another cycle.

============================================================================
1 March
Verified: Another cycle is happening.
Furthermore, verified that the whole list gets done in the first cycle, I
don't see any exceptions.
Also managed to get the thing to fail despite the getVec having returned
all the values:

...
Shell::waitForGetAck: # getVecReturns = (0, 5, 5)
Shell::waitForGetAck: # getVecReturns = (0, 5, 5)
moose: testShell.cpp:808: void testShellAddMsg(): Assertion `ret' failed.
( 3, 3 ) ( 3, 3 ) ( 3, 3 ) ( 3, 0 ) ( 3, 0 ) Abort

So on both counts it looks like the error is not the # of data packets put in
the queue, but either in their contents, or how they are read.

OK, more clues. I replicated the segv, seems like some getBuf_ entries
have not been allocated (are 0x0). Others are fine.

Revisited the printf debug, and this time there is a clear shortfall in # of 
entries come into the getBuf_, despite the extra pthread_cond_wait. Looked
at the logic of it more closely, and there is a possible flaw. Redid.

20 tries of ./moose -c 11:
Failed exit			1 failure
segv				1 failure. Informative. Looks like
	the # of incoming entries was OK, but one was duplicated.
Another segv was confusing. Printfs all over the place.
Yet another suggests that despite all my efforts, we still have not 
	recieved all the inputs.

Implemented a -q option to get MOOSE to quit rather than go to command line
after execution. Used to run a loop to test the failures.
 
set i = 0
while ( $i < 40 )
while? echo $i
while? echo $i >> zug
while? ./moose -c 11 -q >> zug
while? @ i = $i + 1
while? end

Outcome: 
Segvs: 4
Assertions on returned getVec: 2

Modified a bit so I can track # of returned values in case segvs are due to
too few returned entries.
In this run of 40 tries, I get 4 segvs. In two of them the # of reported
acks is 15, not 5. Rest seem OK.

Did again with a cleaned up test, traps the likely segvs with an assertion.
In 40 turns there are 7 traps:

Shell::waitForGetAck: #= 4
( 0, 1) ( 1, 0x0)       ( 2, 3) ( 3, 4) 

Shell::waitForGetAck: #= 5
( 0, 3) ( 1, 1) ( 2, 2) ( 3, 0x0)       ( 4, 4) 

Shell::waitForGetAck: #= 3
( 0, 0x0)       ( 1, 4) ( 2, 0x0)       

Shell::waitForGetAck: #= 5
( 0, 1) ( 1, 2) ( 2, 0x0)       ( 3, 4) ( 4, 5) 

Shell::waitForGetAck: #= 5
( 0, 1) ( 1, 2) ( 2, 3) ( 3, 0x0)       ( 4, 5) 

Shell::waitForGetAck: #= 15
( 0, 0) ( 1, 4) ( 2, 0) ( 3, 0) ( 4, 0x0)       ( 5, 0x0)       ( 6, 0x0)       ( 7, 0x0)       ( 8, 0x0)       ( 9, 0x0)       ( 10, 0x0)      ( 11, 0x0)      ( 12, 0x0)      ( 13, 0x0)      ( 14, 0x0)      

Shell::waitForGetAck: #= 5
( 0, 0x0)       ( 1, 3) ( 2, 0) ( 3, 3) ( 4, 3) 

So that summarizes the problem. As there have been many changes, we
checkin this version. Checkin 2486.

Printing debug messagesfor each thread to ensure it goes around
event loop. It does. But! Many instances of the same
threadIndexInGroup being used more than once. So often that I 
wonder how the calculations ever work.

Clue: There are always 12 calls to eventLoopForBcast.
There are 11 threads.
But, there are a couple of other Groups. Let's print those.
No, we're always in Group 1.
Here is an example:
another Cycle in shellEventLoop
2:1  another Cycle in eventLoopForBcast
1:1  another Cycle in eventLoopForBcast
3:1  another Cycle in eventLoopForBcast
4:1  another Cycle in eventLoopForBcast
5:1  another Cycle in eventLoopForBcast
6:1  another Cycle in eventLoopForBcast
7:1  another Cycle in eventLoopForBcast
8:1  another Cycle in eventLoopForBcast
7:1  another Cycle in eventLoopForBcast
8:1  another Cycle in eventLoopForBcast
9:1  another Cycle in eventLoopForBcast
10:1  another Cycle in eventLoopForBcast

Note that here 7 and 8 are repeated twice, and 0 is missing.

OK, I see it, and also the reason why this has not worked. The flag
(Shell::anotherCycleFlag_) is set on a thread (shellEventLoop) that is
running in parallel with the eventLoop threads. It may get set before
one of the others has had time to stop at the barrier; and it may remain
set while one of the others loops around the barrier again.
I think I will have to move the test for blocking parser calls into
barrier3, which is the only place where the worker threads are cleared.

============================================================================
2 March
First: let's force it to do two cycles through the loop.

In 40 turns there are 7 fails:						# fail
ShellThreads.cpp:110: Shell::waitForGetAck():	4
Msg.cpp: m< msg_size()				3

So the numbers are unchanged. Looks like the problem isn't in handling
incoming entries, but in sending them.
Tried to look at Qinfo::reportQ(): Q is empty at the point of segv.

Could do a printf on AssignVecMsg::exec inner loop to be sure it is
calling Op for all index entries. This should be OK...

Check that Op itself does the right thing wrt sending data.

Possible issue here: OpFunc.cpp:fieldOp uses the Shell::procInfo(),
not the individual procInfos that are thread safe.

After much fiddling, I have fixed it so that the fieldOps send data back
into the thread that handles the call. It works! 
In 40 runs of ./moose -c 11 there is just 1 segv, this happens before 
any of the calls that test getVec.

The current implementation leaves the single 'get' call to be done using
the default ProcInfo for the shell. I think this too may be causing
these (rare) problems.

Also should now go back and see if the cond_wait is needed after all.
Checkin 2487.

First stop: put in an equivalent procInfo handler for regular Gets, for
testing purposes. This is just for testing purposes as it will slow down
operations in real use. There are a couple of ways to avoid the slow down
should the test work: 
	- Add ProcInfo argument to ops
	- Add a special message for single 'get' calls.

Test was good for 50 cycles, then on 51 it stalled presumably at the
automatic quit. Then it ran again for 37 cycles before stalling.
Then 68. So distinct progress but there is still a stall.

Looked at the diagnostics file. It seems to go all the way through
and stop just before printing out the note about the regression
tests.

Also got an assertion fail in testMultiNodeIntFireNetwork().
Rerunning with some printfs so I can pin down where the stall failure 
happens. This goes cleanly through 120 runs without error.

Recompiled with a new option to do unit tests with optimization.
Compiles, runs, but I don't see the occasional stall with 80 runs.

Can I now go back to multinode work?
============================================================================
3 March.
Since this has caused so much trouble, I will first set up to run some heavier
tests on the big multicore machines: gj and ghevar. If it clears all those then
proceed to MPI build. Checkin 2488.

Here is an error from gj:
17
moose: testBuiltins.cpp:307: void testGetMsg(): Assertion `numEntries == 101' failed.
Abort

But ghevar was clear for the first 40.

These tests were for 11 threads. Redoing now with 17:

ghevar: Stalled after 5, no diagnostics. Then reran and it got to 6 and stalled.
Then reran and it got to 16 and stalled. At this point I remembered that the
script dumps output to a file, checked that. Seems like the code dies at the
joinThreads in main.cpp; well after all the unit tests. Next cycle only 0,
stalled same place. Next cycle 5, stalled same place. QED.

gj: Marched through all 40 without problems. Then another set of 40 also OK.

Summary: Most of the bugs seem clear, but there is some problem with clean
closing of threads.

Looked at this, managed to trap it on ghevar once. This happens when
one of the threads has gotten into barrier1, while the rest are waiting on 
barrier3. The master thread waits in pthread_join.
Also managed to catch a crash on testBuiltins.cpp:300:testBuiltinsProcess.
which calls shell->doReinit(). It goes in Shell::waitForAck:69, which is a
pthread_cond_wait.

I'm not sure what to do with these. I tried many many more cycles to repeat
either one, and it did not oblige. 

Proceeding with MPI compilation. 
doing: mpirun -np 2 ./moose -c 1
Immediate, reproducible crash in Shell::waitForGetAck(). 
A GetVec is only seeing 2 values, and at least one is bad.
Control: works fine with ./moose -c 11 -q

Odd issue: AssignGetVecMsg is not compiled in. Maybe I should remove it, I
don't think it is actually used.

One clue: The crash in waitForGetAck expects exactly as many returns
as there are nodes. Doesn't worry about # of threads per node.

Tracked it down by multiple layers of printfs. Turns out to be a simple
'get' call on the root element (for its path). I think that each node is
sending in a response (clue above). Need to find a way to restrict such
calls so they stay on the local node.

This fixes this bug. Now the MPI stuff goes on till the old problem, where
we have to count synapses across nodes.
Checkin 2489.

Cleaned up lots of printf debugging. Checkin 2490.

Progressed a bit. Now the issue is set/get of weights into the
array of synapses. Looks like values put in are not the same as those
read out. 

fdh->syncFieldArraySize at line 887 doesnt assign field values
except on current node.

In the throes of fixing. Problem points:

Shell::doSyncDataHandler: should have different args. Did skel code
Shell::doSyncDataHandler: should call setFieldDimension on all nodes.
testScheduling.cpp:653 Putting in Shell::doSyncDataHandler.

============================================================================

4 March.
The IntFire isn't setting up a ReduceFinfo to handle the updates on the
Synapse dimension.

Tracked things down. The Shell::doSyncDataHandler directly deals with the
object, first doing the reduce, then using the returned value to set 
a new Neutral::fieldDimension value field.

Now it clears the tests for weight assignments across nodes and synapses.
Assertion failure at the output of the calculation. Checkin 2491.

A closer look at the output shows that the weights still are not right. It
just isn't doing an assertion check on them. testScheduling.cpp:700


Printing out lots of the data to show the point at which the extracted field
values disagree with what was stored. It is identical up to the halfway
point (so that would be objects decomposed onto node0). Then it starts out
at the position for the second half, with zero entries 
(hence, no values put where they should go), and when the values do start
coming they are not the ones expected there.
Suggests something bad with indexing in DataHandler. What is odd is that it
isn't symmetric: whatever wrong indexing put in a value, should also be
apparent when we read out the value. Except that the readout is indexed
by the object linear index due to the way we have each entry send out its
value with a field index.

Let's see if I have some unit tests for DataHandler node decomposition.
Nope. So setting them up in testAsync.cpp.
============================================================================

5 March
Made unit tests for OneDim and Field DataHandlers, especially with regard
to their iterators and how. Looks good. Checkin 2492.

Given that I now am sure about what goes on with these, why do we still
get bad weight sequences in the unit test testMultiNodeIntFireNetwork()?

I'll printf debug the info in the DataHandlers on each node.

Checked: The values that come into Shell::recvGet are correctly
	handled. Indices and data values match with the read-out from
	getVec. So the error is upstream of this.
Checked: The start index on the stuff coming into Shell::recvGet
	is correct. That is, there is the right gap of a number of index
	values that are empty.
Checked: The indices on stuff coming into the AssignVecMsg.
	This is in progress. Now it is not printing out node#

Found problem, probably underlies other issues. The fieldDimension has not
been set on node 1. The 'set' command needs to figure out when a field is
global even if the object is not. Tricky.

I really need to keep track of three kinds of fields:
- Regular ones, which are on the objects, which are on the elements.
	These are global or not depending on whether the DataHandler is.
- Element fields, which are on the Elements and are therefore global
	regardless of the DataHandler
	Actually such fields are already done as ElementValueFinfo.
	Need to avoid sending a response from the get operations unless they
	are on node 0. This was done in OpFunc.cpp:skipWorkerNodeGlobal,
	but that will need to be extended.
	However, do need to call the 'set' operations on all nodes.
- Shared Data fields, which are a category I haven't implemented yet. These are
	fields of which one instance exists for each Element, regardless
	of how big the object array is. To some extent the extended fields
	(which are child elements) do this job.

Implemented a really disgusting hack: The ElementFields are all on Neutral,
and they are the first few. Assuming all classes derive from Neutrals, it is
easy to identify functions from ElementFields by their fids. I've just put
in a test for ( q->fid < 12 ) and it works. That is, it clears the test for
getting weights across nodes. Checkin 2493.

Nowit clears the critical testMultiNodeIntFireNetwork. Still has trouble,
but later. Checkin 2494.

============================================================================
6 March 2011
The crash was in testThreadIntFireNetwork, and it was simply
that this test was strictly single-node: it created Elements locally.

Looking to compare funcitonality of testThreadIntFireNetwork and
testMultiNodeIntFireNetwork. They are equivalent, it is just that the first
uses local Element and Msg creation calls, but the latter uses the proper 
Shell API. So I've just set up a check that bypasses the 
testThreadIntFireNetwork when the simulator is running on more than one node.

One issue is that of random seeds across nodes. I need to be able to
seed all nodes equivalently.
The testMultiNodeIntFireNetwork simply comments out the mtseed call,
but uses the cleaner function for setting the seed during the
"setRandomConnectivity" call to the SparseMsg Manager that defines
the connections. So seeds are OK.

Now clears this test, but dies a bit later in testBuiltins.cpp:testFibonacci.
Checkin 2495.

Looked at testFibonacci. Same issue: need to use the ShellAPI to get this
to behave on multiple nodes. I'll do the test for multinode operation here too.
I already have a testMpiFibonacci in place. Unfortunately it needs some fixes
itself.

Should go through and fix all Set/Get calls to use FullIds rather than Erefs.
This is needed as the calls are API functions and work across nodes.

Should make getVec always do the whole vec, not requiring the DataId::any.
In fact it should take the Id as the argument so there is no question of 
specifying the DataId.

For now, still going through crashes with MPI.
Fixed testMpiFibonacci.
Latest problem is with testStatsReduce, which is also
a single-node specific function.
Put in a test for numNodes, now unit tests clear. Need to go
back and put in an MPI version, but 
THIS IS THE FIRST FULL VERSION THAT CLEARS ALL TESTS WITH THREADING AND MPI.

Checkin 2496.

Implemented the MPI version, testMpiStatsReduce. Works with up to
4 cores X 4 nodes.
Checkin 2497.

Converted argument in getVec to an Id. First step in a long process to convert
all the SetGet calls to Ids. Compiles, clears unit tests.
Checkin 2498.

Doing valgrind. Not happy. The MsgManagers need to be cleaned out.
Did that. There is still stuff leaking. I think I need to clear some of the
getBuffers in the final throes of destroying the Shell.

That helps too, but still losing memory. Try doing single-threaded.
That didn't work, it wasn't really single. I think there are just some things
created in the unit tests that don't get deleted.

./moose -s hangs, that would have been the way to find the leaks.
Tried to fix the hang, that made the threaded code crash. Commented it out.

Shouldn't get bogged down here. It is important to fix this, but more important
to get the base code ready for the big API merge. 

Checkin 2499.

Converted argument in setVec and setRepeat to an Id. Compiles, clears unit
tests.

Brahma said: Well, after hearing ten thousand explanations, a fool is no
wiser.  But an intelligent man needs only two thousand five hundred.
                -- The Mahabharata

Checkin 2500.

Still need to convert the basic set and get calls to use Ids.

When I get in to the lab, I need to run the multinode stuff
on gj to see how it scales.
Also need to build a really big regression test for multinode runs.

Documentation.
For python:
	API: Shell::do<name> commands
	SetGet
	Traversal fields from Neutral
	Wildcard operations
For making new classes:
	Example class
	List and description of Finfos
	Explanation of Cinfo usage.
For making new solvers:
	Example ksolve
	Zombie principles.

Need better name for FullId, whose short form confuses with FuncId
ObjId (oid) :	OK but the FullId Goes beyond object to field Object.
Considered several others, but ObjId it is.

Used Id names are:
fid: FuncId
id: Element Id.
did: DataId
mid: MsgId
Possibly confusing one is cid, for Cinfo Id (though it isn't used)
oid: ObjId.

Made the change. Simple but tedious as it affects many files.
Checkin 2501

Working on eliminating Erefs from SetGet. As expected, vast
numbers of files need to be fixed. Mostly compiles through basecode now,
currently done midway through testAsync.cpp.

============================================================================
7 March 2011
Because of the huge number of changes, I'm putting in a mid-stream checkin
as the SetGet changes work through the compilation process. Checkin 2502.

Now through basecode, msg and working on compilation through shell.
Checkin 2503

More compilation, now through scheduling and builtins. Checkin 2504.

More compilation, now done and clears unit tests. Checkin 2505.

Tried regression tests. Turns out that GSL is not compiled with this. 
Fixed compilation flags, recompiled. Now it clears regression tests too.
Checkin 2506.

Ran on ghevar (8/16 thread Intel Nehalem) and gj (4-core Opteron).
Ghevar crashes when run with more than 6 nodes (mpirun -np 6 ./moose -c 1 ).
It is perfectly happy doing mpirun -np 6 ./moose -c 8 for a total of 48 
processes.

gj is happy up to 4 'nodes', then hangs for more.

These seem to be different issues.
Fixed the issue on ghevar: it was an invalid check on # entries on a local
dataHandler, in testMultiNodeIntFireNetwork(). Now works, and ghevar
has just done 32 simulated nodes. 
Checkin 2507.  This is the reference codebase I think for bringing the versions back
together.

Set off on gj with 48 nodes x 4 cores. This went very slowly indeed, 
and it failed with the Sparse matrix transpose.

Tried again on gj with 48 nodes x 1 cores. This went very slowly too, 
and it also failed with the Sparse matrix transpose.

To run it on gj:
/usr/local/mvapich2/bin/mpirun -np 48 ./moose

Before that, to start up mpi: source start_63_mpd
To check if it is running on all nodes: mpdtrace
To clean up the mess: source kill_mpd.


For the lab people to use: added in some documentation  in Shell.h and in 
the Docs subdir.
Checkin 2508.

============================================================================
11 March 2011
Added isRunning and cwe MOOSE fields to Shell.
Checkin 2516.

Next, provide some message access functions. Two approaches: One to put it
in the Neutral fields. Two, to provide a shell function to do it. I think the
Neutral is the cleanest. Should just return a vector of ObjIds that match
criteria, these being the MsgHandlers.

Need to figure out how to run doxygen.

In the meantime, implementing message traversal operations.
Still to compile.

============================================================================
12 March 2011
Compiled Neutral::getOutgoingMsgs and getIncomingMsgs as new fields.
getMsgTargetIds is compiled but I don't yet have a way to specify
'get' calls with an argument for the field name to which the
Msg is to connect.
Also need to put in unit tests. Setting those up.

============================================================================
13 March 2011
First set of tests, for outgoing msgs, fine. Incoming are confusing as it
reports 7 msgs rather than 1 I was expecting. Turns out that I had 
misunderstood the usage of the Element::m_ vector, which actually keeps
track of all msgs. Also some cleanup needed to eliminate the AssignmentMsg
from the returned msg lists, as it comes and goes and its targets change
all the time. Now clears a good set of tests.

Should create an Element for ClassFinfos, as I had earlier. Useful for
looking up Finfo lists, documentation, etc. Showfield would also find this
useful.

============================================================================
14 March 2011.

Checkin 2524 with yesterday's messaging inspection fields, unit tests, and
docs.

Subha confirms that it would be useful to get a list of targets connected to
a specific field on an object.

To make this happen I need to have a 'get' call which takes an argument for
lookup. This will be generally useful, for example in table lookups.

Put in skeleton code for this. The OpFunc and SetGet template parts are
done, but some nasty Shell code remains.

Checkin 2526.

This was the easy part. For the Shell code it may actually be better to go 
with the handleSet type operations using a prepacked buffer to provide the args. 
It would be pretty ugly to adapt the current dispatchGet and handleGet.

============================================================================
15 March
Things to match up:

Type of return value, type of Get function.
Type of arguments, arguments in Get function (which are passed through Set).

Things are taking shape. Need to make sure the new Shell::dispatchGet is
in the header, and the old ones are out.

Need to work on SetGet2 (at least) to enable setVec. Need it for the 
LookupField. Could go further with this and have Field1, Field2, etc.

First step: Get compilation and unit tests to work with the new variant of
the 'dispatchGet' logic, which uses the Set calls.

Compilation done
Unit tests: now stuck in testAsync.cpp:1602 which is to do with
testSetGetExtField.
============================================================================
16 March

Found and fixed issue with extFields.
Now clears the unit tests. Surprisingly painless for such a major change.
Checkin 2529.

Doing a recompile with MPI flags, to make sure it still works across nodes.
OK, still works. Tested with up to (nodes X threads) = 7x2 , 4x4.

Second step: Design for SetGet2::setVec. Here we need a consistent way to pass 
in array arguments.
Given the way the Prepacked Buffer already chunks out the indices, it is pretty
clear: at each index we need a block of memory that has the arguments for that
index in sequence. Everything in the downstream code is already set up to
handle this. Just need to fill in the PrepackedBuffer suitably.

Next I also need to go and fix how setVec deals with putting string vectors
into the PrepackedBuffer.

Could also ask the Prepacked buffer to provide an allocated char* buffer to put
stuff into so I don't have to do an extra memcpy and memory allocation.

Implemented SetGet2::setVec and its unit test. OK. Checkin 2530.

Next: Work on LookupField, a field which has an index for every entry.
This is proceeding, but it shows up a problem with the semantics of setting or
getting a vector when there are 2 args: Do we treat the operation as applying
to an array of objects, each of which gets unique arguments, or do we treat
it as a single object entry, which has the function applied repeatedly.
The latter case might be an array of fields on the object which get all set
or looked up at the same time.

For now I've implemented the 'set' for both options. Option 2 is done in a 
slow, brute force manner. 'get' currently only implemented for the first option,
scanning through an array of objects.

Need to now put in unit tests for all these cases.
Implemented the LookupValueFinfo and put it in the Arith to test in due course.
Now compiles. Checkin 2531.

Implemented unit test, clears it. Added a couple of lines to the API.txt 
Doc file. Checkin 2532.

Next: Fix the string handling of PrepackedBuffer. Done. Checkin 2535.

Then: Set up fields in Neutral for accessing messages and child Elements.

Working on it, the compile fails in a typically obscure way on the
new code.

============================================================================
17 March.
Fixed up and added unit tests for Neutral fields "msgSrc" and "msgDest"
for getting vector of Ids of src and targets Elements of msgs to given field.
Checkin 2536.

Implemented additional Neutral inspection fields for linearSize and dimensions.
Implemented unit tests for these within the existing 
testShell.cpp:testSyncSynapseSize() function. Works. Checkin 2537.

Updated the API document with the key Neutral fields.

Next is the access to class and field info.
Each Finfo should know
	name
	docs
	Type
	Num args ?
	Type of each arg ?
	
	ValueFinfos have to identify set and get Finfos
	SharedFinfos have to identify src and destFinfos.

Clearly, Finfos will need to be individual child Elements, much like the
	Synapse.

Each Cinfo should know
	name
	docs
	parent class
	SrcFinfos
	DestFinfos
	ValueFinfos
	LookupFinfos
	SharedFinfos
	# instances?

Cinfos should be implemented as individual children of a neutral, 'classes'.
Indexed entries would be messy.

Seems I haven't put the doc string into the Cinfo. Must do.

For now: Implemented the Cinfo MOOSE field stuff. Compiles, not
yet hooked up. Checkin 2538.

Seems I left out a couple of files from the checkin. Checkin 2543.

Now to do the implement MOOSE classes for the Finfos.
Then set up the classes Elements.
============================================================================
18 March
Compiled masses of changes for the Finfos to get them set up as MOOSE classes.
Checkin 2544.

Added unit tests for the low-level functions for Finfo and Cinfo fields. After
some juggling, this works. Checkin 2550.

Added some more test in the Unit tests for Cinfo fields. Checkin 2551

Finally got the Class inspection classes to work. They reside in
/classes/<classname>
and each has five child FieldElements named
srcFinfo
destFinfo
valueFinfo
lookupFinfo
sharedFinfo

Each of these is an array, indexed as DataId( 0, <index> ) 
since they are FieldElements.

There is a nasty pending hole in the code, which is that the field 
Neutral::linearIndex
is currently ill-defined. It should resize as soon as the fieldElements or
any other dimension is set, but does not do so now.
Checkin 2554.

Added some documentation to explain these new features. Checkin 2555.
============================================================================
21 March
Next: work on ksolver:
- Multiple instances/compartments
- flux through a barrier
- stochastic engine

Also space:
- Implement basic shapes.
- Automatic geometry generation from neuronal morphology
- Port over readcell and readNeuroML morphology

Also chem model fix ups
- Groups and spatial organization revisited
- SBML import
- mathobject.
- volume handling


For starters, went through and wrote a bit of documentation on the 
ksolver system.

Some issues with the ksolver design:
- Doesn't deal well with object indices. Should explicitly state what it 
	expects: element-to-element messages
- Doesn't have the concept of expanding into arrays just yet.
- Not tested multinode.
- Not set up for multithread.

None should matter for starters. I need an interface plane or port for
communicating with other solvers. This could just be another ksolve system.

- Have to have a way to deal with all molecules scalably.
	- Use a portion of the v vector?
	- Use a set of dummy molecules that build up?
- Has to have outgoing flux and incoming flux terms.
	- Separate sections of the v vector for each.

============================================================================
22 March

Designing ports for interfaces between solvers.

Option 1:
	- Each port represents two sections of the S_ vector for molecules.
		- Section 1 is for incoming
		- section 2 is for outgoing
	- Start of each section encoded in Stoich::portStart_;
	- Same # molecules in each section of a given port, in same order.
	- Incoming molecules are treated as flux per timestep, and handled
		in the calculations by a flux RateTerm, which linearly removes
		molecules till the end of the timestep.
	- Outgoing molecules are integrated during the timestep.
	- During molecule exchange at port
		- incoming section is replaced with incoming vector values
		- outgoing section is sent as is for 'double' outgoing
		- outgoing section is 'floored' for 'int' outgoing. 
			The remainder stays put for the next cycle.
	- During setup
		- Incoming section is tied to a set of 'flux' rate terms.
			- These do not need parameterization, just the dt to
			guarantee complete removal by the end of each dt.
		- Outgoing section is tied to a set of unidirectional
			1st order rate terms to represent diffusion rates.

Option 2:
	- Each port represents one section of the S_ vector for efflux, and one
		vector of integers to look up molecules undergoing influx.
	- Use Stoich::portStart_ to look up the respective vectors.
	- Incoming molecules just added right into their S_ terms.
	- Outgoing molecules integrated during the timestep.
	- During molecule exchange at port
		- Incoming molecules added into their S_ terms
		- outgoing section is sent as is for 'double' outgoing
		- outgoing section is 'floored' for 'int' outgoing. 
			The remainder stays put for the next cycle.
	- During setup
		- Incoming mol indices filled into port vector
		- Outgoing section is tied to a set of unidirectional
			1st order rate terms to represent diffusion rates.


Evaluation:
Accuracy: Option 1 is 2nd order, option 2 is an ugly Euler. But option 1
	may have roundoff/integration error, whereas there will be absolutely
	no molecule loss in option 2.
Data transfer: Identical
Setup: Both need creation of assorted rate terms. More rate terms in option 1,
	option 2 needs a separate table to be created.
Speed: Stoich matrix is a bit bigger for option 1. More molecules and 
	more reacs.
Overlaying data transfer targets: Possible for both. 
	Option 1: This would amount to a shared influx set of molecules for
	all ports, assuming that the molecule set is identical or overlapping.
	Handy for many spines on a dendrite.
	Option 2: This happens effortlessly: influx goes right to target mols.
Compatibility with Gillespie calculations: 
	Both OK. Option 1 is clean, Option 2 will need extra step to flag
	affected reactions for updating transition probabilities.
Compatibility with something with a much finer timestep:
	In both cases, the other solver will need to implement something like
	the flux terms to smoothen influx. Efflux no problem.
Compatibility with something with a much longer timestep: Option 1 is better.

Overall, go with option 1. Higher order accuracy but slightly slower.

Now, setup issues. Ideally we should just introduce two solvers to each other
and let them figure out what to do.
	- Solver A sends B a vector of all molecular species.
	- Solver B sends back a vector with the matching subset.
	- Each uses this subset to build its arrays and reacs
	- Separately, provide each side of each port with a scaling factor for 
		diffusion rates, related to area of port and other factors.
All this to-and-fro stuff, as well as the runtime data, should go via a
shared msg.

Started some coding, still feeling way into implementation. Checkin 2559.

Subha has asked for recoding of rttiType function to return instead a list of
types of arguments to functions in a compiler-independent way. Many files
to change as a result. Compiles, added new unit test for a nasty case, works.
Checkin 2560.
Updated API.txt to reflect changes. Checkin 2561.


The actual picture for ports is more complicated. We may have different
spatial compts handled by a stoich that does diffusion. We may have
different reaction subsets handled by the same stoich, for example parts
doing membranes and other parts doing cytosol. So the port has to be pretty
flexible in accessing any of these.

Options for ports:
1. They are a FieldElement. Each port has the following things:
	assorted msgs
	scaling factor for outgoing rates.
	Info about which part of the Stoich data structures they access
	Possibly some spatial info, or a way to specify associated geom.


Setting up Port class for use in FieldElement.

============================================================================
23 March 2011

Skeleton Port class implemented and compiled. Checkin 2564.
Added in FieldElementHandler for Port within Stoich, and it compiles,
but segvs as soon as I try to run it.

============================================================================
24 March 2011
Tracked down Segv. Was nasty: due to calling of static initializer for PortCinfo
which depended on other static initializers which had not yet been called.
Organized into static funcs instead, and now it is OK. Checkin 2572.

Discussion with Steve Andrews about how Smoldyn handles ports. Seems like it
expects everything to be defined at the model definition stage, rather than
what I've been working on, which defines the molecule species list for the 
port at the model setup stage and does so implicitly. The MOOSE approach says
that molecule species which is defined as diffusible and is present at the
port, will go through if it is also present on the other side.

The MOOSE approach is compatible with loading of different models into
a neuronal morphology and letting them mix up with each other. The models
don't need to know anything about each other.
The Smoldyn approach is designed for complete specification.

There won't be any real difficulty with this for MOOSE provided the molecular
species it supplies at the port are a superset of those coming from Smoldyn.
For unknown species I guess the best thing is to turn them right around.

A key pending issue is to define a map between molecular species Ids run by
different solvers. Some issues:
- merged species. Suppose model A does not care about MAPK*tyr vs MAPK*thr. 
It represents them by a single species. Model B, however, does care.
Cannot just assume equal proportions, as the ratios may even change during a
simulation.
- Same species, different locations within solver. Consider cytosolic vs
membrane-bound CaMKII. However, MOOSE also treats them as different Elements
in order to do the calculations. Pools rather than species is the better name.
- Isoforms. One model worries about them, the other lumps them all. Here they
	can be combined by ratio which will remain fixed during simulation.

So we need to have a species map between and within solvers 
Global:
	- can handle all configs of ports
Pairwise:
	- Easier for a few
	- This is what any individual port would want
	- Combinatorial explosion.

============================================================================
25 March 2011.

Some definitions:
species: A unique chemical species.
pool: A set of molecules of the same species, undergoing identical reaction
	conditions.
Mol, as used by MOOSE: All the pools within a given cellular compartment.
	These could undergo diffusion and could be subdivided by spatial
	compartments. So their chemical environment (local concentratoins) 
	may change but the interactions that they see (reactions) will not.

How to set it up? 
	- Explicit table of species and the pools that represent it.
	- Use unique species identifier for each species, guarantee consistency
		across source models. Basically add a field to each Mol.
		This would require extra input into any kkit-based model.
		SBML uses the term 'Species' to refer to a pool, and SBML 2
		used 'SpeciesType' to refer to a chemical species. SBML 3
		is going to do something still more precise and complicated.
		But I still need to map between species specified in
		different models. If there is an ontology-based spec, fine,
		otherwise explicit mapping.
Decision: 
	- Add a 'species' identifier to molecule. It is set, by default, to
		an index of unique pools.
	- If needed in any simulation, explicitly assign values to this 
		identifier.
	- Later: Find a way to couple this identifier to a global array of
		URIs or other ways to uniquely link species to some ontology.
	- Later: Figure out how to handle combination species as listed above.

============================================================================
26 March 2011.

Another approach: Implement port as a standalone entity, one that could
	even communicate with regular non-solved reactions.
	- Port creates a set of regular molecules to represent the 
		incoming and outgoing pools.
		- Use a 'set' message to assign mol concs on incoming mols
			once every Port dt.
		- Port uses a 'get' to get 'n' molecules from outgoing mols 
			every Port dt.
		- Port sends a 'set' message to zero out mol concs on
			outgoing mols after they have been sampled.
	- Port creates a set of reactions into and out of these pools.
		- Incoming reacs are the special Flux variant of RateTerm
		connecting the incoming pool to the diffusing molecule in MOOSE.
		- Outgoing reacs are just unidirectional FirstOrder reacs
		connecting the diffusing mol to the outgoing pool.

	The Stoich digests these entities in the usual way, with special 
	checks for Port objects. The Solved port has a zombie variant that
	controls the special pools involved in the port.

Issues with this:
	- Doable but adds a lot of work for little benefit
	- Do we want to treat Ports as Elements, or as FieldElements?


Implemented the 'species' field for molecules, and ZombieMol. It is stored as
a vector of unsigned ints in the Stoich. Checkin 2582.

Now to put the bits together. 

Created a SpeciesId class, currently just typedefed to unsigned int.
This is needed for the setup of ports. Checkin 2583.

Next: 
- Use SpeciesId to scan through possible molecules to build up port
- Test port
- Get Python wrapper running.
- Set up geometry things for Smoldyn.
- Look at compiling in Smoldyn
============================================================================
29 March 2011.

Working on Port implementation. Should perhaps have Ports associated with
KinCompartments, since one way of specifying the list of molecules available 
for porting is to take the list of diffusible mols which have a defined 
SpeciesId in the compartment.

============================================================================
31 March. 
Worked on the options, also referred back to earlier design ideas for
compartments.

To reiterate definitions:

Chemical compartment: Domain in which a consistent set of chemical reactions 
apply, but the concentrations need not be spatially uniform or even contiguous.
Equivalent to the term cellular compartment as used in biology.
Examples might be all the apical dendrites, or the dendritic spine heads,
or all early endosomes.

Chemical compartments have an 'inside' relationship to a single surrounding 
compartment. 

Numerical compartment or voxel: Domain within which we assume that the 
chemistry is spatially as well as chemically uniform.

Geometry: an attribute of a chemical compartment, derived from 
the ChemCompt. Equivalent to Smoldyn surfacesuperstruct. Manages set of
child surfaces. 

When drawing or using a self-meshing program like Smoldyn, the Geometry is
sufficient to define the entire 3-D extent of the compartment.

Problem: multiple Compartments can and should share the same Geometry. 
	If they don't then things could get messy with alignment.
Problem: Different things may happen on different parts of the surface of
any given Compartment. Consider ports for spines, membrane vs junction with
soma for dendrite; membrane vs PSD for spine head.

Need to have different Geometries to specify different parts of the
compartment. For example, dendrite length, ports to other solvers, Region

Consider the 


Mesh: child of Geometry, in principle Geometry can have more than one Mesh,
say one for Ca calculations and another for other mols. Manages spatial 
discretization of the Geometry.kkkk

Crucially, the Geometry knows how to spatially decompose the 
compartment into numerical compartments . Given the # of subdivisions and an index it can provide:
	- volume of the voxel
	

Separate trees under Cell for ChemCompts, Geom, and solvers.
ChemCompts: Holds mols, reacs and mesh. Each of these has one-to-one aligned 
	vector expansion for spatial reac-diff models. Each mol or reac looks
	up its aligned mesh element for local volume, used for conc.
	Can be point, line, surface or volume.
	Mols: regular Element array, one per mesh element
	Reacs: regular Element array, one per mesh element
	Mesh: regular Element array.
Geom: Holds many child surfaces in the Smoldyn sense. Provides an API
	for finding out sub-sections as needed
	Surfaces: Manage multiple panels
	Panels: TriPanel, RectPanel, HemispherePanel, DiskPanel, CylPanel etc.
Solvers: These can span ChemCompts, but must occupy at least one full one.
Ports: 

Mapping between these:
Used for:
	Chem compts look up geoms to define vol etc
	Chem compts look up geoms to set up meshing.
	Solvers look up mols on chem compts to figure out what to solve
		Solvers are typically assigned one or more entire ChemCompts 
		as their domain. I don't really see why one would want a subset
		of mols in a chem compt being handled by a solver.
	Solvers look up meshing on chem compts to figure out volume conversions?
		Perhaps this can be done invisibly by the reaction scaling terms
	Ports: Specify a single panel? or a surface? as child or msg.
		Specify ported molecules and porting rates
		Ported molecules are created as copies on port
		Porting rates are set up as reactions on port
		Ports might be able to interact with multiple voxels, but
			this loses info. May need concept of meshed port for
			Smoldyn to deal with it.

============================================================================
2 April 2011
Skeleton placeholders added for Ports. Checkin 2603

Added smol directory for the Smoldyn interface. Checkin 2606.

Able to get it to compile, and to instantiate a SmolSim object. Also able to
instantiate a couple of Mols (which should be Pools). Checkin 2608.

Starting work on geom directory, used to set up the Geometry classes.

Subha reports that Pymoose does not see inherited fields.

============================================================================
3 April 2011
Implemented inherited fields for Cinfos. Some fix up needed in testAsync as
the indexing has now changed. Checkin 2611.

Implemented Geom classes in new 'geom' subdirectory. Checkin 2613
Added SpherePanel class. Checkin 2614

Put in outline of steps needed. Incrementally going through steps.
Checkin 2615.

Added in steps to handle the addition of molecules (nInit) to the system.
Added in steps to add the reaction to the system. Now it should be ready
to do computations with it.

There is a bug in the inspection routine causing a segv, probably because 
I havent defined something. Will figure it out.
============================================================================
4 April 2011
Finally able to checkin code as revision 2617, after many attempts.

Added in steps for setting the reaction rates, also made both
forward and backward reactions.
Still dies in the inspection routine. Checkin 2618.

Added in some more access functions for SmolPool to look up the current
molecule N.

Things to do:
	Steve:
		Figure out why it crashes
		Put in way to get vector of all molecule coords
	Me:
		Proper structure for volumes and compts.
		Compartments
		Buffering through scaling of reac rates
		Updating reac rates
		Clean up nomenclature throughout between Pool, species
		and Mol.

============================================================================
5 April 2011
Following Steve's advice, added a line "smolUpdateSim" and that lets it 
clear the display and subsequently to run it.

There is a pending bug with MOOSE, deleting the SmolSim object. Does not seem
to do with Smoldyn. Checkin 2619.

============================================================================
18 April 2011
Trying to track down deleting bug. It only crops up after we have set the path.
in testSmol.cpp.

OK, tracked it down. SmolPool::smolSpeciesInit tries to assign the local field
diffConst_ to the original diffConst field. But the SmolPool data ptr 
itself is just a cast version of the SmolSim, so this local field doesn't
exist in memory.
Tried to see if the sim knows about the diffConst field. Somewhere in
the libsmoldyn.h library there is a way to assign it (smolSetSpeciesMobility),
but I don't see a corresponding way to get the diffConst from the New
Lab complex.
Options:
	- Have the SimPool be a small wrapper of the Stoich and also include
	a ptr to the sim_.
	- Have the SmolSim maintain another vector of doubles for the
	diffusion constants

============================================================================
19 April 2011
Fixed problem with the SmolPool. I changed it to an independent class with its
own pointer to sim_. This required a new function in DataHandler and all its
subclasses, to copy over a DataHandler but using a new Dinfo for the new
data type.  
Checkin 2628.

Valgrind reported another memory access error, which turned out to be due to
accessing the no-longer extant original molecule object during zombification.
Fixed. Many memory leaks persist, though. Checkin 2629.

Working through leaks. Eliminated many from setVec. Huge
chunk remain from makeCinfoElements:

==3976== 83,316 (11,800 direct, 71,516 indirect) bytes in 295 blocks are definitely lost in loss record 116 of 116
==3976==    at 0x4C27CC1: operator new(unsigned long) (vg_replace_malloc.c:261)
==3976==    by 0x484EBF: Shell::adopt(Id, Id) (Shell.cpp:794)
==3976==    by 0x415D87: adopt(Id, Id) (FieldElementFinfo.cpp:18)
==3976==    by 0x448A07: FieldElementFinfo<Cinfo, Finfo>::postCreationFunc(Id, Element*) const (FieldElementFinfo.h:85)
==3976==    by 0x412ED2: Cinfo::postCreationFunc(Id, Element*) const (Cinfo.cpp:120)
==3976==    by 0x406729: Element::Element(Id, Cinfo const*, std::string const&, DataHandler*) (Element.cpp:77)
==3976==    by 0x4130CF: Cinfo::makeCinfoElements(Id) (Cinfo.cpp:134)
==3976==    by 0x41C7A0: init(int, char**) (main.cpp:207)
==3976==    by 0x41CA6F: main (main.cpp:285)

There are also lots lost from the Smoldyn test.
Checkin now at 2630.

Seems clear that many of the messages to child Elements are not being destroyed.
Also doesn't seem that the FieldElements are being destroyed either.

Turned out that there is a toxic function out there somewhere in the unit
tests, which deletes the root or deletes everything attached to it. I suspect
the syncDataHandler operation. 

Once this is commented out of all unit tests, the whole mess just crashes
when it is time to quit.

Confirmed that this happens without Smoldyn compilation too.
============================================================================
20 April 2011
Still struggling with it. 
Confirmed that it is NOT dual deletion.
Actually I should not delete the Cinfos at all. They are all statically
defined.
Finally put in a check in the destructor for ZeroDimGlobalHandler, to
avoid deleting Cinfos.
That fixed it.

Now need to
- valgrind it
- re-enable calls to test that were causing crash.

============================================================================
21 April 2011
Checkin 2636.
Ran valgrind. Now there are no outright errors, and a leak of only 848 bytes.
But before we go too far with this, I should see if the putative 
Shell::doSyncDataHandler operation is still causing problems. This was called
by testMultiNodeIntFireNetwork.

Reinstated the call.
Now, there are no crashes, but the sync function is still apparently removing
the children of the Root element.

Tracked it down, it was somewhat subtle. The ReduceFinfo has to call 
Element::clearBinding on the shell Element, to get rid of temporary messages
it sets up. But the ReduceFinfo was not properly initializing its 
bindIndex and so it used the default of 0, which represents the child msgs.
Hence all the children of the root element were lost.
Checkin 2637.

Now running valgrind again to see what is leaking memory.
Two things: the messages from Shell during set/get, and the Dinfo<Cinfo>
used by makeCinfoElements. The first should really be taken care of when
I delete the Shell Element during wrap-up.
The latter should not be deleted any more because
the ZeroDimGlobalHandler doesn't destroy its data. Rerunning valgrind:
The second seems to have been fixed, but not the first. 
Checkin 2638.

This also works when compiled with the Smoldyn flags.

Next: Subha needs for the message Elements to provide information about src and
dest fields. Since any given message may have multiple src/dest pairs, I'll
list them all.

Implemented, compiles, clears old tests but no new ones yet for this. 
Checkin 2639.

Working through unit tests. Many done but there is a problem with
the # of entries retured on Element::getFieldsOfOutgoingmsg
for the reverse msg.
============================================================================
22 April
Turned out that my understanding of the shared message semantics was wrong, not
what the program does. Now clears unit tests.
Checkin 2640.

============================================================================
24 April 2011
Back to Smoldyn. Added a few lines to the testSmoldyn unit test to
advance stepwise through the simulation and use the Molecule
access functions to find how many mols are present. Works.

Now would like to get some spatial info about where the molecules
live. This will need another library func, don't have a ready
API for getting the whole lot.

Another useful API function would be to get molecules within a certain
subvolume, or alternatively fill up a vector of Pools with their
individual counts based on coords, as digested by the Pool. The latter
would be more efficient.

Now need to work on the geometry and subdivisions.

============================================================================
30 April 2011.
Checkin 2645.
============================================================================
1 May 2011
Matrix of many pending things in MOOSE, and their dependencies:

Item							Dependency
*Renaming Mols to Pools					-
Array-ifying reaction system.				Shell funcs
+Compartment handling					-
Mesh handling in geometry: voxel data structs		Compartment handling
Replicating regular ksolver in array calculations	Array-ifying
Gillespie solver					-
*Setting up systematic Species info			-
Duplicating a species (as a tracer)			Species info
Parallelize rate updates for Ksolver and Gillespie.	array, replic, Gillesp
Parallelize/multithread regular Ksolver			par rate updates
*Clean up math handler memory leaks			-
Incorporate math handler into ksolver			MathFunc leaks
ReacDiff solver (4th order version)			arrayifying
Geometry extraction from neuronal model: rule sets.	NeuroML/readcell
GUI							-
Merging tool						species
Update kkit reader to deal with species etc		species
SBML reader port and expansion				species, math, compt
Molecule mapper at interfaces.				species, compt
Voxelization of geometry				arrayifying
Volume handling for concs and reaction rate scaling	array, compt
Volume handling for cross-voxel reactions		array, compt
Directions and other details for molecules in Smoldyn	geom, compt
Automatic est of D from molecule wt			Species
Mechanics						geom, compt
Boundary changes					geom
steady-state solver and classifier.			-
dose-response builder					ss solver
scanner for all steady-states.				ss solver
*Regular channel/compt definitions			-
ReadCell reader						-
NeuroML reader						-
hsolver							channels, compts
markov channels						hsolver, channels
Port over signeur					most of the above.


Starting off with compartment handling: See 31 March notes. 
Setting up shared msg from compt to geom(s). Message made, compiles.

For mesh elements: We want: size, centroid, vec of xA to neighbours, vec of
neighbours, shape [linear, cyl shell, sphere shell, hemi shell, tetra, cuboid]
etc. The mesh should be a child Field Element of the Compartment, indexed
to make a virtual array.
Each mesh entry should point to a location in space, referred to a geom?
What to call it? I've called it voxel so far, but mesh would be more apt.

Some mesh forms:
				Dimensions
- linear			1 (might subdivide a 3-D space though)
- rectangular			2
- cuboid			3
- Triangular			2
- Tetrahedral			3
- cylindrical radial		1
- cylindrical radial-axial	2 Concentric axial compartments.
- cylindrical segmented		3 radial-axial-segments
- spherical radial		1
- spherical radial segmented	3 Possibly could have a symmetry to make it 2D

Looks like the way to go is each shape knows how to handle some of these 
meshings. Problem is with composite shapes like a neuron. The neuron as a whole
might know how to do the meshing, but individual cylinders with hemisphere
caps would be in trouble.
Options:
	- Subclasses of ChemCompts that know what is expected
	- Separate mesh classes, children of chem compt
	- separate mesh classes, children of geometry
	- Attributes of geom classes.
	- Separate tree with its own links to the geom and compt.
		- Would we ever need multiple meshes on a single compt?

How to represent a complex mesh vol, like a cylinder with a hole in it for a 
vesicle or a nucleus?


Meshing aside for now. Marching through many many files to convert 'Mol' to
'Pool'. Done. Clears unit tests.
============================================================================
3 May 2011
Checkin 2649.

Think about Species. This needs to be a separate object, with things like
mol wt, diffusion const, membrane localization info, a database # and so on. 
Would like to have a link to it - a msg - from each pool that is of that 
species. Useful for duplicating like a GFP-tag, or a radioisotope for tracers.
Also useful for automatically setting up interfaces at ports between solvers.
Tricky when I want to assign explicit separate diffusion constants.
Tricky in setting up from SBML which might not have the species info.
SBML calls it species-type. It is not a required field.

Setting up a bare-bones Species class. Connects to Pools with the mol wt info,
which the pool can use if desired, for estimating diffusion const.
Checkin 2650.

============================================================================
4 May 2011
Working on cleaning up MathFunc class which has lots of memory leaks.
Fixed one set, much more to go.  Checkin 2653.
More cleanup later, now valgrind only reports the earlier missing 120 bytes
that I still haven't been able to track down. I do hope that the MathFunc
really is OK after the changes I've made in it. Don't know why Raamesh did
it this way at all if it was so straightforward to change.
Checkin 2654

============================================================================
5 May 2011
Got the MathFunc stuff incorporated but not yet tested, in the
ksolver code as a MathTerm. Checkin 2655.
The current version is not thread-safe, will probably have to ask Subha
to rewrite the whole thing.

============================================================================
6 May 2011
Figure out how to test the MathFunc and its implementation in ksolve. 
- Implement a model with nothing but some interesting funcs, like sin(A+B)
- Implement variant on Kholodenko model, replacing MMEnz with MathFuncs.
- Implement rat tracking model.

As a first pass, tested the new MathFunc::op() in unit tests. Revealed that
the system did not deal properly with repeat evaluation requests. Fixed.
Since this was in a loop, also ran this through valgrind. Continues happy.
Checkin 2656.

Trying now to set up a little MM-enz equation using MathFunc and the default
(EE) integrator.
This hangs. GDB says:

167             shell->doStart( 100 );
(gdb) n
^C
Program received signal SIGINT, Interrupt.
pthread_cond_wait@@GLIBC_2.3.2 ()
    at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:162
162     ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S: No such file or directory.
        in ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S
(gdb) where
#0  pthread_cond_wait@@GLIBC_2.3.2 ()
    at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:162
#1  0x000000000048e05f in Shell::waitForAck (this=0x8ec818)
    at ShellThreads.cpp:74
#2  0x0000000000486f96 in Shell::doStart (this=0x8ec818, runtime=100)
    at Shell.cpp:520
#3  0x00000000005459a9 in testMathFuncProcess () at testKinetics.cpp:167
#4  0x0000000000545d62 in testKineticsProcess () at testKinetics.cpp:186
#5  0x000000000041c174 in processTests (s=0x8ec818) at main.cpp:277
#6  0x000000000041c2bc in main (argc=1, argv=0x7fffffffe738) at main.cpp:312
============================================================================
7 May 2011

After considerable messing around, completed the unit test with the MathFunc.
Built in a matching reaction using the regular mmEnz.
Made assorted extensions to Table to do the comparison of the outputs.
Fixed various issues with the MathFunc so as to do the simulation.
Clears the test. But I think the MathFunc is appallingly slow, needs to be 
redone. Checkin 2657.
Need to valgrind.
This looks OK. The same 120 bytes are lost as earlier, unrelated to MathFunc.

Next: Get MathFunc working with solver


Discussion with lab on priorities and efforts:


Item							By
Port over of Biophysics					Upi
Port Hsolve						Niraj
New: Markov channels					Vishaka
Spatial kinetics					Upi

SBML:							Siji, Upi
NeuroML via python:					Aditya, Subha
NineML via python:					defer
Pynn core functions:					Upi, Subha


GUI: core structure redoing				Subha
GUI: kkit: first finish up current dev work		Harsha
GUI: nkit: first finish up current dev work		Chaitanya
GUI: threading						later

Parallel stuff						As the problems arise.



Working on port for Compartment. Way more tedious than expected, still not
done.
============================================================================
8 May 2011.
Had it working except for the unit tests for spatial spread of current.
After a lot of messing around with debugging, turns out to be a nasty bug
with messaging: How does one decide if a message is going forward or back?
Current approach is to ask if originating Element == e1 on the Msg. But
in this case e1 == e2 on the Msg (both are the Compt) so this logic does not
work. Issue comes up only for SharedFinfos where data transfer is
bidirectional, so I'll put in a check for this and ask the user to split up
the contents of the Shared msg.
Implemented this in the Compartment unit test, and it now clears it.
Ran through valgrind, OK.
Checkin 2658.

Ground through the implementation of SymCompartment. Had to go back to
Compartment to set up necessary virtual functions too. Have not yet
made SymCompartment-specific tests.
Checkin 2659.

On now to channels. Need to figure out how to deal with Gates. Possibly
simplest to put them on each channel as a FieldElement, but with the
caveat that a single (dim = 1,1) one of these Gates serves an entire 
array of channels.

Problem is with typical builds of the cells. If each dendritic compartment is
its own Element, then there will be lots of duplicated instances of the HHGate,
something to avoid.

In the current design, the Gate receives a return message but is shared
by many channels. Looks like the Gate is created as a child of the HHChannel.
Avoidance of duplication happens because the Copy function does something
special. This approach works well if there is a library of prototype channels,
and the library would be the location of the one instance of the Gate.

Danger here for pointer access to the Gate: one could delete the original 
entry. Could put in a counter for number of users of the gate. Or a 
message, for reference but not actual data traffic.

============================================================================
9 May 2011
Begun converting HHChannel to new MOOSE form. Need to sort out Gate options.

Option 1: Store readonly pointer. Message to a reference Gate somewhere.
	Load pointer on reinit.
		- More economical. Need a single reference Gate for all copies,
			even if they are on different Elements.
		- Concept is reasonably clean
		- Implementation messy. What to do if reference Gate is deleted?
		- User interface a bit messy, will need to traverse Msg to find Gate.
Option 2: Store readonly pointer. Have child FieldElement with gate.
	Unclear what to do with copies, probably just duplicate the gate 
	otherwise pointer management will be unpleasant.
		- Costly, a separate Gate Element for every Channel Element.
			Many duplications if we have the same channel on many 
			dendritic compartments.
		- Concept is clean: Gate is right there on Channel.
		- Implementation is clean.
		- User interface is clean, it is just a child FieldElement.
Option 3: Store readonly pointer, and the Id of the reference Gate whenever
	copying or copy[] from the reference is done. Reload pointer on reinit.
	Provide a FieldElement to wrap the Gate on each Channel.
		- Almost as economical as option 1
		- Concept is nasty: non-msg link to library element.
		- Implementation is easy, may need a new kind of FieldElement.
		- User interface is clean, it is just a child FieldElement.

Go with option 3 for now.

============================================================================
10 May 2011
Managed to get skeleton code for HHChannel to compile. 
Checkin 2661.

Next need to get the HHGates in there, and then the unit tests.

============================================================================
11 May 2011
HHGates use Interpols. Deciding whether to use Interpols or go with vectors.
Using Interpols:
	- Backward compat
	- Don't need to munge code so much
	- Field access goes through another indirection: can FieldElement
		do it? Not really. The FieldElement assumes a specific level of
		array indexing (which isn't needed here). 
	- Either that or I explicitly create a child Element for
		each HHGate, but there I lose the framework for direct pointer access.
Using vectors:
	- Lose BC
	- Code munging, mostly by way of vector access
	- Need to write local code for interpolation
		- Could convert this into subclasses for functional lookup
	- FieldElement access for Gates is simpler.
	- Would have to treat the vector as a unitary field. Can't index entries
		directly, except through the LookupValueFinfo.
		- But Python is quite happy to handle vectors directly.

Comes down to having to revisit FieldElement access to deal with nesting, and
having yet another layer of Elements on the channel. So go with vectors.

Doing implementation. Added in originalParent_ field into HHGate. Will use
EpFuncs to do field access (and ElementValueFinfos), so that I can 
check so that only the original parent element is allowed to write values.
All others can read it.
This will need a little refinement of the FieldElement creation code
so as to keep track of the original pointer and to ensure that only the
original parent is allowed to delete the pointer.
I've redone the interface for the channels so it looks to the world more like
the alpha/beta/tau/minf form than the raw values. But I need to have the
raw values accessible too, for the purposes of direct table entry.

============================================================================
12 May 2011
Most parts of the HHGate are done, but still need to get the cross-updating
of alpha/beta and tau/minf forms. This will need some algebra.

For now: compiled HHGate. Still no tests. Checkin 2662
============================================================================
13 May
Creation of HHChannels with HHGates as FieldElements is tricky if I don't
	want dummy (empty) child HHGates.
	- Derive a FieldElementFinfo or add a flag about post-creation func.
It is also tricky to have doCreate (for the child HHGates) called from 
	within a DestFinfo function. Race conditions.
	- Allow DestFinfos to register a event in the structuralEvent Q.

Copying of elements with FieldElementFinfos looks like it won't work: will
produce duplicate copies of the FieldElement. Need to test and fix. 
	- The regular Element copy should avoid making the FieldElements,
	as the postCreationFunc will want to create them. But see issue
	with only creating subset of the possible HHGates.
		- I can pass a flag in to the overloaded Element constructor
		that is used during copies, to make it clear that it is a
		copy. This can also go down to the Postcreation func
	- We want to use the regular copy to make the Elements, because we
	know how many children need to be created.

Copying of the HHGate has another issue: HHGates are FieldElements that are
created only when needed. Unless we want to always have the dummy gate present?
If created on need, then the default postCreationFunc will not work.
If created on need, then the access to the field info is OK as is since we
	will have the correct pointer accessed by the HHGates.


Steps:
0. Test what happens with copies of FieldElements. Do they end up with 2 copies?
1. Set up FieldElements for the HHGates
2. Create and test function
3. Test copies
4. Test copies into new dims
============================================================================
14 May 2011
Begain unit test 0.: Copies of FieldElements.
First was to check if data part is copied over correctly.
This is done in basecode/testAsync.cpp:testCopyFieldElement()
This now clears this new unit test. Checkin 2664.

Next was to check if I do get 2 copies of FieldElements. Yes, I do. Now
to fix it by putting in a flag to tell the Cinfo::postCreationFunc that it is 
doing a copy.

Getting close to completing the copy in testShell.cpp.
============================================================================
15 May 2011
Done the test of simple copies. Checkin 2665.

Now to get back to the HHGate and channel. 
Did the skeleton of the FieldElementFinfos. Checkin 2666.

To create stuff safely:
0. Figure out how to assign originalId. Trivial if creating data locally.
1. Check isOriginalId.
2. Figure out how to create FieldIds safely.
	- Could build using elementary commands. 
		- Thread safety issues here because this is a structural call.
		- Not thread safe: Id::nextId, Shell::adopt.
	- Could send this up to the Shell. One option is to 
	find out how to call Shell::requestCreate.send directly, to avoid
	hitting the initAck/waitForAck barriers which cause a race condition.
	Options: 
	a. HHGate to be created by Dinfo so it can be done using regular cmd
		- But Dinfo::create doesn't take any args, we need to assign
		the originalId to this new HHGate.
			See below.
		- But we need the HHGate pointer right away to complete the
		command and put the ptr into the HHChannel::xGate_ field.
		A StructuralQ command will not do this right away.
			- StructuralQ could also have a subsequent assignment op
			to set the gate. Just a destFunc on the HHChannel to 
			inspect kids and extract their ptrs. This func could
			also assign originalId.
	b. HHGate ptr to be created by the HHChannel, so we can deal with all
		setup value assignments.
		- Easy to assign originalId, just make the destFunc and EpFunc.
		- How do we assign the HHGate pointer into the the DataHandler
		of the ElementField objects? Unsigned long?
	c. Protect the HHChannel::createGateFunc with Qinfo::addToStructuralQ.
		This guarantees it only executes on a single thread. Then it
		can happily do Element creation and Shell::adopt using 
		elementary calls.

Summary: 
	Option c is simplest and also most general.

In process of compiling. Then to do tests.

============================================================================
16 May: Compiled. Clears unit tests but the real tests yet to come.
Checkin 2667.

Working on actual tests. The test config was simple enough but I've
run into unexpected queue problems. The argument for the assigned Xpower,
after going into the structuralQ, seems to be messed up. Also the size
of the argument seems wrong at 16 bytes. I suspect that a prepacked
buffer has gotten mixed up with a double.
Confirmed. The difference is that in the regular Assignment operation,
the AssignmentMsg deals with the conversion from the Prepacked buffer.
We've been using the Prepacked buffer for field assignments because we
may have unknown numbers of array entries in there.
One ugly way out might be to put the data portion in the beginning of
the prepacked buffer rather than at the end. This won't work because
the Prepacked buffer needs the size info at the beginning to work out its
own data size.
Further hacks are precluded by the realization that in real runs, the
AssignmentMsg is likely to be shifted somewhere else by the time the
structuralQ comes around. So I need to go back to options above.

============================================================================
17 May
Approach: Make the thread-safe ops more atomic. Specifically, get rid of the
structuralQ requests and instead make Id::nextId thread-safe (not expensive
as we are not going to make too many Ids in any given run). The other
structural ops are adopt, which can be turned into a regular Shell command 
to be passed to the Shell and forgotten, and the operation to delete the
child Element, which needs some thought. Rest can be done on any individual
thread.

============================================================================
18 May
Even the atomic approach doesn't work well, because of node sync. Here is an
old idea revisited: Create all three gates as regular FieldElements.
Just ignore the zero power gates. Do the HHGate data object allocation as
needed. Key thing is to assign the Original Id for the parent when we first
make the gate.

============================================================================
19 May
Fixed up FieldElement creation of HHGates, and ran first pass unit test.
Checkin 2673.

Added in tests for lookup using HHGates. Passes. Checkin 2674.
Added in test for setupAlpha using HHGates. Passes. Checkin 2675.

Found issue with singularity handling in HHGates::setupAlpha. Fixed.
Struggling with test for full HH-squid simulation. Currently commented out
for checkin. Checkin 2676.

Now I've run into a sheduling problem. The initVm_ value is not being
received by channels till after all reinits are done.

============================================================================
20 May 2011
Figured out why the initVm sequencing was off: The reinit does not follow
the same sequencing as the Process, so the assumptions about update order are
flawed. Specifically, in Process we take tick0 and go through the
phase1/barrier1/phase2/barrier2/phase3/barrier3  sequence. Then we do the
same for tick1, and so on. In Reinit, we go through all the ticks in one
go, and then pass around the resulting messages. Will need to fix.
Nevertheless, two Reinits should do the trick. Turns out they do, but the
criterion for value matching is too tight. Changed, now clears unit tests.
Running valgrind. This reveals something that should have been obvious, that
the HHGates need to be deleted. Easy to do, just put it in the HHChannel
destructor.
Perhaps not so easy. Need to have a unique way for the internal channel
destructor to know when it is the base of the HHGates. 
Sorted it out. Valgrind now happy, at least it hasn't added to the existing
leaks that still need sorting.

Now starting on SynChan.
============================================================================
21 May 2011.

SynChan implemented, yet to test.  Checkin 2680.
I want to get in SpikeGen to do the unit tests. Working on it. 
SpikeGen now also compiles, without any unit tests so far. Checkin 2681.
Put in unit tests for SpikeGen. Clears.
Setting up more complete unit test for SpikeGens->Synapses(SynChan).

Running into persistent segv when I try to run this test.
Turns out that the Synapse is hard-coded to use the IntFire::addSpike
function when a spike comes in. Would like something more general.

One option is to have a common baseclass that deals with synapses.
Doesn't even have to be a MOOSE class. 
This could pass the 'addSpike' request down to a suitable virtual func.

Another option is to have a base Synapse class, and then each subclass
be specialized for different parent classes.
	There may be a lot of these:
		- IntFire
		- SynChan
		- NMDAChan
		- MarkovChan

Let's do the SynHandler base class.

============================================================================
22 May 2011

The SynHandler base class is done, but I cannot get the thing to clear unit
tests. I've tried setting the dimensions of the Synapse explicitly but this
does not percolate into the FieldDataHandler. Then I tried making a variant
of the OpFunc that handles setNumField. This won't work in its current form
as the calling Element is the parent, not the Synapse. No way to find which
FieldDataHandler to fix. Defer for now, get back to clearing the unit test.
Checkin 2683.

Still not able to clear the Shell::doStart function for the unit test for
SynChans. Segv. Valgrind does not help pin down where the problem came from
because the queue does not retain info about the calling function.
Unable to run it in non-threaded mode.

==7096== Invalid read of size 8
==7096==    at 0x43FE1D: DataHandler::iterator::operator++() (DataHandler.h:260)
==7096==    by 0x46C607: OneToAllMsg::exec(char const*, ProcInfo const*) const (OneToAllMsg.cpp:41)
==7096==    by 0x416892: readBuf(Qvec const&, ProcInfo const*) (Qinfo.cpp:232)
==7096==    by 0x4169DB: Qinfo::readQ(ProcInfo const*) (Qinfo.cpp:264)
==7096==    by 0x48F9CB: eventLoopForBcast(void*) (ProcessLoop.cpp:53)
==7096==    by 0x4E339C9: start_thread (pthread_create.c:300)
==7096==    by 0x5F1F70C: clone (clone.S:112)
==7096==  Address 0x6638bb0 is 0 bytes inside a block of size 80 free'd
==7096==    at 0x4C26DCF: operator delete(void*) (vg_replace_malloc.c:387)
==7096==    by 0x50A4B3: FieldDataHandler<SynHandler, Synapse>::~FieldDataHandler() (FieldDataHandler.h:37)
==7096==    by 0x4060E6: Element::~Element() (Element.cpp:103)
==7096==    by 0x415D1D: Id::destroy() const (Id.cpp:142)
==7096==    by 0x40779F: Element::destroyElementTree(std::vector<Id, std::allocator<Id> > const&) (Element.cpp:414)
==7096==    by 0x493F88: Neutral::destroy(Eref const&, Qinfo const*, int) (Neutral.cpp:413)
==7096==    by 0x4D9E65: EpFunc1<Neutral, int>::op(Eref const&, char const*) const (EpFunc.h:79)
==7096==    by 0x46C5EE: OneToAllMsg::exec(char const*, ProcInfo const*) const (OneToAllMsg.cpp:45)
==7096==    by 0x416892: readBuf(Qvec const&, ProcInfo const*) (Qinfo.cpp:232)
==7096==    by 0x4169DB: Qinfo::readQ(ProcInfo const*) (Qinfo.cpp:264)
==7096==    by 0x48F9CB: eventLoopForBcast(void*) (ProcessLoop.cpp:53)
==7096==    by 0x4E339C9: start_thread (pthread_create.c:300)
==7096== 
pure virtual method called
terminate called without an active exception

============================================================================
23 May 2011
Moved the doStart function around in the file. It works fine until after the
command, Shell::doUseClock(). Put printf debugging in Shell::handleUseClock to
see if the valgrind error is within it. It is not.

Turned out to be a very subtle bug. One of the SrcFinfos defined for SynChan
was being called in process, but this SrcFinfo had been left out of the
list of Finfos for the class. As a result, its 'send' calls went to the default
Fid which is to destroy the target... I need to make it impossible to call a
SrcFinfo till it has been initialized. Done.

Now I'm in to the numerical tests for SynChan. Implemented doubleApprox
test to be somewhat more forgiving than doubleEq. Clears these tests.
Checkin 2684.

I wanted to make a channel base class too. Only issue is that with SynChan
we'd have to deal with double inheritance.

Added CaConc. Implemented its unit test. Clears. Checkin 2685.
Added Nernst. Implemented its unit test. Clears. Checkin 2686.

============================================================================
24 May 2011.

Working on channel base class. Will have to figure out what to do about the
SynHandler base class for various things dealing with synapses. 
- use a base class that is not a MOOSE class? 
- Have a 2nd stage of base class, one that is both a SynHandler and a channel?
	- We will then have 3 base classe
		SynBase
		ChanBase
		SynChanBase
This should work.

Renamed SynHandler to SynBase. Checkin 2689.
Implemented ChanBase. Checkin 2690.

Checking it out. Nagging bug in the HH calculations, comes out different
on different runs, but takes only a few possible values. Thought it was
failure to init a field, but that wasn't it. Valgrind might find it.
Found it. I had defined Vm both in HHChannel as well as in the ChanBase.
Now clears unit tests. Checkin 2691.
============================================================================
25 May 2011.
Missed out updating some of the test files in other directories with the
conversion from SynHandler to SynBase. Checked in 2693.

Implemented skeleton SynChanBase. This uses dual inheritance at the
C++ level, but as MOOSE doesn't know how to do dual inheritance it replicates
the SynBase fields in its code. Will be a maintenance issue to keep the two
sets of fields identical. Checkin 2695.

Running tests. Fails. The object returned by looking up the dataId of
the SynChan object is not correct.

Tried makeover of SynChanBase, where it inherits from SynBase, and simply
maintains its own entry for ChanBase. Had to reimplement all the ChanBase
calls to point to this internal entry. Still crashes.
============================================================================
26 May 2011.
Some minor cleanups, now it clears unit tests. Valgrind also OK.
Checkin 2698.

Added GHK class. No tests yet. Checkin 2699.
Added NMDAChan class. No tests yet. Checkin 2700.

Looking at HHChannel2D and HHGate2D. They are derived from their 1-D 
counterparts in the old MOOSE, but I think instead that HHChannel2D should 
be derived from ChanBase, and HHGate2D should be de novo.

Inching along implementation. Will use Interpol2D for the
lookup.
============================================================================
27 May 2011
Subha needs proper arguments for LookupValueFinfos... Need to add the
lookup argument in its rttiType function. Done. Checkin 2704.

============================================================================
29 May 2011
Implemented Interpol2D prior to doing HHGate2D etc.  Checkin 2707.
Implemented HHGate2D prior to doing HHChannel2D. Checkin 2708.
Implemented HHChannel2D. Checkin 2709.
Implemented MgBlock. Checking 2711.

With this I have the basic biophysics directory under control.
Next is to work with Aditya and Subha to run models which stress-test
this, even if it is only using the default Exp Euler method.
	- NeuroML spec of network
	- Connectivity functions, higher-order Message calls.
	- Parallel tests and benchmarks.

Then, or in parallel, back to spatial specification of reac-diff models.

============================================================================
30 May 2011
Going on to make a regression test for a network of copied HH neurons. Right
away ran into issues with Shell::doSetClock. The setup of the timesteps
needs to be assured before we go on, but this causes a race condition in 
testShellAddMsg in earlier unit tests. This function and several others should
really be handled as structural operations.

Use regular Set/Get operations to set up the timesteps
Use special procState operations to complete the 'rebuild' call.
doUseClock is done properly as a structuralQ protected function doing
messaging. But see below.
I need a function for clearing the schedule.
I would like a schedule chart on the Clock: basically a lookup table of
	each tick against its target path. May as well be a field of the Tick.
	Shell::handleUseClock would be modified to only append things to this
	path after checking for overlap.
The new messages are put in at the 'rebuild' call. Or maybe I need a separate
call to reschedule and to add things to the schedule, which involves messaging.

Did the simple thing: put in a SetGet::set call to call the Clock func
for setupTick.

Implemented test for building and running an array of HH-type single-compt
cells. Confirmed that the array itself gets built well.
Now getting segv in reinit, when message goes from compt to HHChannel (Na).
It is because the FuncId is 57 whereas there are only 53 funcs known.

============================================================================
31 May.
Tracked down the problem with the crashing copies to an issue with 
OneToOneMsg::copy. Fixed. Also found a new way to crash things: by deleting
the prototype of HHGates before the copies. Should figure out how to avoid this.
Now it clears a major regression test: to set up and run an array copy of
a spiking HH compartment. Checkin 2714.
Next, connect up the compartments.
Doing so. Turns out that the Synapses don't have any allocated DataHandler.
Why? The HHGates were OK. Closer analysis: Not OK. I was not trying ever to
access them, when I did so it turns out that their DataHandler is not allocated
either. Easy to fix: FieldDataHandler::copyToNewDim needs to do something
sensible. In addition, I had to create a post-copy operation to fill in the
new parent of the FieldDataHandler.

Next, went on to making the network. Numerous problems, most seriously
that the FieldDataHandlers were not copied properly. Fixed.
Currently a little stuck on how to check that the connections have been done
and how.


Need to scan all Msgs to ensure that they do the right thing when copied.

============================================================================
1 June
Some minor checks to confirm that synaptic messages are indeed going around.
Looks good. Checkin 2715.

Next: 
- Scan all Msgs regarding copy
- Try out network regression test on multicore and multinode configs.

Trying on ghevar. Compilation error when compiled without unit tests.

Some more compilation fixes, also added the hhNetwork regression test as
a benchmark named hhNet. It tests an array of 1000 of these 'cells'.
Checkin 2718.

Tried running this on ghevar with optimization. It gives a segv. However,
on mishti it is OK without optimization.

Machine		Compilation mode	# threads	Result	time
Mishti		default (non optimized)	2		works
Mishti		optimized		1		works	25.5
Mishti		optimized		2		works	18.2
Mishti		optimized		4		works	25.8
Ghevar		default					Fails
Ghevar		optimized				Fails.


Debugging Ghevar, it fails in innerAddSpike when pushing in the new spike.
One option is to valgrind it on mishti. Did so. No signs of any problem.

Went through Msgs, fixing the problem with copies. Doesn't affect any
of the current unit tests. Checkin 2720.
============================================================================
2 June.
Subha says: The compile command was:

make pymoose BUILD=debug USE_MPI=1

Will test to see where things fail. Tried. Need help with applying gdb.

Fixed Shell:doCreate so it now takes an optional isGlobal argument.
Checkin 2723.

============================================================================
5 Jun 2011
Compiled for pymoose, now the basic pymoose runs OK. The MPI version dies but
it looks like it is in the openmpi libraries.

Several MPI functions will now be stress-tested because of the change above
to default the isGlobal argument to zero.
Did a non-pymoose MPI version. This also dies for more  than one node. This
is because of an error in handling data deletion and inheritance of these
deletion functions.

Carrying on with the unit tests on multiple nodes, other problems:
- Fails on assertion in testCopyFieldElement in testShell.cpp.
This comes down to fixing the synchronization of dataHandlers across nodes.
For now, several important fixes done to the delete operations of 
global DataHandlers, because they get called as base classes of the node-
local DataHandlers. Checkin 2725.

Need to figure out how to have ElementFields be assigned/accessed on
all nodes, even when the DataHandler is node-local.

Working on a Doc for ReduceOperations. About halfway through writing.

============================================================================
6
Done doc. Working on fixing things. Key thing is to enable direct assignment
of FieldElement sizes. This means that the set/get calls for fieldElement
arrays need to be accessible from the FieldElement itself. 

Making a new ReduceFieldElement class as part of this,

Replacing Eref on ReduceBase with Id. Perhaps should be ObjId.

============================================================================
7 June 2011
These changes trickled down to lots of things. Now it compiles but doesn't
clear unit tests.

Some more fixes, now it clears regression tests.

However it doesn't work with computing #fieldEntries in mpi.
============================================================================
8 June 2011
Checkin 2730.

Minor cleanup: I put ReduceBase.h in the header.h file, and went through all
relevant .cpps where I had included ReduceBase and ReduceMax ahead of Shell.h.
Checkin 2731.

Fundamental issue: Why does the testBuiltins::testMpiStatsReduce report 
different totals on different nodes? Can I examine the transfer buffer after the
transfer?

Further tests. Two problems:
- How to deal with ElementFinfos, ones that refer to a field on the parent
	Element. Two issues for this
	- isDataHere should be true even if there is no data on current node.
	- Should only do a single assignment per node.
	- However, some ElementFinfos associated with FieldElements do
		need to be object and node specific.
		- some ElementValueFinfos are used just because we want
		to have the element handy to send on a msg or similar ops
		involving the Element.
		- Zombies use ElementValueFinfos in order to extract their own 
		Id, which looks up the solver tables.
		
- The Reduce code gets confused with globals. It adds everything up. 

Lots of printf debugging put in. Something funny is hapening with 
contents of RecvBuf.

============================================================================
9 June
Starting to make sense after lots of printf debugging:
- RecvBuf entries are identical on all nodes. The earlier code assumed that the
	local node entry was on node zero, and is therefore wrong.
- Entries match regarless of number of threads activated per node. Good.

Fixed the RecvBuf reducing code, using tertiaryReduce. Now all nodes end up
with the same values after reduction. Problem is that this value is wrong.
It also differs depending on how many nodes are used.

Fixed the problem with the ReduceStats test in testBuiltins::testMpiStatsReduce.
It turned out to be that the ElementFinfo assigning fieldDimension was only
going to node 0. In other words it was one of the problems listed above. My
solution was to put code into checkSet to change the DataIds of
affected fields with DataId::any; and corresponding code into the DataHandlers
to deal with it. Still a possibility for multiple assignments from different
threads, not sure how likely this is.

Checkin 2733.

Cleaning up lots of printfs. Remaining 3 warnings on access to zero-sized
synapse arrays are all from Get and Set calls to Element fields:
linearSize
dimensions
fieldDimension.
Still trying to figure out why these calls trigger a call to getSynapse.

For now, checkin 2734.

More clean up of printf debugging. This also meant a cleanup of a number
of calls to getSynapse where there was no data available, as the call
was really to work on Element fields. Some rather nasty template stuff 
followed, by trial and error, till I got something that compiled.
Checkin 2735.

Now after all this, went back to the regression tests with MPI. Fails in
ZeroDimHandler::copyExpand. 
This turned out to be because the source objects were not global. Works when
they are converted to global, but I need a general fix for this.

Now it crashes in the results of the network in rtHHnetwork.cpp. 
There is a problem in copyExpand, it doesn't seem to keep track of 
the node decomposition.
ZeroDimHandler::copyToNewDim and copyExpand both need to do something better
here.

Put in some tests to avoid the problem of making array copies of local data 
onto multiple nodes.  Checkin 2736.

More work on copyExpand. There is a problem copying from globals to locals.
Need a way to pass on the node mode from the Element creation function to
the DataHandler copy function. Could simply propagate a flag through.
============================================================================
10 June 2011

Can I establish rules for copies in all four cases:
local->local
local->global
global->local
global->global

In all cases the sensible outcome is that the entire contents of the Element
are copied over and the array index partitioning is done according to the 
usual rules. In the case of single copies there is no problem, everthing 
copies over just as is, and possibly globalizes if it is a single copy of a
local to a global. In the case of array copies we have the following cases:

local->local
	Trickiest. Start with some partitioning, and end with a different one.
	Will need to copy things from node A onto node B.
	Will need to globalize the original version to accomplish this,
	then unglobalize it.

local->global
	While no funny partitioning is needed at the output, still need to
	do the globalization/unglobalization of the original version.

global->local
	Straightforward. Copy whatever subset is needed for local node.

global->global
	Simplest of all. Copy everything.


Working through DataHandlers to implement the change in copy calls to include
the toGlobal flag. Done ZeroDim.

============================================================================
11 June 2011
Still working through them. AnyDim has not been done but the hooks are put in.

checkin 2471.
At this point clears regular tests, but breaks in the regression tests 
for rtHHnetwork.

Some minor fixes later, and now we have it working correctly and giving
identical output on 1 to 4 nodes.  This is a major milestone. Need to
confirm it works properly on ghevar and on gj.

============================================================================
13 June:
Some things to do
- Get code to run on Ghevar
- Get MathFunc to work with solver.
See long list from 1 May 2011
============================================================================
14 June
Some messing around fixing up the Markov channel compilation.
Compiled code on Ghevar. Still doesn't work with the benchmark test for hhNet.
Still works fine on mishti.
Fixed some minor double-to-int conversion warnings. Checkin 2754.
Added warning messages in Shell::doUseClock in case the provided path has
no Elements.

Dealing with a funny bug revealed on Ghevar: With 12 or more compute threads, 
the system fails an assertion in testShell.cpp:420. Here the totalEntries
in a synapse has not been updated following the doSyncDataHandler call.
getFieldDimension also returns 1, should be 10 in this case.
Oddly it works with fewer threads.
Oddly localEntries is still OK.

Check: what is the thread limit if there size = 5?
Yes, the thread limit goes down to 5.
Likewise, with size = 7, thread limit is 7.

Note that with size = 10, thread limit is 11, not 10.

GDB stepping through it. One bug that doesn't seem to have an obvious
effect is that the iterator
goes through every field array entry, which is what this iterator is
designed to do. This is good for reducing Stats, but not for a field like
maxIndex, which is common to all the array entries.

I think I have found the bug. Subtle. I create a ReduceFieldDimension
object in ReduceMsg::exec, but I rely on the exec to call the
primaryReduce() for the object in order to correctly initialize the
tgtId_. The default, of course, is Id(). Turns out that when final
reduction happens in reduceNodes, it is the root Element whose 
FieldDimension is being set.

Fixed. Works. But regression tests still bomb with 16 cores.
Checkin 2758.

Could do a lot of cleanup and optimization on ReduceFieldDimension. For one,
could put in a flag to indicate that the ReduceMsg::exec need not scan all
fields, or even threads. Later.

============================================================================
Designing a prototype spatial chem model that would deploy the components from
31 March.
: spiny section of dendrite.

Note that the mesh, mols, reacs etc are array entries in a 1:1 mapping to each
other. Mesh entry provides associated mols and reacs with volume and position
info.

/sigNeurModel
	/geom
		/extracell_surface
			/panels
	/elec
		/hsolve
		/compts
	/chem
		/ksolve
		/somaCompt
			/mesh
				/mols...
				/reacs...
				/enz...
				/groups, etc
			/nucleusCompt
				/mesh, etc.
		/apicalDendCompt
			/mesh, etc
		/spineGroup
			/gsolve
			/spineNeckCompt
				/mesh, etc
			/spineHeadCompt
				/mesh, etc
		

Msgs between components:
	- mesh-> mols etc: to update stuff based on vol.
	- mols etc-> mesh: to query vol.
	- mesh<-->geom: to figure out 3D location, to figure out partitioning,
		to generate a 'surface' to represent the mesh on its own.
	- mols etc <--> ksolve: zombification
	- ksolve <---> gsolve: ports
	- elec compts <---> adaptors <---> chem compt components: multiscaling
	- ksolve <--->hsolve: coordinate multiscaling.

============================================================================
16 June
Cleaning up memory leaks using valgrind and Newtonian bisection.
testMpiMsg is clean.
testMpiShell is not. Bisecting testMpiShell.
	Last half of testMpiShell loses 40 bytes.
	Last quarter of testMpiShell loses the 40 bytes.
	Neither of testCopyMsgOps and testWildcard lose the 40 bytes.
	testSyncSynapseSize is the culprit. It calls doSyncDataHandler.

Tried to put Shell::clearBinding of reduceArraySizeFinfo onto the 
	Shell::handleSet. Doesn't fix it.

Did a huge amount of messing around trying to fix this. I now have a version
that doesn't segv but does make valgrind unhappy. I used statically allocated
Msgs to avoid the memory leaks. The current problem with valgrind boils down to
the fact that all three of the temporary messages: AssignVec, Assignment and
Reduce, use the same MsgId, Msg::setMsg. Since this is such a mess I'm going
to check it in as a branch. That was checkin 2764 on branch badAssignMsgs.

Revisited the memory leak. I think I've fixed it, but there are now other
leaks in the Markov code so I can't be sure. Checkin 2768.

...................................................
Back to the spatial kinetics.
Made some initial changes to readKkit.
Begin implementing ChemMesh class. maybe it is a generic mesh class.
Handles volume, a reference to a geometry (probably via msgs), 
Should have a field to specify type of mesh: axial, spherical shell.
Need a way to talk to the pool objects, which will query it for vol 
whenever needed.

============================================================================
17 June
Implementing message querying stuff to get pairs of src/dest DataIds,
needed for detailed traversal of Msgs.
Msg::srcToDestPairs.
Compiles but no new unit tests for it as yet. Also the SparseMsg version is
not yet node-safe.

Also commented out some half-baked changes in ReadKkit. Checkin 2772.

Ran valgrind. OK.

Decide if I need to have multiple Mesh variants:
From 1 May:
Some mesh forms:
				Dimensions
- linear			1 (might subdivide a 3-D space though)
- rectangular			2
- cuboid			3
- Triangular			2
- Tetrahedral			3
- cylindrical radial		1
- cylindrical radial-axial	2 Concentric axial compartments.
- cylindrical segmented		3 radial-axial-segments
- spherical radial		1
- spherical radial segmented	3 Possibly could have a symmetry to make it 2D

Common fields:
	- vector of adjacent mesh indices
	- size
	- dimensions (serves secondary purpose of defining what form of the
		shape should be used: 3D or 2D)
		


Mesh needs to be able to generate surfaces corresponding to each subdivision
Mesh needs to be able to generate subdivisions based on various rules
Mesh needs to be of different classes each of which does the subdivision
	according to internal rules. Initial ones are sphere, cylinder,
	spine, dendrite.
Mesh corresponds to neuronal/cellular functional compartments. So within a
	mesh all reacs are the same.
Mesh supplies diffusion rules between mesh entries, including scaling factors
	for non-cartesian cases.

The ChemMesh class is a base class for different kinds of meshes. These
include CylMesh, SpineMesh, OnionMesh, DendriticTree, and others 
eventually to come.

============================================================================
18 June
Began implementation. Made a new 'mesh' directory. Started with
ChemMesh.cpp to go into this new directory.
============================================================================
19 June
Compiled mesh directory. Checked it in. Yet to put in unit tests.

Completed missing fields in CylMesh. Checkin 2779.

Added first pass unit tests. Checkin 2780.

Added second pass unit tests. These were pretty tricky, lots of fiddly
details had to be fixed in the CylMesh to do with mesh partitioning.
Checkin 2781.

Next: 
	- Add more unit tests for the MOOSE field access of the CylMesh
	- Implement the cuboid mesh.
	- Get them to talk to reac systems. See if there is a smart
		way to set up messaging (other than SparseMsg).
	- Get them to talk to the solver.

============================================================================
20 June.
Added more unit tests for MOOSE field access to CylMesh. Lots of bugs cleaned
up, now clears the tests. Checkin 2782.

Designing Cube mesh. If it is to do more than just make big blocks, I need
to specify a surface within which the mesh exists.

- Perimeter polygon. Like contour lines in z. Each perimeter needs one 
	extra point to define 'inside', that is, where to do the meshing.
- Collection of surfaces, through a geom. Will need somewhat more to figure out
	inside, and it will also be tricky to deal with each surface to build up
	composite.
- Could set up perimeter as either a LookupValueFinfo, or as a FieldElement
	handling a set of geoms. Still is nasty to figure out what is inside.
	
Rather than deal with this, I'll set up a simple vector in which each entry
stores the index that it would have had if it were a complete cuboid, and a 
bitmapped flag to say which of the six directions permits diffusion. The 
construction of this vector is another whole exercise.
I'll need another vector from the full cuboid to the actual mesh set. May
need to use a multi-dim sparse matrix in some cases. This obviates the need
for the bitmapped flag: the requested entries will simply be empty.

Implemented first pass at CubeMesh. Compiles, no tests yet. Checkin 2783.
Working on unit tests.

============================================================================
21 June
Added in first pass of unit tests to CubeMesh. Clears. Checkin 2784.

Now looking at the 4th-order in time and space method of Liao et al.

Can be done, needs new solver system, but can probably use the Stoich to
load up the model.
Things to do:
- Redo ReadKkit to properly deal with ChemCompt and Mesh. Also convert from
	Single to OneToOne messaging.
- Fix up volume handling stuff in reacs and in pools to use Mesh
- Provide Mesh function to propagate changes in # of mesh subdivisions
	down to all objects under it.
- Test under EE integration
- Update Stoich::convertIdToPoolIndex/ReacIndex/FuncIndex functions to take
	the ObjId
- Some Zombie fixes perhaps to make them aware of array entries.
- Provide additional object or features in Stoich to manage updates of
	all array entries. Seems like it is just a matter of extending the
	dimensions of S_ and Sinit_.
- Get Stoich to do its calculations in parallel on each node/thread. In other
	words, make it easy to specify which subset of array entries will
	be updated by any single call. Requires some work on thread safety.

- Get Stoich to handle multiple compartments with distinct reaction sets.
	Possibly do using multiple instances of the Stoich, and have a
	coordinator for integration.

- Implement code for diffusion calculations within the RK5/Stoich framework.
- Implement new code for 4th-order reac-diff calculations using the Stoich
	but different engine.

- Get the surface generation going for the Mesh, for display of 
	compartmentalization
- Incorporate yet more solvers.

.....................................................................
Starting off with ReadKkit. Currently it calls assignCompartments after the
model is loaded with the usual GENESIS object tree. The compartments are
created on the baseId, which is a stoich whose name is passed into the 
readKkit command. I refer to this as /model. Under this we create 
/kinetics, /graphs and all the other stuff from the kkit file in 
a standard call.
Following the reading of the kkit model, we call assignCompartments. This
builds one compartment (and only one) under /model. 

I think the simplest way to do this is to convert /kinetics into a Compartment.

Also we can merge the concepts of ChemCompt and ChemMesh. The ChemCompt
only added fields for Boundaries, which should go into ChemMesh.

/sigNeurModel
	/geom
		/extracell_surface
			/panels
	/elec
		/hsolve
		/compts
	/chem
		/ksolve
		/somaCompt
			/mesh
				/mols...
				/reacs...
				/enz...
				/groups, etc
			/nucleusCompt
				/mesh, etc.
		/apicalDendCompt
			/mesh, etc
		/spineGroup
			/gsolve
			/spineNeckCompt
				/mesh, etc
			/spineHeadCompt
				/mesh, etc
		

Rules for reading and converting Kkit files to this form:
- Read in with /kinetics as a group (Neutral) and the old kkit element tree
- Go through and parse for differing volumes, note these down as required
	ChemCompts.
- Make /kinetics a ChemCompt (merged with ChemMesh)
- Move the entire tree onto the mesh of this ChemCompt. Note that we have
	to move without the original /kinetics parent here.
- If there are any subsidiary volumes, check for identity with a group. 
	- If identical, replace the group with the new ChemCompt and move 
	contents onto its mesh
	- If differing, make a new ChemCompt anyway, and move the objects with
	different volume into its mesh. One can easily identify mols and most
	enzymes to move, but reactions will have to be done on the basis of 
	the vols of what they connect to.

- Extend mesh capabilities to handle these nested situations. 
	- Consider, for a dend spine, we would have a mesh for the spine
		neck and head, and another for the PSD. But this doesn't
		work if we want the whole spine replicated with each mesh entry.
- Container relationships here are assumed by nesting.


.........................................................................
Setting up ReadKkit parse. Instead of putting mols etc as children of mesh,
here I make them sibling. I've assigned size. 
Q: How do objects find their size? 
	- Return msg for size: Simple but slow, uses queue.
		Problem is to make the request without race conditions 
		as the size query itself is waiting for ack.
	- Traverse down parent msgs till we see a ChemCompt, then look for its
		Mesh. Get the size for the matching entry.
		-This doesn't work because even if the traversal is clean I need
		to do a 'get' request which uses the Msg system anyway.
		- Or do the evil thing and look up the pointer... not good.
	- Assign 'size' from the mesh itself, store it locally in the
		relevant Elements. Unpleasant, much duplication.
	- Implement fastGet operation, which uses message traversal on 
		existing msgs but only
		on local objects, and does not use the queue. Off-node fastGet
		must cause program exit, I don't see any way to recover from it.

Implemented last one. It has a lot of promise and should be useful in many
cases. Ripple effects through all Msgs. Also had to comment out regression
tests on ReadKkit due to ongoing work. fastGet not yet tested but compiles.

Checkin 2787.

Added unit test for fastGet. Works. Checkin 2789
Need to add section to test for it to look up FieldElements.
Added section for FieldElements. Revealed problems, fixed. Checkin 2791.
Now let's deploy this for the volume handling for pools and reacs.

Working on the Pool size handling. 
============================================================================
23 June 2011.
Updated Pool to use fastGet to look up mesh entry size. Checkin 2793.

Added unit tests to check that Pool does the right thing using fastGet to
look up mesh entry size, and for conc conversions. Checkin 2794.

I clearly need to extend the interface between Compartment, mesh and pool so
that I can change compartment size or lambda, and automagically rescale both
the pool dimensions, and also the number of entries in the pool (mesh
happens anyway.) As this is a structural change, I think it is best to
pass the work over to the Shell. But this can come later after the
ReadKkit is working again.

I can get ReadKkit to work, but both reac and enz need to have additional
fields for their rate constants, and I need then to fix up the scaling if
the compartment volume changes.

Somewhat premature addition of volume handling to Reac.cpp. Assorted
fixes needed, including the implementation of a consts.cpp file for
numbers like PI and NA. Checkin 2797.

Now back to the ReadKkit.
Lots of errors when I run it again. I need to fill the vol_ vector instead
of trying to assign vols to the pools.

============================================================================
24 June 2011
Cleaned up ReadKkit. Now it clears regression test in gdb, but complains about
odd-number sizes for queue entries. But it doesn't clear in non-debug mode.
Checkin 2799.

Valgrind showed up a number of leaks and bad memory accesses. Fixed. But this
does not fix the problem with odd number sizes for queue entries.
Checkin 2801.

- Need to complete the volume scaling: enzymes.
	- Do unit tests for reacs and enz rate scaling.
- Merge ChemMesh and ChemCompt, using the latter name but the former code.
- Do a mesh expansion, perhaps using a Shell function.

============================================================================
25 June 2011
Vishaka caught an equality comparison between doubles. Fixed. Checkin 2802.

Implemented volume scaling for enzymes. Yet to test. Checkin 2804.

Reconsider enzyme and reac compartmentalization. Should really set up
rate scaling based on compt volumes of each substrate. More tricky is setting
which compartment they belong in, if they transfer stuff between two 
compartments. The two compartments may have different mesh geometries. Assume
these match up in an integral way, that is, the face of one or more mesh
entries are always completely within the other face.
- For enzyme sites, should simply be in same compartment as parent Pool.
	- Enzyme substrates on a different mesh with smaller integral fraction
		size of Enzyme mesh: Make duplicate enzyme sites on same parent
		enzyme Pool, one enz site per substrate mesh entry.
	- Enzyme substrates on a different mesh which is an integral multiple
		of Enzyme mesh: Each enzyme entry does its own thing.
	- Channels interact with 3 meshes: outside, inside, and membrane.
		Assuming integral mappings, we put channel sites on proteins in
		the membrane, and the # of sites is the same as the finer mesh.
		Cases with 3 on one side to 2 on other are Naughty.
  In other words, we need as many enzyme sites as the finer of the two meshes.
- For reacs, they should be in the smaller compartment. Then we always have
	the second case from above, where each reac entry does its own thing.

For now: relatively small change but with lots of knock-on effects:
	Merged ChemCompt into ChemMesh, moved Boundary into mesh directory.
	Checkin 2805.

============================================================================
27 June 2011
Put in the mesh/compartment asignment for Reacs and Enzs in ReadKkit. This
works on 1 thread, fails frequently on 2 threads for a variety reported
reasons. I suspect the msg queuing. Checkin 2808.

Ran valgrind on the entire unit+regression tests. It reports things as
absolutely clean. 

Machine	Compile	#c:n	#tests	clean	fail	oddbuf
mishti	-g	1	10		
ghevar	-g	1	5		5
mishti	-g	2	10	3	4	3
ghevar	-g	2	8		8
ghevar	-g	16	8		8

mishti	mpi -g	2:1	12	3	6	3
mishti	mpi -g	2:2	8	8
mishti	mpi -g	1:4	2	2

This is interesting. When we have MPI comms the problems seem to go away.
I tried to compile for MPI on ghevar, but it didn't work. Should try on gj.


Now to implement Shell::doChemMesh. This is going to do the following:
- Set up the mesh on all nodes: a global.
- Traverse down the compartment hierarchy. Figure out if we have to do
	anything between levels. Usually not.
- Resize all the meshed Elements. Traverse the messages to know which to do.
	- Need to do something intellegent about the # and rate scaling.
- Identify any problem Reactions and Enzymes. Do something about them.
- Do something sensible about ksolves for handling arrays. What about
	reaction sets in different compts?
- Set up the diffusion in the ksolves.


Working on the mesh builder, Shell::doReacDiffMesh, around 725 in Shell.cpp.
============================================================================
29 June.
What to do about the volumes when re-meshing. Clean thing is to get the
average conc of each pool, and the total vol. Then re-apply this to the
remeshed model.


For now, trying to compile. After some effort, compiles and clears
unit tests, but the meshing itself has not been tested.
Also replaced a printf in MatrixOps.cpp with a cout, to get it
to compile with the updated version. Checkin 2816.

============================================================================
30 June
Revisiting reactions and volumes and meshes. Conceptually, the reaction
should only be a single instance defined by rate terms in concentration units.
Likewise the enzyme. The multiplication of individual entities is something
needed only for runtime state variables in the explicit calculation, and for
matching up pools belonging to different mesh entries. By this reasoning,
the pools and enzymes should store their parameters in concentration units,
and have only one set of parameters over the entire array.
If I do retain the explicit calculations, will need to do some nasty stuff to 
	- Figure out which of the incoming meshes is the finest.
	- Make arrays of the reac/enz site according to the finest.
	- Figure out individual scaling factor for each reac/enz instance
		OR pass in volume term along with the conc on each step.
	- Find a way to set across-array fields for rates. Furthermore, these
		would be globals, having to update across all nodes.
		OR: don't turn the Reacs into arrays at all, but internally
		manage an array for state variables. This gets ugly with 
		messaging though.
		The messaging is ugly anyway.


Started out with the unit test for resizing the CubeMesh with a Pool attached.
Various loose ends with DataHandlers and Elements had to be fixed.
Clears the basic test, just replicating a Pool throughout the mesh.
Pool values also replicate. Checkin 2821.

Let's not have explicit Euler calculations for the Reac-diff mesh. It 
should never be done for numerical reasons, and it is going to be a huge pain
to set up cleanly with the Reactions and their messaging, precisely because
they may interface between pools in different meshes. Instead, Reactions and
Enzymes are _not_ to be replicated when meshing. Additionally, they take up
concentration-dependent terms and the terms in 'n' are there for backward
compatibility but not normally used. 

If we take the logic further still, this exercise for pools wasn't really
needed. We'll have the zombies set up the arrays and they can communicate
more directly with the mesh. Anyway, it cleaned up some code.

I think it is time to go right to the reac-diff solver. The design will be
clearer once I have calculations that work.

- Continue to use Stoich as a basis for setting up the system. 
- Modularize the reac updates: 
	- Blocks for each compartment
	- Blocks for cross-compartment calculations, with the indexing set up.
- Modularize the diffusion relationships
	- Start out with a block treating flux as reacs so we can use RKF5.
	- Work out how to fit in the 4th order method.


I think I should do this in a new directory. I want to be able to 
replicate most of what ksolve already does, and be able to replace the
old ksolve with this more general version. Some aspects of multithreading
should also be possible to set up.
============================================================================
4 July 2011
Something to stress test: write a unit test for start/stop of simulations
from the parser thread, where we do this a lot of times to be sure to catch
threading problems with it.

============================================================================
5 July 2011
Wrote and ran stress test for start/stop of sims. It clears the test,
even for 8 cores.
Checkin 2824.

So the start/stop code isn't apparently the problem. Next steps:
- test on ghevar
- Put an assertion on the queue entry odd size to trap it and track it down.

============================================================================

6 July 2011.
Ghevar test is OK, even on 16 cores.
Changed the start/stop test to do two timesteps. This too is OK. Checkin 2825.

The hackForSendTo has turned up in the regression test problem zone.

I put in several more tests in the readBuf function in Qinfo.cpp, to
check that the buffer entry is good. Lots of problem entries come out.
Some observations:
- Only happens with > 1 thread
- Only happens in the rtReadKkit section, at doReinit or doStart.

Tried setting runtime to a value not an integral multiple of dt, to ensure that
the sequencing doesn't suffer from roundoff error. Still get assertion
fail for # of steps, still get lots of 'odd sized' outcomes from ReadBuf.

============================================================================
9 July 2011
Major clue: Looks like ReadKkit is not the problem, it is something that
happens when the model is zombified or when the gsl integrator is built.
============================================================================
10 July
Tracking down error:
- Put in assertions for zero MsgIds in addToQForward and Backward. Doesn't
pick anything, though subsequent readBufs do show zero MsgIds. Is this a
case of Msg corruption?

Tried a couple of runs using valgrind, no problem found.

============================================================================
12 July
Possibly the setup process of the ksolver messes up the messaging. If so, 
then it should be possible to get the same errors if I do the setup as
before, but don't run it. Instead run another model. First run just the
graphs.

Going through the rtReadKkit steps, cutting away parts to find which causes
errors in the clock running. Seems the problem is loading the model after all.
============================================================================
14 July.
Narrowing it down. Looks like the second of the doReinit calls in 
rtReadKkit around line 67, introduces errors into the message queue.

A closer examination shows that the first doReinit does too, just not 
every time.

It begins to look like sending control requests around by messaging is somewhat
dangerous, specially between clock and Shell. There is no test for structural
Msgs, though of course all of these are structural.

The messages received by the Clock for reinit seem OK, but should
check what happens with the ack it sends back.

Note that all these errors occur even when no model is loaded. I have called
doSetClock six times before the first doRenit.

Should go back to analyze why the earlier start/stop tests didn't have a
problem. Is it reinit?

============================================================================
15 July.
Checking if earlier tests caused a problem that manifests only with rtReadKkit.
Commented out all regression tests except rtReadKkit. Still have the bug.
Conversely, if only rtReadKkit is commented out, all runs fine (5 repetitions
of the tests, two threads in each case).

Now commented out all the doSetClock calls. This leaves the doReinit
as the only think happening in the rtReadKkit function. All else has already
been commented out.

Now I get fewer queue entry errors (3 in 10 runs). Also it hangs in doStart
because the clocks have not had their timesteps assigned: must fix or provide
sensible defaults. I thought the default was 1.
When I put back just one clock, I have more errors ( 7 in 10 runs)

p-value: Null hypothesis: we have a single distribution with p as the
probability of an error. In the first set of samples we got 3/10.
Prob of this is p^3*(1-p)^7*(3C10)
In the second set of samples we got 7/10.
Prob of this is p^7*(1-p)^3*(7C10)

Combined prob is the product of these.
3C10 = 7C10 = 10 * 9 * 8 / (1 * 2 * 3 ) = 10 * 3 * 4 = 120.
Suppose p is 0.5. Then we have 0.5^10 = 1/1024.
So total p = (1/1024) * (1/1024) * 120 * 120 ~ 0.014. Rather unlikely.

Does the doSetClock add to this likelihood?  Uncommented all six clock
assignments.
I see a bad queue before doReinit 0/20 times. So doReinit is bad.

What happens if I have in fact loaded in the model?
bad queue before doReinit: 0/10
bad queue after one of the doReinits: 7/10.

doReinit it is.

Checked if there is a problem with the messaging during Clock::reinitPhase2
to send the ack. Used a direct pointer call to the Shell to handle the ack,
instead of sending the ack through messaging. Now it fails 10/10 times.

============================================================================
17 July.
There is a Compartment called 'compt' whose reinit is being called in 
rtReadKkit. This sends out a message. Where did this compartment come from?
Its parent is not root, just the id ahead of it. The parent name is 'n'.
The parent of 'n' is probably root, but gdb couldn't handle it.

I need to write a function to check for and clean up cruft from the unit tests.
First, what happens if I delete 'n' ?
Still croaks. Possibly other cruft is present. Need to do a 
systematic cleanup here. Working on Shell::cleanAll(), and a corresponding
utility func doClean.

Implemented. Reports and cleans more cruft, but the simulation still
croaks.

============================================================================
22 July.
Revisited the reinit calls. Now it goes only to the gsl and to the plot
objects during the reinits in rtReadKkit.

Followed these by putting breaks on the doReinit calls in rtReadKkit,
on Tick::reinit, and Msg::process.

This will introduce calls to requestData from the Table.
Still need to figure out if there are any other things in the queue.

Removed table from sched.
No msg Q problems in 10/10 of the runs using gdb.
No msg Q problems in 10/10 of the runs done directly.

Looks like the get call may be a problem.

============================================================================
23 July

Trying to pin down source of queue problems.
Put in lots of printf debugging. These show that all the reinit stuff seems to
be triggered from thread 1, but to be executed on thread 0.
Put a mutex to protect the queue in addToQforward and addToQbackward.
Doesn't help. This is a surprise. It would seem to rule out cases where the
buffer is trodden on simultaneously by different threads, which had been my
working hypothesis.
Tried valgrind, but as before, it doesn't spot anything, and the sim runs
this time flawlessly. Will try one more time. Again flawless. Heisenbug.

Try censoring input to addToQ... Did that already. I check for bad Msgs.
Try running with 1 core. There are still multiple threads, but only one
process thread. Still has problems.

Tried to put in a printf debug in Qinfo::addSpecificTargetToQ.
It isn't being called. Odd. How does the get return specify its target?  
Seems it just puts stuff in the queue without specifying unique target.
Is the fieldOp doing something bad, such as errors with the PrepackedBuffer?
Why should it only show up in the reinit?

============================================================================
24 July

Redesign of messaging for better type-safety, efficiency etc.

- The Qinfo really needs only the following:
	Source ObjId
	MsgBinding index
	data ptr or reference (implicit size info here, wrt next entry)
  This is because all nodes have the entire message and Element info, and
  can traverse through the message Binding stuff on their own.

Decisions:
1. Contiguous Qinfo and data, or a vector of Qinfos; and data separate?
2. Data in situ on source Object, or copied over to a buffer?
3. Freeform Data, or doubles, or uints?
4. Handling of reverse messages.
5. Message grouping or just universal transfer?
6. Handling of Shell-to-Shell communications
7. Threads and buffers

1. Contiguous Qinfo and data, or a vector of Qinfos; and data separate?
   Separate vector of Qinfos has the following gains:
	- Less likelihood of funny alignment problems => type safety on Qinfos.
	- Can preallocate: faster
	- May be able to return to the idea of synchronous messages where we
		know the Qinfos ahead of time and don't even need to send them.
   Separate vector of Qinfos has the following costs:
   	- Extra send or memory shuffling step to build a send buffer.
Decision: This is a clear case for separate vector of Qinfos.

2. Data in situ on source Object, or copied over to a buffer?
   In situ has the following gains:
   	- One fewer copy in many cases
   In situ has the following costs:
   	- Hard to do multiple readonly lookup/return type calls
	- Need to separately copy over data for send buffer
Decision: Copy it over to a buffer. The in situ version may need copying anyway
	for MPI send buffers.

3. Freeform Data, or doubles, or uints?
   Freeform:
    	- Efficient use of space. Marginal at best.
   Doubles:
   	- Alignment
	- No conversion for most common data type
   UInts
   	- Alignment

Decision: Use doubles. Significant work on Conv< A > needed.

4. Handling of reverse/return messages. Used in get functions.
  Extra flag in Qinfo to say it is reverse
  	- MsgBinding info now inadequate.
  Separate MsgBinding structure
  	- Will we need this for every 'get' field?
  Add return info as part of Msg content
  	- Brings back messy recasting of message direction info.
  Add a field in Qinfo to hold fid, the MsgBinding field now holds MsgId.
  	- Assumes originating MsgId had a unique Src.
 
4a. Handling of single-tgt msgs?
  Similar hack to current version.
  	- Horrible.
  Interpret regular ObjId as Dest, rather than Src.
  	- Loses convention that Src of Msg is always accessible.
  Ban them. Their only purpose was return to sender. See above.
  	- Hm.

Decision: Add FuncId field in Qinfo, it also acts as a flag that the MsgBinding
	field is the MsgId instead. Direction is assumed reverse.
 
5. Message grouping?
	No. For now just have one group.

6. Handling of Shell-to-Shell communications
  	- Special queue and special comms at a special phase in the cycle
	- Regular queue but special Msgs that only connect off-node.

7. Threads and buffers
	Use interleaved indices and a single buffer in two parts: the
	Qinfo part, and the data part. Ah. Problem here with data part, don't
	know its size ahead of time.
	- Can implement an experience-based buffer size
	- Can put in a mutex whenever adjusting buffer boundaries.
	- During Reinit, figure out all the sync buffer sizes and locations?
	- Can do the multiple memcpys as at present.


1---init alloc----2---init alloc----|
becomes, if 1 expands:
1---init alloc----2---init alloc----1---second alloc----|
or if 2 expands:
1---init alloc----2---init alloc----2---second alloc----|

With only 2 threads, could expand bidirectionally from middle, and never have
to worry about contiguity or about collisions:

<----------alloc for thread one----|-----alloc for thread two------>

With a good estimate of size, one could get the alloc about right (with 50%
spare capacity) about 90% of the time. Maybe the best thing is to realloc
as needed, and keep track. First cycle would require a lot of reallocs, but 
later cycles fewer and it would go faster.
	- Rather than do reallocs, set up a fallback vector for each thread
	that is occupied whenever there is overflow. Do the reallocs only
	at swapQ time. Also set up MPI data transfer sizes at the same time.
Decision: Two layer system, one for normal ops, the other if the one overflows.
	Normal ops is preallocated space for each thread, all contiguous.
	Overflow ops is a separate buffer for each thread, expandable.
	All these are addressed invisibly by addToQ functions.
	At swapQ time the normal buffer is reallocated and the overflow buffer 
	contents are inserted into it.
	* Do NOT use interleaved indices. Instead have each thread fill in a
	block. Interleaved will be harder to debug and will be slower.

Before playing with any of this, checkin 2845.
============================================================================
26 July
Setting up data structures. Need to figure out how to handle indexing and
keep track for each thread. I'm inclined to use absolute indexing so
each Qinfo knows exactly where its data is. This means we have to keep track
of the offsets for each thread and do a complete sweep through to update
all indices whenever resizing.

Start out with the overflowData_ and overflowQinfo_ only to handle first tests.
These behave a lot like the current system, with filling in during the process
and clearing out during swap. When stable, set up stats monitoring and then
the preallocated buffers.
============================================================================
28 July
Working through Qinfo.cpp with some fallout on Element.cpp

At Qinfo::innerReportQ, around Qinfo.cpp:459.
============================================================================
29 July 2011
Worked through data transfer and messaging rates. Seems like the required
size of the queues isn't really so big for expected model scales. It may be
better to dump data into individual queues rather than a pre-linearized
data chunk.

Struggling with the compilation. In OpFunc.cpp:fieldOp, we have a problem:
the Qinfo::addToQbackward needs a ProcInfo or at least a threadNum. 
Now, as Qinfo doesn't carry it, I need to figure out how to make it accessible.
Also, how do we find the relevant MsgId? The Qinfo no longer knows the Msg.
The calling Msg never passes in itself to the Opfunc,
============================================================================
30 July
Some options
- Include ProcId or ProcInfo in Qinfo. This specifies thread too.
	- Have a predefined set of ProcInfos, refer to by ProcId? No need.
- Drop ProcInfo argument from Msg::exec, if it goes into Qinfo. 
	Add Qinfo argument instead.
- Provide Msgs with a couple of flags to specify which thread and which node
	they are active on. Quick way to skip the Msgs.
- Add a new kind of Q entry which is inserted, msg-free, with specified
	target. Similar to the old one with the DataId of the tgt in the
	data section.
	- I would have to modify Element::exec a little to deal with the
	cases where the Qinfo::fid is nonzero, to indicate that the 
	Data includes an ObjId.
	- This could be used for 'set': Avoid all that silly temporary Msg
	stuff.
	- In the case of 'get' also is good.

I've put skeleton stuff for all this in, except the Q::proc. Here we
need to have a header entry for the # of buf entries (doubles) needed to
hold a Qinfo.
Redid Qinfo.h to handle the proc, as a ProcId. I actually have enough
space in the 24 bytes for the Qinfo, to put in a full pointer.

Q: When do we assign the ProcId to put into Qinfo? 
A: readBuf is the point at which the ProcId from the oldQinfo (whether
	from inQ or from mpiInQ) becomes irrelevant, and the new Proc is
	available. This calls Element::exec -> Msg::exec -> OpFunc::op.
	So the ops lose the proc info and must therefore not try to send msgs.

Q: When are Msgs sent?
A:
	- Msgs are normally sent in the process/reinit function, 
	which knows ProcInfo. 
	- Msgs may also be sent from EpFuncs, which are passed the 
	Eref and Qinfo.

Things to do tomorrow:
- Rebuild with the new Qinfo data structs.
- Decide whether to set up all ProcInfos with a pid, or to use  a ptr within
	the Qinfo. the ProcInfo is, of course, useless between nodes.
	- Use a pid. It is dumb to pass around pointers in a queue.
	- For now, see if we can get by with just the thread index.


============================================================================
31 July

TDL
- OpFunc.h: 419: Replace direct casting for fid with a Conv< FuncId> call.
- UpFunc.h: Deal with GetUpFunc::op.
- Conv.h: 404: template< class T > class Conv< vector< T > >::ptr()
	needs to be filled out
- Conv.h: Need to sort out a general scheme so that numbers can assign
	directly to the doubles, rather than fill in the memory location in
	their original format.
- Need to sort out threading of script vs main code. I'm now not separating
	the schedule into groups. The script threading should therefore
	send stuff onto a separate queue, presumably thread 0. This will
	have to merge its stuf finto inQ at swapQ time, again protected
	by the barrier. Currently I use thread0 in SetGet.h for Qinfo::
	addDirectToQ.
- Check the memcpies for convs in SrcFinfo and SetGet: size in doubles.
- Check the threadNum also in SetGet::Field::get usage of addDirectToQ.
- Element::exec will have to deal with getVec and setVec.
- Element::exec will have to sort out threads in the direct calls.
- HandleGet will have to deal with getVec, probably not too different
	from current format.
- Figure out if the assorted SetGet::set calls are safe to run without 
	wait for ack.
- Figure out how for SetGet::get call to wait for ack. Implement awaitGottenData

- Implement the two extra variants of Qinfo::addDirectToQ, put in the
	ObjFid stuff for the second.

- Figure out how to decide which thread to use for each DataId of an 
	object, in the various Msg::exec functions.

- For all the Shell API calls, figure out which thread to use. assume 0.
- Shell::digestReduceFieldDimension: unclear which thread to use for send.
	Assuming ScriptThreadNum.

Moved the current development to a new branch:
https://moose.svn.sourceforge.net/svnroot/moose/moose/branches/newQ
Checkin 2847.
Pegged away at it. Now compiles through the basecode directory.
Checkin 2848.
Now compiles through the msg directory. Checkin 2849.

Compiling through biophysics dir. May need to redo all of
SrcFinfo::send, to use Qinfo or even just threadNum instead of ProcInfo.
This has come up because some of the dest functions in SymCompartment.cpp
want to call 'send'.


============================================================================
1 August 2011.

Changed SrcFinfo::send to use threadNum argument. Also set up ThreadId as
identifier.

Using these changes, went through the biophysics directory and replaced the
ProcInfo argument in all 'send' calls with the threadNum. 
Now biophysics compiles. Checkin 2850.

Now builtins compiles. Working on Shell. Issue with Qinfo::addToStructuralQ.
Checkin 2851.

Setting up a mutex in Qinfo to protect the structural Q. To compile

Compiled through the shell directory. Checkin 2852.

Compiled through kinetics directory. Checkin 2854.

Compiled through geom directory. Checkin 2855.

At this point all directories compile, and there are many pages of
linking errors.

Ground through the linking errors. Now it completes the compilation. Promptly
fails when run. Checkin 2856.

============================================================================
2 August 2011.

Planning for threads and interface with script. Options
- Protect all of the addToQ functions with a mutex if it is ScriptThreadNum.
	This mutex will lock all of swapQ to ensure that the copying of 
	Q0 (the script queue) into the inQ is safe.
	- Messiness ensues: The addToQ functions that return a pointer to be
	filled in later will fail because the fillin may complete on the master
	thread only after the worker threads have executed the call.
		Options:
		- Eliminate addToQ funcs that return a pointer. Require that the
		data always be passed in. This will mean allocs in SetGet,
		not a big hardship. But also in SrcFinfo::send. This is
		expensive. Do we use addToQ anywhere else? Seems not.
			Could add a 2 and 3 arg variant of addToQ.
		- Separate function for unlocking mutex.
			Nope.

- In LaunchThreads, add an extra thread to handle script calls. This 
	extra thread is also tied into the barriers, so it is safe for swapQ.
	All Shell API calls go through this thread protected by mutexes.


With this pending, I'm slowly marching through the unit tests in testAsync.cpp.
So far 5 cleared. Checkin 2857.

Fixed up low-level 'get' call. Checkin 2858.

Got regular Set/Get calls working in their unit tests. But the Set call
is currently non-blocking. Need to decide if to keep it this way.
Checkin 2861.

Fixed a key step in passing the correct thread information into the Qinfo in
readBuf, leading to Element::exec. Replaced the generic ProcInfo ptr in a number
of Qinfo functions with the ThreadId to make this explicit. Resulted in 
assorted fixes through the code.  Checkin 2862.

Currently stuck in getVec in testSetGetVec().
This is because the Shell doesn't know that it should be awaiting a vector of
results. Looks like the rest of the calculation is actually OK.

In SetGet::dispatchGet one could set the shell to expect the right # of
entries. Except that I don't know what number to expect. This issue is also
tied up with the problem of sending the ack, which should go only after all 
data has come back.
One simple option both for Set and Get is to simply require the calling function
to wait one cycle (two for Get) of the ProcessLoop.

============================================================================
3 August 2011
The SetVec is working, so check that in. Checkin 2863.
Working now on cleaning up the Qinfo::addToQ functions so that they properly
insulate the threads using mutexes. 
Done. Checkin 2867.

Next is to put in a way using the same mutexes, 
to skip a couple of process cycles for set/get calls.

Implemented, but currently we're not running the process cycle. But it
does handle the default case with clearQ OK. Checkin 2868.

Implemented assignment of Shell::gettingVector_ flag, used it to handle
getVec correctly. Checkin 2869.

Current problem is assignment (setVec) of all the synaptic delays, in 
testAsync.cpp:718.
============================================================================
4 August 2011

Cleaning up setVec/getVec to deal with field indices.
Now it clears this. Checkin 2872.

Put in Shell::clearGetBuf() in SetGet::dispatchGet to clean up after 'Get' 
calls. Now clears a lot more tests.  Checkin 2873.

Fixed up Conv< vector< T > > for T = regular fields and strings. Checkin 2874.

Next failure is at a rather involved test of messaging.
============================================================================
5 August 2011
Fixed messaging bug in SingleMsg. Checkin 2875.

Here we have a problem with the structural Q copy: we don't know how
big the data contents of Qinfo are. This was bodged together for inQ,
but it is buggy, and I see I may need to do it also for mpiQ. Perhaps
fall back to having it inQ independent by guaranteeing an extra Qinfo
entry with the final dataIndex.

Fixed up the structuralQ handling. But it still crashes.

============================================================================
6 August 2011.
I've run into issues with the distinction between single-thread and regular
mode which does have parser plus ProcessLoop threads. Need to clarify.
The doCreate function never returns, I suspect because of this.

Meanwhile, minor fixes checked in as 2877.

Currently I have four inputs to the number of threads and how to execute
	Shell::numCores(), which is a static.
	Clock::keepLooping(), which is non-static.
	Shell::isSingleThreaded(), which is non-static.
	Clock::isRunning: non-static.


NumThreads and numCores are actually independent. numCores should be a hardware
thing and numThreads should really be specified as numProcessThreads,
which could even be zero if the entire simulator including parser are all on 
just one thread.
We could have multiple threads on a single core machine, and we could have
a single thread running on a multicore machine, so numCores is irrelevant
except for determining defaults at startup.

The keepLooping flag should be a static on Shell.
The isSingleThreaded() function should be true if numProcessThreads == 0.

Possible states are:
	keepLooping	isSingleThreaded	Notes
	0		0			Idle, multithread
	0		1			Idle, single thread
	1		0			Running, multithread
	1		1			running, single thread.

The following operations are handled differently in single vs multi thread mode:
	- addToQ and variants do not need to lock mutex in STM.
	- on the flip side, SwapQ does not need its matching mutex in STM.
	- Qinfo::waitProcCycles depends on STM and also on keepLooping.
	- Qinfo::execThread(...) must always return true in STM.
	- Msg::exec and variants, if they have precomputed thread allocations
		for execution of subsets, must recompute depending on # of 
		threads
	- ProcessLoop should not set up barriers in STM.

Steps:
* Static-ize thread info flags, and put all on Shell.
- Separate out Clock operations of running from Shell operations of looping.
- Put in Shell the control operations for looping.
- fix up initial thread specification
- March through operations that depend on threading state.


Converted the flags to static, and separated out Clock operations. Now
clears unit tests to the same point as earlier.

Further movement through unit tests. Now it is stuck with
a Set call in testScheduling.cpp:203.

============================================================================
7 August 2011
Two outocmes of this latest bug:
- It is possible for an Element to insert a Queue entry, and then be deleted
before the Queue entry comes up for execution. Sequence:
	Element gets a process or msg call -> inserts Q entry
	Delete call also arrives -> Goes into StructuralQ
	At SwapQ: StructuralQ is executed, so is Element.
		Q entry is swapped into InQ
	During ProcessPhase2: Q entry is read. The associated Element is zero.
	Crash.
  Solutions:
  	- Check for zeros for every Q entry in ReadBuf
	- When deleting Elements, go through Q and clear entries.
	- It looks like this will be a problem only with Process calls. So,
	  ensure that there is a cycle after the clock run is over, for the
	  calls to propagate through. This won't work though for runtime
	  model changes, which may happen in due course.
	I'll go with the first option for now. Later I think that the third
	option may be do-able with suitable housekeeping.
	
- It is possible for an Element to insert a Queue entry even if it has no
	downstream Msg targets. Wasteful. 
  Solution:
	- In addToQ (but not any of the addDirect options) check if the
	selected msgBindIndex has any outgoing msgs at all. Ignore send
	request if not.

Implemented these. Now gets past the msg bug. Hangs now in the mpiTests,
when threads and ProcessLoop have been turned on. Checkin 2880.

Fixed one problem, which was a bad check for thread number. 
Checkin 2881.

Now up against a more fundamental issue, that of the ack system. It has
relied on the shellThread, which has been removed, to send the condition signal.
But the structure of waitForAck is ugly and involves msg traffic.

On reflection, I can probably get by with just the delay of a couple of 
process cycles in most cases. Let's try.
Major case that does not work with this approach: Blocking parser while the
simulation is running, following doStart.
Currently we have Clock itself doing a checkProcState right at the end of the
process loop, in barrier3.
- Don't want to gratuitously add another mutex in the barrier.
- There is already a message coming from the Clock object to the shell, 
	when the run is finished. Currently it is an ack, could send something
	else to indicate time out.
Options:
	- HandleAck sends the condition signal.

Implemented the HandleAck to send the condition signal. Things move along.
Fixed a race condition with the qMutex. The clearStructuralQ call, which
issues calls to addToQ that lock the qMutex, was itself within a block where
the qMutex was locked. After some thought it seems like clearStructuralQ is
safe outside the lock. Cleared the race. Checkin 2882.

Next bug is to do with reduceQ.

Something fundamental to sort out: The indexing of threadNum. I keep threadNum=0
reserved for the parser thread. Confusion arises between single and multi-
thread modes, because in singlethread mode we must do everything on thread 0,
whereas in multithread mode we currently do everything on threads 1 and up.
For clean handling internally, what we should do is distinguish the
queue for the parser thread, from the queues for the process threads.
In single thread mode, the extra queue won't cause any problems.
In multithread mode, we'll avoido

		Single Thread mode		Multithread
Q0		parser				Parser
ThreadNum 0	parser, process			Parser
Other Qs	-				Process
Other threads	-				Process

Is there a way to use thread 0..n always for Process? This would help with
how the DataHandlers partition load.
Otherwise, just some really ugly condition grunging for each DataHandler.
Did the ugly grunging. Leaves most of the rest of stuff intact.
Checkin 2883.

============================================================================
8 August 2011
Ran into odd and unpredictable bug. This prompted me to go back and do a
valgrind. First thing there was memory alignment stuff, turned out to be
how I handled Conv<string>. Fixed.

Second issue turned out to be that the structuralQ was storing pointers to
temporary Qinfos, not to the inQ as I had thought. Easily fixed once found.
Checkin 2885.

Third issue was in Conv< vector< T > >. Bad handling of zero size vectors.
With this valgrind is happy as far as the unit tests go.  Checkin 2886.

Next to figure out why the tests hang or abort in various places in testShell.
Figured out the 'hang'. It was due to obsolete checks for thread0 in
Clock.cpp to do its main operations. Fixed. Now it crashes earlier.
Checkin 2887.

Restored protection of DirectToQ targets to specific threads. 
Unresolved problem with OneToAll, looks like it goes to the right
places.

============================================================================
9 August 2011
After further tracking, it seems like we have a deeper problem: Every Object
with a send call in Process will dump an entry into the queue if any Object
on the Element has an outgoing Msg there. This crops up in the OneToAll
msg. I can readily enough filter the spurious calls in the Msg, but this is
after the Msg packet has trundled through the queue and all. Is it worth
checking for outgoing Msgs to see if any originate at a given Object?

For now: fixed bug, moved on with unit tests. Now hangs in TestSched::process,
running at testScheduling.cpp:438. Checkin 2888

Had to coment out testMarkovGslSolver and testMarkovChannel. With that, it
compiles and clears unit tests. Checkin 2890.

It also clears regression tests, with the minor issue that the # of output
entries it sends in rtReadKkit and rtHHNetwork is one less than expected.
Checkin 2891.
Now to redo valgrind.
Valgrind is happy. It reports a loss of 104 bytes, 56 of which are presumably
the pthread bytes. Turns out the other 48 are the qCond.

Minor fix for the qCond memory leak, and for command line args. Checkin 2892.

============================================================================
10 August 2011
Minor fix for iterating through target objects in OneDimHandler and 
AnyDimHandler.cpp. Now clears unit and regression tests on 16 threads.

Some prelim benchmarks with -O3.

Machine		# threads	test		runTime
Mishti		1		ksolve		1.55
Mishti		2		ksolve		13.2
Ghevar		1		ksolve		0.8
Ghevar		2		ksolve		6.7
Ghevar		7		ksolve		25.9

Mishti		2		IntFire		190
Ghevar		2		IntFire		154
Ghevar		4		IntFire		84.4
Ghevar		7		IntFire		57.3

Mishti		2		hhNet		19.8
Mishti		1		hhNet		24.7
Ghevar		7		hhNet		26.0
Ghevar		2		hhNet		19.4.
Ghevar		1		hhNet		15.1

Minor fix to runtime for ksolve. Checkin 2897. 

The runtime for the ksolve test paints a rather alarming picture of overheads.
This is for runtime = 1e6 at a dt of 10 sec, so 1e5 steps.
The IntFire run is for 2000 steps. So the overhead there is about 1 sec for 
	7 threads, assuming it scales as in the ksolve test.

============================================================================
11 August 2011
Anyway, the profiling can come later. Next step is to check with MPI.
Several fixes needed to get it to work. Most important is to have an
adjustable data block size.

Rules for block size:
	- Keep track of running average, use blocksize a bit more than this:
		say 1.2x + 10 doubles.
	  ave = ave * (window - 1 )/window + bufsize / window.
	- Keep track of sdev?
	- If bufsize > 2x block size, don't put in running average, but do
		add a resize alert.
	- If resize alert > 2, resize the block size.
	Or
	- Second biggest of last 4 msgs, + 10%.
Actually, this is messier. If we have each node manage its own blocksize then
this will work, but will need monitoring of all incoming msgs.

Coded up these rules. Compiles, clears unit tests in single-node mode,
but runs into problems on multiple nodes. Checkin 2898.

Tracked down first of the bugs, a silly pointer initialization one.
Checkin 2899.

Next bug is a Shell::doCreate one, and I suspect it has to do with sloppy
dealing with the master msg between shells.

Trigger of crash is on testShellParserCreateDelete () at testShell.cpp:40
which is just the doCreate function.
Dimensions seem to come across as 0.
Let's check if all the bytes go. Should be.
============================================================================
12 August 2011

Dies in innerCreate because dimensions is empty.
In testSchell.cpp:40; testShell.cpp:1526.; main.cpp:296

I think what is happening is that the addToStructuralQ is relying on the inQ
being intact, but the inQ is instead being changed with the new MPI data by
the time addToStructuralQ is called.

============================================================================
13 August 2011.
Figured a way around this. Added dataSize_ field to Qinfo. Now system goes
a bit further with MPI unit tests before running aground. Checkin 2900.

Found next bug: Qinfo::updateHistory was bad. Progressed through more tests.
Found next bug: Element::exec wasn't testing target ObjId::isDataHere for
single object operations. Fixed, progressed through more tests. Checkin 2901.

Next bug was an old hack where I had replaced the ack message with a direct
call to the Shell object. This doesn't work on multiple nodes as the
acks from all nodes have to reach node 0. Fixed. Checkin 2902.

Now it seems to be dying in Qinfo.cpp:354, which is a rather
simple resize of a vector. It comes to a bit over 1 Megabyte, should be well
within the memory capacity.

Valgrind points out a trivial little memory leak, fixed.
Possibly a lot of memory is being leaked in the process loop? Check.
Possibly could start with a bigger reserve.
This causes things to work up to 8 nodes, but 8 nodes fails. Looks like in
ReduceBase.

============================================================================
14 August
Fiddled around with reserve size. 
- The reserve process works fine for smaller/bigger sizes.
- The resize crash happens at the same place for smaller (20 doubles) reserve.

I suspect a bad memory access somewhere in the mpiQ.

Tracked it down. I was locking the barn door after... In other words, I was
resizing the mpiQs to the new blocksize _after_ they were supposed to receive
the data of that size. Fixed that. Now it clears various combinations,
including regression tests, up to 4 nodes. 8 nodes has the same ReduceBase
failure. 
Checkin 2904.

The ReduceBase failure is from testShell.cpp:416: testCopyFieldElement().
Fixed an error in initializing tgtId, but it still fails a couple of lines 
later, at 419.

I've run into an old pending issue. There are 'global' fields of Objects,
that actually belong to their parent Elements. Should only set them once.
Currently things hang for a while because I'm setting the FieldDimension
repeatedly, for half a million synapses.

Fixed this. There was an odd hack in SetGet::checkSet which set the dataId to
DataId::any whenever the field was a global field.
Checkin 2905.

But now it hangs in testMpiStatsReduce, in testBuiltins.cpp:488

============================================================================
15 August.

Grinding through the 'Reduce' tests in testMpiStatsReduce. Now it looks like 
the Reduce operations are OK, but many of the synaptic delays are set to zero.

After considerable printf debugging, it looks like the problem is the
set command for FieldDimension not propagating to other nodes. There was 
a puzzle a while back about why I had converted the tgt in checkSet to
DataId::any, for Global fields. Maybe this was why, to have the fields set
on all nodes.

Fixed, involved tracing through multiple levels of thread checking and 
isDataHere. Added a hack: DataId::globalField(), which is set by 
checkSet and alerts downstream calls to let the call through on all nodes.
Now clears unit tests on 1, 2, 4 and 8 nodes. Checkin 2906.

Cleaned up a lot of printf debugging. Checkin 2907.

============================================================================
16 Aug
Some things to do:
- Partition up the SparseMsg::exec by thread ahead of time.
+ Gillespie method.
- Thread-ify the ksolver
- SBML reading
+ Scripting
- With Niraj: hsolver
- With Harsha and Chaitanya: Interface
- Optimize
- Test large-scale parallel sims
- Reac-diff. See moose/threadMsg/msolve.
- Documentation

Started on documentation while things are still fresh. Figured out how to
add stuff to doxygen, wrote up documentation on the threading and process
loop design, including MPI data transfers. Spent quite a bit of time also
putting in figures for this. Checkin 2911.

============================================================================
17 Aug.
Subha suggests priorities for documentation:
	* Clocks
	API
	Building new classes.


Wrote some documentation for Clocks. Put it in the API section. Checkin 2915.

Wrote some documentation for Fields etc. Made a new file : doxygen-API.cpp
for the API section.  Checkin 2919.

============================================================================
18 Aug
Working on Gillespie method. I have a project to compute Kramer time for
the DOCSS database entries, and this will help to do this with the new MOOSE.
Restored some of the KinSparseMatrix functions used by the Gillespie
calculations. Checkin 2920.

Compiled in GssaStoich class. Yet to test. Checkin 2921.
Set up test in regression test directory. Long way to go.

Amazingly, after a few relatively small fixes, it generates a 
reasonable-looking curve for the reaction A <===> B.
Checkin 2922.

Need to convert this into a proper regression test with validation
of output. This will require seeding the rng with a known value.

Reexamined output, compared with GENESIS. The output is obviously wrong: isn't
fluctuating about mean.
There were issues because the useOneWay_ was not being used.
Struggling now to fix this, bugs persist. See ZombieReac.cpp:237.

============================================================================
19 Aug 2011
Still struggling with useOneWay. The obvious updates have been applied to the
code to build the Stoich. To get it sorted, I'm running the regular
Kholodenko.g regression tests using the ksolve, with the useOneWay option. 
It fails to handle the reverse reaction.

Turned out to be a conceptual problem. When I have one-way reacs, I need to
set up both the substrate and product entries for each one-way reac in the
matrix. Did this. Both the ksolve and gsolve tests now seem sensible. Still
need to apply to regular enzymes. Checkin 2923.

Now I've gone through with the cleanup for one-way reacs, applied to 
regular enzymes.  Checkin 2924.
Need a few more regression or unit tests:
- Check a reaction system involving both reacs and enzymes with the oneWay 
	flag set first to 0 and then to 1 and confirm results are the same.
- Do a proper test of the Gillespie system, again using both reacs and enzymes,
	and confirm that the output is reasonable both in mean and in stats.


As a small cleanup, I located some leftover objects created during the unit
tests but never deleted. Now deleted. Checkin 2925.

In order to deal with some of the DOCSS models, I want to resurrect the code
to load in those models. Turns out to be in an archaic version of MOOSE,
dating from 2006, in file conv_cspace.cpp. Brought it over to kinetics
directory to refactor.

Currently at ReadCspace.cpp:333.
============================================================================
21 Aug 2011
Compiled in ReadCspace. Yet to test. Checkin 2930.
Activated the test. Many fixes. Now clears it. Cleanup after test is pending.
Checkin 2931.

Cleaned up after test. Also set up to return Id of created chem model by
ReadCspace::readModelString. Checkin 2932.

Did a valgrind check. Still clean.

Added a check for a CSPACE file type in regression tests. Checkin 2933.

============================================================================
24 Aug 2011

Working on implementing a model using ReadCspace and Python. Slow going. Some
thoughts on the language construction.

- restore pwe, ce, le, show.
- Quick way to show and traverse messages.
- Help( class ) to print out notes on each field and on the class.
- A filtered list of fields so that they look more like what the user would use
- Get rid of the Array suffix. Everything is an array.
- Help( func ) to print out notes on each Shell func
- Help to print out instructions for MOOSE help
- Equivalence of an id and a path.
- Substantial discussion with Subha about handling FieldElements.
	An example is Synapse. Synapses are always created as child
	FieldElements of IntFires. By default the size of the Synapse array 
	is 0. Each IntFire Object can have a different number of Synapses.
	How to represent all this in python?

Checkin 2943.


============================================================================
25 Aug 2011

Implemented Shell::doSetParserIdleFlag( bool isParserIdle) to tell system to
wait in the ProcessLoop if nothing is going on.
This can be inspected by static bool Shell::isParserIdle() on all nodes.
Some cleanup for SetGet<>::innerStrSet for 4, 5 and 6 arguments.
Checkin 2948.

Added code so that when LoadModel sees a kkit file and is asked to use 
the 'gsl' solver, it sets up the full calculation including plots, plotdt
and the GslIntegrator.  This followed by cleanup in the regression tests.
Checkin 2950.
Lots of little bugs in the ReadCspace code. Now looks better but still to 
confirm. Checkin 2952.

A possible major bug: If reinit does not find any targets, it hangs.
Tracked it down. Not reinit, but handleUseClock. Wasn't sending ack if it
didn't find targets.

Now trying to get stochastic calculations to work from pymoose and from the
ReadCspace. Something funny happens during zombification at ZombiePool.cpp:255.
============================================================================
27 Aug 2011.
Working on homework/KRAMER/2011/23Aug_test_newQ/run1.py.
Here, the system is fine if it loads model only.
Gssa stoich seems to run but no stoch.

Try a simple binding reaction regular and cspace then stoich.
Did reac.cspace. Did reac1.py. This uses gsl, seems OK.
Did reac2.py. Runs a little slowly but looks promising, seems stoch.
Did reac3.py, explicitly set the # of mols to 100. Looks good. 
	Also matched up shape of curves, looks good. Now to try enz reac.

Did enz.cspace and enz1.py. Uses GSL, seems OK.
Did enz2.py. Crashes.
============================================================================
28 Aug 2011.
Tracked down a bit. It crashes in the reinit with a CSPACE defined a--b-->c
reaction.

Turns out problem with enz2.py was that I was loading the wrong reacn, and
then trying to plot a nonexistent object. Odd that it should crop up only at
reinit.

Another thing to check: will it work for the same reac defined in kkit.
Yes, seems to. enz3.py.

Checkin 2955, which basically puts in the ReadCspace fixes.

Next key thing: get the volume scaling fixed.

============================================================================
31 Aug 2011.

Updated CubeMesh to have a flag 'preserveNumEntries' which decides whether nx
or dx will remain the same when the cube is resized.

For conc scaling, I need to replace several things:
	- Messages formed from reac to pools should be OneToAll. A single
		reac will deal with all pool entries in the compt
	- kf, kb should be removed, Kf, Kb are now native units.
		- setKf should now use the volScale
		- setConcKf should not use the volScale
	- the volScale function should iterate over all reactants. 
		- Figure out what to do about the n-1 power used for scaling.
		Perhaps that should not be n-1.
	- ReadKkit and ReadCspace will need fixing for their setups of reacs.
	- Need a sensible default for pool mesh entries for their volumes.

============================================================================
1 Sep 2011
Running unit tests. Some things to add:
- Test out the case where CubeMesh::preserveNumEntries is true in 
	testShell.cpp.
- Test out preserveNumEntries in testMesh.cpp.
Checkin 2964.

============================================================================
4 Sep 2011.
Added test where CubeMesh::preserveNumEntries is true in testShell.cpp.
Checkin 2965.

Added further test in testShell.cpp:testShellMesh to change volume of mesh and
watch what happens to pool amounts. Unfortunately at present the 'n' of the
pool is the preserved quantity, and accordingly conc changes. Arguably the
conc should be preserved. Anyway, passes unit test with current convention.


Use cases for mesh changes:
1. osmotic swelling. Here we preserve n, so conc changes.
2. test stochastic effs at different volumes. Preserve conc, change n.
3. remesh without changing total vol. Here we preserve overall n, and try 
	to preserve spatial distrib. This could be done also with conc fixed.

Options:
- concInit is native form, n is native form. This is just nonobvious
- Both nInit and n are native, but the base mesh Element has a way to 
	tell it to resize while keeping either n or conc fixed.
- Forget it, just use nInit and n as native forms, and proper sequencing of
	conc assignment from script.

============================================================================
10 Sep 2011.
Added in minor unit test for preserveNumEntries in testMesh.cpp.

Some thoughts about DataId nesting

- Have a single long as the DataId
- The DataHandler knows its own indices
- For nested arrays foo[i]/bar[j], the DataId for bar has a portion that
	identifies i, and a portion for j. Should any operation require access
	to foo[i] as the parent, we can extract i.
	- This is recursive.
	- This applies both to regular nested Elements, and also to 
		FieldElements like synapses
	- Implementation decision: should I bitmap it or modulo it?
	- As before, ragged arrays are permitted but we require knowledge
		of the max# entries of the ragged dimension(s)

- For Synapses and similar FieldElement classes, the parent and child
	DataHandlers need to be able to refer to each other.
	- Also it is possible as before for the actual data to be stored on
	the parent, and the FieldDataHandler just does a lookup from parent.

- Iterators march through DataId, prompted as before by DataHandlers.

- Thread blocks are linear chunks, except for FieldElements, where they
	are referred to parent blocking.

- Node blocks likewise.

........................................................................
Now back to practicalities. 
- Set up OneToAll msgs in ReadKkit for reacs. This fails for MMEnz when we
	send msg from the parent enz pool to the enz site. MOOSE does not yet
	allow backward direction for a simple msg.
- Fixed up the MMEnz issue, by creating a hack for AllToOne msg which is simply
	a OneToAll msg connected backwards.
- Fixed ReadKkit which was incorrectly setting coords in the CubeMesh.
- Fixed ReadCspace which was incorrectly setting coords in the CubeMesh.

Still need to put in a regression test for ReadCspace.
Implemented Osc.cspace. Working on regression test.
A bit stuck here with a memory access error that gdb doesn't pin.
Tried going back and commenting out the previous test. Doesn't help.
Found bug: the cspace reader was dying if it found extra spaces. Fixed the 
model, not the cspace reader. Now trying to get it to correctly read the
oscillatory model Osc.cspace. Putting in corrected volume handling in
ReadCspace, currently stuck in the ReadCspace.cpp:testReadModel() unit test.
It is having problems reading and converting in the reacn rates.
This was relatively trivial mistakes in the set/get routines.

============================================================================
13 Sep 2011
Working still on getting the Cspace loading to work, but there is a deeper
problem lurking. I need to set up reacs volume conversion. Basically, in terms
of # molecules/sec,

flux( #/sec ) = kf * #_1 * #_2 *...

And in terms of uM/sec, i.e., conc units, we have

Flux (uM/sec) = Kf * conc_1 * conc_2 * ...

This is a little problematic: what is the volume to use for the Flux? Clearly
it has to be the same as whatever pool is being affected by the flux.

So in terms of molar units we have
Flux ( umoles/sec ) = Kf * Vo1 * conc1 * other concs?
That is odd and usymmetric. I don't like.

Suppose we assume that the Kf is defined for unit volume V, applicable to
all reagents. The flux is then
Flux (umoles/sec) = V * Kf * conc1 * conc2 * conc3 * ...

Suppose now that the volume of the whole system changes to Vnew. We have
Flux (umoles/sec) = Vnew * Kf * conc1 * conc2 * conc3 * ...

dconc1/dt = -Kf * conc1 * conc2 * conc3 * ...
	which is incompatible with 
dconc2/dt = -Kf * conc1 * conc2 * conc3 * ...
	if vol1 != vol2.

Problem is to get kf in terms of Kf and vols.

#_1 = vol1( m^3) * 1e3 * conc_1 (uM) * NA / 1e6 = vol_1 * conc_1 * NA * 1e-3

Or, 
conc_1 = #_1 * 1e3 / ( NA * vol1)

etc.

flux( #/sec ) = Flux (umoles/sec ) * NA * 1e-3

So:
flux = NA * 1e-3 * Flux = 
	NA * 1e-3 * Vunit * Kf * 
		#_1 * 1e3/( NA * vol1 ) *
		#_2 * 1e3/( NA * vol2 ) *
		...

	= NA * 1e-3 * Vunit * Kf * (1e3/NA)^numReactants * #1 * #2 * ... /
		(vol1 * vol2 * .. )

But flux is also = kf * #1 * #2 * ...
so divide out.

1 = NA * 1e-3 * Vunit * (Kf/kf) * (1e3/NA)^numReactants / ( vol1 * vol2 * ... )

Rearranging and setting Vunit to 1 m^3, we get

kf = ( Kf * (1e3/NA)^(numReactants-1) ) / (vol1 * vol2 * ... )
where all vols are in SI.

Suppose vol1 = 1, and numReactants = 1
kf = Kf. Good.
But if vol1 = 1e-15, we get kf = Kf / 1e-15. Bad.
This happens because the assumed vol of the tgt is still unity.

In summary: The rate equation depending on concs is inherently asymmetric,
and will need scale factors for different compartments. There is an implicit
assumption of a reference compartment i, for which the relationship
dconc_i/dt = -Kf * conc1 * conc2 * conc3 * ...
is true. If volumes differ between compartments, the dconc for the other
compartments will need to be scaled.

So, despite its convenience, the conc version of the equation is less clean
than the # version of the equation. I'll stick with the # version underneath
because it is appropriate for stochastic calculations, and also because it
avoids these issues.

I should look at how SBML does it.

Reaction design is to have a single Reac object per chemical reaction, 
regardless of its spatial considerations. 
Example 1: 
There would be a single Reac
defining A <---> B in a given chemical compartment, whether the compartment
were meshed as a single voxel or as a million. Here A and B are meshed 
identically so we just need a single volume term to convert rates.
Example 2: 
There would be a single object for an enzyme in a membrane that
transfers molecules across the membrane. Here our enzyme reaction spans three
chemical compartments: inside, membrane, and outside. 
Example 3:
Again, like example 1, consider A <---> B in a given chemcial compartment,
but now allow A and B to have different meshes. This is a peculiar case and
for now we should ignore it.

In either case, the idea is that a single rate term should account for all the
individual meshed up reaction transitions. In other words, the rate terms
should be in intensive units like concentration.
The job of the scaling calculations is therefore to allow the solver to
ask of the Reac or Enz object, how to set up individual rates in each mesh.
Or between meshes, for the inter-compartment reaction.
These individual rates should be in extensive units, like molNumber.

These fundamental considerations aside, I moved ahead with implementing
the volume scaling in ZombieReac and ZombieEnz. However the latter fails 
because it doesn't have a mesh entry to look up. More generally, I need to
rewrite Reac::volScale to extract volume of each target Pool, rather than
the volume of the current mesh entry.

Even this isn't sufficient. I need to specify which mesh entries to use for
each pool, since the extracted kf and kb will go to defining the reaction 
rates.  This is easy within a compartment, the pools will all have the same
mesh index though the volumes of each entry may differ.

The diffusion terms are also easy, they are computed separately in a sweep
through all mesh edges. Reactions don't apply here, just diffusion coeffs.

The hard part is reactions spanning compartments. Here the reaction meshing
routine must match up mesh entries on either side (sometimes 3 at a time).
It must apply these mesh indices to the appropriate reactant molecules.
Here the system will scan through space and generate the sets of mesh indices.
Then it must confer with the reaction to match the indices to the reactant
pools. Then generate the rate terms for each mesh index combination.

============================================================================
15 Sep 2011
Implemented a number of conc conversion utility functions. Checkin 2971.

Implemented fixes for Enz volume conversions. Now clears unit tests and 
regression tests up to the point where the output is generated. Unfortunately
the output is still garbage.
Checkin 2972.

Fixed some plot names. Suddenly it (the CSPACE reader) works though the
units are off. 
But the match with GENESIS is poor in the time axis, though the waveforms 
match very well qualitatively. Possibly numerical.
 
Turns out to be simple: I was earlier plotting n, rather than conc. Now
fixed. I needed to redo the GENESIS reference simulation with a much smaller
dt to get a better match.

Did this. Some printf debug cleanup. Now it clears regression tests.
Checkin 2974.

Next steps:
- ksolve replication
- Reac-diff calculations using simple rk5 for space
- tests
- Stoch calculations in parallel.
============================================================================

16 Sep 2011
Some minor cleanups of volume handling, to remove the requirement for Enz and
Reac (and zombies thereof) to be associated with a mesh entry. Clears
unit and regression tests. Checkin 2975.

General scheme for changing vols and remeshing:
	- Things to update:
		Pool dimensions
		Pool 'n'
		Rates for enzymes and reacs
		Solver dimensions
		Solver n and nInit
		Solver rates
		diffusion coupling
		output and graphics

For starters, I'm going to allow the compartment volumes defined on the 
solver to be changed. Later I'll set up messages from the compartments to the
solver that request this change.

Steps:
	- Scan through all pools connected to this compt, update S_ and Sinit_.
	- Scan through all RateTerms. Each rescales itself to the new vol,
		by checking its reactants and seeing if they are in the 
		affected compartment(s). Note that this magically propagates
		to the ZombieReacs and ZombieEnzs, since if anyone checks them
		they will find the new rates.

Implemented volume rescaling in the Stoich. Compiles and clears unit and
regression tests, need to do a specific test.  Checkin 2976.

============================================================================
17 Sep 2011. 
Rescaling volume debugging slowly progressing. Now it hangs with a negative
mol-n  in in in MMEnzyme1::operator().

Looks like the Km in MMEnzymeBase should be in # units. Also in
MMenz.cpp and ZombieMMenz.cpp.

Tried running Kholodenko.g with different volume term. Doesn't
seem to make a difference.

Is there a memory issue? Ran valgrind with the base unit tests. Still clear.
Ran it on the regression tests. Nothing turns up till the negative mol-n.

Something funny about Km for MM-Enz. It should be in # units too.
============================================================================
18 Sep 2011
Much messing around to fix up the volume scaling. There were problems with
reinit. There were problems with volume assignment. There were problems with
numerical accuracy. There were plain old bugs. Anyway, it now works
through the regression test, and in the process much other cleanup has
occurred. Checkin 2977.

Regression test for cspace model is taking too long to run. Worked on the
model, reduced runtime by a factor of 6 or so.  Checkin 2978.

Time to get going on the reac-diff calculations. For starters, I'll just
consider diffusive coupling between different solvers. In other words, if there
is a cross-solver reaction, it has to be split into a local reaction or two,
and then diffusion to move molecules between solvers.
Likewise, any compartments within a solver are replicated uniformly for
all instances. If I want to do different meshing of two compartments I'll need
two different solvers.

Stages:
- Just set up replicates of data structures
- Set up calculations on replicates. This will work with separate spines.
- Set up calculations on replicates using GSSA.
- Set up diffusive updates
- Combine using an RK4 fixed-timestep with interleaved updates
- Put in ports for different solvers to talk to each other.
- Figure out how to multithread replicates.
- Figure out how to multinode replicates.

Some multithread design:
Assume we set off solver on selected # of threads. The solver itself has to
know which threads it is allowed.
The GslIntegrator::process is the entry point for calls to the solver. It is
a regular 'process' call, could be called from all the process threads 
simultaneously.
The gsl_odeiv_evolve_apply function is called there. But the gsl func
has to be done on a single thread.
What we do want is that as soon as the gslFunc is called from the single
gsl thread, it latches onto all the available threads and deals with the
appropriate part of the data.
	- Send signals out to existing threads
	- Spawn and rejoin threads
	- Ignore this granularity of threading. 
		Instead run separate gsl_odeiv_evolve_apply funcs, with
		independent instantiations of the internal variables, each
		for a different mesh location.
		This is simpler but does not afford multithreading to big
		signaling models which would be good candidates for division of
		rate updates.
		- Here we need to change the GslIntegrator to be a lightweight
		class with a stoich ptr and a ProcInfo ptr. Allocate one
		per thread.

Could also possibly clean out the y_ vector from Stoich.
============================================================================
19 Sep 2011
Write some timing code to test threading options
- For spawning and joining threads
- For cond-wait on existing set of threads 
	- using a barrier.
		Usage: barriers numThreads numJoins numBusy
		Seems to be about 15 usec per loop around the barrier on
			my laptop.
		Tested using 
			time barriers 2 1000000 3. 
		Similar results come up if numBusy is 2 or 4.
	- Using a master thread to unleash the crew.

Checkin 2979: the barriers test.

Modified a bit to do same thing using joins. Way slower. Checkin 2980

Usage: ttime numThreads numJoins numBusy doBarrier

Ran both on mishti (Core 2 Duo U9400@1.4Ghz) and 
ghevar (8 core Xeon E5520@2.27GHz)

						User Times:
numThreads	numJoins	numBusy		Barrier = 0	Barrier = 1
						2core	8core	2core	8core
1		100,000		3		7.6	3.7	0.04	0.02
2		100,000		3		7.7	3.8	1.45	0.8
3		100,000		3		8.2	5.4	2.7	1.5
4		100,000		3		10	5.7	3.9	2.5
5		100,000		3		14.1	10.7	5.2	4+-1
8		100,000		3		28	20.5	8.8	7+-1
16		100,000		3					40


So the thread creation and destruction costs about 40-80 usec per cycle,
starts becoming linearly dependent on # of threads > 4.

The barrier approach costs about 15-30 usec per cycle, scales sublinearly 
up to 8 threads.

This means that, at least in Linux systems, I should be able to use the
thread create/join approach for particularly large reaction systems. The
use of the existing barriers for reac-diff calculations is even better,
considering that the infrastructure exists.

- Should time the updateV function with different size models in a single
	voxel, to get an idea of when it make sense to multithread them.

Looking closely now at how to set up multiple threads in ksolve.
- Set up an array of the GslIntegrator data structs, one per thread.
	- Each knows its own index, preferably even its ProcPtr.
	- Called in the GslIntegrator:stoich function.
- Stoich::gslFunc needs to have an arg for threadIndex. Will need it
	from the Process
- Stoich::innerGslFunc should take the threadNum as arg.

- Stoich should NOT use a shared y_ for the data, issues with the different 
	threads.

============================================================================
20 Sep 2011
Finally (after years of fiddling with it) figured out why we cannot just put 
the S_ vector into the GSL for computation and to avoid memcopies. Turns out
GSL passes back different versions of the state (y) vector, internally 
allocated, as the numerical integration proceeds.
Sometimes y is the same as the S_ vector, other times not. GSL needs to 
maintain both vectors for its internal calculations, so we cannot even take
the shortcut of memcpy( S, y ) when the pointers differ. In principle we could
use the returned y vector for the updating, but now we need to keep track of
this for the inspection of field values, which looks up S. We don't even
want to retain the y pointer instead of S for inspection, in case the solver is running on a different thread than the function which wants to inspect the
field value.
Checkin 2981.

Implemented code to pass ProcInfo into the Stoich::gslFunc. This is needed to
deal with multiple GslIntegrators handling different subsets of the reaction
spatial mesh. Checkin 2982.


Need to clear up: is there one GslIntegrator per meshEntry or per thread?
Seems like per mesh entry.

Spent significant time working through numerical methods for Reac-diff 
systems. Some disappointments, e.g., with the Liao, Zhu and Khaliq 2006 paper,
but a good general revision. There is also a potential lead in:
Solving reaction–diffusion equations 10 times faster. Aly-Khan Kassam.

For starters, I can use the Method of Lines
with the current framework, if I add in terms for the diffusion. The problem
is that the MoL uses a single giant ODE system whereas my thought is to use
separate ODE solvers for each reac system. In essence, I use first-order
(Euler) summation for the diffusion terms and add these onto the 
5th-order RK5 solution for the reactions. Tau for diffusion terms is 

tau = lambda^2 / 4D. If we use subdivisions of 1 um, and D = 1 um^2/sec we have

tau = 1 * 1 / 4 = 0.25 sec. The typical synchronization timestep in MOOSE
is 1 to 10 msec, so the Euler summation should do fine.

Working on the implementation.
Turns out that the GssaStoich should also come along rather nicely here.
First pass: cleared regression tests with just a single mesh entry.
Checkin 2983.

Working on regressionTests::rtReacDiff.cpp
============================================================================
21 Sep 2011.

rtReacDiff is coming along. The first test, the creation and running of 
multiple instances of the models, is OK on creation but running seems to
have a problem. I think that the model construction fails to bring in the
initial values for all the pools, and only sets up the ones on index 0.

Still fighting with array handling stuff in rtReplicateModels.

============================================================================
22 Sep 2011
Working on ZombieHandler and pool zombification. Seems like a good way to
have zero overhead zombies that use the parent solver, but keep track of
# of entries.

Did this. Many messy fixes followed, to get all the zombie pools made
and initialized properly. Checkin 2985.

Should revamp all of the DataHandlers, install policy in them for node
and thread decomposition, sort out iterators and dimensionality, deal with 
the DataId stuff too.

Subha wants FieldElementFinfos added to the Cinfo fields. Did it,
not yet tested. Checkin 2986.

I need a reference model that can be tested for individual mesh calculations,
and also does something sensible when I set up reac-diff calculations.

As a formal completion of the regression test with independent little models,
I tested the output values of the different mesh locations in 
rtReacDiff.cpp:rtReplicateModels. Surprisingly, the results differ from run
to run. Major issue here, possibly to do with mixing of intermediate values
due to threading.
That was a good guess. The v_ vector in Stoich.h was being used by both 
threads. Set up to dynamically allocate it. Later might preallocate.
Checkin 2988.

For reac-diff: will need local neighbour list. Better as a stencil.
Issue crops up here: when do we do this. It needs input of mol concs from all
interfaces; rather selective exchanges per node.


============================================================================

23 Sep 2011
Implemented the basic function for updateDiffusion, that is straightforward.
Scheduling is tricky.  Need to be careful of 
when the transfers happen between threads. We need the mol # for each 
species and mesh location, (S_ matrix), to update the rate term v.
Seems like the obvious solution is to have a vector of diffusion terms for each
species. This can be updated both within the thread and using messages.

One option is to have it scheduled through the GslIntegrator. I've done this
but it is not clean and needs additional fields to be specified that have
nothing to do with GSL.
Another option is to create another solver helper, Diffuser. Like the GSL
it will be scheduled and will drive the stoich functions. This would need
to be split up into numMeshEntries entries.

A third option is to have the ChemMesh object itself drive the stoich. Problem 
here with the number of mesh entries, but the obvious thing is to have
the mesh entries be scheduled and drive it.

============================================================================
24 Sep 2011

Setting up diffusion calculations with the third option above. Use a message
from the ChemMesh to the Stoich (there could be more than one Stoich target).
Perhaps implement something like a fastSet to follow down multiple local
Msgs? This is what I want to do anyway with the 
CubeMesh::updateDiffusion iteration.

Sequence will be: Process->MeshEntry->parent ChemMesh-> scan through msg
tgts -> Stoichs-> diffusion calculations.

Main reason to do it this way is to set up the scheduling and thread
decomposition properly. It also lets the mesh figure out how to specify the
stencil for the diffusion terms. Note that a single mesh may handle multiple
stoichs, but we'll deal with this later. Checkin 2993.

Implementing regression test: testDiff1D.  Analytical solution:

c(x,t) = { c0 / (2 * sqrt(PI.D.t) ) }.exp(-x^2/(4Dt)

Note that integral(-inf, +inf)( exp( -ax^2 ) = sqrt( PI/a )
So we have a = 1/(4Dt) and the integral is 2 sqrt( PI.D.t ) so this
works out to integral(-inf, +inf) c(x,t)dx = c0.

In order to set it up for a finite initial amount, we have c0 = cmid * dx



Ran into roadblock here. The FieldDataHandlerBase does not execute process
calls. Need another way to get the calls into the stoich in a way which uses
the threads effectively. 
Even if it did, how would one decide how to partition threads? 
Like OneDimHandler?

============================================================================
25 Sep 2011

Reac-diff starting to work. Going through error terms.

For dx = 1 micron, # segments = 21:
for dt = 0.1:
root sqr Error on t = 1 = 0.0133225
root sqr Error on t = 2 = 0.00461179
root sqr Error on t = 3 = 0.0026447
root sqr Error on t = 4 = 0.00188755
root sqr Error on t = 5 = 0.00200928
root sqr Error on t = 6 = 0.00313812
root sqr Error on t = 7 = 0.00497474
root sqr Error on t = 8 = 0.00726361
root sqr Error on t = 9 = 0.00984374
root sqr Error on t = 10 = 0.0125965

for dt = 0.01:
root sqr Error on t = 1 = 0.0293132
root sqr Error on t = 2 = 0.0104859
root sqr Error on t = 3 = 0.00607502
root sqr Error on t = 4 = 0.00423246
root sqr Error on t = 5 = 0.00357299
root sqr Error on t = 6 = 0.0040164
root sqr Error on t = 7 = 0.00541802
root sqr Error on t = 8 = 0.00744706
root sqr Error on t = 9 = 0.00985837
root sqr Error on t = 10 = 0.0124959

For dt = 0.001: 
root sqr Error on t = 1 = 0.0309297
root sqr Error on t = 2 = 0.0110765
root sqr Error on t = 3 = 0.00641892
root sqr Error on t = 4 = 0.00446861
root sqr Error on t = 5 = 0.00374003
root sqr Error on t = 6 = 0.00411993
root sqr Error on t = 7 = 0.00547308
root sqr Error on t = 8 = 0.00747161
root sqr Error on t = 9 = 0.00986352
root sqr Error on t = 10 = 0.0124882

Oddly, the results get better at larger dt, but there are much of the same
order whether we use 1 to 100 msec dt. Part of this may be since we use a
high order ODE method. Anyway, the outcome is that I don't have to worry much
about the updates of the diffusion system.

Tried now for 
For dx = 0.5 micron, # segments = 41:
dt = 0.1:
root sqr Error on t = 1 = 0.00624078
root sqr Error on t = 2 = 0.00250449
root sqr Error on t = 3 = 0.00150366
root sqr Error on t = 4 = 0.00105265
root sqr Error on t = 5 = 0.000999846
root sqr Error on t = 6 = 0.00166516
root sqr Error on t = 7 = 0.00289127
root sqr Error on t = 8 = 0.0044467
root sqr Error on t = 9 = 0.00620283
root sqr Error on t = 10 = 0.00807393

dt = 0.01:
root sqr Error on t = 1 = 0.00337378
root sqr Error on t = 2 = 0.00138091
root sqr Error on t = 3 = 0.000826444
root sqr Error on t = 4 = 0.000638089
root sqr Error on t = 5 = 0.000920407
root sqr Error on t = 6 = 0.00173873
root sqr Error on t = 7 = 0.00294765
root sqr Error on t = 8 = 0.00444198
root sqr Error on t = 9 = 0.00613543
root sqr Error on t = 10 = 0.00795478

dt = 0.001
root sqr Error on t = 1 = 0.004319
root sqr Error on t = 2 = 0.00177055
root sqr Error on t = 3 = 0.00105967
root sqr Error on t = 4 = 0.000792555
root sqr Error on t = 5 = 0.00099915
root sqr Error on t = 6 = 0.00177518
root sqr Error on t = 7 = 0.00296457
root sqr Error on t = 8 = 0.00444674
root sqr Error on t = 9 = 0.00613151
root sqr Error on t = 10 = 0.00794457

This is reassuring, results have converged rather nicely especially at small
times when the gradients are steepest.
I'm a little concerned about the start/stop process, if it mangles time.
Anyway, set this up to do a silent test for the root square error using
dt = 0.01 and 41 segments of 0.5 microns each.
Note that the long-time calculations will also build up error due to the 
edge effects of the finite diffusion volume.

Try moving input point to edge. This should let us compute the falloff using
fewer points, important in higher dimensions. 
This works. The error even starts out smaller.

For n dimensions, we have:

c(x,t) = { c0 / ((4.PI.D.t)^(n/2)) }.exp(-x^2/(4Dt)
where x is a vector now. Let's try it.

First pass: set it up with 1 D in a corner. Works. Checkin 2995.

Then: try with 2D. Fails. Looks like a stencil error; that is, seems like
molecules are spilling over to the wrong parts of the pool mesh. Particularly
at boundaries.

OK, stencils it is. in Stoich::updateDiffusion, need to check
differently for boundaries, in 2 and 3-D diffusion.

Rather than do the stencil checks in Stoich::updateDiffusion,
should define Stencil as a more sophisticated class and have it
internally return the appropriate index to use.

============================================================================
26 Sep
Setting up stencil classes. Still to do 3-D. Nearly ready to try.

============================================================================
27 Sep 2011

Stencil Class implemented, clears regression test in 2D. Checkin 2997.
Extended tests. Now clears regression tests in 1, 2 and 3D. Checkin 2998.

Minor fix for includes. Checkin 2999. Also confirmed it runs on 8 and 16
threads.

I now need something testable for reaction-diffusion.
The keywords "analytical solution reaction-diffusion" yield 49 hits on PubMed.
Number 3 and 5 and possibly 9 and 14 look OK. There is also the 
calcium diffusion paper (ref 20) of Mironova and Mironov in BiophysJ 2008.

Not able to get something analytic right away. Made a little ad-hoc reaction
and set up initial conditions to produce what should be a distinctive output.
Output seems reasonable. Checkin 3000.

Some release notes:
This is MOOSE beta 1.4.0. At this point MOOSE has been used in several large
network and multiscale research simulations and this beta release is primarily
one for refinement of the current code framework. This beta release sees
many GUI developments including:
Run-time visualization and interaction GUI for cell models in 3D
Run-time visualization and interaction GUI for signaling models
Improved layout of the GUI, with a convenient Simulation Control Toolbar.
This beta release also has simulator core object developments including:
Efield object used in simulating field and EEG electrodes, ported from GENESIS
PulseGen enhanced with arbitrary numbers of levels, delays, and width.
Classes added to support stochastic synapses with short-term plasticity.
Hsolve updates for better single-neuron and network modeling.

This is likely to be the final beta release, and following this MOOSE will
switch over to a new code base which supports multithreading and MPI. 
Thus the next development cycle will be a release-candidate whose main
focus will be to replicate the current features on the new code base.

============================================================================
28 Sep 2011
Working now on the update to DataIds. From the notes from 10 Sep:

- Have a single long as the DataId
- The DataHandler knows its own indices
- For nested arrays foo[i]/bar[j], the DataId for bar has a portion that
	identifies i, and a portion for j. Should any operation require access
	to foo[i] as the parent, we can extract i.
	- This is recursive.
	- This applies both to regular nested Elements, and also to 
		FieldElements like synapses
	- Implementation decision: should I bitmap it or modulo it?
	- As before, ragged arrays are permitted but we require knowledge
		of the max# entries of the ragged dimension(s)

- For Synapses and similar FieldElement classes, the parent and child
	DataHandlers need to be able to refer to each other.
	- Also it is possible as before for the actual data to be stored on
	the parent, and the FieldDataHandler just does a lookup from parent.

- Iterators march through DataId, prompted as before by DataHandlers.

- Thread blocks are linear chunks, except for FieldElements, where they
	are referred to parent blocking.

- Node blocks likewise.

Use cases. Here the DataId description applies to the last portion of the path.
Note that the last entry could be a fieldElement or a regular Element:
no distinction in the DataId, only in what the DataHandlers do with it.

Path				DataId		bitOffset	numBits
/foo				0		0		0
/foo[23]			23		0		5
/foo/bar[23]			23		0		5
/foo[23]/bar (bar has 1 entry)	23		0		0
/foo[23]/bar[55]		55+(2^6)*23	0		6
/foo[23][55]			55+(2^6)*23?	0		11
/foo[23][55]/bar[10]		10+(2^4)*55+(2^10)*23
						0		4
/foo[23]/bar[55]/zod[10]	10 + (2^4)*55 + (2^10)*23
						0		4
/foo[23]/bar/zod[55][10]	10 + (2^4)*55 + (2^10)*23
						0		10

One possible change is to have multidimensional arrays at any level just
be linear internally, using modulo arithmetic. In other words each level has
a single bitmask of its own, and figures out what to do internally. Lookup
will be faster this way. Also more compact. Then it would be:

Path				DataId				numBits
/foo				0				0
/foo[23]			23				5
/foo/bar[23]			23				5
/foo[23]/bar (bar has 1 entry)	23				0
/foo[23]/bar[55]		55+(2^6)*23			6
/foo[23][55]			55+max*23			11 or whatever
/foo[23][55]/bar[10]		10+(2^4) * (55+max*23)		4
/foo[23]/bar[55]/zod[10]	10 + (2^4)*55 + (2^10)*23	4
/foo[23]/bar/zod[55][10]	10+max*55 + (2^10)*23		10

An essential part of this design is that the Id+DataId is a unique identifier,
but it is not complete until it matches up with the Element and its DataHandler.
The Id+DataId part will uniquely specify any given object, but the 
dimensionality of self and parent have to come from the DataHandler and Element.

Note that the DataId of the parent can easily be extracted from the DataId
and the bitOffset. Just blank out everything beyond the bitOffset.

Discussed with Subha. He points out that if we have a sparse occupation of
the indices, we may run out of space. Here is an example worst case: a highly
detailed spine model set in a single highly detailed neuron model set in a
large coarse-grained network of neurons:
1e7 neurons:		23 bits
1e3 compartments:	10 bits
1e3 mol species:	10 bits
1e9 molecules:		30 bits
			-------
			73 bits

So, in principle, it is possible to occupy more space than a long long provides.
For now we'll start with a long long (64 bits) as the DataId.

Implementing DataHandler::iterator, we run into some more issues:
- What is begin()? If we assume that each DataHandler deals with the entire
data block it should be DataId( 0 ). Block and thread decomposition can use
this as a base.
- What is end()? Again, if we assume the DataHandler deals with the entire data
block, then even a ZeroDimHandler has to know the full hierarchy of sizes and
indices of everything that is above it. I don't want to have such a dependence,
there will be all sorts of synchronization issues when we change mesh sizes and
the like.
	This implies we want to split up the ZeroDimHandlers.
	Consider Element zod in: /foo[0..25]/bar[0..60]/zod[0..14]
	- Assume that there is a single Id referring to all the various indices
	up to and including the one on zod itself.
	- Should Element Zod handle different DataHandlers, so that each one 
	is [0..14]? We would then be stuck trying to iterate through the 
	whole set, as they wouldn't know about each other.
	- This is effectively like a higher-order DataHandler. Looks like 
	a mirror of the Element tree using MetaDataHandlers. This is 
	bad if we want rapid field access from DataId.

	Suppose Element zod in: /foo[0..25]/bar[0..60]/zod[0..14] is just
	one of 25*60 such Elements, each with a unique Id. In other words,
	the DataId now only needs to supply its lowest few bits for the local
	lookup. Why bother then about the other bits? But, this gets rid of
	the utility of making all elements ArrayElements.

	Consider this tree: /foo[0..25]/bar[0..60]/zod[0..14].
	- Let there be a single foo Element, with a OneDimHandler of 25 entries.
	- Let there be 25 bar Elements, each with a OneDimHandler of 60 entries.
		Or could even be a ragged array.
	- Let there be 60 zod Elements on each bar element; a total of 1500.
		Each has a OneDimDataHandler of 14 entries..
	
	Now we access /foo[11]/bar[12]/zod[13]
	From the path, we come to an Id for the appropriate zod. It is actually
	/foo/bar[11]/zod[12]. Hideous.
	This zod has a DataHandler with 14 entries, and we pick entry 13.

	Suppose Element zod is actually a FieldElement. Here we do need the
	next level up for getting at parent Element index. We would use
	/foo/bar[11] for the Id, and then the DataId would provide the lowest
	index of 13, and the parent one is 12.

In other words, the main change would be in having multiple Elements with the
	same name but different indices.

	Another option: DataId contains two parts: a full bitmapped index,
	and a linear index. There is only one Element with a given name,
	regardless of the indexing up and down.
	- Lookup of data will be fast through the linear index.
	- If we have FieldElements we can extract the field index fast, 
	and the linearIndex will refer to the index of the parent data handler.
	(This looks a lot like the current system).
	- If we need to generate the full path we can use the bitmapped part
	of the index and traverse it.
	Problems with this:
		- How do we generate a DataId? 
			Iterators in Process: here the linearIndex is known
			Paths: here the bitmapped index is known
			Messages: Typically bitmapped.
		- Should we make the DataHandlers map rather closely to the
			Element indexing? In other words, not a solid block
			but instead multidimensional arrays?
			- Slower lookup as it will always need the bitmaps.
			- Profusion of Handlers?
		- How do we make sure the two stay in sync? 
			Could do JustInTime and leave a flag for the unknown one
			to be computed only when needed.
		- It is ugly for the Element path stuff to be coming from the
			DataId. But no other way once Elements abdicate indexing
		- Same issues as before arise with traversal. Do we do 
			traversal by marching through parent Elements? 
			How do we do that? We would need Elements to get the 
			parent DataHandlers.
		- LinearIndex is volatile, changes if the element tree changes.
		- What do we do with ragged arrays?
			This would be the responsibility of the Data Handlers.
			It gets messy for linearIndex.
			- The combination of linearIndex and bitmapped Index
				helps for the lowest level of indexing.
			- It gets rather nasty if there is a ragged array up 
				high on the tree. Fast lookup is hard either
				way, but bitmaps less so.
			- We currently don't even address this issue. We
				assume a very flat tree structure.
		- Should we redo the DataHandler::iterator to directly return
			the relevant data ptr, and act on this? It would
			address several of the speed concerns.
		- Lacking a linear index, how do we do node and thread 
			decomposition?
		- Have a nested DataHandler structure to deal with the various
			possible cases?
			- Zero
			- One
			- Two
			- Nested
			No, won't work. Will instead have to do vector


Summary: 
	- Stick with proposed design for bitmapped unsigned long long DataId.
	- Write DataHandlers to deal with matching indexing to the Elements, 
		preferably minimum number of DataHandler classes.
	- Provide a data ptr incremented within the DataHandler::iterator for
		fast data access, much faster than dereferencing the DataId.


Valgrind reports a problem with setRepeat on Ids. Added a unit test in
testAsync, and confirmed it. Turns out to be an issue with Conv< T > where
T is a class whose size is smaller than a double. Basically the memcpy tries
to copy out memory in chunks of doubles, which is 8 bytes. Valgrind flags this
because it is copying part of the field from unknown sources. I wonder
if there is a way to tell templates to do something different 
if sizeof( T ) < 8. It seems I've instead just brute-forced it for
int, short, bool etc. Should do the same for Id. Done.
The memory leak is gone but there is a nagging valgrind complaint about an
uninitialized value, which I though I had fixed. Perhaps just needs a 
recompile. This takes over half an hour to run, so let's set it off again
some other time. 
Checkin 3002.

============================================================================
29 Sep 2011. Trying to do whole compile. Stuck in Element.cpp.
============================================================================
30 Sep 2011. Slowly going through compile. Lots of design issues came up and
were resolved in doing so. Have now cleared the ZeroDimHandler.cpp, many more
to come.

Compiled OneDimHandler.cpp except for the last function in it, 
rolloverIncrement.
============================================================================
01 Oct. Compiled OneDimHandler.cpp.
============================================================================
02 Oct.
A bit of an update to the use cases for the indexing. If I add a dimension, 
it is usually the top-level one. 
That is, /library/bar/zod[0..10] gets copied over 200 times and becomes 
	/model/foo/bar[0..200]/zod[0..10]. 
I may then expand, in which case the foo becomes foo[0..40], etc.
I may also do the following:
	/library/pong/ gets copied over 200 times to give
	/model/pong[0..200].

This means that going from a 1-D to a 2-D Handler, I would usually put the
new 'n' into ny, rather than nx.
Doesn't much affect current design, but will need to keep track.

Got TwoDimHandler to compile.
Got AnyDimHandler to compile. 
	In this there is a bit of fuzziness about what happens when we 
	reallocate in various ways. We would like the data at old indices to
	be intact. This is not possible if isGlobal==0. It gets messy at
	higher dimensions even if isGlobal==1. Deal with it later.

Got DataHandlerWrapper to compile.

============================================================================
03 Oct

Working on FieldDataHandler.
FieldDataHandlerBase::process.
Need here a way to extract root portion of dataId in order to do iteration.
Actually that went much easier using the iterator from the DataHandler. 
Perhaps could do same with other DataHandlers, if there isn't too much of a
speed penalty.

next hurdle: setFieldArraySize is used only once in the entire code base, just
in sparseMsg.cpp. 

getFieldArraySize is used more often.

Cannot replace getFieldArraySize with sizeOfDim(0), as the latter func is 
for the maxFieldSize.
============================================================================
4 Oct
Further slow progress on FieldDataHandler , now on rolloverIndex.
============================================================================
5 Oct
Revisiting the whole iterator business. It is turning out to be ugly and
inefficient.

There seem to be just four use cases where the DataHandler iterates
through stuff:
One is the Process. This is already done with internal iterators.
Two cases involve calling OpFuncs with specified arguments. 
These two cases are Element::exec and OneToAllMsg::exec.
This will do:
foreach( ThreadId threadNum, Element* e,
	const OpFunc f, const double* arg, unsigned int argIncrement ) 

A fourth case is the ReduceMsg::exec, which wants to go through
all targets and call ReduceBase::primaryReduce( ObjId ) on each.
I could probably cast this into a form compatible with the previous
foreach.

Implementing this. It greatly simplifies the code and will also improve 
efficiency a lot.

As always, the FieldDataHandler stuff needs some extra work. Here the key
thing will be to
a) implement a Process for the FieldDataHandler that operates from the 
	parent ptr, and does the iteration through all field entries on a given
	parent object, starting from the parent object as base.
	This will be passed in to the DataHandler::process of the parent.
b) Implement a FieldOpFunc that again operates from the parent ptr and deals 
	with the iteration using the parent object.

This is somewhat nasty but note that it is nicely recursive.

Finally managed to work through all of the Handlers. Some few functions still
dangle. It now compiles through to Cinfo. Checkin 3015.


============================================================================
6 Oct
Compiles through to testAsync.cpp. Checkin 3017.
Compiles through the basecode directory. Checkin 3019.
Compiles through the msg directory. Checkin 3021.
Compiles through the shell directory. Checkin 3022.
Compiles through biophysics directory. Checkin 3024.

Having to backtrack: in testShell.cpp.
============================================================================
7 Oct
Compiles through builtins. Checkin 3026.
Compiles through kinetics. Checkin 3027.
Compiles through ksolve. Checkin 3028.
Compiles through all libraries, but not the link step.

Implemented FieldDataHandler::foreach using excessively clever OpFunc derived
class to deal with iteration through arrays of fields. Link step still
has several functions to go. Checkin 3030.


I need to formalize the path traversal rules for DataHandlers and Elements for
assigning indices. First let's finish the compile, then go back to this,
esp with things like set/getFieldArraySize, leading on to the 'path' functions.
DataHandler::isDataHere( DataId di ) is also a concern.
Will need to deal early with having ragged arrays.

First, compile the whole mess. Compiled.

Now stuck on FieldDataHandlerBase::localEntries().
Some kind of foreach and OpFunc should work for this, but I don't see
it right away.
============================================================================
8 Oct
Implemented another iterator, DataHandler::getAllData, in order to be able 
to handle the FieldDataHandlerBase::totalEntries. This compiles, currently
stuck with odd problems in unit tests. Checkin 3034.

Need to set fdh->numFieldBits when we assign maxIndex. Need to compile.
Checkin 3036.

============================================================================
9 Oct
Nasty bug with testSetGetVec. Element::exec goes through it, expecting to
iterate through one argument per vector entry. However, there is only a single
argument (the Fid) given, and so it goes awry. In the old iterator it used
the linearIndex which actually didn't work properly, so the two errors 
cancelled out.

Fairly extensive changes to fix this in a general way. Compiles, clears this
bug, but now stuck a little further on. Checkin 3037.

More progress. Now stuck with getVec. Checkin 3038.
Now cleared the testSetGetVec batch of tests. Checkin 3039.

Now clears testSetRepeat tests. Checkin 3040.

Now clears testSendSpike and a few others. Fails in testSparseMsg. Checkin 3041.

Still struggling with testSparseMsg, something to do with the delivery of
events ?
============================================================================
10 Oct
Tracked it down: it was because of the change in syntax for vector 
field assignment for ragged arrays. Checkin 3042.

Fixed problem in testDataCopyZero, when going from zero to one dim: 
Turned out to be a fairly fundamental problem with the Dinfo::assignData.
Checkin 3043.

Fixed bugs with copy and resize, especially pertaining to assigning data to a 
larger dimension with tiling. Checkin 3044.

Further progress, had to skip a unit test on foreach for the FieldDataHandler
because it needs a valid Element argument. Checkin 3045.
============================================================================
11 Oct
Fixed subtle problem in setting up the ClassInfo objects for introspection.
Turned out that the hard-coded resizing of the FieldElements for each Finfo
was not propagating to the totalEntries, and hence subsequent lookup failed.

With this we clear testAsync.cpp. Checkin 3049.

Now into testShell. Stuck in Shell::handleReMesh. The isssue is that here
we need to resize the pools, adding a new dimension for the mesh. Usually 
when we add a dimension (by the 'copy' function) it is done at the root of 
the tree. In this case though, it is the top of the tree that is expanded.

For now, set up a special Element operation not to resize, but to 
appendDimension, and insist that this can only happen from dim0.

Revisited this, went back to resize, but did things in a more sophisticated
way.

Now clears tests through to testMesh. Checkin 3050.

I keep running into the issue of synchronizing FieldDataHandler sizes. Should
implement an Element or Id operation to do this.
Element::syncFieldDim();

Done. Now clears all single-thread tests. Checkin 3051.

Ran into issues with thread handling. Made the fixes in OneDimHandler, and then
it was clear that the same code would extend to the Two and Any Dim cases.
So did a clean sweep and made a new baseclass, BlockHandler, that deals with
a lot of common code in One, Two and Any Dim. Clears tests up to previous
point. Checkin 3054.

Now stuck in ReduceMsg::exec. The vector of extracted DataIds is odd, having
35 entries rather than 10.

Issue was with the totalEntries. Had to do the syncFieldDim.

Now stuck with shell::doCopy. It seems to copy structure but not the 
values. Checkin 3058.
============================================================================
12 Oct
Stuck for a while in testShell.cpp:testCopyFieldElement. Looks like the 
problem may really be with getVec on multiple threads.
Also check why we are doing a getVec on the orig rather than the copy.

A bit more analysis: The problem is probably with the setVec even before the
copy. Turns out that the copied version actually matches up perfectly with the 
original: both have the wrong values.

Looked at it. There is a fundamental problem with trying to SetVec using a
FieldDataHandler: it uses the foreach, and I would have to know how many
entries partition for each thread if I do the assignment sequentially by
entry sequence. So I need to go back to reconsider how to do this with a
fixed 'block' type vector assignment, even if many entries are blank.

For this to work, we need to stipulate that the maxFieldEntries_ and
associated bitfields are valid at all times.
This will require callbacks from FieldDataHandlers and a guarantee that any
function that messes with the field array size is immediately followed by a
check on maxFieldEntries. 

Should these change, we have another kind of problem: DataIds will
point to something different. The same will happen for any kind of 
dimension resize.

Options:
	- Fix maxFieldEntries and dimensions once and for all at creation time.
		- This is nice as we don't have to do nasty node sync stuff.
		- This may cause problems if we run over. Trap and die?
	- Permit explicit dimension resizing, all applied from Shell rather
		than generated internally. User has to redo all ObjIds of
		resized objects if this happens.
		- Trap and die if we exceed.

Try option one.
Problem here: When I create an entry of the parent class, how do I define the
size of the expected array? It would require the Element to know ahead of time 
that it is going to have a FieldElement or two.

Option two it is then.

Minor elaboration: Should I define a default size for the array? 
	Advantage: May be enough always, like Ticks
	Neutral: No cost to defining a larger size, just bits used in DataId.
	Advantage: Usually do not have to remember to  resize.
	Problem: Likely confusion in cases where the default is too small and
		one gets out of the habit of resizing it right after creation.
Decision: Yes, define a default size.

============================================================================
13 Oct
Implementing above. Now stuck where I left off last. Checkin 3061.

Fixed some issues with the foreach field assignment. Now stuck much
earlier, in testSetGetVec(). Checkin 3062.

Cleared this. Cleared also the earlier problem with threads in 
testShell.cpp:testCopyFieldElement. Checkin 3063.
Now stuck in testSyncSynapseSize. Fixed. Now clears testShell. Checkin 3064.

Another, important thing: get rid of Qinfo::execThread. Should instead
refer to the DataHandler to decide if it wants to deal with a request on a 
given thread. Did. Gets stuck late in unit tests.

Progress. The Shell had been assigned thread0, which was not being selected once
the main process loop began. Now crashes in testMpiStatsReduce.

Fixed various issues which all boiled down to the thread selection in
DataHandlers. Now gets all the way to testMultiNodeIntFireNetwork.
Checkin 3066.

Sundry further fixes, now clears unit tests. Checkin 3067.

Fails to produce output in the rtReadKkit. This turns out to be due to the
ZombieHandler::foreach not being defined, which oddly enough is needed 
because the msg type for plots in rtReadKkit was OneToAll. 
Changed that to Single, but the basic issue of lacking foreach in ZombieHandler
remains.
Now it clears rtReadKkit, but fails in rtReplicateModels. I suspect this is
the same ZombieHandler::foreach problem. Checkin 3068.
Tracked it down. It is a subtle bug: We need the linearIndex for each zombie,
and it tries to refer this to the parent, which always sends back zero.
Zombies need to encapsulate the original DataHandler, with the data
cleared out. It will retain the hierarchy info. Later.

Managed to get it to clear unit tests and regression tests.
Checkin 3069.



A silly bit of refactoring: should replace Eref::index() with Eref::dataId().
============================================================================
14 Oct
Now need to implement the path hierarchy, including some tests.

Option 1: Have a counterpart to 'dims' in the DataHandler, that specifies
pathPosition. All positions that do not have an explicit dim size greater
than one are assumed to be one. Thus:

/model/bulb[0..1]/gloms/glom[0..2000]/cells/mitral[0..25]/dendrite[0..200] 
  0       1         2       3          4      5              6
would be in a 4-dimension AnyDimHandler.
Dims would be 2, 2000, 25, 200
pathPosition: 1  3     5   6

If one had a 2-D matrix at some point we would use for the dends:
/cortex/v1[2]/layer4/cells/pyramidal[10000][10000]/dends[200]
0   1     2     3            4           5             6 
This is a  4-dim handler too.
Dims would be	2	10000	10000	200
pathPosition	2	5	5	6

dims convention: highest index is closer to the root.
pathPosition convention: as above: zero index is root.
Note that the two go in opposite directions, but it is with reason: the
dimension have to scan the zero index fastest for iteration. The 
pathPosition would like to keep the root index identical in all cases.

The test cases are
	- Returning path
	- Generating ObjId from path
	- Generating ObjId of parent
	- copying to build up the arrays, indexing thereof.
	- Moving
	- Creating new Elements: option on a single twig or across all.

How to set up the pathPosition?
	Requirements
	- ZeroDim: No numbers needed. It is the twig of a single dim all the way
	- OneDim: Single number
	- TwoDim: Two numbers
	- AnyDim: Matching vector to the dims.
	- FieldDataHandlerBase: single number for level of self. Other levels
		refer down to parent.
	When to do
	- Part of the constructor: Good, ensures we don't have orphans
	- Separate function: needed anyway for moves and copies.
		- Don't permit moves unless the parent dimsize matches the
		root dimsize of the movee.
	
	What to do
	- Just provide all the numbers in a separate virtual function. 

============================================================================
17 Oct
Implemented path hierarchy code, compiled through the basecode directory.
Checkin 3072.

Compiles all the way through, now to grind through unit tests. Checkin 3074.

Steady progress through unit tests. Now in testAsync.cpp:testDataCopyAny.
Checkin 3076.

============================================================================
18 Oct.
Doing unit tests. Transpires we need another argument in copy:
suppose I copy
/library/cells/pyramidal/apical/dends[200]/channels[10]
to 
/cortex/v1/layer4.
with multiplication factor 1000 to get:

/cortex/v1/layer4/pyramidal[1000]/apical/dends[200]/channels[10]

I know that the copy began at pyramidal, because I would issue the command
	copy( /library/cells/pyramidal, /cortex/v1/layer4, 1000 )
The first argument defines the depth of the new array. 
But my DataHandler::copy for the channels has to know that the 
startDepth was 3. It could just as well have been 'cells' or 'apical'
with depths of 2 or 4.

Before going through all this, let's checkin. Checkin 3081.

Made changes. Compiles. Checkin 3084.

Steady work through testAsync unit tests, especially the DataHandler
tests. About halfway. Checkin 3086.

Cleared all of testAsync.cpp unit tests. Checkin 3087.

Now stuck in testScheduling.cpp:691.
============================================================================
19 Oct.
Now clears all unit tests, stuck in the rtHHnetwork regression test. The
problem is that when an n-copy is made of a cell with an array of synapses,
the dimensions of the cell are changed but this does not propagate to the
synapses.
Checkin 3088.

Some cleanup in ShellCopy.cpp. Now it clears unit and regression tests.
From the 14th: The test cases are
	* Returning path
	* Generating ObjId from path
	* Generating ObjId of parent
	* copying to build up the arrays, indexing thereof.
	- Moving
	- Creating new Elements: option on a single twig or across all.

============================================================================
22 Oct 2011.
Cleared unit test testShell.cpp:testErefToPath(). Checking 3099.
Now working on generating an ObjId from a path.
Work on Shell.cpp: around 690.

============================================================================
23 Oct 2011.
Working on handling the indexing. Implementation is at odds with existing
wildcarding implementation, will need to reconcile.

Did so. Still doesn't clear unit tests, but checking in as there have been
many changes. Checkin 3100.

Clears unit tests. Still have some dummy functions for DataHandler::pathDataId.
Checkin 3101

Some progress into regression tests, but not cleared. Checkin 3102.
============================================================================
24 Oct 2011.
Fixed pathDataId error, now clears regression tests too. Checkin 3104.
Added in a unit test for nested copying into arrays, to build up model. Clears.
Checkin 3105.

Made a somewhat more strenuous test of the assignment of values into this
nested array. Works. Checkin 3107.

Did a valgrind test. Lots of leakage. Turns out that ZeroDimHandler wasn't
freeing stuff. This cleared up most of it.
Found another leak in OneDimHandler::resize. Now clean.  Checkin 3109.

Implemented code for and test for Neutral::parent to obtain the full
ObjId of the parent of an Eref. Works.

Should also implement a broader function for Neutral::children, which
at present only returns a vector of Ids. Would be good to return a vector
of ObjIds and restrict them to the ones specifically below the parent
index specified in the argument Eref.

Renamed the 'foreach' call with 'forall', which is more accurate. Checkin 3111.

Added in a test of path to ObjId conversion following a move of objects. OK.
Checkin 3114.
============================================================================

Subha needs: a clear API of how path-based addressing is translated to the
new dh_branch system, traversal of messages and elements.

6 Dec 2011: Did some documentation of this API. Checkin 3220.
============================================================================
28 Dec 2011.
Wish list of things to implement:
- New Synapse code with lookup rather than queue
- Batched messaging for axonal transmission: club a lot of events.
- Implement proximity activity synaptic capture learning rule
- Implement standard STDP learning rule
- New IntFire with dendritic domains
- MPI
+ Conc units for reactions
- ReacDiff solver to permit compartment-specific reacs.
- SteadyState solver
	- Automated dose-response generation
	- Estimates of all possible steady states of system
- SBML I/O
- NeuroML I/O
- kkit13


Started out working on conc units for reactions. I have a utility
function "convertConcToNumRateUsingMesh()"
which does the conversion either way, but currently reacs (and enz)
use mol#s for the rate unit. The design is to have only the pools
replicated for each voxel in the mesh, the reacs and enzs remain as singleton
Elements.
- What to do for stochastic calculations?
- What to do for exp Euler calculations? 
- What quantity (n or conc) to use for pools?
	- The last two are easy if all calculations are in the same volume.
	EE works fine in conc units, so pools could be in conc units.
	- If pools are in different vols and in conc units:
		- We would need the ratio of vols in the ODE.
		- Need factor also to use in EE calculations if calculations
			are done in 'n.
	- The last two could also be one in 'n' units.
		- EE wourks fine, but need to have rate scale factors to use
			for the calculations
	- 
	



============================================================================
29 Dec 2011.
Began work on converting Reacs to use conc units. The reac side of the story is
now sorted out, but for the solver system I still need to figure out how to
retain the authoritative Kf and Kb values: in the solver data structs, or back
on the ZombieReac.
Compiles and clears unit and regression tests. Checkin 3226.

On solver data structs: We may have different volume meshes, so would
need either a volume term in the calculations, or precompute different
RateTerms. Either way, the Kf and Kb values would be for reference, not
for calculation.
Are we likely to have different RateTerms? Probably not, but will need to
incorporate different volume scaling terms.

On Reac: Would need to refer back each time there is a remeshing.

I think this belongs on the Reac. Redo the zombie.
There is also the concern that (#/cell) units are meaningless in a reac that
describes an entire mesh. Each mesh entry will have its own kf and kb if the 
volumes differ between mesh entries. Even if they are the same, the values 
are dependent on mesh dimensions.

Structure of kinetic models:

/base	:					solver, 
					could be Ksolve, Gsolve, or other.
	/kinetics:				Mesh
		/mesh				MeshEntry
		/boundary			Boundary
		/<poolname>	 		Pool
			/<enzname>		MMEnz
			/<enzname>		Enz
				/<enzname_cplx>	Pool
		/<reacname>			Reac
		/<sumtotname>			SumTot
		/<group, pool, reac: recursively>
	/compartments:				Neutral
	/compartment_<i>			Mesh
		/mesh				MeshEntry
		/boundary			Boundary
	/geometry: 				Neutral
	/groups: 				Neutral
	/graphs: 				Neutral
	/moregraphs: 				Neutral


OneToOne Messages go from pools/requestSize to the matching MeshEntries/size.
compartments doesn't seem to be doing anything, it should have been holding the
compartments.
Geometry doesn't have anything in it now.
Groups: Not doing anything now.
Info: not present now.

============================================================================
30 Dec 2011.
x The readkkit needs to direct the reacs to the compartment (ChemMesh) for size,
and the pools to the MeshEntries on the compartment.
	This isn't right. The reacs need to query their sub/prd pools
	for volume, and in this process they handle different volumes too.
	So current function is a good start on this.
. Need to make unit test for Reac volume scaling
	- Need to go back and fix the scaling. The function should go to each
	of the substrates, get their volume, and then work out the scaling for
	kf. Likewise for kb.
- The ChemMesh needs to be linked to a suitable Geometry
- ReadKkit needs to insert ChemMeshes/compartments under the master /kinetics
	tree, and to put the embedded pools and reacs under their compartment.
- ReadKkit needs to put reacs that span compts into the smaller compartment.
- I need to build some good unit and regression tests for all this.
- Need to simply insert solver and have it traverse its children as its default
	path. 
	- The solver should itself figure out what to do with the mesh and 
	geometry.
	- Solver needs to set up messages or other links to the solved 
	ChemMesh objects, to handle remesh requests.


Checkin 3227. So far I've only begun on the unit test for reac volume
scaling, need to fix up the function that asks the reactants for their volumes.

Need to have API for geoms and meshes to talk to each other so that 
changes on geom reflect on the mesh, and when the mesh is general, like a 
CubeMesh, we can apply arbitrary geoms to define the limits.


============================================================================
31 Dec 2011.
Working on reac rate/volume conversion.
See also discussion on 13 Sep 2011.
Ran into a problem with it because one needs to specify a meshIndex, but
which one? If the reactants are on different meshes then we have a big
problem of matching up. What is this function used for anyway?
If it is for the generic kf and kb of the bulk terms, then we can use
a simpler function that ignores meshIndex.
If it is for the specialized job of handling transfer between different
compartments, and generating terms for those reactions, this is a job
for the solver and meshes to work out. Here we would have the additional
issue that the rate terms for reacs are not constant, so the regular
solver replication approach would need an additional scaling term to
deal with volume.

Use cases:
traffic between bulk and PSD (with an anchor protein): 
	1 vol for bulk, other vol for PSD having complex and anchor in it.
Transport via channel
	1 vol for outside ion, 1 vol for inside ion, surface for channel
Receptor-ligand binding
	1 vol for ligand, surface for receptor, surface for product.
GProtein activation
	Ternary complex (memb) + GTP (bulk) <---> complex (memb) <---> Galpha.GTP (memb) + GbetaGamma (memb) + GDP (bulk)

The basic idea is that both in the case of the single-mesh-entry bulk
reactions, and in the case where the solver does a straightforward
replication of reac terms, they all have the same kf and kb rates.
So we will just need to use the first index of the mesh.
In the case of nastier mesh alignments, the reac will need an additional
scaling term which can be condensed into something like another
reactant. The job of the solver and meshes is then converted to figuring
out which reactions need this treatment, modifying them, and generating
the array of scaling terms.

After some pen/paper analysis, we come back to the point from earlier: the
Kf, Kb are defined for a certain volume. If molecule A1 is in this volume,
then

dA1/dt = -Kf.A1.A2.A3...  + Kb.B1.B2... // Regardless of v2, v3 etc.

Then, dA2/dt = dA1/dt * v1/v2
etc.

flux = d.nA1/dt = kf * nA1 * nA2 * ...

Converting to conc:

flux = dA1/dt * v1*NA = kf * A1*v1*NA * A2*v2*NA *...
Cancel out v1*NA

dA1/dt = kf * A1 * A2 * ... * v2*NA * ...

But 
dA1/dt = Kf * A1 * A2 * ...
So

Kf = kf / ( v2 * NA * ... ); 

kf = Kf * ( v2 * NA ) * ( v3 * NA ) * ... // More terms as needed.

So which pool do we pick for A1?

Assume the smallest.

Implementation in progress. I have replace the original 
convertConcToNumRatesUnsingMesh
Need to get the stuff to compile first. Done.
Need to clear unit tests for reac and add more tests for multiple vols.
Clears original unit tests but fails regression tests. Checkin 3228.
============================================================================
01 Jan 2012
Absolute can of worms in ReadKkit.cpp. The rates in kkit are saved as
kf, kb. These get assigned to Reacs, which refer back to the pool vols to
set Kf and Kb, but unfortunately at this point pool vols are meaningless.
Then the pool vols get setup, leaving discrepancies between kf and Kf.
To add to the fun, the MOOSE vol units are SI (mM) while the kkit units are
usually uM. 
Furthermore, NA in MOOSE is a good best current value, but NA in kkit is 6e23.
So conversions between kf and Kf differ for the same model in MOOSE and kkit.

Need to do some post-loading fixups.

Begun, with ReadKkit::convertRatesToConcUnits. Still doesn't clear test.

============================================================================
06 Jan 2012
Still working on regression tests relating to volume conversion.
Got the conversions for Reac to work. Now fails on MMenz, which is odd as those
should use conc units natively. However, the kkit storage of values is as
k1, k2, k3 so the conversion fails there.


============================================================================
7 Jan
Much progress on setting up the ZombieReac so it, not the solver, maintains
Kf and Kb. Also redesigned it so that scaling Kf and Kb refers the problem of
rescaling individual voxel rates to the solver.
Still stuck on rtReadKkit.cpp:83.

============================================================================
8 Jan 2012
Now it clears the regression test stuff for checking reac rates,
back to dealing with the MMEnz rates.

Now clears the Kholodenko test both for ReadKkit and for running it.
Stuck in Cspace, may be I now need to fix up enzymes.

Fixed enzymes. Now problem with pools, which were converting to and from 
micromolar. Converted all to millimolar, playing havoc with unit tests.
Fixed. Regression tests are also nasty. Need to put in a unit conversion
function into the Table, perhaps just a scaling factor. 
Did that. Now able to go through unit tests. Regression tests now stuck in
the readCSPACE conversions.

============================================================================
9 Jan 2012.
Finally clears the rtReadKkit.cpp regression tests. Now fails in 
rtReacDiff.cpp:rtReplicateModels.
Fixed up some concscales there too. Now clears all unit and regression tests.
Checkin 3237.

From 28 Dec 2011:
Wish list of things to implement:
- New Synapse code with lookup rather than queue
- Batched messaging for axonal transmission: club a lot of events.
- Implement proximity activity synaptic capture learning rule
- Implement standard STDP learning rule
- New IntFire with dendritic domains
- MPI
* Conc units for reactions
- ReacDiff solver to permit compartment-specific reacs.
- SteadyState solver
	- Automated dose-response generation
	- Estimates of all possible steady states of system
- SBML I/O
- NeuroML I/O
- kkit13

Rather alarming that it has taken 2 weeks for just one of these things.

Some more specific items based on plans to do SigNeur calculations and
the traffilator stuff in spatial systems:
- Branching 1-D diffusion
- Different volume meshes
- Generating 1-D diffusion meshes from neuron morphology
- Arbitrary boundary 2-D square meshes
-> MPI and thread partitioning of diffusive calculations
- Stochastic spatial calculations.
-> Interfaces between sections handling different reac systems
-> Transport and conc gradient calculations

Starting with MPI and thread partitioning of diffusive calculations.
Stoich::updateDiffusion is called from the Process of the MeshEntry. The
MeshEntries are presumably the entities subdivided among threads.
MeshEntry::process --> ChemMesh->updateDiffusion-> Stoich::updateDiffusion
--> Stencil::addFlux -> stencil1()

Here the thread partitioning happens at the specified meshIndex, which is
from the e.fieldIndex used in MeshEntry::process.

The stencil operation is where the values add up. Here we would wish to 
accumulate stuff for inter-node messaging, but need to know the
partitioning beforehand.

As the ChemMesh is where the stencil setup and flux calculation happens, it
is clearly there that the partitioning to other nodes needs to be assessed,
and from there that inter-node messaging must proceed.

============================================================================
10 Jan 2012

In CubeMesh::buildStencil, should examine the edges of the node range. There
should put in an alternative to the various Stencil forms that handles 
messaging. This message should originate from the Stoich, and go into the
Stoich. The dest part is relatively easy, as the sender has all info that the
recipient will need to put the required flux to the required place. Could even
club a lot of this. 
Sending is a bit tricky as the Stoich will need to know its own Eref and
Qinfo and so on, but the original call is through the MeshEntry::process.

============================================================================
11 Jan 
How is Stoich load divided among threads?  How does this relate to the flux
calculations? Worked through this and added to the doxygen programmers
guide. Will need to send out the flux messages at the first clock, 
triggered by MeshEntry::process. With suitable message dests in the 
Stoich, this will all be there in time for the second clock when the
GslSolver::process is called.

Stuff to pass down to Stoich::updateDiffusion: ThreadId, for the
send call.
Stuff to add to the Stoich data structure: Its Eref, for the send call.
Stuff to consolidate: 

We currently have a bottleneck in the destination fluxes all converging on
the Stoich. Later.

Rather than devise a new stencil for off-node entries, one option is to 
create dummy S vectors for them. The message would then not be flux, but the
value to populate these dummy S entries. Problem is with multidimensional
edges.

Assume we can set these up. These will have to be figured out at the ChemMesh,
but it is the Stoich that has to send the messages out.

The time to send stuff is at the end of Clock1. The messages are needed in
Clock0 to deal with the diffusion calculations.
So the Stoich::clearFlux function is the right time. Will need to put in the
additional stuff for threadId and Eref, which actually the GslIntegrator has
handy and can pass in.

Would be nice to do some consolidation here, for all the S values for all
the GslIntegrators on this thread. Probably only a minor optimization.

Underlying this, and perhaps should be done first: We need a solver setup
class. Could be the Stoich, or something else. Needs to deal with the message
setup and handling Stoichs, Mesh, Gsl and other integrators, and of course the
chemical system.

Stoich:
Already deals with chem
Already has most of the data structures needed for diffusion internally
Already nearly 1000 lines long.

Separate class
Will have to orchestrate Stoich with extra func calls. Both good and
	bad aspects to this.
Keeps the functionality somewhat better separated: the molecular traversal
	to the Stoich, and the system traversal to self.
Consider the SigNeur to be one variant of this. Here the Stoich as well as
	an hsolver have to be handled.
Consider a reaction system with multiple reaction sets in different
	compartments. This will need multiple Stoichs.

Seems clear enough that I should make a separate class. In
the earlier incarnation of MOOSE there was a KineticManager.

Let's design some defaults:
1. Simple reaction tree: Kinetic Manager builds the volume stuff and sets up 
	solver.
2. Reaction tree with spatial stuff on it: 
	Kinetic manager builds a deterministic PDE solver system
3. Reaction tree with 'hints' as fields on the ChemMesh parent of the 
	signaling tree.
	Hints may also help assign # of nodes and threads.
	Sets up a multi-solver model with the hints indicating whether to solve
	each sub-tree using GSL, GSSA, Smoldyn or whatever.
4. SigNeur?	Neuronal model with usual solver, with 'channel' placeholders
	to indicate name of signaling model in each compartment.
	Signaling model done as in 3.
	Adaptors defined in sig models, work on local channels.

Checkin 3242.
============================================================================
12 Jan 2012
Revised structure of kinetic models:

/base	:					Manager
	/<solver_name>				Solver
	/<compartment_name>			Mesh
		/mesh				MeshEntry
		/boundary			Boundary
		/<poolname>	 		Pool
			/<enzname>		MMEnz
			/<enzname>		Enz
				/<enzname_cplx>	Pool
		/<reacname>			Reac
		/<sumtotname>			SumTot
		/<group, pool, reac: recursively>
		/<compartment_name>
			/mesh			MeshEntry
			/boundary		Boundary
			/<poolname> 		Pool
			etc, recursively.
	/geometry: 				Neutral
	/groups: 				Neutral
	/graphs: 				Neutral

Basic principle is that compartments and reactants are the biological building
blocks, so let's keep that as the backbone tree.
I would like to put the solver under the relevant compartment, but we may
have a single solver manage multiple sibling compartments. For example,
PSD and bulk. I could insist on a containment relationship, for example put
the whole lot into the spine, but this is hard to extract from kkit models.

Added ChemMesh::buildDefaultMesh to make it a little easier to set up models.
Checkin 3248.

Compiled in SimManager. Doesn't do anything yet, only stubs for tests.
Checkin 3249.

Next step: Use the SimManager utility functions in ReadKkit and
ReadCspace. 
Next: Fix up Shell::doLoadModel to correctly use the SimManager.

============================================================================
13 Jan 2012
This is now working for the original model, but I've run into problems due to
the earlier hack
for changing volumes. The correct way to do this is to have the stoich and the
Mesh/meshEntries talk to each other much as the chemical system does.
Or perhaps the SimManager needs to talk to the Mesh.

For now, patch up the regression test and move on.

Checkin 3252.


============================================================================
15 Jan 2012
Designing a message from ChemMesh to the SimManager. This needs to
indicate 
	- # of voxels
	- Size(s) of voxels
	- Proposed node partitioning? Or is the the job of SimManager?
		Seems that SimManager should do this. It knows more about
		the rest of the model, and the required solvers.
		Problem is that the connectivity of voxels is known only to
		the ChemMesh. Is there a way to abstract this so that the
		SimManager can deal with the node balancing itself?

Sequence of steps in voxel partitioning:
1. SimManager gets # of nodes and # of threads per node from Shell.
2. SimManager gets model size and likely dt from Stoich. Uses to estimate load.
3. SimManager gets # of voxels from ChemMesh.
4. SimManager also gets load info for other calculations. Puts all this together
	to decide on partitioning.
5. SimManager sends # of available nodes to ChemMesh. Mesh partitions voxels.
6. Mesh sends back partitioning info and the tables of which blocks of data
	need to be messaged between which nodes.
7. SimManager across nodes sets up messaging between Stoichs

============================================================================
16 Jan 2012
How to define node partitioning: 
	- Simple brute force would just be a listing of nodes and mesh entries.
	This is bad because it will need to be screened anyway to establish
	transfer lists.
	- Next would be through a list of boundaries for each node pair:
		left node, right node, left mesh entries, right mesh entries.
	This seems pretty general even if possibly tedious.
	Could ask the ChemMesh to send back only those node pairs where current
	node is present.
	In practice, left (current) node is known. Need a vector for other node,
	a vector for # of entries per boundary, and a catenated vector
	of mesh entries for each node pair.
	These data transfers are exemplars of when specific node-to-node
	transmission is desirable. For now SimManager just grabs the Id and 
	accesses directly.

Now running into a number of setup ideas:
	- Redo fastGet to take string field specifier, and not need existing Msg
	- Implement a fastSet.
	Point is to have a node-local, fast and general function that is
	thread-local. Would be useful for things like model managers and
	setup, where we want to do stuff locally.

Current status: trying to compile. Need to get the various ChemMesh 
subclasses to handle requestMeshStats and HandleNodeFinfo.
============================================================================
18 Jan 2012
A few issues come up.
1. Broad one: need clearer demarcation between messages meant to go 
	locally, and those meant to go across nodes. Local 'set' messages
	still need to go into a queue because of threading issues, unless
	I can guarantee that it is on shell setup thread. Alternative is to
	do hard pointer lookups on objects to do operations, which is both
	ugly and thread-unsafe.
2. What to do about meshing for un-solved molecules. Currently some ugly
	shell functions deal with things, but I would like to move all this 
	out of shell. 
3. Execution of structural commands off-shell. To do this we need to have a 
	safe way to execute ops on objects other than the shell. 
	The Qinfo::addToStructuralQ will work on any object, but has to be 
	called when the system is on a single thread.
	- Could simply use a mutex to protect the structural Q.

Tried setting it up with a mutex. It now clears unit tests but stalls
(due to small dt, but it still runs) in the rtRunKkit regression test.
This is likely due to not setting dts somewhere in the SimManager. Checkin
so we can keep moving on.
Checkin 3261.

Still tracking down why the plotdt isn't set up correctly.
============================================================================
20 Jan 2012

Need to have a clear demarcation between setup operations (doLoadModel etc)
and messaged operations.

- Setup ops cannot be messaged at least in the present manner. There will be
	race conditions as the inner setup ops like doCreate are blocking.
- Setup ops cannot go in parallel. They have to run on the master node and
	may refer to lower-level Shell setup ops like doCreate.

- A fastSend would be nice. 
	- This would be an alternative to extracting the pointer to the obj
		to manipulate it.
	- This would encode all lookup relationships in msgs
	- Sets up a common API for multi-obj ops.
	- Big concern that it should _not_ be called by regular msgs.

Perhaps a cleaner alternative is to make the various doCreate and other
functions messaging compatible. 
	- All these funcs do point into a DestFinfo, such as handleCreate.
	I haven't tested this for multithread, multinode safety when called
	from a non-shell object. But I think a 'Set' call will go to all nodes.
	- In many cases we do need to block till a sequence of messages is done.	- 

I have a clash of cultures here. 
	- direct call of object functions in ReadKkit. Not thread safe, assumed
		called only from parser thread.
	- Ack-protected thread-safe messaging.
		- Shell::doCreate etc which uses messaging wrapped in acks.
		Likely to hang if called within messaging.
	- Set/Get calls, which insert stuff into messaging and use 
		Qinfo::waitProcCycles.
		Likely to hang if called within messaging.
	- Regular messaging.
	- Structural messages, which achieve safety by putting things on a
		single thread, but thus make locks likely.
	
============================================================================
21 Jan 2012
Tracked down a problem: Calling Stoich::set_path twice. The first
time seems OK, the second time round all pools are now zombies and are
not recognized even though the ChemMesh still has messages to them. So
it all gets confused. 

I do need to set up the Stoich in the first pass just to get it to estimate
the chem load. So the part of the stoich where it messes with the chem mesh
needs to be removed and put out with the SimManager.

There is quite a bit of compartment related code in the Stoich. Most of it
looks like stubs, but there is a substantial chunk in 
Stoich::setCompartmentVolume(). All should go away. Not clear if I need to 
have any kind of zombification or other internal digenstion of compartments
or meshes.  For now leave out of zombie structure and aim to have the regular
messaging take care of it.

Here's a chicken-and-egg problem: I want the model structure sorted out in
Stoich to decide how to do the node decomposition of the mesh. However, the
Stoich wants to know the pool meshing to load in (and zombify) the pool,
since this is when it brings in the Ninits for each mesh entry.

Analyze use cases. In almost all situations I start with a single-compartment
reaction definition and want the solver to spread it out. There is a problem
with this only when I have an a-priori spatial distribution. Here too the
SimManager has to figure out how to reallocate the model among nodes.
Conceptually though it is cleaner to have the whole model already split up.
OK, this boils down to deciding whether the pool array resizes into voxels
as a direct operation when he ChemMesh does so, or whether this operation is
mediated only through the SimManager. Earlier too the operation was actually
handled through the Shell. 
	Again, the voxelization is easy but the node partitioning is not, and
should be done through SimManager.
Let's go through SimManager. This means that:
* Need a pool Sinit vector sans voxels in the Stoich. Could take over the concs
	so we are not stuck with the volume conversion issues.
* Need to be able to do first pass set up using the Sinit vector.
- Need to be able to revoxelize any time, and this function is just what is 
	used to build it the first time too.

Once this is done I need to go back to the chem systems and redo them all 
to use conc units. At this point only the Reacs do.


Next step: Write out the calculation and messaging stuff on
Stoich to figure out how to set up the messaging. I think that
there will have to be a message from the ChemMesh to it,
rather than to the SimManager, to define internode diffusive messaging.
This is happening around Stoich.cpp:826.

Also now need to tell the GslIntegrator only to operate on meshIndices
that exist on current node.
This is a little tricky. I need to create GslIntegrators only for the 
meshEntries on the local node. Instead of creating a bunch of dummy ones,
I'll have to provide each with its associated meshIndex.


============================================================================
22 Jan 2012
Working on the mesh. Realized I need to transmit vector< vector< T > >.
Implemented converter and unit test for it. Checkin 3263.

Current effort is to have things set up so that they can again run simple
simulations, and slowly build up to meshed models, and finally to the 
multinode case.

Gone through it. Now it clears the first test where it does a calculation,
and hangs later when it tries to change the system volume using the
compartmentVolume function which no longer exists. Checkin 3264.
============================================================================
23 Jan 2012
Fix to converter for vector< vector< T > >, pointed out by Niraj. I'm surprised
the original worked. Checkin 3268.

Continuing from 15 Jan. The Re-meshing operations sequence.

Geom changes 
	-> send Msg to Chem Mesh
	OR direct remesh assignment on a ChemMesh 
	-> Send msg to Pools to remesh:
		Presumably a Neutral call to reconfigure DataHandler.
		This will have to go to the Shell, somehow, to propagate to all
			affected nodes. Some nasty node balancing may ensue.
	Send Msg to SimManager to remesh. 
		-> Send Msg to Stoich to remesh
		This too has to undertake nasty load balancing, not of
		DataHandler, but of contents.
		-> Send Msg to GslIntegrator to reconfigure DataHandler too.
		Again, has to go to Shell to orchestrate balancing.
		
// This one lets the DataHandler figure out its own node balancing.
// Works fine for all symmetrical cases where there is no relative organization
// required for local nodes. They come in a block as defined by a linearized
// version of the n-dimensional Handler.
DataHandler::innerNodeBalance( numData, myNode, numNodes )

// This one imposes load balancing. Usually also imposes change of the 
// DataHandler to a different class. Arbitrary node arrangements possible.
// Note that the form of command means it can be applied on all nodes.
DataHandler::innerNodeBalance( myNode, vector< short > nodeAssignment );

Both these functions need a handler through Neutral. The role of this handler
is to transfer data out to other nodes, and coordinate harvesting of external
node data to fill in local node portion. Frequently these functions change the
size of the DataHandler.

The Neutral function must also tell the messages that their targets have moved.
Most messages are already globalized, but some sparse messages may need
also to be reorganized. Perhaps defer this? The sparse message should have a
higher-order definition, which can be reactivated.

============================================================================
25 Jan 2012
Currently a bit stuck because the pool compartment resize is not propagating
through to the DataHandler as it should. 
This is in testBuiltins.cpp:470.

The ofid->fid is zero. This is wrong.

Tracked it down. Turns out that the addToStructuralQ did not know about
direct Q insertion cases, and was therefore missing out the ObjFid data
required to execute such calls. With this fixed the code now clears unit
tests. The code point is now back in the regression tests where I need to 
reformulate the remeshing calls to be called from the Mesh object. Key
advance is that these structural calls can now be invoked from regular objects.
Checkin 3270.

Now it isn't computing the original, single mesh-entry kinetic model.
I looked at the trace and though it is calling the Stoich::innerGslFunc the
values in the S_ vector don't change.

The v vector does obtain nonzero rates of change. 
yprime is all zeros though. Why would SparseMatrix::computeRowRate fail?

Doing a valgrind. Clues there in zug.
============================================================================
29 Jan 2012
A long struggle to clean out an uninitialized value reported by valgrind. 
After some days tracked it to the change in Reac.h where I introduced 
concKf_ and concKb_ as the authoritative values, but I hadn't initialized them.
Still doesn't clear regression tests.
Checkin 3274.
Ran valgrind through to point of assertion fail in regression tests. No
memory problem nor use of uninitialized value.
============================================================================
30 Jan 2012
Finally figured out why the reactions were not advancing. Printed out stoich
matrix, looks like none of the MMEnz terms are in it. There is something
odd happening in the MMenz::zombify.
============================================================================
31 Jan 2012
Tracked down bug. Somewhere along the way, the functions 
getOutputs and getInputs were set up to catenate new targets onto the vector,
rather than to clean it out and start anew. I had been assuming the latter.
Need to check with Niraj if this was intentional. If not, remove. Anyway,
with this known it was easy to fix. At this point it successfully loads in
a single compt model, runs it correctly. Now to move on to volume and mesh 
handling.
Checkin 3275.

============================================================================
1 Feb 2012
Working on remeshing. Now a simpler message from the MeshEntry to the Pool,
to tell it to resize, renode, and rescale volumes.
This should really be a node-local message.

This brings up a general point about node-local and global messages, perhaps
even subset target messages.

At this time the entire contents of the queue are broadcast. SwapQ generates
a single queue for local processing and transmission. Transmission always
means broadcast.

We could do this indiscriminately, binary or graded. 
Indiscriminate: All messages go to all nodes including self.
	Wasteful
	Problematic when we want to send node-local requests around
Binary: Messages either go to local node only, or to all nodes including self.
	Wasteful if there are subsets of target nodes: only on big clusters
	SwapQ costs a little more as it has to fill distinct buffers
	Fixes problem with node-local requests
Graded:	Messages go to whatever subset of nodes is specified.
	Possibly efficient if there are large groups of nodes.
	SwapQ costs a lot more as it has to fill multiple buffers, and contents
	may overlap.
	Fixes problem with node-local requests as a special subset.


Let's try out binary. It will involve rethinking the multinode data transfer
protocol a bit, as we may have local ops to do in addition to what has come
in from other nodes. Additionally the 'idle' cycle at the time of node0 
data transfer, could be used for node-local messaging on all nodes.

For now, work out the remeshing code on a single-node basis.

Did the remeshing code. It breaks the little volume-handling that there
already is in the regression tests. Anyway, checkin 3276.

Fixed the volume handling test. There was a field mismatch between zombie
and regular Pool. Checkin 3277.

Working in mesh/testMesh.cpp:testReMesh().
Skeleton set up to create a pool and tie it to a mesh. Does OK so far.
Next to remesh. 
============================================================================
2 Feb 2012
Got the remesh working in a unit test, without the full scheduling going. 
Checkin 3279.
A continuing issue here is that the structuralQ calls end up being executed
out of order unless one explicitly tells Qinfo to wait a couple of cycles.
Not sure how to deal with this.

Next step is the regression tests.

Some fixes to get back to the point of interest, where I want to set volumes
of the hsolved system. I had to fix ReadKkit and some of the zombie
conversion routines. Checkin 3280.

Now onto the core conversion steps.
1. Changing the volume. I have it propagating into the ZombiePools, need to
put in a test just for changed numbers and stable concs.
2. Changing the volume. Still need to connect to the Stoich itself, so that the
	reaction rates also scale.
3. Changing mesh size. This too will involve connecting to Stoich itself,
	and will also mean remeshing the GslIntegrator.
4. Linking Geoms in
5. Providing suitable time buffers.


Now trying to change reac volumes
	From Stoich: Need to go through all RateTerms, use getReactants,
	query volumes. But it doesn't tell us what the reference rate was.
	From individual Reacs: Need to implement updateRates function to
	query substrates and products, and do scaling. In the Zombies this
	has to then update own entry in RateTerm table.
	Advantage here is for cases with multiple compartments, each compt will
	manage its own reacs.
	Very minor advantage also that numKf can be updated as soon as 
	rescaling happens.
	Issue is that I now have to rebuild the messaging to the reacs.

Done all this, now it clears the checkReacRates section with the rescaled
volume.  Checkin 3281.
Next to do the enzymes.

============================================================================
3 Feb 2012
Put in skeleton messaging code for the ChemMesh to call "remesh" on MMenzs.
Checkin 3282.

Put in the MMEnz::remesh code. Still to get it to clear regression test.

Now clears the test in rtReadKkit.cpp:rtRunKkit()
for resizing the compartment, without changing the number of voxels.
Checkin 3283.

Working on handling remesh requests in the Stoich. Skeleton messaging
set up. Had to fix conversion of vector< vector< T > > again.  Checkin 3283.
Added messaging and remeshing operations to the GSL.
Next to actually use these in a test, and deprecate the existing ReacDiff tests.
Setting up unit test in testSimManager.cpp.

Compiles and runs without crashing, but I now need to put in some verification
that the diffusion and multiple voxels are happening.

Put it in. Much cleanup ensued. Final bug was a good one: The max field size 
for the FieldDataHandlerBase of the MeshEntry was not getting updated when
the CubeMesh was resized. So the bitmask was rolling over at 63, and this
meant that the mesh entries above 63 were being done as 0-36.
Which mean that those indices were executed twice.
Which gave a numerical error.
Checkin 3286.

Some cleanup pending, specially to see if I can now avoid explicitly calling
the setRepeat to assign the Stoich to the gsl. Yes, this works.
Also added another rescaling of the same system. Does it seamlessly, but
the numerical error is now a bit more, about 1.1e-8.
Did another variant of this, same total length, halved the spacing. It
works too. Checkin 3288.

Also a fix required for numEntries of zombified pools when they are resized.
Should behave like regular pools there, which means that things like SetVec
should work too.
Did this. Basic fix worked, but there was a complication with how 
ZombieHandler::copy was implemented. Fixed that too. 
Clears unit tests again. Checkin 3289.

Then need to clean up the ZombieEnz code.
	Skeleton of this done. But not yet integrated into ksolve, nor tested.
Checkin 3290.
Further steps on incorporating: Checkin 3293.
Much cleaning out needed still on ZombieEnz. Some more done. 

Then need to clear all regression tests.
Then need to go multinode.

============================================================================
7 Feb 2012
General cleanup of ReadCspace to use SimManager correctly. 
Led to fix in a bug in Shell::doReinit.
Now clears unit tests. Checkin 3294.

Further cleanup to get the changes in ReadCspace through regression tests 
till it is back at rtReplicateModels. Checkin 3295.

Now I'm stuck with a small numerical discrepancy in the calculations for
rtReacDiff.cpp:rtReplicateModels. Tried many things:
	- Keep all voxels init conditions identical
	- Check if stoich and gsl are called.
	- Zero out diffusion consts
	- Check enzyme parameters
	- Meshing of enz cplx (was bad, fixed).
	- different # of threads
	- Change voxel vols.
	- Ran single-compt version in genesis, saved, reran in moose-g3.
	- Ran the thing as a single-compt model.
Possibly an error with the enzyme zombification setup.
Yes. Confusion between Id of enzyme site and enzyme molecule. Fixed. Now
goes one step further in regression tests. Checkin 3296.

Now working on testDiff1D. Here I test the CylMesh. Still stuck.
Fixed. The CylMesh wasn't defining a stencil for diffusion, so diffusion wasn't
happening.

Now clears testDiffNd for 1 to 3 dimensions. Checkin 3298.

Now clears testReacDiffNd, but that doesn't do a quantitative check.
Anyway, with this all regression tests clear and it is checkin 3299.

============================================================================
9 Feb 2012
Some minor fixes to ReadKkit to get the system to correctly handle more .g
files from Doqcs. Added a couple of regression tests for this. Checkin 3302.

Trying to catch a bug with zombification of BufPool. The function 
zombieBufPool::zombify doesn't get called.

============================================================================
11 Feb 2012.
Tracked it down, I was testing for the wrong molecule. Now clears regression
tests.
From 9 Jan: Wish list of things to implement:
- New Synapse code with lookup rather than queue
- Batched messaging for axonal transmission: club a lot of events.
- Implement proximity activity synaptic capture learning rule
- Implement standard STDP learning rule
- New IntFire with dendritic domains
- MPI
* Conc units for reactions
- ReacDiff solver to permit compartment-specific reacs.
- SteadyState solver
	- Automated dose-response generation
	- Estimates of all possible steady states of system
- SBML I/O
- NeuroML I/O
- kkit13
- Branching 1-D diffusion: a neuronal morphology geometry and mesh
- Different volume meshes
- Arbitrary boundary 2-D square meshes
-> MPI and thread partitioning of diffusive calculations
- Stochastic spatial calculations.
-> Interfaces between sections handling different reac systems
-> Transport and conc gradient calculations

============================================================================
14 Feb
Minor updates to regression test for reading kkit models, minor fix to
ZombieBufPool creation. Checkin 3306.

============================================================================
18 Feb 2012
Minor updates to the documentation for zombies. Checkin 3315.
============================================================================
20 Feb 2012
Added another regression test to read tabsumtot.g. This tests tables as
well as sumtots. So far the skeleton of the test is there, and reads the
model only. Clears regression test but complains that it doesn't know how to
handle the xtab. Checkin 3317.

Next: fix Readkkit to
	* read table
	* Handle sumtotals
	- Read coords
	- Read annotations


ReadTable stuff in progress, still skeletal.
Working on Table::process. Lots of stuff pending.
I think the clean thing to do is to have 4 classes:
TableBase: Handles the basic stuff for field access and file I/O. Base class
for the others.
	Table: Receive and record inputs. Used in lieu of plots.
	StimulusTable: Generate output waveforms and events
	Interpol: Lookup values using interpolation, send results out. Can also
		be set up for FastGet.



============================================================================
21 Feb 2012
Implemented TableBase.
Redid Table.
Some temporary patches to ReadKkit so it clears unit and regression tests for
now. Checkin 3319.
Created StimlusTable, derived from TableBase. Incorporated into ReadKkit.
Clears regression tests for setup, yet to test for its running. Checkin 3321.

Issues with inheritance. When overriding a parental DestFinfo, the
fids should match up so that messages can go to the correct function
of the derived class. They don't.

Patched this in DestFinfo top. Need to make corresponding fixes in Cinfo.

This works but the quantitative comparisons with the expected
result don't.
============================================================================
22 Feb 2012
Some fixes to the interpolation routine. It now correctly deals with 
StimulusTable input to Pools. It almost seems to handle sumtotals correctly,
but the t=0 value is wrong. I've hacked this out for the purposes of clearing
the rest of the regression tests. Checkin 3324.

Fixed the initialization at t=0. Now the quantitative values still come
out wrong.
I think this is the millimolar conc issue. Furthermore, I really need to check
whether the StimulusTable is connecting to conc or n for controlling values.

Fixed ReadKkit conc scaling for tables and deciding whether to connect
n or conc. Checkin 3327.
============================================================================
23 Feb 2012
Another wishlist, directed towards what it would take to write up:

Spatial sims:
	- Neuronal geometry		2
	- Irregular boundary mesh	2
Numerical engines			
	* GSSA				1
	- Smoldyn			2
	- SigNeur			2
	+ Hsolve			Niraj
I/O
	- SBML				Harsha
	- NeuroML			Aditya/Niraj
	- NineML			Subha
	- Data I/O			?
	- SigNeurML			2
GUI
	+ Demos				All
	+ Kinetics			Harsha
	- Neural			Chaitanya
	- Network			?
	+ MOOGL				Chaitanya
Platforms
	+ Multithread			OK
	- Multinode			4
	- Compile on 3 OSs		Harsha

DOCS

Total time				15 weeks



Immediate next step: How to set up annotations and display coords on chemical
entities, and to use this for kkit reader.
Options:
	- Give every object a string and xyz coords. GENESIS does it.
		- Easy, wasteful, but won't matter much. A bit ugly.
	- Use individual ext fields. 
		- Very wasteful, won't matter much.
	- Make a general annotator class with xyz and a text string. Dangle
		this off whatever you wish to annotate.
		- General, wasteful, won't matter much. Can add fields like
		colour and shape/icon and it would be general.

Implemented option 3. Seems to work. Need to do a regression test based on
one of the existing models.

Implemented the regression test. Much cleaning up, works now.
Checkin 3333.

============================================================================
24 Feb 2012.
Dealing with a bug in getting Msg Element paths, reported by Subha. The
immediate bug was fixed, but there are issues still with the   
localEntries function which always returns 1.

============================================================================
25 Feb 2012.
Worked on keeping track of localEntries. This turned out to have multiple
cascading effects including some nasty initialization sequencing issues and
a redoing of the various Msg class destructors. Good thing of this effort is
that the structure for MsgManager data is now sensible, and one could iterate
through or other wise examine all Msgs of each class. Need to work out API for
that.
There is a pending valgrind (memory) problem in a unit test, handle later.
For now it clears both unit and regression tests. Checkin 3336.

============================================================================
26 Feb 2012
Look at GSSA. Easy enough to extend to many mesh entries, as we did with
Stoich. Hard thing is to sort out diffusion at junctions: perhaps borrow
from tau-leap and/or Avrama's MesoRD. Either way, approximations. Also will
have to do the stochastic calculation of diffusive mol transfer just once, and
then inform both voxels about what happened. Good thing is that each mol is
independent at this stage.

Got GssaStoich.cpp to compile with changes to handle arrays of mols.
Still to come:
- set up a data handler to let it spoof being in an array.
- Deal with diff.
- Threaded mtrand?

============================================================================
27 Feb 2012
Added a bit of documentation about OpFuncs, EpFuncs, and UpFuncs. Some more
yet to come. Checkin 3340.

Added a data handler that will get all threads to converge on to a single
object, in this case the GssaStoich. Checkin 3342.
============================================================================
29 Feb 2012
Some fixes to class definition Cinfo for RC and PIDController classes, to
fix up memory leaks. Still stuff remains. Checkin 3350.
Some more fixes to RC and PIDController. now most memory leaks gone except
for one whose origin is hard to find, no source line. Perhaps if I run it
single-thread. For now moving on to the pymoose segv bug.

I think this comes up because the messages get deleted before some of the 
other parts of the simulation. Fixed. Put the Msg deletion code back into
the Msg class as a static func that all the derived functions call, so we
work around the limitation on accessing virtual functions in the base
class destructor.
This now clears the squid demo python test, but not the kkit test. But the
kkit test reports a different kind of error than the earlier segv.
Checkin 3351.

============================================================================
2 March 2012
GSL provides nicely wrapped RNGs suitable for multithread use. Their
default RNG is our good friend mt19937, so this should be OK to use. I'll
have the GssaStoich use the GSL RNG interface.

Working again on GssaStoich. Dealt with issue of multithreaded random numbers:
used the GSL RNG, which is the Mersenne Twister by default. But I need to 
have a good way to seed all the rngs differently. Checkin 3354.
Did seeding at reinit. I check that we're on thread 0, and then I use mtseed
once, and then I use mtrand for each of the gsl seeds.

Diffusion: As I do this periodically, I can't use the GSSA to compute it.
Instead I go through each species and compute the probability of n molecules 
diffusing, over the entire timestep. Similar to my old adaptive stochastic
method. Any changes have to reflect back to the GSSA to clean up dependencies.
Depending on diffusion rates, that could be expensive.

What would be nice would be to provide 'portals' as Steven Andrews and I had
considered for Smoldyn to MOOSE connections. That would let me model a bunch
of spines on a dendrite, using GSSA and RK5 respectively.

For now, getting the GssaStoich object tests going. Reorgainzed the
SimManager model building sequence to work on this. Now it clears the
regression tests. This means it also successfully loads a model into GssaStoich,
but no info yet on whether it computes it correctly. Checkin 3356.

Next trying to run it. Crashes.
Some cleanup later, it generates reasonable looking output. Still need to
validate statistically.

============================================================================
4 March.
Runs are all identical. Need to find why it isn't doing a proper randseed.
Fixed. I was using longs to extract the values from the mtrand. Turns out
that the value is only in the upper 32 bits, and the seeding function
only uses the lower 32 bits. Which were always zero.

Added a volume term to the tests. This seems to work, except at the 
smallest volume where it has rounded up a molecule. Checkin 3359.

============================================================================
7 March
Minor bugfix reported by Subha: the ChemMesh::dimensions field shadows the
existing Neutral::dimensions field. Replaced 
ChemMesh::dimensions with numDimensions. Checkin 3366.

Minor cleanups of compile warnings in basecode.
There is still a clash of the Neutral::dimensions field. Since were in the
business of cleaning up naming, I will now call it objectDimensions since that
specifies the quantity better. Made change, clears unit and regression tests.
Checkin 3367.

============================================================================
8 March
Tracked down nasty bug with assigning compartments to enz cplxs.
Requires us to set up parent pools and then go back and use the same dest
mesh for the child enz cplxs.
Currently a little stuck in regressionTests/rtReadKkit.cpp:764, revealing that
n1 has a repeated entry.
============================================================================
9 March
Fixed repeated entry in the meshEntry messages, fixed the misallocation of
volumes for enz complex pools. Implemented a regression test for the AabX
reaction, so that I can map to the specifics of the python test as well.
Clears regression tests. Checkin 3377.
============================================================================
13 March
The GssaStoich has stopped working. Seems its 'process' doesn't get called,
due to threading partitioning. Unclear why, and why it worked earlier.

Seems I need to get the ZeroDimParallelHandler to manage the GssaStoich,
rather than the ZeroDimHandler. 

Implemented a setup chain for this. The Cinfo for GssaStoich has to set up
a flag, the useInternalThreading flag. When the Element constructor runs across
this flag it uses the ZeroDimParallelHandler.
============================================================================
14 March
Getting segvs and assertion errors at random places when I run it. 
Tried valgrind, nothing bad till the assertion/segv.
Tried running with 1 thread: runs fine.
Seems like the thread separation within the GssaStoich is bad.

Fixed the crashes, but now it doesn't generate any responses.
============================================================================
15 March 2012
Finally got regression tests for multiple threads apparently working with the
GssaStoich. Still need to do
- multicompartment calculations on multiple threads
- stats of output.

============================================================================
23 March 2012.
Tracked down an obscure bug in the optimized compile that caused segvs at
initialization. The Cinfo initializer for setting up classes was dying when
loading in the 'docs' array of strings. Fix is equally obscure, I just set
up temporary const strings rather than refer directly to the array.
Checkin 3386.

This seems to fix the segv on start in pymoose as well. Progress!
Now the segv on quit. There seems to be a problem in 
Qinfo::swapQ relating to vector< double >::reserve.
Did some printf debugging. When this happens, the numQinfo seems too big, but
not something to cause a memory allocation failure. Note it is happening
on thread 3. However, on thread 1 we are already on the exit handlers.
Can't figure out where thread2 is other than in the processEventLoop. I think
the issue is in how pymoose is closing threads.

Ran into problems with these changes in regression tests. Had to reinstate
reduceQ. Also put in a fix, yet to test, for SumTotals in GssaStoich.
Checkin 3388.

Now the problem is dependencies on buffered but variable molecules,
like those controlled by a table, and on sumtots. Basically need a dependency
table from pools to reacs. This would also help if we ever change the conc
of a buffered molecule from the script: it is an operation that happens 
in many scripts.

I've implemented something to test handling of sumtotals and tables with 
GSSA, but it doesn't do its stuff in the pymoose runs.
Checking in so I can debug it in the C++ regression tests.

Turns out that the function updateDependentMathExpn never gets called. The
problem is that the atot for the system starts out at zero, and so we simply
skip all the calculations. We need to have it so that whenever there is an
external assignment to a pool conc the system updates right away. Unfortunately
this only applies to GssaStoich, not to the parent Stoich, which is what
the ZombiePool sees. For a patch could put in a helper function that calls a
virtual function...

Did so. After some cleanup, it works. I have a little regression test for it
but I really need to have a way to handle evaluation of regression tests
stochastic runs, other than looking at the output.

============================================================================
24 march 2012.
Turns out that the gsl solver (Stoich.cpp) also doesn't handle changes to
conc correctly. One-line fix, seems to work.

An unexplained crash on loading with stargazin_psd6.g for gssa. Brought the 
model into the regression tests.
Turns out this model also crashes in debug mode, good.
Tracked it down to the changes to conc, which should apply to the y_
matrix only for variable pools, not for FuncPools or BufPools. Now clears
regression tests. Checkin 3394 

============================================================================
25 march 2012.
Puzzle with 2 values being returned for each timestep by ZombiePools and their
ilk, but only for Gssa. Checked, the problem is that the getConc function
is called on both threads. It is true that the GSSA itself is to be called
on all threads, but why the ZombiePools?

Turns out that the ZombieHandler uses the parent DataHandler's execThread call.
This will usually be the right thing to do but not here.

Could refer the getN call to the Stoich, and let it figure out if the thread
needs selection. But the GetOpFunc call itself will be on the ZombiePool on two
threads, and will get two responses even if the Get does not do anything.
Approch: extend the execThreadCall that already goes to the parent so that 
we know which id is being manipulated, and let the parent DataHandler handle it.
Actually even this won't do. The ZeroDimParallelHandler explicitly does NOT
know how to handle the threading and data decomposition of its objects.
Perhaps the 'hasInternalThreading' argument to Cinfo should instead be a 
static function passed in by the class, to help work out threading.
return ( thread == threadNum_[ dataId ] ) // do stuff.

============================================================================
26 March 2012
Put in the framework for handling this, put in a hack for the immediate issue of
keeping the execution to a single thread. Seems to work.
============================================================================
02 April 2012

Updated ReadKkit to handle graphs from enzcplx, and also something to correct
the init conc of enz complexes. Added some of this to regression tests, clears.
Checkin 3404.

This version still does not seem to handle a production run correctly. I'll do
a comparison now with the GENESIS output of the run.
============================================================================
03 April 2012
Implemented another regression test to confirm that the enzcplx calculations
and graphing match. Clears this. But this leaves unresolved the issue of 
mismatch in a run where the intermediate values had been saved.
Checkin 3405.

Trying to tackle this in a new regression test. Haven't yet checked in.
============================================================================
04 April 2012
Added the new regression test for enzcplx calculations if they have a non-zero
initial value.

This still did not fix the problem case. Found that the kkit.g script held
an invalid volume term for occasional enzyme complexes. This didn't 
affect kkit, but does affect reading into MOOSE. So I use the parent pool 
volume  which is what I should have done all along.
============================================================================
06 April 2012
Tracking down segv when reiniting HHChannels in pymoose. Put in some 
assertions, will need to recompile in debug mode.
============================================================================
07 April 2012
Fixed silly bug in ReadCell: Cm value was being put into Rm. Now output looks
reasonable. Next to put in the extra messages for some of the more complex
channels.

Set up framework for specifying additional messages from channels defined for
ReadCell. This involved updates to ReadCell, and the construction of an
Mstring class.
Checkin 3413.
============================================================================
08 April 2012
Testing ReadCell framework, still in progress. Seems to be making some of the
channels properly but doesn't yet match with genesis.
Checkin 3415.

There was a major issue with reinits. Specifically, the reinits were not
going through the same careful separation into distinct ticks for each
cycle, as the process calls were. Redid the reinit system, this has broken
some early unit tests. Fixed those up to test the new version.

Now the runtime channel initialization also looks fixed. There are 
still discrepancies in the traces, but not much at the start.
Checkin 3420.

============================================================================
09 April 2012
Several fixes to do with SymCompartment reading by ReadCell, and then in the
SymCompartment code itself. Unfortunately still nto working right.

============================================================================
11 April 2012.
Added TableBase::loadXplotRange to pick up a specified subset of a specified
plot. Checkin 3438.

Noticed that StimulusTable was not compiling. Tracked down to a Makefile
error.

============================================================================
12 April 2012.
Added a little regression test in rtTable.cpp to confirm that loadXplotRange
is working. Checkin 3442.

Did a little work on StimulusTable to let it deliver repetitive stimuli.
Checkin 3445.

============================================================================
13 April.
Minor fix to Stimulus table for spurious error message.  Checkin 3450.
Fix to scheduling so that changing dts can proceed without zeroing out internal
times. Clears regression test, still to test with sim. Checkin 3451.
============================================================================
22 May.
Minor fix to regression test.
Checkin 3488
Looking at loading multiscale models.

Note that SED-ML allows insertion of things into an existing model it loads.
	Also allows loading of multiple models, though it runs one at a time
Distinction from SED-ML:
- End result is a declaratively defined model.
- Composes a model using the constituents. May replicate them.
- Interactions between composed instances of models are defined.
	- Note that NineML may define interactions between cell models: synapses
	etc.
	- These interactions are typically simple linear mappings.


Base model setup
- Specify source models in NeuroML, SBML etc.
- Specify replications of source models
	- Parameterize replication: scaling, distributions, transforms...
- Specify regions where the models interact
	e.g., subset of dendites and spines
- Specify how structure of one scale of model defines structure of other scale
	e.g., neuronal model defines space in SBML model.
- Specify entirely new computations that are not part of any of the constituents
	e.g., NO diffusion, extracellular fields
	
	For many of these, need wildcards and operator-based matching.

Specify mappings at interfaces between different kinds of entities
	- Setting a relationship between molecular conc and channel conductance
		should be independent of the level of detail of each.
		- Conc could be single compartment or spatial
		- Channel conductance could be at different compartmental 
		resolution
	- Current to concentrations
	- Synaptic activity to concentration
	- Compartmental Vm to fields
	- rate models to and from spiking

Specify mappings at interfaces between similar entities, different numerics(?)
	- # concs to regular concs
	- spatial distribs to bulk concs
	- Stoch to deterministic

Numerics.
	- Specify compartmentalization? That is part of base model setup too.
	- Specify numerical methods and decomposition for all parts.
	- Specify sync time between different methods
	- Specify parallelization options.


Points of contact with other standards
	NeuroML, SBML: Base models
	SED-ML: Output mapping. Runtime control and parameterization.
	NineML: Several aspects of model composition. 
		Interface entities (synapses)
		Wildcarding options

Some questions:
	- Extensions to an existing ML, or a new one?



============================================================================
11 June 2012
Things to do

Dt calculations. Currently working on ksolve/EstimateDt.
	Done it, compiled, but it needs base class for pool, reac and enz.
+Base class for zombies: for wildcards and any tree traversal function.
	Should use virtual funcs so as to reuse most of the Cinfo functions.
		Problem is that then I need a wrapper func for the Cinfo to
		call, which internally calls the virtual func.
		Worth it though.
	Should efficently eliminate zombie process calls.
	Zombie will have to handle a ptr, not just be subclassed from solver.

Enz should be base class for MMEnz, and should be able to swap between.
kkit reading option : for compt models
segvs: To get it to work. Harsha has provided list:
slow msgs: 
	- In addToQ, give SrcFinfo:send the ptr to Q rather than copying data.
	- in swapQ, don't do memcpys. Find a way to pass ptrs to MPI to stitch.
	- Redo Conv so it uses the original ptr, doesn't copy it.
	- Figure out how to eliminate lockQmutex in addToQ.
	- Msg::exec decision on direction, src, thread.
	- SparseMsg::exec precompute thread and node portions.
Autoscheduling: Just give a clock # to each thing. System is smart enough
	to ignore unused clocks with lower #s.

Global objects: for efficient channels
	- Define a DataHandler in which the copy operation just does a pointer,
		and which does reference counting.
Array and non-array fields
Fix leak found by valgrind. I think Zombies are not having their
	data cleared out properly.


============================================================================
15 June 2012.
Checkin 3553 with the EstimateDt code.

Implemented a skeleton baseclass for Pool. Will need to redo the
existing classes to use it. Handles almost all the interface fields and msgs.

What to call new pool base class?
PoolBase:
	+ Leave 'Pool' naming as is. Lots of tests and examples.
	- Redo any path traversals that used to use 'Pool'
	. Gives default status to EE pool (but it already has this status).
	. Have to remember to use PoolBase for generic traversals.
	. Unambiguous about what is the base. 
Seems that PoolBase it is.
First pass compiled. Checking 3554.
Now compiles completely, lots of files fixed. Segv towards end of unit tests.

============================================================================
16 June 2012.
Got the updated PoolBase system to work, clears unit and regression tests.
Still isn't used in ZombiePool. Checkin 3555.

Got the whole thing to compile. Clears most unit tests but stuck on
testSimManager::testRemeshing.

Really need to redo the meshing.
	- Have the GSL and Stoich deal with many mesh entries, do not change
		the zombies. Their data handlers may need updating, but the
		objects don't.
	- Figure out who triggers the mesh resizing. When this happens, it
		should affect everything in the compartment. If zombified, it
		should happen by sending suitable messages to the GSLIntegrator
		and to the Stoich. If not, will need to send messages to the
		Pools.

============================================================================
20 June 2012.
Need to get the basic stuff going before I mess with remeshing. So for now
comment out the test testSimManager.cpp:testRemeshing. Also commented out 
rtTestChem and rtReacDiff from regressionTest.cpp: there is a recurring
problem with gsl hitting negatives.
Now clears regression and unit tests.
Stage set now for fixing up the remeshing and routing it through the 
solver.
May also need to implement a new DataHandler that doesn't touch the
data, just provides appropriate lookup options to the solver.
Instead of this, added a field isOneZombie to Dinfo. It uses this field to
tell itself to ignore resize calls on the zombie entry, and just do a single
copy.

============================================================================
21 June 2012.
Re-activated the testRemeshing unit test. Worked through the bug, now clears
unit tests. Checkin 3575.

Sorted out meshSplit. Issue: which is authoritative: concInit or Sinit?
Seems at present I must update concInit when zombifying. Done it.
Now clears regression tests. Checkin 3579.

Let's look at the problem with gsl hitting negatives. That was rTestChem 
in the regression tests. Alas, now there is a new one, with ZombieFuncPool.
Probably because I haven't refactored its zombification too.
Fixed that, now crashes somewhat later in rtReadKkit.cpp:705.
============================================================================
22 June 2012.
Turned out to be a simple bug: I was comparing Finfos of the same field on
a ZombiePool and a ZombieBufPool. Earlier they were different, now because of
the common base class they are the same. Fixed. Checkin 3581.
Now to tackle Harsha's list of segvs.

* Acc1   ReadKkit::undump: Do not know how to build 'kchan
* Acc2   void ReadKkit::buildSumTotal assertion
* Acc3   Do not know how to build 'stim'
* Acc16  Do not know how to build 'stim' : also kchan.

*Acc30  Id ReadKkit::buildEnz assertion
*Acc33  int ReadKkit::loadTab Assertion

Acc55  ReadKkit::undump: Do not know how to build 'calculator'

* Acc67  Error: ZombieMMenz::zombify: No substrates (I think there was no
substrate in the model, an enzyme connected to product)

* Acc81 to Acc84  python: ReadKkit.cpp:738: Id ReadKkit::buildGroup(const
std::vector<std::basic_string<char> >&): Assertion `pa != Id()' failed.
	Loads now, but unhappy about -ve diffconst. This was a flag for
	telling the object to use a global diff const rather than the value on
	the pool. I'll just zero out negative diff consts.

* Acc85  Error: ZombieMMenz::zombify: No substrates
	This was fixed with the acc67.g bug.

Then put in compartmentalization option for ReadKkit.

Fixed acc2 problem. Checkin 3583.
Fixed acc30 problem. Turned out to be problem with handling indexing. Added
	fallback routine to treat the whole name, complete with index,
	as a string.
Fixed acc33 problem. Silly mistake with handling the -continue flag for kkit
	reads of tables. Checkin 3584.

Fixed acc67 problem: replaced failure assertion with a warning and did some 
	cleanup if the substrate messages fail. Checkin 3586.

Fixed acc81 problem: I think it was probably the indexing issue. However, also
	turned up a problem with -ve DiffConsts. Checkin 3587.

Implented 'stim' class handling (actually a PulseGen) to deal with 
acc3. Now it loads correctly but doesn't run. Checkin 3589.
============================================================================
23 June 2012.
Now to put in a dummy kchan.
Also set up Reacs in a heirarchy for zombies. 
Also Subha may ask me to put in a flag so that the fix for indexing in 
pathnames is optional.

*Proofs for book chapter.
Ashesh
*student examiners
DBT
============================================================================
24 June 2012. 
Put in dummy kchan. Fixes acc16 and acc1. Checkin 3595.
============================================================================
26 June 2012.
Checked issue with accession 85 reported by Harsha.
Turns out that it uses the enzyme as its own substrate.
Related problem in the traffillator models
osc_cspace_ref_model.g,  traff_nn_diff_TRI.g
which are products of their own enzyme activity.

As these are all legitimate models, I need to allow the zombies to handle
such cases.

Accession 68 has square bracket problems. I think I should undo the hack
in the fallback function for indexing. Instead I should convert the square
brackets to underscores in the first pass of ReadKkit.


Working to add a 'version' field in ReadKkit. Done.

Can't figure out where the problem arises that causes failure of acc85
to add a message from enzyme mol to enzyme as substrate.

Made a really simple model, with an enzyme that is its own substrate in
one reac, and its own product in another.

Tracked it down. It is a bug in the kkit model file acc85.g. I use REAC eA B
rather than REAC sA B for an enzyme that is its own substrate. In GENESIS
this is fine because both eA and sA are zero. 
I don't want to do a special case in MOOSE for this one model. Should fix model.

============================================================================
29 June 2012
Some more fixes to the regression tests, because the kkit naming changes had
broken them. Checkin 3607.

Trying to get accession 85 to work. Even with the fix on eA, it fails,
this time because of a negative concentration.

Checked if self substrates and products are a problem. Used model 
self_sub_prd.g
This runs fine and generates correct results matching GENESIS.

Something else is wrong in accession 85.

In the meantime, someone has messed with the file output of TableBase.
Fixed and put in an alternate function 'plainPlot' to make them happy.


Put in a helper vector< Id > idMap_ to track down the affected molecule in
accession 85. It is something called Heaviside_dup and is a SumTotal
coming off one molecule, and is also a substrate of an enzyme.
============================================================================
30 June 2012.
Looked at it again. The dup comes of a molecule that is the product of a 4th
order reaction. Perhaps an error there? Also the Kf is very high.
Tried to manually lower the rates. Still gives a segv.

============================================================================
10 July 2012.
Put in fields for method and runTime in SimManager. At this point reading
these fields makes sense, but it doesn't do the right thing when writing them.
Checkin 3722.

============================================================================
15 July 2012
Update to enz model.

Minor updates to layout of trafficking models in Demos/Genesis_files.
Checkin 3777.

Fixed the dangling clocks in SimManager. I hadn't properly updated all of these
so some of the kinetics/mesh examples were taking a very long time to run, as
they were locked to a 1e-5 timestep. Need to do a 'clear'.
This brings up a few things to fix in scheduling.
- Resched
* Get all clocks and associated dts. Should be easy with a getVec on the
	Ticks, but there should be a nice way to organize it.
	Done, using a ReadOnly field on the Clock.

============================================================================
19 July 2012.
Multiple fixes to unit and regression tests, following from reassigning clocks
to different object classes. 
Checkin 3793.

============================================================================
20 July 2012
To switch num methods: Decide between swapping Stoich and GssaStoich,
with zombifying/unzombifying.

Direct swap:
	Escape reparsing entire reaction tree
	Need to build clean unzombification code.
	Might lose data if we convert between two high-level methods keeping
	track of individual molecules.
Zombify/unzombify:
	Need to do for any of the other kinds of method change.
	Uses existing code for building

The latter is more general, as it goes through the MOOSE reference object
definitions.

I've begun work on a general zombify function that is defined at the PoolBase
level. It generally swaps one pool class with another - no restrictions.
leaves assignment of the Stoich still to the derived zombie class, and 
also rescheduling. Good general way to swap either way.

Begun implementation, so far only for pools. Compiles. Crashes in unit tests.
Now clears unit and regression tests.

Need to put in some auto scheduling.
In testSimManager.cpp:testZombieTurnover().
Four tests there.
// the classes of all objects in the sim: properly converted back and forth.
// Adding new objects and incorporating them with a refreshPath
// Deleting objects and reconciling their absence
// Memory leaks when we round-trip.

Check Id( "/model/kinetics/k/J/J_cplx"

After the obvious bugs are fixed, I have a nasty race condition. This
happens when I try to invoke the SimManager::setMethod, which in turn
calls assorted structural ops.

============================================================================
22 July 2012.
Worked on a new framework for process vs. build call handling.
Key change is that we identify the 20 or so Shell::do<Func> calls, and
the Set/Get calls, as the only things that the Parser talks to.

In order to get moving on this, I've commented out the tests for changing
the integration method for a chem model from testSimManager.cpp:183.
Clears unit and regression tests now.

Set up buildQ branch in order to play with the build call handling.
General approach: 
- Replace build calls with their threadless equivalents one by one.
- Set up MPI calls within the build calls.
Is there a way to test which thread one is on?

Starting with Shell::doCreate. This relatively small change
clears unit and regression tests.

Also did Shell::doDelete. This too clears tests. Checkin 3807.

Need to put in mutexes in these so that the Parser can call
these safely during the Barriers.

============================================================================
23 July
Put in the threading stuff for the new buildQ calls. Moderately nasty.
Trying to compile. need to work out about protectedAddToStructuralQ.

============================================================================
24 July 2012
Compiled. Fails very quickly in unit tests.
Marching through SetGet.h to get it to work again. Currently in Field::get()
around line 320. But compare with fastGet a few lines down.
Done, now on to LookupField.
Done, now need to set up a sensible Qinfo that will tell targets that the
call is direct.

Done, compiled, with copious warnings. Clears first few unit tests but 
doesn't do the 'get' properly. Anyway, let's check this in. Checkin 3808.

Now proceeds past the 'get', fails in testSetGetVec. Checkin 3809.

Got first level of getVec to work, now fails with ragged array of
synapse delays.

I thought I fixed it with a much more complete getVec to handle field
arrays, but still isn't working.

Fixed it now. Needed careful attention to how DataHandler deals with
field indexing.
Now clears through till the testLookupSetGet. Checkin 3810.

Now clears all of testAsync.cpp. Fails in testMsg. Checkin 3811.

============================================================================
25 July 2012.
Fixed minor issue of initializing the mutexes for Qinfo. Now goes
on till testShell.cpp:1694.
Fixed this: EpFunc.h::GetEpFunc1, made it subclass of 
LookupGetOpFuncBase< L, A >.
Now clears tests all the way to testMesh.cpp:350. Checkin 3812.
The problem here is with the propagation of mesh changes to 
stoich and to the reaction system, including mols and reacs. The propagation
is done through messaging. This violates the expectation that all structural
ops should be done in a single blocking call. Patched it with 
Qinfo::waitProcCycles, but I'm unhappy about this.

Now fails unit tests at testShell.cpp:985, testShellAddMsg. I think I just
need to now make this a direct blocking call.

Working through the Shell functions. doStart is a problem: it waits for an
ack message from the Clock to say sim is done.

Deferred this issue. With the doAddMsg fixed we go on till 
testScheduling.cpp:754.
I think this too is a 'start' issue. Need to address cleanly.

Somewhat stuck with testShell.cpp, doing unpredictable things around line 1012.
Ran valgrind, nothing there.
Seems that this is a threading issue after all. The parser thread is going
through checking on the unit tests, while the process thread has gone
ahead and may issue a 'reinit' call at an unexpected place.
doing different things.

Fixed doCopy. This cleaned up some of the uncertainty.
Now it goes either in doMove (yet to be fixed) or at testShell.cpp:1085.
doMove is tested before, should fix it too. Checkin 3813.

Fixed doMove.  Now reliably fails unit tests at testShell.cpp:1085. 
Checkin 3814.

After a lot of messing around, turns out that the problem was due to
corrupted scheduling. I had used a hack for a shortcut to monitor 
Clock::isRunning and Clock::isDoingReinit. Now clears unit tests
till testScheduling.cpp:983. Checkin 3815.

Fixed Shell::doUseClock.
Now stuck in doSyncDataHandler, but also other places.

Replaced the functionality of doSyncDataHandler with updates within the
SparseMsg itself, much cleaner.

After a bit of messing with testBuiltins.cpp:testStatsReduce(), it now
clears unit tests. Not regression tests. Checkin 3816.

Next: Fix up regression tests
Speed up messaging: Implement benchmarks for this.
	- Large array of single msgs
	- OneToAll
	- AllToAll
	- Sparse
	Clean up the conversions, minimize data copying and conversion.
	Eliminate at least one barrier.
Fix integ method swap.
Tackle MPI
Space

============================================================================
26 July 2012.
Turns out the problem in regression tests is the deferred issue of 
dealing with mesh resizing.
Should I invent a special instant-send option in SrcFinfo?
Did so. Worked, but there are a huge number of cases of addToStructuralQ
that I had to eliminate. I'll have to figure out how to handle those.

Clears unit tests and most of the regression tests. Checkin 3817.
Now clears all regression tests. Pending issue was a funny initialization
of default chem mesh size of 64. Checkin 3818.
With this phase 1 of the conversion is done. However it doesn't work with
python. Minor fix later, it runs but it fails an assertion on exit.
Ran valgrind. 

Benchmarks: Arith array to self. First pass at writing it, not
yet compiled. In msg::testMsg.
============================================================================
27 July 2012
Various bits of setup and cleanup so that the benchmarks can be run.
I've also inserted flags so that even in debug mode, the regression and unit
tests don't run by default. Instead one uses the -r and -u flags on the
command line.
Benchmark syntax for message tests:
-b msg_<msgType>_<size>
where 'msgType' is one of Single, OneToOne, Diagonal, OneToAll and Sparse,
and where 'size' is the # of nodes of the fully connected network. The
	# of travelling msgs is therefore size * size * numSteps. 
NumSteps currently is hard-coded to 100. 
Checkin 3819.

Did some benchmarking, the key findings are
- SingleMsg is about 100 times slower than SparseMsg
- More threads slower. Linearly so for most, sublinear for SparseMsg.
Need to verify the message passing. Done so, for small numbers of cycles.
Turns out that the doubles go happily up to over 10^300, so I can deal with
n = 1000 for 100 steps. Takes 11 seconds with OneToAll, and only 7.5 with 
Sparse.
Compiled with optimization. 
	Sparse Takes 	3.2 seconds on 2 threads, and 4.3 with 1.
	OneToAll takes 	5.7 seconds with 2 threads and 4.7 with 1
	OneToOne takes 	6.6 seconds with 2 threads and 6.6 with 1
	Diagonal takes 	11.3 seconds with 2 threads and 8.9 with 1

For the Singles, doing only n = 100 for 100 steps.
	Single takes	4.4 sec with 2 threads and 4.1 with 1
For the Singles, doing only n = 500 for 100 steps.
	Single takes	sec with 2 threads and 450 with 1
	About 30 seconds was just to destroy these messages, possibly 
	a similar time or longer for setup.

Checkin 3820.


Specific optimizations:
- Avoid copying the separate thread buffers into a big one.
	This will help only if we have a small fraction only going by MPI
	Will need double buffering.
- Eliminate the direct q handling stuff in Element::exec
- Have SrcFinfos directly insert stuff into the data Qs?
- Condense Qinfo
- Have info about targets on the Element, indexed by msgBindIndex.
	Tells us if offnode, and possibly target threads.
- Revamp Conv. 
- Ring buffer synapse queue with fixed intervals.
- Actually by far the biggest message efficiency step will be to have solvers
	that conduct their own ticks and only synchronize with the rest of
	the world every 1 or 2 milliseconds.
	- Solvers will have to take over plots as well, to dump data with a 
	finer timestep than 2 ms.
	- Hsolvers will essentially never emit more than 1 spike each tick,
	so just need to specify the timestamp accurately.


First pass at profiling. This upsets someof the plans above.
SingleMsg benchmark: 
%time            self sec   # calls    Func name
33.05      1.95     1.95 107048778     SingleMsg::exec(...)
16.36      2.92     0.97 107073742     Msg::getMsg(unsigned int)
15.08      3.81     0.89    10793      Element::exec(...)
12.46      4.54     0.74 107981150     Id::operator()() const

Only Element::exec is on my list of plans.

For the Sparse msg, it is even less encouraging: 
13.25      0.56     0.56 100001072     Eref::data() const
12.89      1.10     0.54   100000      SparseMsg::exec(... )
8.71      1.46     0.37 200102640      Element::dataHandler() const
8.71      1.83     0.37 100000466      OpFunc1<Arith, double>::op(...)
8.35      2.18     0.35                BlockHandler::execThread(... )
8.11      2.52     0.34                BlockHandler::data(DataId) const

I might be able to do a little with SparseMsg::exec.

Tried the intFire test, doesn't terminate in 15 min. Eventually done around 20,
but failed to produce the gmon.out file.

============================================================================
2 Aug 2012.
Now to move back to integration method conversion, aka, de-zombification.

Working on setting up a base class ReacBase.

============================================================================
 3 Aug 2012.
Got staff to install Ubuntu 12.04, back to slowly fixing up the
ReacBase and related files.

============================================================================
5 Aug 2012
Baffling earlier notes. I think they were to say that I was unable to get the
Python to link MOOSE, even though the system happily cleared unit tests.
Subha tried some fixes to the Makefile, doesn't help.
Anyway, for now back to the zombify/unzombify work. Checked in something
for the ReacBase, not yet tied with the other Reac classes. Checkin 3824.

Looks like Reacs are now oK. On to enzymes. Checkin 3825.
Checked in skeleton for EnzBase too. Checkin 3826.

Implemented the zombie stuff using base classes for MMenz. Assorted 
odd crashes, suggest ptr problem.

Yet to implement regular enz zombie stuff.

============================================================================
6 Aug 2012.
Fixed up MMenz zombie stuff, tedious set of bugs. Put in a couple of unit
tests. There is a random crash happening earlier too. Still to clear the
zombie conversion of regular enzymes, but this was a necessary first step.
Checkin 3827.

Now clears unit tests but not regression tests. Checkin 3828.

============================================================================
7 Aug 2012
Fixed up a series of rather subtle bugs found in the regression tests. All
had to do with volume conversions during zombie creation. The trick was that 
I was using a temporary object, zombier, as an argument to the assignment
routine in the zombify function. This zombier however did not have the context
of volume info and other messages that the original object did. Solution was
rather simple, replace zombier with the original object oer. Since the 
zombification routine only used the object to look up volume and message info,
it did not change it at all. Now clears all regression tests.

Should put in a couple of additional zombification tests for GSSA, and 
also include some actual calculations in these tests. Otherwise on to next
phase.

============================================================================
12 Aug 2012.
Stuck on a couple of fronts.
- Still cannot run with Python. When I import MOOSE into Python 
	I get an error of the form:
	ImportError: /usr/lib/libgsl.so.0: undefined symbol: cblas_sdsdot
	The odd thing is that MOOSE compiles and runs fine through unit 
	and regression tests on its own: it is only in PyMoose mode that
	these problems arise.
	The forums state that the key thing is to have -lgslcblas in the	
	linking stage, which we do indeed have.
	This is really a problem, as I did a complete OS upgrade just to be
	able to run the python GUI. Now I cannot even run PyMOOSE.
	Things tried:
	- Lots of messing around with Makefiles, including reordering -lgslcblas
	- Updating Python and entire system. Reinstalled python.
	- Local compile and reinstall of GSL

- Even outside of python, crashes in pthreads on pthread_join. This is after
	clearing all unit tests, and as far as I can tell valgrind is happy
	through all the tests as well.
	Things tried:
	- Put in a properly initialized pthread_attr_t in all the 
		pthread_creates, rather than NULL.


Give up on these. Now doing some pending things:
Set up the SimManager::setMethod command to correctly handle converting to
and from the GSSA as well as GSL. Clears unit tests. Checkin 3843.

Next: Get the SigNeuro sims going using a neuronal model for signaling geom.
This is going to be awkward to test till I have the Python going.

Tasks:
	NeuroMesh
	* NeuroMesh skeleton implementation
	- NeuroMesh read structure from loaded neuron model
	* NeuroMesh stencils 
	* NeuroMesh Y junction diffusion
	- NeuroMesh directed diffusion/transport.
	- NeuroMesh talk to a sphereMesh or similar for nucleus or soma.
	- NeuroMesh hemispherical junction diffusion
	- NeuroMesh talk to CubeMesh or similar for NO-type free diffusion
	- NeuroMesh talk to MatrixMesh for Glu-type membrane bound diffusion
	SpineMesh
	- Standalone SpineMesh with 1 to 5 compartments: 
		neck, head bulk, head membrane, PSD, PSD membrane
	- Neck with optional zero volume and surface, only diffusion constrict
	- Communicate with NeuroMesh
	- Set up so SpineMesh can also sit flat on a regular dendrite for
		non-spiny synapses.
	- SpineMesh structural changes?	
	SimManager
	- Specify chem sim in NeuroMesh geometry
	- Specify SigNeur sim with only Ca effects on kinetics
	- Specify SigNeur sim with partial bits of cells
	- Specify SigNeuro sim with bidirectional chem<-->elec signaling
	MatrixMesh

============================================================================
14 Aug
Implementing CylBase to set up NeuroMesh. Will also retrofit into CylMesh.
Fixes some errors in volume estimation.
Also implemented NeuroNode as a standalone file. Compiles with
CylBase and NeuroNode, clears unit tests.
Now about line 320 in NeuroMesh.cpp.

============================================================================
15 Aug.
Compiled NeuroMesh skeleton  code. Doesn't yet do the key operation of scanning
a cell model, nor to generate the stencil. Most of the rest done.
Checkin 3851.

I've run into rather nasty stuff in the NeuroStencil, dealing with the 
diffusion scaling by area and volume, since the voxels are not uniform as
they were with the CylMesh. Actually if I take the different diameters at
either end of the CylMesh seriously it too should not have uniform volumes.
I thought about dropping the careful conical section stuff. Turns out that
even if I drop it for the intermediate section, I still have to deal with
it at the ends. No change in complexity, perhaps a bit more storage. So I
may as well deal with the full case of conical sections.

============================================================================
16 Aug 2012
Some different ways to get the list of child nodes:
- vector of child nodes on each NeuroNode.
	Simplest, slowest, most memory
- Number and index into array of child nodes
	Most complex, medium speed and memory
- Assume 2 child nodes, store them locally.
	Simplest, fastest, least memory, what to do if more than 2? 
		Fall back on option 2.
		Which turns this into something really complex.

Go for the simplest.

Subha gave his compilation configuration. Turns out it differed from mine in
that my system used GNU 'gold' as the linker, instead of the traditional 'ld'.
When I removed the 'gold' package both the bugs from 12 Aug go away: it 
is loaded properly by Python, and it does not segv on exit.

Implemented and compiled the NeuroStencil. Quite messy. Checkin 3853.

Need to build a unit test to be sure stencil is doing the right thing
 in the wide range of possible branch/end/middle diffusion. 
There are already likely issues to do with dummy nodes.

Now to set up model loading.

============================================================================
19 Aug
Libraries needed:
gsl
numpy
matplotlib
pygraphviz
QtOpenGL stuff: python-pyside-qtcore ,qtgui
python-qt4-gl


Now it loads MooseGui! But fails with kinetics models, suspect ongoing work
by Harsha.
Some minor checks for zero elements (typically after deleting objects) in
SetGet.h. Checkin 3859.

Added unit tests for CylBase. Fixed lots of bugs in implementation. Works.
Checkin 3861.

Added unit tests for NeuroNode. Straightforward. Checkin 3862.

============================================================================
24 Aug.
Should take over length assignment of NeuroNode since this has to match the
coordinates. Done. Checkin 3893.

Finally exercised the NeuroStencil::addFlux function. Runs without crashing,
but output is not right. most values are zero, some are uninitialized.
Checkin 3895.

Did some more tests on flux handling. Now I simply print out flux values.
They are still wrong, despite some fixes.

============================================================================
26 Aug.
Tracked down fundamental issue with flux calculations: how to handle 
cross-section area.

============================================================================
29 Aug.
See:
Mass and Heat Transfer: Analysis of Mass Contactors and Heat ..., Volume 10
 By T. W. Fraser Russell, Anne S. Robinson, Norman J. Wagner. CUP.
Page 211 ish.
============================================================================
4 Sep 2012.
Conical diffusion still pending.
Added ChemMesh::setEntireSize so that the field is now read-write. It is a bit
ugly, uses buildDefaultMesh internally.  Checkin 3921.


Need to fix issue with null FieldElements: Accessign them should not
cause segv. For example:

Ca = HHChannel( 'Ca' )
>>> le('Ca')
Elements under /Ca
/Ca/gateX
/Ca/gateY
/Ca/gateZ
>>> foo = element( 'Ca/gateX' )
>>> foo
<moose.HHGate: id=712, dataId=0, path=/Ca/gateX>
>>> foo.min
Segmentation fault (core dumped)

============================================================================

10 Sep 2012

Notes from INCF meeting demos.
- The graphics are excruciatingly slow. Something is clearly very wrong. Even
old GENESIS graphics are much faster. 
- GL click to select takes forever, is inaccurate, gets item behind rather than
	in front, many other issues.
- GL should really run in another thread so it doesn't bring everything in the
	interface to a grinding halt.
- Need to update without stopping.
- Subha pointed out issue of Ids vs elements. Will this speed up handilg
- Kinetics models: The reactions cannot be clicked to view, except in one
of the compartments. 
- Loading models also takes a very long time, even fails for some big models
	with around 500 molecules.
	The same models load within a second from the command line.

============================================================================

12 Sep 2012
To do for Release 1.0.0:

Documentation: Tutorial and GUI Help systems.
SBML read and write
HSolve tests
NeuroML and .p files load into Hsolve by default
Graphics cleanup.
Kinetics model editing and building
Toggle Gillespie/GSL

Release 1.01:
Documentation: Main commands and objects. Help system.
Hsolve create/delete cycling
NeuroML write
More graphics cleanup
SigNeur operations
Threading cleanup.
GUI: Chem compt view

Release 1.02:
Documentation: Scheduling.
GUI: Channel view

============================================================================
13-14 Sep:
Seems like I should have a motor function in the in Stencil class.
Needs similar info to diffusion but calculations not identical. 
See ~/homework/PRAGATI/NOTES around 13 Sep.
Typically motor will be in addition to diffusion.

Back to testing the NeuroStencil. It is still doing funny things, not zeros
in the test for uniform conc.
Tracked it down. Turns out it was close enough to zero, the error was just
rounding error. Put it into unit test rather than printf debug.

I'm trying to figure out another test for the NeuroStencil here. Possibly
can do something analytical for a cone using our approximation.
- Should be able to do branching of cylindrical segments obeying Rall's Law to
	 achieve equivalence of dendrites, so that the decay outward is 
	equivalent to a single long compartment, which we can estimate.

Next, need to put in infrastructure for motors in the stencils.
	Test cases: 
		Uniform dia cylinder, different compt lengths.
		Branching cylinder. At junction each branch gets # proportional
			to area.

============================================================================
15 Sep 2012.
A more structured approach to releases.

Each release will have six major components, and we'll subdivide development
efforts among these six for clarity.
To do for Release 1.0.0:
Component	Targets
Documentation	Tutorial and GUI help (For neuron and chemical modeling GUIs)
I/O		SBML and Kkit read, NeuroML and .p read
Numerics	Hsolve functioning, Gillespie/GSL switch.
GUI		Cleanup. Faster load and click to select.
Basecode	Scheduling clean up
Project		Release build

Release 1.01:
Component	Targets
Documentation	Main commands and objects. Help system.
I/O		SBML write
Numerics	solve create/delete cycling
GUI		Move to top bar panel for run and plot control. Side panel
		only for field info.
Basecode	Threading cleaning
Project		Deb dependencies for user compilation and build


Release 1.02:
Component	Targets
Documentation	Example files for how to load and run different kinds of models
I/O		NeuroML write
Numerics	SigNeur: spatial diffusion
GUI		Chem compartment view (new panel showing geoemtry of chem compt
Basecode	Cleaner access to msgs and clock ticks.
Project		Whatever deps for Mac compilation and build

etc.
============================================================================
15-16 Sep 2012
Continuing work on NeuroMesh. Set up a unit test, beginning the implementation.

Clears first set of unit tests, which check for the qualitative aspects
of setting up the mesh. Lots still to do by way of checking quantitative stuff.
Checkin 3949.

Put in the test for NeuroMesh diffusion calculations. Runs but output
is wrong.
============================================================================
17 Sep 2012.
Sorted out major issue with NeuroMesh calculations. Output now is better,
but two issues:

1. The junction of the compartments to the soma adopt the soma diameter as
their proximal diameter. Not right. Should be their own diameter.
2. The decay of conc along dendrite is OK for first half, then the
remaining dendrite segments seem to be connected up wrongly.

Checkin 3950 anyway.

============================================================================
18 Sep 2012

Need to have the NeuroMesh handle a few specific, explicit policies for how
to deal with junctions and branching.
- Fallback mode: 
	- Use cylinders. Diameter is just compartment dia.
	- Place somatic dendrites on surface of spherical soma, or at ends
		of cylindrical soma
	- Place dendritic spines on surface of cylindrical dendrites, not
		emerging from their middle.
	- Ignore spatial overlap.
- Default mode:
	- Use frustrums of cones. Distal diameter is always from compt dia.
	- For linear dendrites (no branching), proximal diameter is 
		diameter of the parent compartment
	- For branching dendrites and dendrites emerging from soma,
		proximal diameter is from compt dia. Don't worry about overlap.
	- Place somatic dendrites on surface of spherical soma, or at ends
		of cylindrical soma
	- Place dendritic spines on surface of cylindrical dendrites, not
		emerging from their middle.
		
- Trousers mode:
	- Use frustrums of cones. Distal diameter is always from compt dia.
	- For linear dendrites (no branching), proximal diameter is 
		diameter of the parent compartment
	- For branching dendrites, use a trouser function. Avoid overlap.
	- For soma, use some variant of trousers. Here we must avoid overlap
	- For spines, use a way to smoothly merge into parent dend. Radius of
		curvature should be similar to that of the spine neck.
	- Place somatic dendrites on surface of spherical soma, or at ends
		of cylindrical soma
	- Place dendritic spines on surface of cylindrical dendrites, not
		emerging from their middle.
		

Put skeleton code for NeuroMesh::geometryPolicy in.

Put in some more elaborate printf debugging to check the tree structure of
the test model in testNeuroMesh. Turns out to be OK. Addresses point 2 from
yesterday.
Next: put in dummy nodes. Checkin 3953, 3954.
Now to put in the geometry policy implementation.
Done that.
Now testing against analytical solution. Wrong.
Also mass conservation fails, though this isn't evident on single-stepping.
Should try simple cylinder calculation, but at this early stage the
model here is like a simple cylinder.

============================================================================
19 Sep 2012
Moose project policy overview:

Routine schedule
1. Before each checkin, every contributor: run unit tests: Don't break the build
2. Automated, every night: 
	1. Fire up virtual machine for Debian. Download code from SourceForge
		and build.
	2. Run unit and regression tests.
	3. Run Valgrind on unit tests
	4. Automated updates of checkins onto web page: can it put weekly stats?
3. Every week:
	1. Full install of Moose and latest dependencies into naive Ubuntu
		virtual machine.
	2. Run unit and regression tests.
	3. Benchmark tests
	4. Valgrind on regression tests.
	5. Update web page.
	6. Bug tracker/issue discussion
	6. Later: Steps 1-2 on Mac and Windows virtual machines too.
4. Every month:
	1. Release goals assessment
	2. Mac rebuild
	3. Windows rebuild on Virtual machine
	4. Fedora rebuild using tarball
	5. If ready, release! 
	6. Update web page.

Code policies in brief:
	- CamelCase convention
	- Private variables have have an underscore after them: int foo_;
	- Use Doxygen for in-code documentation. Aim to describe all functions
		and all key variables, as well as giving a brief class spec.
	- Put in lots of assertions
	- Make unit tests for all key operations, aim for test-driven design.
		The MOOSE unit test framework is very simple: in every directory
		there is a test_stuff.cpp file which coordinates tests for that
		directory. The test_stuff() function is called from main.cpp.
	- Make an example script for every new class and key feature.
	- Subversion hosted on SourceForge for code management. 
	- Check in stuff frequently. Do not break the build.
	
............................................................................
MooseGui Help sections
Loading models
	(What to click)
Example models
	(List and outline)
Running models
	Run
	Reset
	Stop
	How to plot variables
	The model tab and the Plot tab
	Simultaneous viewing of plots and model
Viewing and Editing models
	Child and Parent objects, object hierarchy
	How to view and edit variables and parameters
		Editable fields
Chemical Kinetic models
	- Overview
	- Model layout and icons
	- Compartments
	- Pools
	- Buffered pools
	- Sum total and other mathematical operations
	- Reactions
	- Michaelis-Menten Enzymes
	- Mass-action enzymes
	- Compartments
	- Merging models
	- Changing numerical methods
	- Changing volumes

Neuronal models

About MOOSE
Credits
............................................................................

Commented out the failing unit tests so that I can go on with other fixes
for the release. Checkin 3957.
Ran valgrind --leak-check=full ./moose -u
Most gratifying:
==2931== All heap blocks were freed -- no leaks are possible
............................................................................
Working on examples for model loading and for solver swaps.
This uncovered a crash-on-exit bug. Fixed.

Tracked problem with swapping between GSL and GSSA. Nasty dependence on
message traversal in Pool::zombieSwap because we need to get volumes for
the old Pool::setNinit. Message traversal is a bad idea here because we use
temporary Erefs during the swap.
Will need to change Pool to use nInit internally rather than concInit. 
Still issue of funny concInit table in solver.

The change to use nInit internally has cascaded horribly, though it is probably
the correct thing to do. It involves having the mesh inform the changed
pool what its old volume was. Changes to mesh, to pools, and even to SrcFinfo
to add a missing fastSend.

This clears unit tests but now GSSA calculations fail utterly. Turns out the
GSL was also too big by about 18 orders of magnitude, just that this doesn't
faze the GSL integrator like it does GSSA. So I need to get rid of the 
reliance on the concInit vector in Stoich.

Did that, but it wasn't the problem. Problem was that concInit was set in the
pools during object creation through the kkit reader. Changed that to 
nInit and all was well. Including being able to switch from gsl to gssa.

Checkin 3959.

============================================================================
20 Sep 2012
Working on WriteKkit.cpp.
Skeleton in place. Need to replace couts with fouts and to begin to pass in
Ids.
Put in the fouts. No real stuff passed in as yet. 
Checkin 3968.

Now partial implementation in place. Put test in regression tests.
Unfortunately some other changes have caused errors, in rtTestChem (line 889).

============================================================================
21 Sep 2012
Fixed errors in regression tests resulting from changes to volume handling
in Pool. Checkin 3972.
Almost set up reac and enz messaging.
============================================================================
22 Sep 2012
Now it generates a runnable version of the Kholodenko model, but graphs don't
work yet. Checkin 3980.
Graphs work but their colour isn't yet converted.
Now I have graph colour also converted. Checkin 4001
Now to put it all into SimManager and extract runtime parameters.
A little remaining stuff to get StimTables in.

============================================================================
23 Sep 2012.
Extracting runtime information is done, but there is a fair amount of 
reworking of SimManager to do. Needed to bring hsolve and cell models into
its ambit. Checkin 4005.
Before I sign off on writeKkit, 
- Check on a big model with regular enz.
- I should set it up to handle Stimulus Tables.  Need test model
	Set up stimulus table test using tabsumtot.g, already in regression
	tests. Looks OK, but now I also have to get the sumtot to work properly.
StimulusTables now work. Sumtot not yet.

Sumtot assertion on messaging revealed a benign bug in the ReadKkit.
Now the regression test on writing tabsumtot.g clears too. Checkin 4013.
Still need a way to automatically check the validity of these conversions.

Also did acc8.g: Here ReadKkit stumbles: the version of the model
is old and ReadKkit cannot handle some of the comments. WriteKkit
doesn't complain but the output file is messed up. When I replace
acc8.g with a newly saved version the genesis output is OK.
Instead of acc8.g I use Osc_cspace_ref_model.g. This
goes through the read-write cycle fine, works. Checkin 4014.

============================================================================
25 Sep 2015

Subhasis provided instructions on converting documents using eeeeeeeemax:
C-c C-e b (Control+c followed by Control+e followed by b). 
Or you can press Alt+x and enter: org-export-as-html in the minibuffer.

I've used it to fine-tune the kkit12 documentation. Checkin 4022.
============================================================================
26 Sep 2012
Harsha reported a bug in WriteKkit. Fixed. But there is now an issue with
the gui, that stores only 100 seconds for runtime. Checkin 4034.

Now I have a bug in Downloads/sumtotal.g. Harsha has reported correctly that
the system does not set up the sumtotal messages here.

============================================================================
27 Sep 2012.
Fixed sumtotal bug reported by Harsha. It was because the linear class 
hierarchy in MOOSE does not let me recognize ZombieSumFuncs as related to
regular SumFuncs. Checkin 4044.

Bug fixes:
the lookupFinfo for 'neighbours' lookup should not do an assertion around
Element.cpp:559. Actually this is OK, the calling function should test for
the finfo before calling the Element::getNeighbours.

Cinfo should build in the ReadOnlyLookupElementValueFinfos. Trivial, all I
had to do was to derive them from LookupValuFinfoBase rather than Finfo.

Fixed other bug with path of ticks. Still is a bug there, in that
the number in the ematrix is 16, not 10. Will discuss with Subha.

Working on cleaning up scheduling. Main aim is to ensure we don't do any
ticks if there are no objects on them. Huge mess, but I've come a good way
along. 
Now it seems OK. checkin 4060.
============================================================================
28 Sep 2012.
Some more scheduling cleaning. There is a Heisenbug in the scheduling tests,
in that it sometimes misses out a timestep. Haven't tracked yet. It has made
it hard to run valgrind on regression tests as it tends to catch it.

Fixed a bug with GssaStoich.cpp where it would get into an infinite loop
if there were no molecules present.
============================================================================
29 Sep 2012.
More documentation stuff.
============================================================================
30 Sep 2012.
MooseGui documentation now more or less OK.
For the kinetics documentation I should really make a little model with
all the possible icons. Done. Called it kkit_objects_example.g.

Used it to make up the icons. Updated the Kkit documentation. I think that is
it for documentation for this round.

============================================================================
01 Oct 2012.
Harsha had pointed out a problem with going back and forth between GSL and
GSSA. Turned out to be a problem with unzombification of SumTotals. 
Fixed that. Unfortunately there is still a problem when actually running the
model.
============================================================================
02 Oct 2012
Some fixes to the HDF5 code.
Revisiting the (small) memory leak in the unit test setupTicks that valgrind
detected. Still unable to clear it. Continuing issues also with the 
runtime: the call for data at reinit is not adding an entry to the table.


Now back to the neuronal diffusion code. Plan to redo as sparse matrix.

i : index for left voxel
j : index for right voxel
k : mol index
Dk : Diffusion const for molecule k.
Fijk : Flux from i to j for molecule k
Sik : Molecule counts
VSi : volscale
Ai : Cross-section Area of voxel (sub-cylinder) i.
Aij : Area at interface between i and j. This is symmetric and is the
	smaller of Ai and Aj. So one could do with a vector of min area of 
	junction with parent.
Li : Length of sub-cylinder i.

Lij : Length from cylinder i to j. Obtained in a heuristic as:
	Lij = 0.5 * ( Lmax * Amin / Amax + Lmin )
	Reasoning is that if we have cylinders of the same area, then the
	concentration gradient is linear. But if we have very different area
	cylinders (e.g., small dendrite on soma), then most of the conc
	gradient is on the smaller cylinder and the gradient on the large
	voxel will be short and steep.

With all this, the # flux equation for the diffusion going from i to j is:

Fijk = -Fjik = Dk * NA * Aij * ( Sik / VSi - Sjk / VSj ) / Lij

We could extend this further if we have motors. The flux equation for a 
motor is

Fijk = -Fjik = Vk * Sik / ((Li + Lj)/2)

Where Vk is the transport speed of molecule k
and we use geometrical cylinder lengths because we assume the motor just
chugs along at a fixed speed regardless of gradient.
This will typically be used for a very small number of cargo complexes, and
I'll want to expose the cargo to some off reaction processes that can then
separate out the components.

To compute this, I'll put Lij on the sparse matrix. I'll need to precompute
Aij and VSi. Given the odd access sequence of the Stoich (with various 
threads) it is a little awkward to march through the sparse matrix, but
anyway much better than the previous incarnation of NeuroStencil.

Should look into building thread/range specific subdivisions of SparseMatrix
for faster access and thread safety.


============================================================================
03 Oct 2012
Made the SparseMatrix version. Far cleaner and easier to understand, probably
faster too.
Now handles the linear diffusion calculations.
The assignment of the sparse matrix for branching seems wrong.
Fixed it. Now the test seems to be getting close, down to the numerical
issues of spatial discretization.

============================================================================
04 Oct 2012.
NeuroMesh branching cell unit test built. There are assorted numerical issues
 with getting the test grid size right, but finally it seems OK. Checkin 4109.

============================================================================
06 Oct 2012.
Designing for neuronal traffic. This is to be a function implemented on top
of the NeuroMesh.
Based on the discussion with John Mercer, I need to do the following:
- Have stochastic movement of molecular 'blobs' back, forth, on, off, etc.
	Do this by making a standard set of reactions between compartments:
	forward motors, backward, on and off.
- Assemble and disassemble the molecular 'blobs' with varying composition. 
	Assume the blob remains intact during traffic.
	Note that this creates a new kind of simulation entity, where 
		each entity of a given kind may have a different composition
		of molecules. PoolBlob.
		This is similar to rule-based modeling, see Hlavacek et al.
		We're not going the full way with the rule-based models at
		this point, just making, moving and decomposing the entities.
		Specifically, no reactions on the composite entities. Yet.

There are two ways to go about this.
- Multiple instances of single particle Gillespie, one for each 'blob'.
	Here we just let the blob go about its operations, and we do an 
	independent set of calculations for each blob. This will need lots of
	Gillespie solvers, as many as molecular blobs. Which is itself a 
	variable.
- Multi particle Gillespie, in which we do the regular Gillespie calculations
	but keep track of specific molecular 'blobs' that are part of the
	stochastically reacting pools.
	So, on each firing involving a blob we pick one of the blobs to
	do the transition. 
	This requires a new subclass of Pool, which maintains a list of 
	individual entities. RulePool.
	This requires additional features in the Gillespie solver, to pick
	one of the entities on the RulePool whenever there is a a transition.

The Multi particle Gillespie approach seems best for now. It lets me set up
arbitrary reaction schemes for each compartment in the mesh, using existing
infrastructure.

Design of Blobs.
	- They are single-particle, hence stochastic.
	- Note that the PSD might also be implemented in this manner, but it
		would have to allow internal reactions as well.
	- The simple way to do this is to have them like compartments, with
		a bunch of transport reactions that bring each molecule in.
		When the blobbing reaction fires, the (integral) molecules in
		the compartment are assembled into the blob, and the remainder
		stay put till there are enough for another blob.
	- Release of blob is a reversal of this process. Also easy.
	- Mostly based on existing formalisms. The only new thing the
		blob does is to vacuum up the integral molecules.
	- For the case where we have specific binding 
		sites, with limiting stoichiometry of on-reactions:
			- We could maintain a compartment-specific set of
			target 'sites' (just pools) with a suitable conc
			to ensure stoichiometry.
			- Sites with arbitrary capacity are just buffered.
	- Need a trigger to convert compartment contents into the blob.
		This would probably
		need to be a rule that requires a certain composition
		of filled sites in the blob, and then applies a certain
		transition probability.
			- Rule also has to decide if this blob-grouping is
			allowed to keep generating blobs, or only one.
			- The only-one case would be easy if the sites are
			zeroed out as soon as the blob is formed. 
	- We would like a way to have new scaffold molecules come in,
		and when they do, a stoichiometric set of new sites appears.
			- Scaffold molecules can also be regular pools that
			are transported in.
			- Problem with the scaffold going out again. How to
			deal with things that may have bound to its sites?
				- Can get this to work if the scaffold _is_ the 
				compartment. Then all the individual site
				reactions play well with the rest of the 
				chemical system.
				- Do not allow exit of scaffolds. Entry would
				correspond either to synthesis, or transport in.
			- Problem with multiple scaffold molecules. Which one
				 owns which sites?
				Option 1: Make separate scaffold/compts for
				each molecule.
					- Need an array of them.
					- May need to expand/contract.
					- Needs additional sim infrastructure.
					- Linear increase in sim cost with more
						scaffold molecules.
				Option 2: Put multiple scaffolds in the
					compartment.
					- Don't care which owns which sites.
					- At 'firing' time, just randomly sort
						out a set to take.
					- No particular extra sim cost.


To do:
	Implement RulePool that manages an array of specific entities: MolBlobs.
	Implement MolBlobs that keep an array of pool Ids and their 
		stoichiometry.
	Implement MolBlobTriggers that manage a list of pool Ids, and spawn/
		kill MolBlobs.
	Implement extension to Gillespie to accomplish all this.
	Test case 1: simple scaffold formation/destruction system implemented
		both as regular combinatoric set of pools, as well as with
		MolBlobs.
	Test case 2: simple diffusion implemented with the Multi particle
		Gillespie set up in a cylinder. Compare with analytical.

Note added later:
	I should permit reactions of specific states of scaffolds in and out
	of the system. On the rule-based compartment side, we have detailed
	site occupancy information. On the bulk side we have whatever
	combinatorial subset is modeled, including cases which meld together
	sets of sites. For example, CaMKII phospho states expressed just
	as CaMKII_12.P_y. This does not preclude specific limited interfaces,
	like just the newly synthesized molecule and just the degraded form.
	It allows more flexibility.

============================================================================
07 Oct 2012.
Prioritizing. 
SigNeur: needed for Spatial pattern sims. Specifically the Stargazin model
	and friends.
Traffic: Needed for consolidating Pragati's two models. Also tie to expts on
	motion of molecules.

Stuff to do to get SigNeur code working.
	+Diffusion in subsets of dendritic tree
	SpineMesh implementation
	Coupling of different meshes/compartments: spines and dends
	+Reimplementation of adaptors.
	(?) Specify signaling compts in .p
	Specify entire thing in a NeuroML extension.
	SimManager features for setting all this up.
	Array calculations in Gillespie solver.
	Coupling between stochastic calculations in spines and deterministic
		 in dends.
	Test case 1: Diffusion in and out of spines
	Test case 2: Ca computed simultaneously in elec and chem models.
	Test case 3: Replicate model from Neural Networks paper
	

Tons of stuff to do but I'll go with the SigNeur code. Starting with 
setting up a signeur directory and an Adaptor in it.
Checkin 4119.

============================================================================

09 Oct 2012
Coupling meshes/compartments is the key step.
- How to define it in as SBML/kkit compatible a manner as possible
	- Use a single kkit model that defines all compartments, and uses
		permitted inter-compartment transport reactions. These include:
		- Exchange and binding reactions
		- Enzyme-catalyzed steps
		- Note that diffusion and Motors would have to be excluded
			here. Even with diffusion we do not permit molecular
			transfer without a special binding step.
	- Within-compartment transfer includes:
		- Diffusion. Implicitly defined in kkit from diffusion const.
		- Motors. Cannot currently define properly in kkit.
- How to solve it.
	- Reactions between compts need to know which mesh entries correspond.
		Examples:
		- Spines on a dendrite
		- Dend/spine signaling with extracellular diffusion
		- Compartments on either side of a membrane.
	- If both sides are deterministic, compute the rate of each reaction
		across for a each molecule across the given boundary. 
		This is almost identical to what the reacs
		do anyway. To do this the reacs need mol concs on either side
		of the boundary, at the finer of the two mesh divisions. Assume
		for now integral mesh matching.
		- Provide this rate term to both the solvers, with appropriate
		summing up for the coarser grid.
		- Keep this rate fixed during the entire interval between
		the synchronization between solvers.
	- If one of the solvers is stochastic, and the other is not,
		then the reac should be stochastic. I think it can be within 
		the regular Gillespie form, just taking a non-integral # from
		the deterministic side. The stochastic side obtains mol concs
		from the deterministic side, and does the usual calculations.
		The # actually changed are used to update the deterministic
		side. 
		In rare cases this update will give a negative conc on the
		deterministic side. If so, we roll back the increment by the
		minimum number of molecules, and now have to update both the
		stoch and deterministic sides. The stoch update will cascade
		to multiple affected reactions, as usual.
	- If both solvers are stochastic proceed again as above. Do the
		Gillespie calculation on the smaller voxel side.
	- If one solver is Smoldyn, its entire volume is a single division.
		I think Smoldyn will let us have reactions with an external
		pool. We assign this pool conc each sync step. Use Smoldyn
		to decide how many molecules actually transit each time.
		Still have potential issue of local -ve concs.

- How to set it up.
	Sim Manager will need hint about whether to load in a multicompt
	model into multiple solvers or a single one. Substantially different
	setup in the two cases.
	- Stoichs themselves will need to traverse paths differently, 
		If they are in multiple sover mode, then they have to set
		up lists of cross-compartment reactions and identify
		whether they are on the sender or receiver mesh.

	- Message between Stoichs: 
		Send vector of doubles of pools needed for cross-compt reacs,
			for a given meshIndex. Or all indices?
		Recv vector of doubles of cross-compt V, ie, reac rates.
			In the case of rk5 calculations, V is the rate at the
			start of the time interval, and remains fixed.
			In the case of Gillespie calculations, V is the 
			(integer) # of transitions on each reacn.
		Send Rollback vector of doubles if recved rate V gives a 
			negative anywhere, esp for Gillespie. The Rollback
			vector contains the permitted values of V.

Currently we have an issue as all reacs and enzymes seem to be set into
the kinetics compartment. Error.

Working on this. 

============================================================================
10 Oct 2012.
Got reacs to go into correct compartment.
Got enzs to go into correct compartment.
Both were an issue with readKkit. 

============================================================================
13 Oct.
Using revision 4125, getting close to release candidate.
Ran valgrind -u. Still has the 96 bytes of leak. Acceptable.
Ran valgrind -r. Not so good. Loses 576 bytes, seems to be in the ReadKkit 
	tests. Barely acceptable.


============================================================================
14 Oct.
Steps for stoich communication
* Set up test having A<===>B into different meshes.
* Set up messaging between Stoichs. Test should create message.
- Set up ode calculations. Test should directly call update operations and see.
- Set up traversal with listing of cross-compt reactions and pools
- Set up Manager. 
- Set up Gillespie-ODE calculations. Test should call update ops.
- Set up rollback for Gillespie-ODE calculations. Test rollback ops

============================================================================
15 Oct 
	Set up the test for A<==>B into different meshes.

To set up Stoich paths restricted to one compartment.  Options:
	- Set compartment base as path. 
		- It will digest all contained objects that have not yet 
			been solved. 
		- Every time it runs into a reac with a dangling message, 
			it will traverse the msg to the target pool.
			- Assume reac is assigned to correct compt. This is
			the one which has the finest mesh size.
			- Possible problem of assignment of enzymes. Currently
			put, logically enough, into compt which has the 
			enzyme molecule. By this rule we would possibly have
			different parts of enzyme in different places.
			- Best solution is to allow either case. If we get
			it wrong all that happens is we do fewer calculations
			at a coarser grid size. So it would be nice but
			not critical to optimize a bit by correctly placing 
			the reac during setup time.
		- Assumes common base for all the related compts. Consider
			dendritic spines. They have to be in a single compt,
			indexed by spine # in some way.
	- Special command specifying both full path and compartment base
			Doesn't seem necessary, see above.
		

I've now set up the framework for the messaging between Stoichs. Doesn't
actually do anything, just the placeholders for the handlers and 'send' calls.
Tested that the shared message is created.

============================================================================
16 Oct
Some minor refactoring of Stoich.cpp, to clean up some overly long functions.
Checkin 4137.
============================================================================
21 Oct
Based on the Charm++ rewrite that Pritish is doing, I will need to revisit
how the Stoichs work. Currently they take one meshIndex at a time as an 
argument that the Process method passes in via GSL. The Charm++ framework
instead treats each processor as a completely separate object, ignoring
possible shared memory. If I take this idea to the Stoichs, then each has
a range of meshEntries that it deals with, and handles all the iteration
internally. In this framework the inter-Stoich operations are much the same,
since in either case the equivalent info has to be passed around.
However, the intra-stoich operations can be done much more cleanly and quickly.
Also, the Stencil design for diffusion can be much improved.
Also, the numerical integration of the diffusion within the Stoich can be
done perhaps using the GSL, fixed timestep Runge-Kutta or something.


Reran Unit tests. Still the same 96 bytes from testScheduling
Reran regression tests. Still the same 576 bytes from ReadKkit::buildSumTotal.


Another design option: Merge the diffusion and the compartment exchange
calculations. Motors should also fall in this framework.

Similarities: 
	Rates can be expressed in terms of regular kf and kb, 
	Rates parameters need special calculations to set up at meshing time.
	The rate calculations cross processors
	The rate calculations cross solvers (GSL<-->GSSA<-->Smoldyn)
	The rate calculations cross boundaries between compts
	A subset of molecules are involved
	May need many-to-one mesh exchanges.
	
Differences:
	The setup calculations differ.
	Diffusion and motors need calculations among voxels within a Stoich.
	Diffusion/motors only use A <---> B. Exchange calculations may involve 
		many mols
	Diffusion involves most/all molecules. Compt exchange involves few.
		Motors involve few.
	Diffusion across compts will need special matching.
	Motors will typically handle Blobs/RulePools later.

Implementation:
	Cross solver/processor/boundary calculations in all cases 
		will be shared, and mol-specific.
	Need a special general-mol subsystem for within-processor diffusion
	Need another specific-mol subsystem for within-processor cross-voxel
		movement like motors. Either that or put the motors on another
		solver and use the diffusion framework plus cross-solver
		framework to talk to other molecules. Defer.
	Need the core ODE/GSSA update calculations.
	Split up Stoich into:
		- StoichCore: This has the code for set up reac system,
			zombification and unzombification.
			It sets up the Stoichiometry matrix, the RateTerms,
			and the Sinit_ and S_ arrays for all the molecules.
			MOOSE interface only for path.
			If there is one StoichCore per processor, then 
			one would like to share the Stoich matrix and RateTerms.
		- Diffusion: For diffusion handling within a processor.
			This defines the diffusion, and sets up rate calculation
			framework, but does not compute it.
		- Boundaries: For boundary handling between 
			processors/stoichs/compartments
		- Integrators: GSL, Gillespie etc.
			These independently set up the calculations. A bit
			different from what I do now in the GSL, where I leave
			the calculations to the Stoich. More like the GSSA,
			which is a subclass of Stoich and does its own
			calculations. I think the latter is better.


Implemented StoichCore. Basically chopped out lots of stuff from Stoich.
Working on implementing GslStoich, derived from StoichCore.

============================================================================
22 Oct 2012.
Following discussion with Pritish, some more design decisions about how to
partition things. In Charm++, there are things that are one per node,
others that are one per thread/processor. 
Charm++ tries to avoid having the user deal with explicit multithreading.

Option 1:
Separate the StoichCore further into StoichCore and StoichPools. 
The latter deals with the molecule arrays.
StoichCore is present one per address space/node
StoichPools are present one per thread/processor
	All the StoichPools on a given node share a const StoichCore ptr.
	So StoichPools are NOT derived from StoichCore.
	Each StoichPools handles the storage of a given range of meshIndices.
GslStoich is present one per thread/processor
	Has a pointer to StoichCore and the matching StoichPools

Option 1.5:
GslStoich is derived from StoichPools. So it has the pools but encapsulates
the pool assignment operations.

Option 2:
StoichCore is present one per address-space/node.
	StoichCore manages all the voxels on a node.
GslStoich is present one per thread/processor
	Each GslStoich points to the StoichCore, but only updates a 
	section of its mols.
		Fragile partitioning.
		Shared StoichCore should really be read-only.

Option 3:
Separate StoichCore into portion that only does the setup and RateTerms.
One StoichCore per node.
GslStoich is present one per thread/processor
	Has a pointer to StoichCore 
	Manages the pool arrays itself.
	GslStoich becomes bulky.
	May want Pools to be available to some other solver.

Conclusion: Choose option 1.5 even though it has most components. 

Starting on implementation of StoichPools.

============================================================================
23 Oct 2012.

Setup sequence:
Create a StoichCore. 
	Assign its Path
Create a small array of GslStoichs, one per thread.
	Assign the StoichCore to each of these.
	Assign some kind of meshing to each.

Issue: If we have distinct GslStoichs, how does the 
	Stoich::convertIdToPoolIndex function work? We cannot ask the singleton
	stoich to do it for us as it isn't handling the pools. But we 
	do not know which of the GslStoichs to use.
	- Partly a mismatch between global containers (Elements holding pools)
	with local specific entities (the stoich)
	- If the pool has the Id of the handling Stoich rather than the ptr,
	we could look up a thread-specific stoich, but it wouldn't help.
	- We could consult all the stoichs to find which is handling this 
	pool meshIndex. May well be on another node.
		- This shifts responsibility from DataHandler to the Stoichs.
	- We could ask the DataHandler of the current ZombiePool what to do.
	It could indicate which Stoich to use. It could also say that it isn't
	on the current node. What then? 
	The current code doesn't deal with this situation.
		- Set calls are easy: just ignore.
		- Get calls go two ways: through messaging and direct to
		parser. In both cases they return the value. No way to 
		either ignore or say no-value.
		- Alternate is to have a query call go out to check presence.
		- Or, pair< node, thread > ObjId.locate()

Turns out this isn't an issue. Each ZombiePool has its own stoich field. 
	Unless we're doing something funny with it.
============================================================================
24 Oct 2012.

Made the GslStoich, compiles. Now to set up some tests for this whole
subsystem. So far I have StoichCore, StoichPools, and GslStoich. Between them
they should handle most of the functionality of the original Stoich.

Set up a test in testKsolve.cpp, doesn't clear it yet. Checkin 4182.



Can clean up n and nInit access by having innerGetN and innerGetNinit on the
GslStoich. But I still need to handle DiffConst, and Species. Either I
make the StoichCore accessible from the GslStoich, or I provide these as
virtual funcs from the StoichPools class.
	We have to choose between various kinds of ugliness:
	- Providing a bunch of virtual wrapper set/get funcs in StoichPools, and
	- Using the Id::value() as an integer index for all these funcs.
	Or:
	- Exposing the StoichCore from the StoichPools as a virtual func.

The virtual wrappers have the merit that they let us handle meshing within
the GslStoichs. More complicated threadwise lookup is then easier.

============================================================================
25 Oct 2012.
Added ZPool class, so we can incrementally convert from the old Zombie class.
Compiles.

Setting up the model zombification code in StoichCore. I will need to
refer to parent of the StoichCore for the GslStoich or GssaStoich when
zombifying pools.  Other issue is to assign the correct GslStoich to each
Pool. When remeshed, I would have to have a distinct GslStoich matching the
zonation of the threads.

============================================================================
26 Oct 2012.
Got the GslSolver system to load in a model, yet to see if it did so correctly.

Numerous fixes later, GslSolver now clears unit tests comparing output of
a simple reaction with analytic solution. Checkin 4190.
Added in clones for ZombieEnz and ZombieMMEnz into the GslSolver fold.
Checkin 4192.

Working on SumFuncs. This is messy. Should define a generic func system base
class like I have for Pools etc.

============================================================================
27 Oct 2012. 
Cleaner design for functions: Define a FuncBase in moose/kinetics. This 
provides a virtual FuncTerm to handle its operations. Derived classes can
simply adopt the FuncTerms hitherto used in ksolve. Then no zombification
needed other than removing them from the schedule list. 

I should now be able to get rid of ZombieSumFunc.

Compiles, and clears unit tests. Still haven't deployed the GslStoich
in a big way.
============================================================================
28 Oct 2012

Redid a few more FuncTerm and FuncBase derived classes. Did a make clean, this
turned up a lot of dependencies needing fixing. Clears tests. Checkin 4207.

Now I need to get GslStoich to take over all these things and confirm it
does everything that Stoich does. Then change gears to building in the
compartmental and diffusion calculations.

GslStoich now takes over all the things, at least in principle. Tests to
commence.

Replaced the old Stoich with GslStoich in SimManager. Breaks unit tests.

============================================================================
29 Oct 2012
Struggled a bit with testSimManager:testRemeshing(). Turns out I need to
define my policy for handling the per-thread entities like solvers. 

Where does the remesh call come? To the GslStoich ie, StoichPools; and 
also to the ZPools. 

Policy for solvers (here, GslStoich) is clear: Make one on each thread,
at creation time.

Policy for pools to be worked out. Options:
- Flat array over all memory space, manage by index
- Subdivide as per thread. Separate sections of DataHandler.

Also, who controls it:
	- The solver: remesh call to solver then subdivides pools
	- The pools themselves: Remesh call goes to each pool entry.
	- Some thread-level subdivision: Remesh goes to DataHandler.
		
If we do one solver per thread, then by default it will subdivide its managed
pools by thread.

Data Handler design options:
- One per Element, and it figures out which sub-address-space to use
- One per Thread, so a small array on each Element.

In both cases a key problem is how does a message figure out which
node/handler to ask for. Currently the Handler maintains the policy. If we
have one Handler per thread, then how?

Suppose we go with:
- One Element per node
- One DataHandler per thread: there is a small array of Handlers on the Element
	- What if there is a single object? Most of the Handlers will be empty.
- Element carries decomposition policy info, if in doubt can refer up.
- GslStoich is on a OnePerThreadHandler.
	- Handles the pools already assigned to its thread
- Remeshing request comes to StoichPools, as it knows best about policy.
	This simply means that the ZPools ignore the message.
	But I would have to then always define one ZPool per thread, since
	each needs to know its stoich. 
	No, just need matching ZPools and GslStoichs


For now: Just resize the S and Sinit and y vectors.

Good progress with this. Have had to disable testSimManager.cpp:testRemeshing()
because it tests diffusion which isn't yet on. Incrementally working through
unit tests. Those cleared, now grinding through regression tests which are
significantly nastier. Currently it fails to work with MMenz.
============================================================================
31 Oct 2012
Inching ahead with regression tests. I suspect there is something missing
in my Func zombification. Fixed.

Now a small error due to lack of reinit value update on SumTotal.

Now clears all unit and regression tests, except for the diffusion tests.
I've disabled those till I code in the features for that.
There is a segv on exit.
The GUI does not work with the new classes. Will need to work with Harsha
to fix. Will checkin anyway.

============================================================================
01 Nov 2012

Lots to do:
Track down segv on exit
Convert GSSA to ZPools etc; eliminate old Zombies and Stoich.
Put in intra-solver diffusion
Put in between-solver transfer of stuff.

Start with intra-solver diffusion, then between solver stuff. Idea is that I
should tinker with the interface for just one of the solvers before converting
the GSSA solver as well.

I'm thinking of just using a SparseMatrix for all diffusion, regardless of
mesh type. The entries would indicate vol scaling between voxels. If I do this
I should perhaps invent a SymmetricSparseMatrix, thus halve storage. But it
would slow things down for lookup.

Need a vector of flux across all mesh boundaries, equivalent to v. This would
be used just twice, on either side of the boundary. May be faster to compute
on the fly. Easier too.

flux = D[mol] * ( N[other]*kf[i,j] - N[self]*kb[i,j])
Can I eliminate one of the sparse matrices? 

flux = D[mol] * area_scale_term[other,self] * ( conc[other] - conc[self] ) /
		 dx[other,self]

The area term and dx term can be merged into 'adx.
To convert conc terms to N, we do:

flux = D[mol] * adx[other,self] * ( N[other]/vol[other] - N[self]/vol[self] )/NA

So in principle the calculation only needs one sparse matrix. The job of the
mesh is to provide this sparse matrix (instead of the earlier stencil).
The sparse matrix also makes it easy to handle irregular boundaries, e.g., in
a cubic mesh.

From this we need to be able to provide updates to yprime for every given 
MeshEntry. That is, self.
For every given value of the MeshEntry, we look up a row of terms from the
SparseMatrix. Even better, all terms other than the diffusion const are the
same for all molecules.


============================================================================
02 Nov 2012
Minor fix resolves issue of segv on exit. But took a long while to figure it 
out. Had to do with the updated ordering of deleting element trees twig first
rather than root first.

Making good progress on diffusion framework within stoich. It looks a lot
faster and cleaner than old one. Key issue is to get pointer to the mesh.


Now the code compiles and clears unit tests, without actually being exercised.
The ChemMesh subclasses need to define a useful getStencil function.
Checkin 4239.

Opportunity here to do completely general 1,2,3-D boundaries in the CubeMesh,
and other good things. 

============================================================================
04 Nov 2012.
Put in a prototype getStencil function for the CylMesh. This revealed need to
change its interface, done. Checkin 4242.

For the CubeMesh, the design depends on whether I want a truly general mesh
with arbitrary borders, or just a cube of points. I assume it is the first.
Options:
	1. Define each mesh point individually, sequencing arbitrary, record
		coords for each point, use sparse matrix to indicate which 
		points are connected.
	2. Embed entire system in a 3-D mesh. Individually define mesh points
		by a) indices into 3-D, and b) identifiers for stencil.

Both these have the problem of how to find an index, given a point in 3-D space.

Worked through options. Going for a simple SparseMatrix for the whole thing,
even if we do the default cube.

Need to revisit getNeighbours (now nicely encoded in sparse matrix) and
m2s and s2m.

============================================================================
06 Nov
Compiles now with reasonable CubeMesh code, fails in testMesh:testCubeMesh.

Now clears fairly stringent CubeMesh unit tests. Would like to do tests with a
sphere imposed on it. Later.

============================================================================
07 Nov.
Fixed up and extended the unit tests for CubeMesh. In due course I will need
to come back to it to provide an interface for generalized 3-D geometries. 
For now move on. Should really check it in.
Next: Revive testSimManager::testRemeshing.

Did so. Now it is going through the motions but getting the wrong answer.
Not off by a huge amount.
Checkin 4247 after temporarily commenting out the testRemeshing.

Fixed up some stuff in the testRemeshing, still doesn't work. Because the
mesh is cubic, the fix ended up being identical to the earlier version.

============================================================================
08 Nov.
Slowly grinding through unit tests.

============================================================================
09 Nov
Fixed an error in the handling of recalculations for changed mesh size.
Now strugging to work out a reasonable dt.
============================================================================
10 Nov
Now calculations are OK enough. Clears unit tests, some regression tests
still to revive.
============================================================================
11 Nov.
Working through regression tests. It is clear that the new numerical approach
is actually worse than the previous one. I need a smaller dt in all cases to
get to the criterion. I'm trying to use the RK method along with the 
flux, to handle all the time-advance calculations but this seems wrong.
Will later try something cleaner but for now I'll take half the RK timestep
for the diffusion calculations, and use forward Euler for the calculations.

Now clears all unit and regression tests. Indeed the accuracy is better than
it was.

Spent a while tracking down the long-pending memory leak in the unit
tests. Finally caught it: using an unallocated vector index for the queue,
that was nevertheless safe because of how vectors are assigned.

The regression tests still lose 192+168 bytes, all from ReadKkit it seems.
Deal with it later. Checkin 4269.

Next:
Cross-boundary diffusion
Cross-boundary reactions.
============================================================================
14 Nov.
I have a skeleton for how junction messages may go back and forth.
This will used an array of class SolverJunction, each of whose entries 
is accessed through a FieldElement.
The hard thing here is to figure out how to set up the SolverJunctions.
Cases:
Description		Solver A		Solver B
Multinode diff mesh	GSL same model		GSL same model
Different compt chem	GSL A, mesh		GSL B, mesh
Different compt diffn	GSL A, mesh		GSL B, mesh
Diff compt reac+diff	GSL A, mesh		GSL B, mesh
Different solvers	Solver A		Solver B
Motors			any of the above.

The key thing is to embed all info needed to set up the junctions in the 
object definition of the model, and to update whenever mesh changes.

Use case is the SigNeur system. Here I have the combo case of different
compartment reac+diff. There are also special diffusion interchanges through
the spine neck. Will need a special SpineCompartment class for that.

To identify reactions: Just follow the messages. 
To identify diffusion: 
	- Nonzero DiffConst.
	- Identity:
		- Molecule with identical name? 
		- Molecule with identical SpeciesId?
		I don't yet set up SpeciesId from ReadKkit. Easy though.
		But this just regresses the problem: Who sets up the species 
		info? SBML might.
		-> So for now just take the name.
	- Diffusion within, but not across compts, or special rates across 
		compts.
		- Compt sets a single global scale factor for diffn consts
			across junctions, over and above geometry.
		- Junctions permit more fine-tuning, beyond what is automatic.
To identify mesh entries on the junction: Options.
	- SimManager does it based on preprogrammed ideas. Not general.
	- Rely on the 'geometry' objects. 
		- Shared panels? But we don't always have clear-cut panels
		that match: cylinder of dendrite and cylinder ends of spine.
	- Find subdomain of space where junction should lie, then scan mesh.
	- Query to compartment with other one as arg.
		Return vector< pair< meshindex, meshindex > >.
		where reacs or diffusion can occur across the pair.
	-> The last option works best. The compartments can use internal
		logic or reference to geoms, etc, to build mesh pairs.
To specify which kinds of ChemMesh: put the correct ones in place.
To specify which compartments have a stoich, and of which kind: 

current layout is messy:
/model	
	/kinetics
		/mesh
		/<pools>
		/<reacs>
		/<enzs>
		/<groups (deprecated)>
	/compartment_<1,2,3...>
		/mesh
		(ToBeImplemented:
		/<pools>
		/<reacs>
		/<enzs>
	/stoich
		/stoichCore
	/geometry
	/graphs
	/groups

I want to traverse this to figure out how to set up stoichs. Preferably
stoichs should explicitly reside within the compartment they manage, but
a stoich may manage multiple nested compartments so it isn't easy.
- Simple approach: Permit a stoich only to manage a single compartment.
- Compromise: Stoich can manage either a single compt, or all the compts in
	a model provided they are all single voxel compts.
	- Should implement a default single-voxel ChemMesh
	- Should encourage use of ZombieSwap for changing mesh properties.

-> If there is a stoich on the /model, it is a global stoich
-> Otherwise there should be a stoich within each compt.
-> SimManager has to traverse the tree and tell the stoichs about each other
	so they can set up interfaces by referring to their Compartments.


Working on implementation of baseclass fuctions for handling the junctions
between two StoichSolvers.
==========================================================================
16 Nov.
Implementation taking shape. Current issue: where do I store the vector of
RateTerms that define the flux? 
- On original StoichCore
	- Has much of the infrastructure for managing reacs, including some
	but not all of what I'll need for handling rate changes.
	- Will need to do some fixes so as to use only part of the rates_ 
	vector for the regular calculations.
	- Need new stuff to handle changes in diff consts.
- On GslStoich
	- Neither here nor there: will need to sub-map between the RateTerm
	vector and the SolverJunction that each entry belongs to.
- On the SolverJunctions
	- Logically reasonable subdivision
	- Need to rebuild entire management structure from StoichCore.
-> Store it on the original StoichCore.

Specific issues with handling rate changes for cross-compartment reacs:
	- A single rate may control many RateTerms. 
	- Or, a single RateTerm may control many but not all cross reacs. 
		This is cleaner. It is similar to what we have in the regular
		reacs anyway: a single RateTerm controls the calculations in
		all voxels. 
		- The Junction handles this part of the calculation anyway.
		- The Junction should identify the rate term and then the
		range of voxels to apply, for outgoing fluxes.
	- Updates on the parameters can control the RateTerm in the usual way.
-> Good, the cross compartment reacs are done comfortably in the current
	framework.
	
Specific issues with handling DiffusionConst updates.
	- Single update will control multiple RateTerms as well as multiple
	meshEntries. The multiple RateTerms are one for each Junction, as the
	meshing may differ.
	- Other implementation: tap into the DiffConst vector from the 
	StoichCore. 
	
==========================================================================
17 Nov.

General architecture for solver communication taking shape:

- StoichPools is the base class for GslStoich and in due course for
	GssaStoich and Smol. 
- StoichPools has a vector of FieldElements called SolverJunctions. These
	can talk to one another through the 'junction' shared message. 
- All reaction and diffusion communication between solvers, including internode
	communication, goes through the SolverJunctions and their messages.
- Communications happen through 'process'
- Data transferred is currently the increment in the remote object in a time dt.
	This amounts to a Simple Euler calculation.
- Junctions are built using the StoichPools::addJunction, which takes the 
	target StoichPool Id as an argument.

Designing tests for this.

Checkin 4281.
==========================================================================
19 Nov.
Incrementally building in the structure for setting up the junctions based on
the object structure of the simulation. Long way to go yet. Checkin 4282.
Starting on unit tests just for the junction setup. Fails already.
Fixed that. Looks like junctions are now OK.

Further elaboration of junction setup in StoichPools:;addJunction.
Skeleton code laid out, significant work to do to fill in.
==========================================================================
20 Nov.
Really stuck on how to build the junction mesh entry list. Want to avoid
N^2 scan of all possible positions.
Use cases:
	- NeuroMesh and SpineMesh.	// neuronal model
		Sort by axis with largest variation, for both meshes
		March up axis on each mesh, doing a match.
	- NeuroMesh and CubeMesh	// intra to extracellular diffusion
		Make list of NeuroMesh entries within CubeMesh
		Use coord lookup trick to find (s2m)
	- SpineMesh and CubeMesh	// intraspine to extracellular diffusion
		Same as above.
	- CubeMesh and CubeMesh		// General case.
		Compute line between centroids
		Pick axis.
		Confirm no overlap
		Assign matching plane
		

Tackle CubeMesh in all its horribleness first.
New functions:
	double nearest( x, y, z, unsigned int& index )
	returns the distance from the specified point xyz to the nearest point
	in the CubeMesh. The meshIndex of the nearest voxel is passed back.
	Assumption: the mesh is (nearly) cubical. Won't break otherwise, but
	the distance isn't so useful if you're looking to find junctions.
	Finds this in 3D by using the standard grid subdivision and estimating
	resultant coords. If within, returns directly. If outside, finds the
	face where the line from xyz to middle crosses, and then just scans
	through that section of the 'surface' vector to locate the point.
	In 2D it is similar.
	Speedup if the 'fullmesh' flag is set. Then it just uses the crossing
	point on the face to find the nearest.
	In 1D it just returns centre-to-centre distance.

	bool inside( x, y, z )
		Easy, the s2m function does this.

	unsigned int numDims() const

	int compareMeshSpacing( const ChemMesh* other )
		-1: me smaller. 0: equal. 1: other bigger.


Got the reference code in for CubeMesh. Compiles, fails unit tests.
==========================================================================
21 Nov.
Cleaner idea for finding contacting surfaces: 
1. Define the intersecting cuboid, if any, between the respective CubeMeshes.
2. Allocate entire volume with uint voxels at the coarser mesh size. 
3. Scan through the coarser mesh surface, putting meshIndices into the
	relevant locations of the intersecting cuboid
4. Scan through the finer mesh surface, looking up locations on intersecting
	cuboid. If occupied then add pair to list.

Cost: linear in number of surface voxels. Memory will be steeper but not
too bad either.

Issues:
	- What if the meshes intersect, not just abut?
	- How to deal with cuboid centre offset? Should I shift positions
		toward each other on the centroid? Or just define a generous
		intersection radius?
	- Extend to neurons in a volume. Specially for surface emission.

For what it's worth, compiled and cleared unit and regression tests
for previous implementation. It just bypasses the actual mesh handling at
this point. Checkin 4287.

Implemented but not tested the new algorithm. For it to work I need also to
get the surface_ vector set up in the mesh. Did it. Checkin 4288.

Marching through tests. Now at crux point:
How to deal with the zero volume of intersection cuboid in cases where there
is perfect abutment.
1. Make intersection cuboid bigger by 1 voxel each way. This way it will
	capture all the relevent voxels. Easy.
2. When the fine-mesh surface scans the intersection cuboid, look for 
	abutment rather than overlap, on a suitable stencil.
	Moderately easy. 6x more checks isn't too bad.
		Or, at time of coarse-mesh filling, could also paint outer
		abutting voxels and inner abutting voxels with neighbour
		indices.
		- Could 'paint' singleton abutters with actual index, and flag
		others. But then how do we distinguish between surface and
		abutting voxels?
		- intersection cuboid could have pair< index, flag >
			where flag tells if point is surface, inside, or 
			outside. Could even have an extra case where the flag
			field too is an index, for 2 outside points.
		- Note that we can have up to 3 outside points for a 2-d 
			surface, and 5 for 3-d surfaces.
			
	2a. Warn if there is actual overlap.



Did some systematic unit tests for the functions setIntersectVoxel and 
	checkAbut, which are at the heart of these calcuations. Clears.
	Fails the subsequent unit test to actually define the meshEntries at
	a diffusion junction. But it seems close.
==========================================================================
24 Nov
Cleared the unit test now for generating the mesh junction vector. 
Checkin 4291.

Next:
	- Set up code to harvest the mesh junction vector for use in the 
		solver calculations. See GslStoich::matchMeshEntries.
		Should perhaps migrate up to the StoichPool
	- Set up test code for 3D junction.


==========================================================================
25 Nov.
Some documentation for the mess of data structures I'm using to pass data around
between solvers.

Wrapped up implementation for setting up the mesh-related arrays in the
SolverJunction. Yet to compile. Now compiled.
*Next: Unit test for this setup.
*Next: set up diffusion across junctions
*Next: test diffusion across junctions
Next: set up reactions across junctions
Next: test reactions across junctions
Next: Return to CubeMesh using a general voxel assignment routine and
	automatic set up of s2m_, m2s_, surface_ and m_.
==========================================================================
28 Nov
Trying unit test for setup of SolverJunction. Error because the surface_
vector of the CubeMesh is not being set up properly.

==========================================================================
30 Nov
Seems to report spaceIndex rather than meshIndex.
Worked through, fixed it. Now clears unit tests, though I have lots more to put
into testKsolve.cpp:testMatchMeshEntries. Checkin 4312.
Put in additional unit tests into the same function, just sailed through.
Next: set up diffusion across junctions

This went smoothly till I started actual calculations. Then major issue 
arose: I need to specify a scale factor for diffusion between every two
coupled voxels. Currently no way to do so.

==========================================================================
1 Dec
To handle the diffusion scale factor, I need to 
	- Figure out direction
	- note h/2 for each of the meshes in the selected direction
	- Decide smaller of two XAs

Added in VoxelJunction for passing around the diffusion scale factor.
Cascading changes in many files. Clears unit tests with the exception
of the preliminary ones to test cross-junction diffusion.
Checkin 4316.
	

2 Dec.
Stuck on compilation.
Issues with varying mesh sizes, see testMesh.cpp:1285.
Issue with the calculations.

==========================================================================
3 Dec: Now it seems like the sent message with the diff info isn't being
received on the target SolverJunction.
Fixed.
After some effort, incorporated the n-dim analytic diffusion test from the 
regression tests. Now it clears this, though not with stellar accuracy.
Takes a 15x15 diffusion calculation and splits it among 9 (that is, 3x3)
solvers.
Anyway, major step cleared.  Checkin 4321. See 25 Nov for next step, 
reactions across junctions.

Here the crux step is exemplified in ZReac::makeHalfReaction,
where it calls stoich_->convertIdToPoolIndex.
The Id is obtained through regular messaging, but the PoolIndex
will be invalid because it isn't in the path of this Stoich.

One option is to return a flag for the PoolIndex which tells us
to put the reaction on a special temporary rates_ list.
Furthermore we need to be able to encode the target stoich/solver that
does handle the pool. This will be needed to decide which Junction should
take it forward.

==========================================================================
4 Dec.

A perspective view:
1. I need to push mol# around in all cases, not just reacs. Currently 
	diffusion calculations use the symmetry of the flux term, but for better
	numerics and GSSA I need to use mol#. 

2. I need to do calculations only on one side. Don't duplicate them. The
	Pool transfer also becomes asymmetric.

3. It is too messy to predefine any given pool, let alone a solver, as
	being Leader or Follower in these calculations. Complicated meshing
	and reactions will throw things out regardless, and any given pool may
	be involved in multiple fluxes. So, all pools and
	solvers have to be prepared to be involved in some calculations 
	locally, and to have fluxes dictated to them by remote calculations.

4. Do the calculations in the following sequence
	Init: Send out current Pool values
		Init_handle: recv current remote Pool values.
	Process: Do all computations, local and cross. Send out fluxes.
		Process_handle: recv fluxes. Update Pool values.


Where to put the transferred mols?
- In S_: 
	Easy to incorporate with ongoing reac and diffn calculations
		But how meaningful are ongoing reac calculations? Will the
		S_ term be buffered?
	S_ becomes bigger: nmols gets bigger with X reacs, and nmesh with diffn.
		nmols: not a problem, only a few
		nmesh: Will need to break S_ mesh indexing, or have lots of 
			blanks. But possibly the sparse matrix stencil will
			rescue this, so minimal number of extra mesh indices.
		
- Separate data struct:
	Excruciating to incorporate.
	Can employ distinct numerical methods.

While there are possibly only a few junctions,
	Need selective execution of the extra reac terms on junction voxels.
	

*1. Get data into S_ vector. Done skeleton for it.
*2. Write up the messaging stuff and the Init phase.
3. Extract setup info to pad out the S_ vector
4. Use setup info to add terms to the stencil so that the regular diffusion 
	calculations will see the transferred pools
5. Modify the delta_V calculations to harvest instead from the changes in the
	terms in the S_ vector.
6. Check that it still works
7. Get back to doing the reaction calculations.

Successfully did 1 and 2, clears unit tests. Checkin 4324.

==========================================================================
S_ along surface only, expands to handle additional mols for reacn interface.
	These are nominally in the same voxels.
S_ also expands by a few mesh entries but not having any reactions, to handle
	abutting locations for diffusion interface.

rates_ expands too, for the additional reacs. Invoked only at boundary.
N_ expands to handle the stoichiometry of the additional reacs.


Call chart for setup:
StoichPools::addJunction( Id other )
	-> DerivedClass::vAddJunction: doesn't do anything!
	- Validates
	- Sets up messages
	- vBuildReacTerms: // Todo: Add to the N_ matrix and to rates_.
	- SolverJunction::setReacTerms
	- other->vBuildReacTerms
	- other->SolverJunction::setReacTerms
	- findDiffusionTerms: figures out which mols diffuse
	- SolverJunction::setDiffTerms
	- other->SolverJunction::setDiffTerms
	- ChemMesh::matchMeshEntries: Figures out surface abutment
	- SolverJunction::setMeshIndex: stores surface abut info
	>- SolverJunction->setSendPools: stores info about pools to transfer.
	>- SolverJunction->setRecvPools: stores info about pools to transfer.
	- other->SolverJunction::setMeshIndex: stores surface abut info
	- expandSForDiffusion: Add meshEntries to S_ for diff to abutting mesh
	>- other->SolverJunction->setSendPools: See above
	>- other->SolverJunction->setRecvPools: See above
	Todo:
		Add new reac terms in vBuildReacTerms
			Expand S for new pools but only in surface voxels.
			Add rows and cols to Stencil.
		add xa term to diffusionMesh for junctions,or fill from Junction
			No, it will have to be on the mesh. The stencil is 
			there too

	
Call chart for runtime:
	GslStoich::init
		foreach Junction
			harvest outgoing pools
			send msg
	StoichPools::handleJunctionPoolNum
		GslStoich::vHandleJunctionPoolNum
			// Assigns values. 

	GslStoich::process
		Advance sim using Gsl etc. // To be fixed to handle X reacs.
		updateDiffusion // To be fixed to deal with more entries
			foreach meshEntries // To fix to use bdry Entries too.
					// Also to add rows&cols to Stencil.
		SubClass::vUpdateJunction
			foreach Junction
				foreach meshIndex
					stoich_->updateJunctionRates 
					// (Reacs, to remove)
				foreach Voxel on surface
					GslStoich::updateJunctionDiffusion
					// Perhaps to remove this too.
				foreach transfer vector v
					multiply by dt // Remove
				Send msg to junctionPoolDelta
					// Fix to send actual change in concs.
				
	StoichPools::handleJunctionPoolDelta
		Junction::incrementTargets
	

The Stencil lives on the ChemMesh subclass. I can expand it so that it accounts
for all junction diffusion terms as well. Or I can have each junction
maintain its own little Stencil.
- Single big stencil:
	Easy and clean to compute
	marginally faster
	More opaque: can't easily check and alter any one junction.

Related, will need to expand getMeshEntrySize to include junction voxels if
	I have a monolithic Stencil, otherwise will have to partition.

Overall, a single big stencil is far easier to subsequently fold into a better
	numerical method than a bunch of separate calculations. So let's go
	monolithic and hope for the best.

Implemented the extended Stencil fillup. Yet to compile, let alone test.
==========================================================================
7 Dec. 
Compiled and cleared unit tests on Stencil fillup. Yet to put in tests for it.
Checkin 4335.

To do the diffusion, should I buffer or float the external meshEntries?
- Buffer: 
	I won't be able to measure the change in conc. 
	The fixed conc may be more consistent with actual behaviour. 
	Same as computing the flux term using explicit Euler, if one only
		does one timestep advance.

Float: Only differs if the computation is done using multiple timesteps. Here
	a second order option would be to compute values on both sides and
	exchange estimates: effectively a trapezoidal approximation.

Clearly, have to float it if I want to estimate how much it has changed during
the timestep.

Unit test: *confirm that the correct number of S have been set up
	*Confirm that the correct entries get exchanged
	Confirm that the extended stencil is right.

Full test: clean out the old diffusion calculations, replace with the new
	stencil form.


==========================================================================
8 Dec.
Added unit test to confirm mesh data transfer of Pool #s. Works, data gets
to the right place. Check 4336.

Checking that stencils are properly extended. Seems OK for the limited
unit test in testMesh::testCubeMeshExtendStencil(), but fails in the more
complete test in testKcolve::checkDiffusionStencil: 633

Fixed, partly. But now it seems like the meshIndices of the abutting 
neighbour are rotated.
I'll have to see what the VoxelJunction vj vector in CubeMesh::extendStencil
was doing.
==========================================================================
9 Dec.
The values sent over seem to be OK, even if the meshIndices are scrambled. So
let's move on.
The nans are happening for every single voxel.
Discrepancies in allocated size for S_, Sinit_, and presumably y_.
That might account for it.

Put in various assertions to check for these, They actually seem OK.
Try valgrind.


Valgrind doesn't help

Going through the upidateDiffusion since at Process, only the 
diffusion-connected entries are nans.


==========================================================================
11 Dec.
This turned out, after really tedious debugging, to happen because the
volumes in extendedMeshEntrySize were all zero. Which turned out to be 
because I had declared it as unsigned ints.
With this we get sensible numbers transferred over. Next to figure out 
the change in junction concs following the diffusion calculations, and use
it to send over delta.
Implemented something for this. It isn't yet right, but error in mass consv
is small. Possibly I'm on the right track.

One possibility: I'm doing the diffusion calculation on both sides of the
junction. Should it be just one?


A	B	C	D

A and B are in compt1, C and D in compt 2, B and C are the interface voxels.
The two compts see:


A	B	c		b	C	D


Now 'c' gets a new value, this change is transmitted over to C. This would
conserve mass because any mols lost from c end up in B and A.
Vice versa is also true. So there's some other issue to cause the lack of
mass consv.

==========================================================================
13 Dec
Added a 1-D diffusion test in testKsolve.cpp. This too doesn't work.
Symptom is that both voxels across a junction have the same conc.

==========================================================================
15 Dec
Picking out details of 1-D test in 
testKsolve.cpp:testOneDimDiffusionAcrossJunctions. 
After a single timestep in case where there
is a uniform gradient from 100 to 109, it seems OK.
Stencils look OK
But its mass consv fails when doing calculation in another way.
Points indexed 5 and 6 both take up on the same value. 

Fixed, I was using the wrong arrany (for targets rather than recv
entries). Now clears unit test for 1-D diffusion across a junction.
Fails still on 2-D diffusion.

Will put in starting defaults to better check the data transfer at init.

Did so. Something goes wrong for dummy meshEntries used to do the
diffn calculations.

==========================================================================
16 Dec 2012. Data transfer at INIT is happening to the first tile, but not
to the others that I've tested. See testKsolve.cpp:979.
Possibly the addJunction function isn't finishing the symmetric junction.
I know it is being created as I count num_junction.

==========================================================================
17 Dec 2012.
painstakingly traced all the recvd messages. 24 go, correct: 12 junctions and
data each way. But fieldIndex is only 0 or 1. 

Tracked down. Thoroughly insidious bug, I really need to revisit fieldindex
stuff. I had set a number of 2 for the default # of field Elements. When 
the thing exceeded this number it just truncated it. I need to handle this
better and it has to work across nodes. Cleanest is to have
a separate unsigned int always assigned for it.

Anyway, for now hacked in a limit of 16 junctions.
With that we are not stuck with the calculations after 1 timestep.

      p svec[25] $1 = (double &) @0xda1cc8: 181.40000000000001
(gdb) p svec[26] $3 = (double &) @0xda1cd0: 185.90000000000001
(gdb) p svec[27] $5 = (double &) @0xda1cd8: 190.90000000000001
(gdb) p svec[28] $6 = (double &) @0xda1ce0: 195.90000000000001
(gdb) p svec[29] $7 = (double &) @0xda1ce8: 178.40000000000001
(gdb) p svec[30] $8 = (double &) @0xda1cf0: 264.60000000000002
(gdb) p svec[31] $9 = (double &) @0xda1cf8: 265.5
(gdb) p svec[32] $10 = (double &) @0xda1d00: 266.5
(gdb) p svec[33] $11 = (double &) @0xda1d08: 267.5
(gdb) p svec[34] $12 = (double &) @0xda1d10: 238

==========================================================================
18 Dec
Worked out a logical error in the diffusion algorithm:
	- Transfer mol# from abutting voxels over
	- Compute local diffusion including these abutting voxels.
	* Compute change in mol# in abutting voxels
	* send back to originating solver and apply delta to its mol#
The items in * are wrong. 
Put more generally, I have a variety of cases depending on solution method.
General principle is that the more detailed method does the calculation and then
sends over DeltaMol#, and the less detailed just sends over mol#.
This will also work better for cases where I have reactions, where I do need
to send over deltas.
	1. Deterministic, Deterministic: 
		#A -> B
		#B -> A
	2. Gillespie, Gillespie:
		#A -> B
		DeltaB -> A
	3. Smoldyn, Smoldyn:
		?
	4. Deterministic, Gillespie:
		#A -> B
		DeltaB -> A
	5. Deterministic, Smoldyn:
		#A -> B
		DeltaB -> A	
	6. Gillespie, Smoldyn
		#A -> B
		DeltaB -> A	

Looks like the general approach is #A -> B; DeltaB->A. Can I apply to 
Deterministic, Deterministic case?


For now: simply commented out the transfer of the deltaMol#. Now it clears
all unit tests. Fails in a regression test for cylinder diffusion, probably
need to update the cylinder class.

Since I have gone a very long time without a checkin, I'll put this in after
commenting out the cylinder regression test. Checkin 4348.

==========================================================================
22 Dec.
Some cleaning up later, we now have the unit and regression tests clear with
the asymmetric calculations for diffusion. Checkin 4350.

List of pending things here.
- Implement cross-compartment reactions
- Set up sim manager to handle coupled solvers
- Extend to CylMesh and NeuroMesh
- Write SpineMesh, extend to it.
- Put in stuff for SigNeur.
- Extend CubeMesh to figure out surfaces from Geom classes and bitmaps.
- Convert GssaStoich to use new zombie classes
- Derive GssaStoich from StoichPools
- Get GssaStoich to talk to each other through diffusion
- Get GssaStoich and GslStoich to talk to each other through diffusion
- SBML read and write
- Reorder KKit reader to put things more sensibly into compartments.

==========================================================================

24 Dec.
Doing the cross-compartment reactions.
1. Assume that the off-solver pools do not figure in the elist that is
sent to StoichCore::allocateModel. 
2. Put the offending pools in new poolIds.
3. Put the offending reactions in a special subset that is not run
	normally.

Pools: separate voxel or same voxel? In other words, does it live in
	the same S_ and Sinit_ as the other pools in the voxel?
	- Hard to implement stoich calculations across voxels, the structure
		is designed to be self-contained.
	- The pool will be in a separate list anyway.
	- The pool maintains its own volume anyway.
	- This amounts to institutionalizing the idea that S and Sinit are
		not simply the pools within a voxel, but the pools with a solver.
		Usually the two overlap perfectly.

The StoichCore for a compartment has to know about all reacs and pools,
even if they are not used in a given compartment.

Data structures:
- Put all pools in S_ and Sinit_, including offSolverPools
- Put all reacs/enz in rates_, including reacs going offSolver.
- Record offSolverReacOffset_. Later reac terms are offSolver.
- Record offSolverPoolOffset_. Later pools are offSolver.

Functions:
- in setPath, traverse the elist to find offSolverReacs and offSolverPools.
	Trim the offSolverReacs off elist. This is in locateOffSolverReacs.
- Do the allocation so that the offSolver entries are at the end of the 
	respective portions.

Runtime:
- The extra reacTerms will run based on SolverJunction::reacTerms_ vector.
	Each solver will go through its set of junctions to find the terms.
- Ext solver pools are updated just as usual
	- Input values come through targetMols_ * meshIndex for each junction.
	- Delta values go through the same.

==========================================================================
25 Dec.
Set up first pass code for cross-compartment reactions. Current StoichCore code 
finds the reactions, finds any off-solver pools that they talk to, and puts 
these pools into the idMap for the StoichCore. They also are put into the 
S_ and Sinit_ matrices. 
Clears a really simple new unit test that all this is done correctly.
Checkin 4351.

Next: Set up Junctions to handle cross-solver terms.


==========================================================================
26 Dec.
Did the setup for cross-solver terms. Yet to test it, but it does clear all the
older unit tests. Cleaned up a lot of cruft in Junctions.

Still need to activate the handling of the additional reactions only in
the boundary voxels where the cross-solver reactions take place. Need to
implement a serious unit test for this.
- May need to do a 'unique' on the pools in case we have a higher-order 
reaction.
==========================================================================
29 Dec.
To check for cross reactions: just do single voxel reaction on each side.

==========================================================================
30 Dec.
Setting up cross reaction computations. Failed simple test. Turns out there
is a design oversight here. Both the reaction rate updates and the solver
calculations differ depending on voxel. One size does not fit all.
- Each voxel computes a different subset of reactions, depending on which 
	cross-compartment reactions apply. In principle we need to track this
	subset for every voxel.
	- If many voxels, most voxels will have the local set only.
	- Typically only a few groupings of reactions will be needed.
	- The Junctions don't have the reaction subset data in a useful form. 
		Instead they store the set of meshIndices to which they apply.
	So I need to 'invert' the data representation, so that each voxel
	knows which junctions apply to it. Then the voxel can find which 
	reactions to solve.
- I use a common stoichiometry matrix N_. This has info about all the
	reactions that the compartment might choose to use, mapped to all the
	pools including all proxy pools. 
	- For 'core' voxels, the solver only calculates the core set of pools.
		The proxy pools neither need to be updated, nor do they 
		influence the calculations.
	- For 'Junction' voxels, the solver has to calculate all pools including
		proxies, even if those proxies are not used at this junction.
		This is because the solver and stoich matrix want a single 
		contiguous array.
		In principle I could do a whole lot of copying to select only
		the required pools for the solver.

How about setting up each voxel to be much more ignorant of these housekeeping
details. Precompute all the applicable combinations of N_, rates_, and  pools.
Problem here is that the data transfer becomes more complicated. For every
pairing of pool vectors across junctions, I need a separate set of 
junction vectors. 
 
Most models are likely to have just one or two kinds of cross-reaction terms.
For example, the dendite compartment will typically interact with spines, 
and with soma. Possibly with extracellular matrix.
How many combinations? Small enough.

GslStoich handles a set of StoichPools, one per voxel. Each StoichPool
	keeps track of S, Sinit.
GslStoich handles a set of StoichCores, one for each category of voxel. The
	StoichCores keep track of all the reactions including cross-solver
	reactions, on any given voxel.
	The StoichCores all share a common subset of molecules and reacs
GslStoich handles a set of Junctions, one per interface with any other Stoich.
	Each voxel category has to provide a subset of data to the Junction.
	I need to encapsulate this.


Or: Eliminate cross-solver reacs from the purview of StoichCore altogether.
Have them handled in a stupid way by the Junctions.
Problem is that this won't deal well with different kinds of solvers. The
solvers have to take responsibility for handling cross reactions and diffusion.

Outcome: Keep the basic single-voxel solvers simpler, have more versions 
of them managed by multi voxel solvers.
SolverBase -> GslStoich
StoichPools becomes VoxelPools and removes the junction stuff.

Things to check:
1. have the GslStoich, handle the setReacKf type calls, rather than
the StoichCore. Reason is that there are multiple StoichCores to update.
2. Calls to Gsl solver if the data is changed but the structure is the same.
	I think a reset is needed somewhere. I'm concerned about internal
	storage of some state variables or differentials.


Setup sequence:
- Reaction system and compartments and their meshes should already be defined.
	Mesh should already have
	spatial stuff set up, though it can be updated later.
- Assign junction messages
- Assign path.
	- Use it to fill out the zeroth stoich. This is the one that does the
  	  zombification.
	- At this stage the StoichCore records, but does not build up the
	  off-node reactions and molecules.
	- This level is enough to do single-compartment calculations. Don't
	  need further builder intervention.
- Call the builder function. 
	- Extract junction information
	- Figure out how many stoichs are needed, based on path and junctions.
	- Fill all Stoichs up using copies, not zombies.
	- Assign Gsls


Begun this huge refactoring operation. Split up the GslStoich.cpp file
into GslStoich, GslStoichSetup, GslStoichNumerics, and GslStoichZombies.
Still trying to compile Numerics.
==========================================================================
31 Dec.
Grinding through refactored code. Compilation through till the testKsolve.cpp.

Finally compiles. Crashes rather quickly in unit tests.
==========================================================================
1 Jan 2013. Shame to still be working on this tedious stuff.

Refactored and fixed up compilation, various setup issues. Now sets up the 
path of a test simulation but isn't handling assignment. But things are 
coming together.

==========================================================================
2 Jan 2013. Finally some headway in unit tests. Does the basic setup and 
single compt run stuff.

==========================================================================
10 Jan 2013
in testKsolve.cpp:testInterMeshReac. Many fixes later. It is not doing a segv,
but it isn't transferring molecules to compartment B.

Seems like the GslStoich in compartment B is not being called for init
or process. Fixed. Still isn't tranferring molecules. still the size
of the transfer pools is zero, even though I've seen the values be assigned.

==========================================================================
11 Jan 2013.
OK, I see what has happened. The reac pool info is indeed set up, but the
sendMeshIndex stuff is not. So nothing is transmitted.

I should write a separate unit test to make sure all the contents of the
Junction are set up properly for 
- reacs across junction
- diff across junction
- both reac and diff across junction.
- Both reac and diff across multiple junctions.

==========================================================================
13 Jan 2013.
Workin on testKsolve.cpp:testJunctionSetup.
The junctions are not finding abutting points, possibly because of the size
mismatch between the two meshes. For now state that they are identical on 
the x=0 face, achieved by making the second mesh long and thin.

The intersection defined in CubeMesh::matchCubeMeshEntries is wrong in two ways.
	- It doesn't have any voxels included, though of course there should
		be the volumes of the two compartments themselves.
	- The corners are wrong: {0,-1,-1},{2,2,2}
		Should be: {-1,-1,-1},{3,2,2}, or perhaps
		{0,0,0}{2,1,1} to be really precise.

The CubeMesh::surface_ vector is empty. Why? OK, fixed it. Turns out that there
was no way to test for a zero-dimension (i.e., single point) CubeMesh. With
that fixed the unit tests march along to testKsolve.cpp:testInterMeshReac.
==========================================================================
14 Jan.
Further steps. Now there is a hiccup in GslStoich::fillReactionDelta.


==========================================================================
15 Jan.
I need to write out a flowchart from the setPath call on downward, to be sure
that things are set up correctly. Need to deal with cases where there is no
compartmentalization and diffusion too.

1. Create system
2. Set up mesh
3. Create stoichs
4. Set path on each stoich
5. Add remesh msg to the stoichs
6. Add junction between stoichs.
7. Set clocks and run.

Issue: StoichA_->pools_.size = 2, {2, 1}
Should have been 1, {2}


Much marching through bad code later. The StoichCore::updateRates simply
does not update the proxy pool, even though the reaction is called.

Fundamental issue with StoichCore for the junction. Need to cleanly define
what each does, and whether to incorporate the proxy pools and the
cross reacs into numVarPools and rates_ respectively.

I've been retrofitting to put in the extra reaction in.

Cleaner:
1. At time of setPath, the GslStoich and StoichCore can already work out which 
combinations of StoichCores will be needed. This is because the set of 
reacting compartment Ids are known to the StoichCores.
1.1 The basal StoichCore should probably never be used for computation, since 
	it contains the rateTerms_ for all the reactions including all 
	cross-compartment reactions. These rateTerms will probably refer to 
	empty or misaligned pool entries. Unless there is a single junction.
1.2 The basal StoichCore should spawn off truncated versions applicable to
	each compartment combination. For 3 compartments this will be:
		bare, j1-2, j1-3, j2-3, j1-2-3.
	the last being the basal StoichCore.
	1.2.1 Doing this combinatorially is a Bad Idea unless we can guarantee
	that there will never be more than, say, 4 compartments abutting any
	other. Consider things near a dendrite: spine, membrane, NO/AA diffn,
	mitochondria, soma...  So it won't work.
	1.2.2 To generate the applicable combinations automatically is too
	horrible. Simpler to have the user specify the list, otherwise use
	brute-force combinatorics. This turns out not to bee too bad,
	since it isn't all possible combinations, only sorted ones.
	1.2.3. X-reacs will only go up to triplets. Most are just pairwise.
		Can we enforce pairwise? It will eliminate MMenz type entities
		such as ion channels, where a steady-state high-traffic
		approximation holds and spans 3 compartments.
1.3 When it spawns off versions, subsets of the rateTerms vector are removed.
1.4 When it spawns off versions, subsets of the proxy poolEntries are retained.
	This means that their poolIndices change.
	This means that the corresponding reactant indices in the x-reaction 
	rateTerms will need to change.

2. The AddJunction has now to work with a set of pre-defined OdeSystems.
	2.1 These have to specify which compartments they handle
	2.2 Pairwise is easy.
	2.3 Triplets and in fact all levels are generated simply by going
		through all junctions and appending onto a signature for each
		voxel. These are then compared against the signatures for the
		odeSystems and thus assigned to entries in the pools_ vector.


Working on StoichCore::spawn. Issue here: I can't really manage multiple
RateTerm arrays, since it will mess up field access. In other words, if I
have multiple such arrays, then any field change needs to update all of them.
One way around this is to have all the spawned StoichCores use the pointers
from the original.
Similar problem does _not_ arise for the S_ and Sinit_ arrays: the only pools
we can access are always aligned the same.

==========================================================================
17 Jan.
Implemented code for spawn. If I am to reuse pointers from the master 
StoichCore, then I cannot have different indices for the x-compartment 
reac pools. Instead make a separate vector with the affected indices, used
for transmission. This ends up wasting the numerical engine cycles as it
will try to update unused pools. Which is one of the things that the separate
StoichCores were intended to fix.
Action: Since the API is unchanged, I'll get the simple form of this implmented
	now and never mind the inefficiency. Later benchmark and see
	if it matters.

Set up spawn, compiles, croaks in tests.

==========================================================================
18 Jan,
Much fixing later, now croaks about where it did earlier with setting up the
interMeshReac. 

Now gets much further. Need to now assign the correct ode to each pool_
entry.
==========================================================================
20 Jan.
Now back at the usual crash site, in testKsolve.cpp:testInterMeshReac.
Still not seeing an increment in molecule b.

Turns out that I still need to sort out data structures for the off solver
(proxy) pools. The N_ assumes they follow right after the nVarPools, but 
what of the bufPools and funcPools?

Anyway, another minor fix and now we clear the testInterMeshReac(),
but I'm not convinced that the values are right. Volumes were to be different.
Key thing is that info is passing back and forth now, and the reaction is
proceeding across the junction.

==========================================================================
21 Jan.
Clears the testInterMeshReac. Currently stuck on StoichCore.cpp:unZombifyPools.

==========================================================================
22 Jan.
Fixed that, and a minor fix with field naming. Now it clears unit tests
but fails regression tests. There is something familar about the kind of
problem here, where things seem mostly in place but the model doesn't give the
right output. But I can't recall.


==========================================================================
23 Jan
Issue with mixing up var pools with all pools.
Related issue with mixing up reinit with setup for odeSystems.
See GslStoichNumerics.cpp:GslStoich::reinit
and GslStoichSetup.cpp:GslStoich::setPath.

With a temporary bodge here, the regression tests advance to the
previous point of problem, the Kholodenko model test in rtRunKkit().
==========================================================================
24 Jan
Not much progress with the regression tests. Ran valgrind and it thinks that
I'm copying from unallocated memory. Tried a little fix, no change.
==========================================================================
27 Jan.
Despite fix, same error. I need to deal with allocation of N_ in Stoich,
of S_ and Sinit_ in VoxelPools, and y_ in GslStoich in a much more tightly
coordinated manner. Further, I have to explicitly put the proxy pools for 
reactions right after the regular varPools and before FuncPools and BufPools.

Let's tighten up design, now that we have most of the bits working.
One key fix would be to do a special extra lookup for any reactions past the
core set, in which there is an indirection in the computation of the rates v[i].
Currently the index defined by the rateTerm is used directly.
In the fix, any indices beyond the coreReacs would do an indirection so as to
	point to the pool in the current system.
Furthermore, the N_ vector will need to be updated to match.

Suppose we have 4 reaction compartments, ABCD.

	    A                           D
	( M1, M2 ) <-----R4------> ( M1, M5 )
	    ^
	    |
	    |\
	    | \
	    |  \
	    R3	R3	
	    |    \
	    |     ------------------
	    |                        \
	    V                         V
 	( M1, M3, M6 ) <----R5---> ( M1, M4 )
	   B                             C
Where
	R3 = M2 <---> M3 + M4	is on A
	R4 = M2 <---> M5	is on A
	R5 = M6 <---> M4	is on B

and molecule M1 diffuses between them all.

		--------------------------------------

On compartment A we have the following ODE systems
Vol = 1e-15
N Core:	    R1 R2 R3 R4 
	M1[             ] [ v1 ]
	M2[             ] [ v2 ]
	M3[             ] [ v3 ]
	M4[             ] [ v4 ]
	M5[             ]

N A (local mols only)
	    R1 R2
	M1[       ] [ v1 ]
	M2[       ] [ v2 ]

N ABC ( For R3 )
	    R1 R2 R3
	M1[          ] [ v1 ]
	M2[          ] [ v2 ]
	M3[          ] [ v3 ]
	M4[          ]

N AD ( for R4 )
	    R1 R2 R4 
	M1[          ] [ v1 ]
	M2[          ] [ v2 ]
	M5[          ] [ v4 ]

		--------------------------------------
On compartment B we have the following ODE systems
Vol = 3e-15
N core: 
	    R6 R7 R5
	M1[          ] [ v6 ]
	M3[          ] [ v7 ]
	M6[          ] [ v5 ]
	cplx[        ]
	M4[          ]

N B (local mols only ).
	    R6 R7
	M1[       ] [ v6 ]
	M3[       ] [ v7 ]
	M6[       ]
	cplx[     ]

N BC ( for R5 )  Note that this is the same as N core.
	    R6 R7 R5
	M1[          ] [ v6 ]
	M3[          ] [ v7 ]
	M6[          ] [ v5 ]
	cplx[        ]
	M4[          ]

		--------------------------------------
On compartment C we have the following ODE systems
Vol = 5e-15
N core: This is the only system here.
	    R8
	M1[    ] [ v8 ]
	M4[    ]

		--------------------------------------
On compartment D we have the following ODE systems
Vol = 2e-15
N core: This is the only system here.
	    R9
	M1[    ] [ v9 ]
	M5[    ]


------------------------------------------------------------------------------
Will come to this later. For now, I was able to move forward in regression
tests. I created a standalone mmenz.g regression test and confirmed that it
failed. Traced this back to the setup code on ZMMenz.cpp. Fixed. Now it
clears the mmenz regression test, and also the Kholodenko test, but it
croaks further on in rtRunCspace.

Finally! Clears unit and regression tests, on a single thread only.

==========================================================================
28 Jan 2013
Dating back from 22 Dec:
List of pending things here.
* Implement cross-compartment reactions
+ Set up sim manager to handle coupled solvers
- Extend to CylMesh and NeuroMesh
- Write SpineMesh, extend to it.
- Put in stuff for SigNeur.
- Extend CubeMesh to figure out surfaces from Geom classes and bitmaps.
- Convert GssaStoich to use new zombie classes
- Derive GssaStoich from StoichPools
- Get GssaStoich to talk to each other through diffusion
- Get GssaStoich and GslStoich to talk to each other through diffusion
- SBML read and write
- Reorder KKit reader to put things more sensibly into compartments.
Also:
- Figure out pool composition and motors.


First thing: run valgrind for unit tests. It reports a bad memory access 
in the new stoich code, not surprising as I haven't cleaned it up. With 
a fix valgrind is now completely happy.
Also discovered that I hadn't checked in a fair number of new code files
arising from the recent updates. But SourceForge is currently not responding.

Fixed settings. Checkin 4384.

Ran valgrind for regression tests. Loses about 350 bytes, somewhere in 
ReadKkit buildSumTotal.

Sim manager and coupled solvers.
	For now this is easy: put GSL on every compartment by default. 
	Option to have a single solver of specified type.

in buildGSL, need to traverse through all compts to make solvers.

Set up traversal function for picking solvers upon which to build solvers.
Unit test cleared for the traversal, but not yet tested for setting up the 
solvers with it. Checkin 4388.

==========================================================================
30 Jan 2013
Put in the junction identification and adding code. Compiles, clears old
unit tests but yet to test the new code. Checkin 4389.

Next: a regression test for this. I'll set up a suitable model in kkit with
multiple compartments. I'll set up the LoadModel to load in the model without
actually completing the model building, so that script operations can fine-tune
things like meshing and setting compartments to inherit stuff. Then test out 
that the model is built and computes correctly.

==========================================================================
31 Jan 2013.
Made regressionTests/multicompt_reac.g, a kkit script implementing the
multicompartment reaction scheme of Dec 27. Let's load up and see if
the correct setup is done.
Made regression test in rtReadKkit.cpp:rtTestMultiCompartmentReaction().
This is uncovering a lot of bugs. 
Turns out all objects are being put into the first compartment.
Turns out that graphs are also made under first compartment. Not sure why.
Turns out that all objects have an 'info' object under them.
This together accounts for the large number of objects (63) in the tree.

So I need to have the kkit reader do something sensible with the objects
and compartments, such as moving objects under the compartment that owns them.
Will also need something to decide when compartments contain others. Not
so easy from kkit.

A bit stuck. Regression test fails because there are repeats in the list of
pools sent to StoichCore::allocateObjMap. I've gone through to find where,
and I have cleaned up some of the repeats, but they do remain.

==========================================================================
1 Feb 2013
Able to complete regression tests, at this point not really testing the 
multicompartment code other than loading it. I've cleaned up the interface a 
bit too, the loadmodel call can handle it so far. Earlier it needed a second
call to the SimManager to build the model.

==========================================================================
3 Feb 2013
Added snippet to show setup of kinetic solver in a model constructed
explicitly using Python. This uncovered a bug in the 'n' field assignemnt
in the solver, which I fixed. Also uncovered numerical inaccuracy in
non-solver version of the snippet, which I also fixed using smaller dt.

Starting to put in a serious unit test which checks out all sorts of
things about the created multicompt model. See rtReadKkit.cpp.
==========================================================================
4 Feb 2013
Working on the rtMultiCompartment.cpp regression test. It has been moved to a 
separate file because of all the specialised headers.
	assert( gs->ode()[0].stoich_->getNumProxyPools() == 0 )
fails, it is 3 instead. I need to clear out proxy pools in the derived stoichs.

Done it. Clears regression tests. Much more to put in the MultiCompartment test.
Checkin 4399.

==========================================================================
5 Feb 2013.
Added more detail to MultiCompartment test. It is gratifying, the system has
correctly split up the reaction system.

Next to run the calculations.

==========================================================================
8 Feb
Problem turns out to be that the StoichCore::spawn function does not
clean out the appropriate columns (reacs) out of the N_ matrix.
It should do so to match to the change in the reacs.

Working on fixing the SparseMatrix::dropColumns function.
==========================================================================
9 Feb.
Seems like I've fixed the SparseMatrix::reorderColumns. Clears unit test.

==========================================================================
10 Feb 2013
Turns out quite a bit more fixing was needed to the 
SparseMatrix::reorderColumns. More recoding, more unit tests,
now clears those and regression tests. The multicompartment reaction test
is not crashing any more, but gives the wrong result. Checkin anyway to get 
stuff on record. Checkin 4413.

Tracked down an issue: the Kb for R3 is 300 rather than 0.1.
But this is perhaps just a minor problem with the vol conversion calculations
compared to kkit. I put in a whole lot of additional quantitative tests for
parameter values, and they are ok especially reac rates in # terms where
in doubt. So it may be the solver after all.

M5 goes to 0.5 uM
M4 goes to 0.5 uM
M6 stays at 0.
M3 stays at 0
M2 goes to 0.5 uM.

This is precisely what we get if there is no communication at all between
the compartments.

Turned out the pools were indeed referring to the local-only solver, 
ode index 0. But when I hard-set them to the correct one, the 
molecules are drained out of the first compartment without actually making it
to the other compts.
Need to work out exactly what the transfer ops should be, and make sure
they happen. Need to follow process.

==========================================================================
11 Feb
Did so. Zero size vectors are being sent out from compartment A to tell the
others about the delta. As far as I could tell, there are no messages coming
from the compartments having pools participating in reactions located in other
compartments. Need to have the regression test explicitly check setup of
all the messaging and Junction stuff.
==========================================================================
12 Feb
Worked through the various steps of the setup. Turns out it is doing all OK,
but the spatial criteria for junctions are failing, which is actually 
the correct thing to do. 

One option is to make a "PointMesh" or "WellStirredMesh" (the latter is more
accurate) and have it able to diffuse and react with any other overlapping one.

Another option is to have a flag on the CubeMesh to make it more forgiving
of the overlapping cases.

Did the latter. Easier for now.

Now the cross-compt reacs occur, but rates are wrong. I've checked with
a lower dt, still doesn't help.

Simplified multicompt_reac.g model by reducing most reac rates to
nearly zero, only R3 remains what it was. Now the GENESIS and MOOSE
calculations match. From here, incrementally put back stuff. Checkin 4416.


==========================================================================
14 Feb: Put back R8 in compartment C.
After a bit of analysis, it transpires that the outcome is identical to the
run where R8 was stopped, with the exception of molecule R4 which now 
builds up to a value about 5% higher than in kkit. Higher by almost
precisely 0.45 uM to reach 0.55, actually.  0.55 is what one would get if
one took the final value of M4 = 0.1 uM when R8 is off, and equilibrated with
the 1 uM from M1C.

What happens if we start M4 out at 1 uM, and turn R8 off again?
At t= 60, M5 = 0.5; M4 = 1.1; M6=0; M3=0.1663; M2=0.2496
Here again, all are identical, except now M4 is the sum of its initial value
and the 0.1 coming from R3. Very odd.

I think we might be able to get this effect if the proxies on compartment A
were communicating changes out to B and C, but not getting any input back,
and specifically, not being zeroed out each timestep.

This predicts that I should be able to do a similar manipulation by
starting M5 and M3 out at nonzero values.
Put M5 at 5, M3 at 3. Predict should go to 5.5 and 3.1663 respectively.
Fail. M5 = 3: not predicted; M3 = 3.165 as predicted.
But M5 has M1D to exchange with at 1 uM. So 3 is actually OK for the system.

Interpretation is therefore good, that the original counterparts of the 
proxies don't send info back. Now to track it down.

Turns out that I had forgotten to set up the INIT action for the GslStoich. 
Put this in and the model runs correctly.
So let's wrap up this regression test and move on.
Added in tests for graphs, clears them. 

Added in minor test to confirm that the correct solver is selected in
each compt for doing cross-compt reactions. 
Also some cleanup of accumulating regression result files. Perhaps should
add a flag so that they are deleted unless specially requested. 
Checkin 4419.

Dating back from 22 Dec: Updated list of pending things here.
* Implement cross-compartment reactions
+ Set up sim manager to handle coupled solvers
- Regression test for calculations for cross-compartment reac-diff.
- Extend to CylMesh and NeuroMesh
- Write SpineMesh, extend to it.
- Put in stuff for SigNeur.
- Extend CubeMesh to figure out surfaces from Geom classes and bitmaps.
- Convert GssaStoich to use new zombie classes
- Derive GssaStoich from StoichPools
- Get GssaStoich to talk to each other through diffusion
- Get GssaStoich and GslStoich to talk to each other through diffusion
- SBML read and write
- Reorder KKit reader to put things more sensibly into compartments.
- Figure out pool composition and motors.


Now to do reac-diff test. Will need to set up something I can replicate in Kkit.
If I base it on the multicompt_reac.g, then the geom is something like:

                   D
		   D
		BBBA
		CCCCC

Or I could even try this in 3D. Avoid complexity for now.

==========================================================================
16 Feb.
Two steps to take here:
- Set up the code so that a volume change or remeshing rebuilds the junctions.
	If I do this then I can set up the above model after loading in the
	old multicompt_reac.g
- Adapt the reader to separate the junction setup phase from the model loading
	phase.

Info: At model reading time, the 'addJunction' command is called by the 
SimManager::buildAllComptSolvers from SimManager::build from ReadKkit::read
from Shell::doLoadModel.

The first half of SolverBase::addJunction is fine: it creates and puts basic
info into the Junction. Some of it would have to be redone if the reaction set
changed.
The second half sets up the mesh abutment and decides which pools are sent
around. The only tricky thing here is to know which junction to use.

Did the refactoring to separate out the junction configuration from the
junction creation. Minor hiccup to be resolved: I need to extract the exact 
solver pair. Unfortunately the default messaging command getNeighbours gets
the Id, not the ObjId. So I don't know which of the junctions it is.

Implemented SolverBase::updateAllJunctions which uses rather grungy
messaging code to get at the full Erefs of the junctions. Now to test.

Set up rtMultiCompartment.cpp to also try out the reac-diff code. Currently
clears setup but the test doesn't go any further.
==========================================================================
17 Feb 2013
Putting in tests for X-reac-diff.
Seems that the remeshing has failed miserably.
Incrementally worked through components of cross-reac-diff system 
multicompt_reacdiff.g as tested in 
rtMultiCompartment.cpp:rtTestMultiCompartmentReacDiff()
Clears setup part after many fixes. Now to run it and see. I will have
to make an explicit spatial discretization in kkit to check the output values.

==========================================================================
18 Feb 2013.
Trying to run it, crashes. What should I use for dt?
D = 1e-12 m^2/sec. mesh size = 10 microns.

dist = sqrt( D * t ) = 1 micron per second. So a dt of 0.1 should be fine.

stepping through calculations. The v vector is clearly building up into 
oscilations and instability. Even after I reduced dt to 0.01 sec.

I think I've found two errors in the diffusion calculations.
1. GslStoich::updateDiffusion(): vOther comes to the entire volume of the
	other compartment, rather than the volume of one of the voxels.
2. There is a sparse matrix to define the stencil for diffusion. This
	isn't right, one value is 1e-5 and the other 1.
==========================================================================
19 Feb 2013
Working on fixing up the errors. Homed in to CubeMesh::extendStencil.
This has been refactored. But its call order also needs to be fixed.

Put the code in order, not beautiful but enough to try out.
Now grinding through unit tests. A lot of those have been broken by these
changes. Currently there is an issue because coreSize is given by 
nx*ny*nz whereas m2s is given by whatever I put in it, in a test.
==========================================================================
20 Feb 2013
Lots of cruft uncovered in the Cube mesh code, including some useful unit
tests that were commented out. Basic issue is ranking of the mesh geometry,
the m2s_ vector and the s2m_ vector. Here s is space and m is mesh, and the 
two vectors map points in space to and from the mesh indices.
s2m is always meant to be the size of the enclosing cuboid. But a lot of its
entries will be EMPTY (~0) when they fall outside the included volume of the
mesh.
m2s is always meant to be just as many as included voxels.
The coreStencil should only refer to included voxels. It is a square matrix
and should be exactly m2s_.size() on each side.

Currently we don't think much about the geometry, so for now continue to 
ignore it.

Ranking in order of assignment:
s2m -> m2s ->coreStencil
When the enclosing volume is filled in or resized, we have to
rebuild s2m to include everything. Later could include geometry at this step.

When s2m is changed, we rebuild m2s and the coreStencil

When m2s is changed/assigned, we rebuild s2m and the coreStencil. But this is
discouraged as one could end up with non-contiguous geometries.


Did this and other cleanups. The unit tests go a lot further. Stuck now with  
testKsolve.cpp:1068: void testDiffusionAcrossJunctions(): 
Assertion `sj->abutMeshIndex()[0] == 30' failed.
This seems to be an issue with how the abutMeshIndices are generated.
I think expandSforDiffusion needs work here, to deal with more than one
junction.
I replaced numVoxels with pools_.size(), but this will fail if there is another
round of resizing. Need to revisit. 
But it clears unit tests. Fails verbosely on regression tests.

==========================================================================
21 Feb 2013
Added substantial additional tests to the rtTestMultiCompartmentReacDiff()
to be sure that the stencils are being set up correctly. Unfortunately
some other cleanup has led to one of the junctions not finding any pools
to send, though they are there. Trying to track. Twice I've gone through
and when single-stepping the data does seem to fill in, but the thing
fails anyway. Odd.

It loses it in checkAbut with ix = 2, 0, 1  and meshIndex = 4.

It loses it somewhere to do with setting the intersection volume, the
surface, and setIntersect.
==========================================================================
23 Feb 2013
Finally tracked this particular bug down. Turns out that the CubeMesh
decided whether it was 1, 2 or 3 dimensions, and accordingly generated a
Surface. This failed when I tried to connect sideways onto a bunch of pools,
organized like this.

                   D
		   D
		BBBA
		CCCCC

So here many of the pools would have been classified as 1-dim, even though the
reactions of interest are along y.
Anyway, with this fixed I now have the regression tests clearing and 
generating some possible reac-diff plots. Yet to quantitatively test them.
Checkin 4426.

Next to make the explicit model and compare.
==========================================================================
24 Feb 2013
Made explicit model, discretized_reacdiff.g. Time-course doesn't match. 
Continuing issue with rate scaling. I'll compare rates in MOOSe model with 
those in the discretized model.

==========================================================================
27 Feb 2013
Cleaned up the function convertConcToNumRatesUsingMesh.
Stuck in rtMultiCompartment.cpp:testMultiCompartmentReacDiff.
The Kf and Kb have been converted to correct values, but the kf and kb are
not scaled to the new mesh size.

==========================================================================
28 Feb 2013
Fixed scaling to new mesh size. Clears assertions in regression tests, but 
does not match the explicitly discretized reac-diff calculations.


Let's try with a very tiny diffusion rate to see if individual
compt reacs are all doing the same correct thing.
order in compt C seems scrambled, but values of M4 match.

Molecules in compt C
MOOSE	M4 height rank (0 lowest)	M1
4	2				0
3	3				0
2	1				1
1	0				3
0	0				2
GENESIS
4	0				2
3	0				3
2	1				1
1	3				0
0	2				0

Hm. Perhaps it is just backwards. I see, the MOOSE ordering is increasing in
x, while the GENESIS ordering was decreasing in x, for compts B and C.

Compartment B
MOOSE		M6	M1
2		1	0
1		0	2 -] Very close
0		0	1 -]

GENESIS
2		0	2 -] Very close
1		0	1 -]
0		1	0

Compartment A: single compartment, values match.

Compartment D: Values and order both match. As expected.

Files: discretized_reacdiff_lowdiff.g
multicompt_reacdiff_lowdiff.g

So, the discrepancy comes from diffusion. Next: see if diffusion
happens at all. Compare the diffusion-less plot with a diffusion-full plot
from MOOSE. 

No, doesn't match. So it isn't just that diffusion is not happening.
It is happening incorrectly.

Two options:
1. Look closely at diffusion code to be sure that multiple targets can be
handled. Specifically, 
	Y does the system make multiple proxy diffusion mols, 
		one for each direction?
	Y Are the diffusion constants OK
	Y Is the return flow of mols going to the correct object
	Y Are non-diffusive molecules left alone?
2. If this fails, then make another variant of the reacdiff model that has
	only diffusion, no reac.


Need to check GslStoichNumerics.cpp:394.
Problem that we have identical but large values of n in neighbouring voxels.
This turns out to be a problem mostly with the initial conditioins. Later
they differ.

==========================================================================
02 Mar 2013.
Next test: Decompose a reac-diff model in two ways: one with a single
compartment with a cubic mesh, another with many interlocking compartments.
Ensure that the outcome of the calculations is the same.

Really need to first scan all diffusion rates to find the subset which are
nonzero. Then use this subset only to do do the diffusion calculations.
That will reduce these calculations by 50 to 90%.

==========================================================================
03 Mar.
Turns out that the values handled by vHandleJunctionPoolDelta are wrong.
The compartment B->C vector is OK, but A->B, A->C and A->D have only the
chemical term and not the diffusion term working.

When I go back to the send stage, it turns out that there are nozero
and changing values in the S matrix, but they are identical between the
local and proxy voxels: Except for the B->C case.
Perhaps something in the indexing if there are multiple targets?
OK, turned out that I was requiring that the diffmesh had multiple entries
to do any of the diffusion calculations.
Fixed. Confirmed that diffusion terms are now passed around. But the output
still does not match the GENESIS one with explicit discretization.

==========================================================================
04 Mar 2013
Checked it in anyway, since the code nominally clears unit and regression tests
and I've done a lot of work on it. Checkin 4436.

Check if the dt is dubious. Reduced it 10x in both. No change in
Genesis outcome when dt went from 1 ms to 0.1 ms. 
Very small change in MOOSE outcome when it went from 0.1 s to 0.01 s.
So it isn't dt, it is model structure related.

Item from  28 Feb:
	- Is the return flow of mols going to the correct object?
	I can say that different fluxes are going to different objects.
	I can say that the reaction terms going from A to (B and C) are
	identical, as they should be, and to D is different, as it should be.
	From the code structure, the diffusion terms are sent off in the same
	vector as the reaction terms. 
	From previous tests, the reaction system on its own (no diffusion)
	was OK. 
	The diffTerms_ entry in all cases points to M1, as it should.
	The sendMeshIndex_ entry in all cases is the correct voxel.
	So, though I cannot vouch for the values, the flow of diffusing 
	molecules seems to be between the correct objects.
	- Are non-diffusive molecules left alone?
	As far as I can see, there is no point at which they would be updated.

With these tests, it seems that the data transfer is OK but I have not 
tested the stencil-based diffusion calculations that occur within each solver.

	To test stencil calculations:
	Y Do the stencil entries refer to the correct mesh entries?
	Y Are the scaling factors all correct? 

With these tests, the next question is whether the correct entries are being
transmitted from the remote solver to the local proxy.

==========================================================================
05 Mar 2013.
A brief hope that the issue was simply a mismatch in the diffusion term 
calculation. Turns out not.

Now let's follow the entries being transmitted. First look at what arrives.
That seems OK.
Now look at what is transmitted.
Something wrong here. This is what goes out:

A: 	M1 to B, D, C  for diffusion 			???

B: 	M3 to A for reac.
	M1 to A for diffusion, 1 voxel.
	M1 to C for diffusion, 3 voxels.		???

C: 	M5 to A for reac
	M1 to A for diff

D: 	M4 to A for reac
	M1 to A for diff
	M4 to B for reac, 3 voxels
	M1 to b for diff, 3 voxels.

What comes into the vHandleJunctionPoolNum actually looks OK.

The bad ones are not received. Looks like the junctions are made but not
connected. So, an error but not one that should cause numerical issues.

For now: Two fixes and then go on to doing the diffusion-only tests to
track this down.
Fix 1: Precompute nonzero diffusion molecules, scan only through those when
building inter-solver transfer vector
	Done. This was easy, but in due course I need to update the 
	vector of non-diffusers when diff consts change.
Fix 2: Figure out why the incorrect data is sent as described above. Fix.

Diffusion-only tests:
	1. Same system, zero out all reaction rates. Check that M1 stays flat.
	2. As in 1, but now set M1 to zero except in compartment A.

Implemented 
Test 1 confirms and partly identifies problem. All the states of M1 go down.
Some start flat and then accelerate down, others go down exponentially.

So we need to look for some process which is losing molecules.
Compt A loses them the most and fastest.

I think I got it: vOther is not the 10 micron cube. It is the original
size of the entire compartment.

Working on getting the no-reac multicompt diffusion system in place.
segvs.

==========================================================================
07 Mar 2013.
Fixed problem with vOther. The values were actually being calculated correctly,
but were just being pushed back into the existing vector. It required that the 
CubeMesh::extendedMeshEntrySize_ vector be cleaned out when rebuilding.
The no-reac multicompt diffusion model now gives a nice flat line.
Does it work with the full reacdiff model? Yes! Finally. Checkin 4437.

Now to clean up a bit. The regression test that has given so much grief should
do a proper quantitative comparison with the reference curves. Done.

Checkin 4439.

Dating back from 22 Dec: Updated list of pending things here.
* Implement cross-compartment reactions
* Set up sim manager to handle coupled solvers
* Regression test for calculations for cross-compartment reac-diff.
+ Extend to CylMesh 
+ Regression test for CylMesh
+ Extend to NeuroMesh
* Regression test for NeuroMesh
* Write SpineMesh, extend to it.
- Extended regression test for cross-compartment reac-diff with patchy compts.
- Put in stuff for SigNeur.
- Extend CubeMesh to figure out surfaces from Geom classes and bitmaps.
- Convert GssaStoich to use new zombie classes
- Derive GssaStoich from StoichPools
- Get GssaStoich to talk to each other through diffusion
- Get GssaStoich and GslStoich to talk to each other through diffusion
- SBML read and write
- Reorder KKit reader to put things more sensibly into compartments.
- Figure out pool composition and motors.


Working on CylMesh, converting to stencil, need to put in areas.
Did that too. Clears unit tests and regression tests, including one using
the CylMesh that wasn't working earlier. But I haven't yet implemented
any of the cross-solver calculations for CylMesh.
==========================================================================

12 Mar 2013
To check: Can we move the extendStencil, clearStencil etc to a base
class for all mesh-based meshes? I think they do the same thing.
Also extendedMeshEntrySize including the vector for the extendees.

Compiles.  Clears unit and regression tests.

Should make a unit test for some of the new things in CylMesh.
For that matter in CubeMesh too.
Should also do renaming: ChemMesh: ChemCompt
base class for meshed ChemCompts: MeshCompt

==========================================================================
13 Mar 2013.
Checkin 4454.

Added unit tests. Checkin 4455.

Added unit test for setting up CylMeshes end-to-end.
Next need to sort out CylMeshes  with CubeMeshes.

Renamed ChemMesh to ChemCompt. Lots of files changed. Compiles, clears tests.
Checkin 4458.

Working on MeshCompt.
NeuroMesh needs lots of fixing to use stencils properly. I thought this was
already done.

Compiled in MeshCompt class. Yet to fold into class hierarchy.

Folded into class hierarchy, so far just CubeMesh. Clears unit and regression
tests. Checkin 4460.


Clearing out cruft from ChemCompt. Found a hard-coded stencil term there,
probably obsolete now that I use the sparse matrix for it in the case of
meshes. Certainly incorrect to have in case I have a Smoldyn compt.
Somewhere a unit tests fais.


==========================================================================
14 Mar 2013.
Cleaning out use of Stencil throughout the code. A bit tricky because the old
Stoich used it, and GssaStoich depends on Stoich. Cleaning out Stoich is yet
to come.

That was a bit of a diversion. Back to core track: Put the CylMesh now on
the MeshCompt. Done. Went smoothly, clears all tests.

Now converting the NeuroMesh. Implemented buildStencil and it compiles, but
fails the unit tests.

==========================================================================
15 Mar 2013.
Some fixes later. The code clears all tests but NeuroMesh isn't really being
tested properly yet. Checkin 4465.
Next: Put in better NeuroMesh tests. 
Then: Put in the cylinder and NeuroMesh interaction with CubeMesh
Then: Regression test for NeuroMesh with a cell and chem model.
Then: SpineMesh and interaction with NeuroMesh.
Then: Regression test with combined Neuro and SpineMesh.
Then: Talk to elec model for SigNeur functionality.
==========================================================================
18 Mar 2013
Put in the first set of NeuroMesh tests, for a non-branching cell. Clears it.
Checkin 4466.
Working on second set. Here I try to do a calculation for diffusion in the
branched cell.

==========================================================================
19 Mar 2013.
Managed to get neuromesh diffusion calculation to work. Checkin 4467.

Now looking at vector algebra to find an expression that will let me find
z on a cylinder given x and y. 

Another approach is to use given xyz from a systematic scan of the CubeMesh,
and check distance from cylinder axis. Check if it is equal to radius. That may
be better.

==========================================================================
20 Mar 2013
Getting there. 
Define c = nearest point on cylinder axis.
ccube = centre of cuboid voxel.
px, py, pz: Coords of plane of intersection of cylinder with cuboid voxel.
r: Point along c-Ccube at a distance equal to cylinder radius, from c.

The key vector calculation is to find px, py, pz, and use them to 
estimate the area of the intersection of the cylinder with the voxel.
Alternatively I could find the points on each cuboid voxel axis, which are 
a distance r from the axis of the cylinder.
==========================================================================
21 Mar 2013

Simpler brute force approach, has the merit that it simultaneously does an
approximation to the surface area. Also works directly for NeuroMesh. Also
works for conics.

1. March through a series of positions on the axis of the cylinder. 
	- Do this at intervals of some fraction of dx, the CubeMesh voxel size,
	but we also want to relate to lambda along cylinder as the cylinder
	is also voxelized. dx/10 for starters. Let the interval be h. 
	- position = r = x0 + k.( x1 - x0 )
	- k goes from 0 to |x1-x0| in steps of h.
	- Could also do the caps of the cylinder in similar manner.
2. Obtain unit orthogonal vectors to the axis.
	- Take x0. Check if 1,0,0 is parallel to axis, if so, use 0,0,1.
	- Get cross product v0. Get cross product of v0 with axis, call it v1.
	- Normalize v0 and v1.
3. March through angles theta such that interval is the same distance h.
	- dtheta = h/a (in radians)., where a is radius of cylinder/conic at 
	that position along axis.
	- p = v0.cos(theta) + v1.sin(theta) + r.
4. Take each such point p and ask CubeMesh which voxel it is. 
	- Sum up # of points on each voxel to establish intersection area.
5. Would like to start with a filled cubic mesh and eliminate interior voxels
	too. Need to do ahead of the above process.
	Would also like to scale down volume of partially occluded voxels:
	This would require to monitor each voxel volume individually.

Did points 1 to 4, put in unit test. Answer is close (within a percent)
but not precisely what I expected. Checkin 4469.

New issue is that these area terms which are close but not precise, are not
really what the diffusion calculations use. Nor are they used, as far as I
can see, by the chemical reactions, though they should be.

Deeper issue: There is no way now to handle reactions between voxels with
different volumes. Or from a 2-D membrane surface.

==========================================================================
22 Mar 2013.
Fortunately this is orthogonal to the implementation of these calculations.
I'll set up the NeuroMesh to do the same calculation while I think about how
best to set up something useful.

==========================================================================
28 march 2013
NeuroMesh now has the basic code set up. Now to test and tighten up.

Test reveals all sorts of mess. Working on it.
==========================================================================
2 April 2013
Finally got first unit test on 'nearest' to work for linear NeuroMesh.
Further tests:
'nearest' on branched NeuroMesh: Doing rigorously. Also added test for
	indexToSpace.
Area on branched NeuroMesh

Then on to SpineMesh.
==========================================================================
4 April 2013
Got the more complete unit tests for 'nearest' on branched NeuroMesh to work.
Got the unit test for area calculation ( matchMeshEntries() ) on branched
NeuroMesh to work. Checkin 4474.

Thoughts for spineMesh.
Need a class of SpineEntry. Make general or make specific?
General: Handle multicompt spines, spherical head, etc. Treat as multiple
	CylBase, but have to do something special for head.
Specific: A single cylindrical spine-head. Ignore shaft except as 
	spatial position and diffusion barrier. Always assume a chem
	compt for PSD at surface of spine head.
	Derive from CylBase but need to add area for PSD/synaptic zone.
	Stencil here is trivial, no diffusion internally. Instead have the
	stencil handle diffusion via neck to dendrite, and separately to PSD.
	No diffusion between spines.
	The PSD solver will need to do the N-transfer to spine
	The dendrite solver will need to do the N-transfer to spine, as well.
	In both cases, the spine solver will do the calculations for diffusion
	and usually use stochastic (integral) calculations.
	For this to work, I need to refer to the same data in the SpineMesh
	from two different avatars, one to tell it to be the spine bulk, and
	the other for the PSD.


To construct: Update NeuroMesh::setCellPortion to extract out spine compts,
	and to track which NeuroNode they are with. This should go into an
	internal store.
	Then we need a function to return a suitable data structure to the
	SpineMesh to help set it up. Preferably by msg to ensure linkage.
	Then we need a function from the constructed SpineMesh to make a 
	PSDMesh. Preferably by msg to ensure linkage.

Minor cleanup: changed field name GslStoich::diffusionMesh_ to compartment_.
==========================================================================
5 April 2013

Working on SpineEntry class. There is one SpineEntry for every dendritic spine.
Here I'm trying to get it to matchCubeMeshEntries.
This requires a way to match the disk on top of the cylinder. CylBase needs
updating. Added this option to CylBase::matchCubeMeshEntries. Now
to compile and test.

==========================================================================
6 April 2013
Compiled without the SpineMesh stuff. Clears unit tests. Checkin 4477.
Compiled with the SpineMesh stuff. No tests yet. Checkin 4478.
Tests: 
	*Test SpineEntry setup.
	*Test creation of Spines.
	*Check location of Spines
	*Check for coupling between NeuroMesh and SpineMesh
	*Check coupling between SpineMesh and enclosing CubeMesh.

Then make PsdMesh from SpineMesh.
==========================================================================
7 April 2013
Tested and fixed up SpineEntry. MatchCubeMeshEntriesToHead and ToPSD.
Still to do matchup to NeuroMesh.
Working on it. Much additional code needed in NeuroMesh to generate the
spineList.
==========================================================================
8 April 2013
spineList message mostly done, and spineEntries are created. Yet to put in
the calculation of the parent voxel of each spine, which may be a finer
subdivision than the electrical compartment. Clears unit tests so far.

Now to fix up the 'parent' information for each spine. With that we're
clear to set up the matchSpineMeshToNeuroMesh stuff.

==========================================================================
9 April 2013.
'parent' information checked.
Coupling between NeuroMesh and SpineMesh checked. Checkin 4482.
Coupling between SpineMesh and CubeMesh checked. Checkin 4483.

Next: make a PsdMesh. Slight extension of SpineMesh where the 
	SpineEntries are duplicated. Begun on it.
Then:
	SigNeur
	Regression Test of whole mess with kkit and .p file loading
	SBML reader/writer.
	GSSA update for meshes, eliminate old code.
	GSSA interface with GSL
	SigNeurML startup.
	Smoldyn interface
	Pool composition and motors.

==========================================================================
10 April 2013
Skeleton PsdMesh done. Yet to compile or test.
==========================================================================
11 April 2013
Skeleton PsdMesh compiled. Yet to test. Checkin 4485.
Added in bare-bones test for PsdMesh. Clears. Need to add further tests.
Checkin 4486.

==========================================================================
14 April 2013
First set of unit tests for PsdMesh cleared, quite painlessly.
Second set of unit tests was nasty. Tracked down to the following ambiguity:

psd_.push_back( CylBase( *x++, *x++, *x++, 1, 0, 1 ));
In what order do the arguments get taken and incremented? Turns out not 
defined. So replaced it with the form
psd_.push_back( CylBase( *x, *(x+1), *(x+2), 1, 0, 1 )); x += 3;
With this fixed, all unit tests clear.

On to SigNeur. Most of its capabilities have been subsumed into the
various meshes and associated solvers for reacdiff. Key remaining
role is mapping of fields and objects. This was a simple linear
transform with slope and offset, plus the capability to sum over
diffusion compartments as well as over time. Adaptor.h from moose-g3.
We already have a version in the signeur directory. 

Other aspect of it is to set up the entire model, including meshes and solvers
and adaptors. Should this be in SimManager or in Shell::LoadModel?

LoadModel should do stuff from files.
SimManager should take existing model definitions in memory as objects, and 
build up the full model with solvers. It needs assorted specification fields,
many of which are already there. Also needs to know which path to use,
currently assumes a certain object tree based on self.

Some mappings needed:
- Neuro, spine and PSD details (need for each):
	Which chem model (Id?) is base of each.
	Subset of elec model to use
	Solver to use
	Timestep to use
- Adaptors
	- ( Model, object, field) pairs, 
	- input offset, output offset and scale.
- Globals: 
	- Sync timestep.
	- Neuro model (Or is it just a predefined child path?)
	- Lambda to use for main neuro model


Options:
1. Have the entire definition set up through objects and msgs, just
	parse through them to set up solver
	- Allows set up through Python
	- Any XML spec would only have to set up the objects and msgs.
	- At present I don't have flexibility for spines and PSDs. The
		meshes will just set up whatever is there in the cell defn.
2. Make a master SigNeur object with various FieldElements to set up these 
	mappings
	- Set up again through Pthyon, but would feed into the SigNeur object.
	- Likewise XML spec. 
	- Would probably end up setting up the messaging anyway, so this is
		really just another layer.
3. Make a dedicated file loader that does all this.
	- Not really an option.
	- Instead feed through 1.

So option 1 it is. 


==========================================================================
17 Apr 2013.
Next step is a test model with all elements in place. Problem is how to make
it numerically testable. For the test let's just go for getting the model
set up and behaving phenomenologically. If possible let's define the model
entirely in code. Later we should have a regression test model that matches
my Neural Networks paper.

Model:
Elec:
Single elec compt soma with HH channels. 5 spines on it. GluR on spines. Ca
	pool in spine head fed by GluR.
Chem:
	PSD: GluR pool.
	Head: Ca pool. GluR pool. 'Enz' to take to PSD. Reac balances with PSD.
	Dend: Ca pool binds to 'kinase'. Kinase phosph K chan. Dephosph by reac.
		Ca is pumped out into a buffered non-reactive pool
	
Diffusion:
	Ca from spine head goes to dend

Adaptors:
	Electrical Ca in spine head -> Ca pool in spine head
	Chem GluR in PSD -> Electrical GluR Gbar
	Chem K in soma -> Electrical K Gbar.

	
==========================================================================
18 Apr 2013. 
Building test.
Set up skeleton cell model including Ca and spiking synaptic input to spines.
Trying to plot and schedule it.

Some issues with synapses.

Finally got the system responding to snaptic input.
Now to check on the calcium.
A little tweaking and this now works too.
Onward to setting up the chemical system. After that, the various solvers.
Here I could do a relatively simple reference, just compare solver outcomes
with the regular messaging/exp Euler calculations.
Ca levels are currently unrealistic, need to scale down to account for buffer.
Checkin 4492.

==========================================================================
21 Apr 2013.
Put in the chemical portion of the SigNeur model. Had to incorporate default
return values for NeuroMesh, SpineMesh and PsdMesh to return even before the
cell is defined. Seems to go through the construction process fine.
Checkin 4493.

Doing this, it fails to scale with volume. I need to take a call about whether
to have the mesh ownership defined simply by object tree structure, so that
the first 'parent' mesh class of a pool, enz or reac is its mesh. Key
problem with this is that then changes in volume are propagated by traversal
of tree, rather than messaging.

Tried to put in one. Doesn't seem to change things.
==========================================================================
22 Apr 2013.
Tried to figure out failure of scaling. Odd, unreliable behaviour. Finally
threw valgrind at it, and there is a problem. After a fair amount of 
elimination turns out that an earlier unit test using meshes already sets 
up a memory error at simtime. 
This was in ksolve/testKsolve::testInterMeshReac().

==========================================================================
23 Apr 2013.
Finally tracked it down using assertions in the location indicated
by valgrind. abutMeshIndex has invalid values.

Much messy tracking later: 
in GslStoich::expandSforDiffusion, the abutMeshIndex is set to an invalid
value. Should check if there are any diffusion pools at all.
Made the change, now clears unit tests.

Uncommented out the SigNeur tests that had been the original problem.
Still not finding the correct numSpines at testSigNeur.cpp:270.

Should redo valgrind just to be sure that it isn't any silly memory problem.

==========================================================================
24 Apr 2013
Valgrind is happy now, till the assertion fails with numSpines. This is where
we were a couple of days ago. Now is the message going out?

The message goes out but perhaps one problem is that within the message another
goes out. The system should handle this cleanly but...
Unfortunately I can't just comment it out because another test uses it,
suprisingly without ill effect.

Try to have the request emanate from the NeuroMesh rather than the
SpineMesh.

Did this. Probably a worthwhile excercise anyway, but the problem isn't
fixed.

==========================================================================
25 Apr 2013
Basically any messages I send bomb. Let's see if a direct call hack works.

Well, it sends the info to the correct place. But the # spines returned is
still incorrect. With gdb confirmed that the entry array is correctly set
in the SpineMesh and PsdMesh.
==========================================================================
26 Apr
Horrible mess cleaning up neuroMesh, spineMesh, and PsdMesh and their
contacts and updating of pools. Finally set it up and the correct number
of assignments occur, clears unit and regression tests.

Next: Better separate out chem mesh system so I can run as a simple single
solver model (or even non-solver model) to check it works reasonably. 
To do this have dummy meshes with the right volumes but no subdivisions.

Then: Add in adaptors to the simple single solver version.

Then: Put in chem solvers

Then: Put in elec solver.

Then: regression test with equivalent model loaded from kkit model 
	definition files.

From 9 April, the following steps:
	SigNeur
	Regression Test of whole mess with kkit and .p file loading
	SBML reader/writer.
	GSSA update for meshes, eliminate old code.
	GSSA interface with GSL
	SigNeurML startup.
	Smoldyn interface
	Pool composition and motors.


Separately, thinking about redoing the messaging yet again, with the idea of
single-thread operation within each core, and just a common Charm++ or MPI
type interface between cores.
Various options for messaging: 
- Message delivery
	- Instantaneous: go through the Msg dispersal upon call
		- Avoids a data copy (except the args passing itself)
		- Breaks the semantics of messaging: 
			- Targets may change before end of cycle.
			- Execution ordering hard to figure out.
			- Off-node messages get messy to deal with
				instantaneously, if we want to keep the 
				behaviour consistent regardless of nodes.
		- Cascading messages work right away.
		- Infinite loops possible: system will hang.
	- Scheduled: Put the data into a buffer and go through on schedule.
		- This requires that a SrcFinfo be allocated for every outgoing
			msg. Or that it somehow set up an available buffer.
		- If the data are stable, could use pointers rather than buffer.
		- Cascading messages will need multiple cycles to complete.

- SrcFinfo::send
	- Send command doesn't convert args.
		- if Instantaneous: The args are now only available as separate
			variables, so we have to ask the Msg for an iterator
			and march through it. It was messy last time we did
			this, but most of the mess is now contained in the way
			Elements deal with Msgs. Different targets are always
			on different MsgFuncBinding entries, which are easy
			to iterate through.
			The iteration through the targets may be eliminated if
			we permit only 3 kinds of target mappings: one, all or 
			sparse. Then the job can be done by the 'send'
		- if Scheduled: The SrcFinfo with its internally buffered
			args is passed to the Msg, and the Msg does its
			iteration, passing the dest objIds and
			destFuncs to the SrcFinfo to operate on.
		- In both cases: SrcFinfo typecasts the OpFunc and executes 
			with the args it holds.

- SetGet::set
	- Does not convert args.
		- Does the appropriate typecasting on the destFinfo, just
			passes the args directly in.
- SetGet::get
	- Already doesn't convert arg, and directly returns value from a func.

- OpFunc/EpFunc/UpFunc
	- 'op' just executes the stored function with suitable args.

- Conv: 
	This is pretty hideous right now, but I need something like this
	for the 'serialize' class below.
	For this version I only need the conversion to and from a provided 
	buffer, rather than the various access functions used now.

- SerializeArgs:
	- Converts an arglist into a buffer. 
	- Converts a buffer back into an arglist and calls an OpFunc with it.

This version should be called brisc.

==========================================================================
27 Apr
Separated out chem, elec, mesh etc for the unit test. Clears.

==========================================================================
29 April. 
Checkin 4507. Also ran unit tests through valgrind, all clear.

==========================================================================
06 May
Working on makeChemInCubeMesh for testSigNeuro.
==========================================================================
07 May
Got skeleton of makeChemInCubeMesh to work.  Checkin 4522.

Next: Set up diffusion equivalent reacs between compts and run to be sure
that the chem system is working.
Trying, setup done but it generates many Nans.

Put in sompe printf debugging of mol and reac pars. 
Mols have a problem, haven;t found yet.
==========================================================================
11 May
Fixed up a lot of conversion errors in the SigNeur test, one reaching into
Enz. But it still doesn't work. There is a possible memory assignment
error since the kf_ of one of the reacs gets set to a substrate value.
May need to valgrind this. Did. No leaks or errors. Some other unpleasant
thing is happening.

Found one more bug, I was multiplying by the volscale factor rather than 
dividing during reinit to set kf_ and sub_ etc.. No more nans but numbers
still odd.

Further cleanup, yet more bugs found, another in the enz. Now MOOSE and
GENESIS match. This exercise has been painful but highly productive of 
bugs (fixes) for the default kinetic classes.
Checkin 4537.

Removed ChemCompt from kinetics directory. It wasn't doing anything there.
Harsha had pointed this out. Checkin 4538.

==========================================================================
12 May

A bit of rearrangement. Ran the model again with a solver. Worked.
Next: 
	+ Run elec model with solver standalone.
	+ Run combined model with adaptors, no solvers, cubeMesh
	+ Run combined model with solvers in cubeMesh
	- Run chem model with solvers in NeuroMesh including diffusion.
	- Run combined model with solvers in NeuroMesh including diffusion.
	- regression test with equivalent model loaded from kkit and
		.p model definition files.
	- regression test with NeurNetworks paper model loaded from kkit and
		.p model definition files.


Added solver to elec model. It runs but gives divergent output for Ca and 
	spineVm. Will take up this concern later.
Checkin 4539.

Set up cubeMesh model with adaptors. Turned out I had to update the Adaptor
code to handle field requests.
==========================================================================
13 May Working on putting more reasonable numbers in the adaptors.
Compiles and runs, look at adapt.c.
==========================================================================
16 May
Bugfix on array indexing for synapses, reported by Subha. Still to figure out
if it works in the PyMoose context.

Seems not to, though it is correctly reporting the indexing if we put brackets
for indexing the synapse. Tried another thing. Still doesn't work in
python.

==========================================================================
20 May
Tested the scaling by adaptors. Works. But the simulation isn't responding
to these changes.
OK, fixed: minor scheduling typo.

Now it is doing interesting stuff, but could do more.
Setting up a diffusion reaction between spine and dend.
That helps.
Issue with SynChan: Does not change when you assign Gbar. Turns out that there
is a normalization calculation that was only being done at reinit. I have now
set it to renormalize every time Gbar is altered. Now it seems to work.
Checkin 4552.

==========================================================================
21 May
Tried a couple of different dts for the electrical part of the model, just to 
see if it is reasonably robust. Unfortunately there is a fairly large 
divergence in chem properties, but not elec. This is odd because I changed the 
elec dt.

Tracked the reason for the divergence. The spine Ca levels, which are
generated by the elec model, diverge a lot. I need to check if this is a 
minor scheduling issue or something wrong with the calculations. The tau
for the caPool is 10 msec: should be quite slow and happy. Some of the 
difference late in the run may be due to longer spike bursts, but the
divergence starts before this.

==========================================================================
25 May.
Minor fiddling around with the sigNeur test, trying to use msgSrc from the
CaConc to the adaptor rather than a request from the adaptor to the CaConc.
This causes only a subtle change to the 10usec_adapt.plot results.

==========================================================================
26 May.
Tackle the problem: run a 5 usec run to see if it has converged.
Yes, converged. 20 usec isn't too bad. I don't understand why the dt has to
be so small though.

Let's now get the solvers on board.
The ksolver just worked.
The hsolver is in the right general range, but does not match the Ca
	influx profile obtained using EE and a 5 usec dt.
	I wonder if there is a problem with the adaptors.

No, turns out that the Gbar isn't being changed in the hsolver when I
alter it in the synchan object. Unclear if it works for the hhchan.
Checkin 4561.

Did some gdb tracing. Looks like the specified SynChan is indeed getting the
Gbar message and is indeed doing the right thing in its calculations.

Two possibilities come to mind: The hsolver is sending the plots for the wrong
spine, or there is a mess-up in the messages it handles.

I'm plotting the Ik of the affected SynChan in a new run.
Turns out that there is something very wrong here. It goes up to 100x of the
expected value.

- If we give only one axonal stimulus (by changing refractory period of
spikegen) then the response matches closely.

- Even at 87 ms stim interval the problems with spine Vm kickin
Added other printf deubgging. 


==========================================================================
28 May

Lowered plotdt to 1e-4.
Turns out the problem with huge Ik is actually due to wild oscillations in the
Crank-Nicolson calculations of the solver. Once these settle the value is 
actually almost identical to that of the EE method. What I don't understand is 
why these oscillate at the frequency of the plotdt.

I suspect a numerical issue here in how the C-N method talks to synchans.
Cranking back the plotdt further to 2e-5, same as simdt.

Oscillations are still hugely evident. Now let's go back to the original high
stimulus rate, to see if the oscillations explain the discrepancy.

Result: Only partially. I think the rest is a matter of subtle differences in
the basal excitability. Let's test.

Turns out the basal differences are not so subtle. I needed to pump in 
-10 nA to get the system to start out a bit like the EE model, but then it
doesn't go bursty towards the end. So the hsolve makes quite a bit of
difference here too.

In a side calculation, I ascertained that the very rapid spike of synaptic
current would indeed cause a rather drastic voltage swing even at the
slow onset of the synaptic channel opening.
Conductance:: 2e-7 mho
driving potential: 80 mV
So, Current: 160e-10 A.
Cm = 6e-13 F
I = C dV/dt, so dV/dt = 160e-10/6e-13 = 26 mV in 1 usec. Quite enough to make
the solver unhappy.

So I need to proceed on two tracks.
1. Work with Niraj to clean up solver.
	This needs for me to make a python implementation of this
	model, with all its instabilities and parameter sensitivities,
	as a good test case for the hsolver.
2. Make this model a bit more robust, so I can use the solver anyway.

Then I can go on to setting up the model in a neuronal geometry.

Checkin 4568.

Working on HsolveInstability.py
Currently about to fill in the spines.

Did that. The output is nearly identical to the EE plot from SigNeur.plot.
Now to throw in a solver.
Did. Replicates even the small discrepancy between hsolve and the EE method 
from the unit test.
Checkin 4573.

==========================================================================
29 May
Confirmed that the Ca discrepancy between solver and EE continues even at a
much smaller dt: 1 usec for the EE and 10 usec for hsolver.

working on putting in the chem signaling model in testSigNeur.py.

==========================================================================
2 June 2013
Got the test chem model to work, lots of little bugs in setup in Python.
Added a test to Shell::doCreate to avoid creating duplicate objects with
the same name.
Checkin 4578.

Next: 
	* confirm python chem model works with solver.
	* Set up adaptors, confirm python model matches unit test model.
	* remove unit test model. Too slow to have it in there.
	- tweak Python test model to run much much faster while still
		exercising multiscale features.
	- Get all solvers on board, make sure it is reasonably close
	- Get the neuroMesh version working in EE and with solvers.
	- Make regression test to compare against published multiscale model.
	- Hand over to XML model builder.

At this point the python model with adaptors matches that from the unit test.
There is still a small numerical divergence, but I won't pursue that here.
Checkin 4579.

Removed unit test runs with the models, but I do create them and check
some aspects of sizing.

==========================================================================
4 June 2013.
Fiddling with testSigNeur.py to try to get interesting multiscale effs within
1 second.
==========================================================================
5 June 2013.
Made an interesting looking testSigNeur. Generates the most complicated
patterns. Testing if it is robust to longer timesteps.

Here is the grid

Elec Dt ChemDt		Notes
1 usec	10 usec		Reference.
1 usec	100 usec	Slight cumulative offset in DendVm but shapes match 
			fine., probably due to small chem difference in kChan_p
1 usec	1msec		One extra non-continuous firing cycle in DendVm. 
			About 15% difference in dendKinase and thus kChan_p
			would account for it. toPSD differs by much less, <5%.
10 usec	10 usec		Nearly perfect match, very slight cumulative offset
			in DendVm. Less than in 1usecX100usec case.
10 usec	100 usec	Again excellent match, slight cumulative offset.
10 usec	1 msec		Essentially the same as the 1usecX1msec case.
100 usec 100 usec	Very close to reference. Some cumulative offset.
100 usec 200 usec	Reasonably close, a fair cumulative offset.
100 usec 500 usec	Adds extra non-continuous firing cycle.

Outcome of this:
The elec model is pretty robust now, EE handles it well even out to 100 usec.
The chem model isn't so clean. EE handles it out to about 200 usec. The
	qualitiative behaviour isn't too bad out to 1 msec.
The 1usecX10usec case has converged, and is a good reference also for solvers.

Setting off solvers. Ksolve is happy, hsolve not.

The discrepancy in chem stuff is mostly very small.
At spineCa differs a lot (> 10%) with reference. Thus toPSD is very different.

So this is down to the hsolver.
Let's look at init conditions

Niraj suggests trying symcompartments.

==========================================================================

9 June 2013
Much fiddling to get symcompartments to work. Several fixes needed to the
hsolve: it was odd because it only knew how to parse regular compartments,
but solved them internally as symcompartments. Still doesn't match with EE.
Checkin 4599.

Discovered a granularity in table lookup not in the solver, but in the
EE. Shows up in the Na channel. 

t	Ike-9	Vm
172	2.988	-0.07384
184	2.936	-0.07389
192	2.885	-0.07394
201	2.834	-0.07399
211	2.785	-0.07404
222	2.737	-0.07409

This is a 50e-6V step.
The stepsize in the table is precisely 150e-3/3000 = 50e-6.
Why should this happen in EE and not hsolve code? Does the hsolve interpolate?
Anyway, I've saved this version of the testSigNeur for reference.
Yes, the hsolve interpolates. Good thing too.

OK, that little discrepancy seems cleaned up by the use if useInterpolate
flag on the HHGates in the testSigNeur.py file. We still have a substantial
discrepancy in responses to the stimuli. This is presumably what Niraj
is trying to track down.

Turns out there is a substantial discrepancy just with the current injection
into a passive compartment.
Went back to Asym compartments to check if those work properly. Yes, they do,
after yet another a fix to the Hsolver set up code. Checkin 4602.

Looking at compartment Im. 
In EE, it is almost 1e-9, that is almost the entire injected current.
In hsolve, it is 3.5e-12, that is, a tiny fraction of the injected current.
The deltaV is about 0.4 V implying an input res of 400Meg. Nothing on the
list matches:
	Rm		Ra		Cm	
Head 15915494309.2 397887.35773 6.28318530718e-13 4e-06 5e-06
Shaft 1.59154943092e+11 39788735.773 6.28318530718e-14 2e-06 5e-06

There was a bug in execution order of the code, established through 
printf debugging. Fixed, some improvement in values, but not right by a long
way.

Im in head2 in EE: 0.99e-9
Im in head2 in hsolve: 3.5e-12 Same as before.
The values of Ra are all reasonable, and the biggest is 4e7.
deltaV is better, at 0.12V , which implies an input res of 120 Meg. 
The closest value here is the Ra, which is about 1/3 this.

Values in head2:
		Im	Vm	deltaV	inputRes	Notes
EE		1e-9	0.066	0.12	120e6
Hsolve		3.5e-12	-0.0137	0.043	44e6		Close to shaft Ra.


Finally I gave up trying to match the GENESIS code, and redid the MOOSE
SymCompartment calculations using a very simple bit of my own code. 
I won't be surprised if it is inaccurate at branches, but for the present
calculations it has the merit of actually working. Still some issue with
the reported value of Im which is 1e-9 here but something close to zero in
the hsolve. Won't worry about it. Checkin 4603.

Now to get back to doing the rest of the model.
Put back the ion channels. It works! The Hsolver is almost identical to the
EE version with SymCompartments. Even the Ca matches.

* Next: separate out into a snippet showing how to set up a SymCompartment
	model. We haven't had this yet.
* Then go on with the chem part of the model, and push to see where the 
	approximations go.
After than we can finally begin work on the NeuroMesh.

Implemented testHsolve.py. This reveals issues with changing dt in the
Hsolve, which Niraj should address. It is a useful little snippet anyway.
Checkin 4604.

Went on to include the chem part of the model. Unfortunately differences
persist. These have to do with the electrical part of the model, specifically,
how the potential builds up over multiple stimuli. I am inclined to suspect
the SymCompartment. 
Ran with smaller plot timestep to see if we're running into instabilities.
Seems not. Is it a SynChan issue? So let's just go on with the model.
I may need to rebuild the whole mess in GENESIS to track down.

Another clue: I reduced KchanGbar at setup, by a relatively small amount.
It led to wild firing. Perhaps the adaptor that controls this doesn't work with
the Hsolve????
Confirmed. Gbar isn't controlling anything. Home in now on this....

Tracked it down. The functions and everything were defined, but not registered
as fields on the ZombieHHChannel. Now there is a close match between
EE and solvers, but not perfect. Ca trace in particular has some differences,
(about 10% on spike peaks) and these do not seem to converge with smaller dt.

Anyway, with this we are set to go on.
Timesteps: 
1. Holding elecDt fixed at 50 usec.
ChemDt	
	1e-4 (reference)
	1e-3	Pretty close
	2e-3	Adequate. A single extra spike at start due to issues with 
		Adaptor update.
	5e-3	Ugly but OK: the very long time causes the plots to be blocky.
			Here the plotDt is finer than the adaptors or chem.
			But every Ca spike is followed, even unto the 
			amplitudes.

2. Holding chemDt fixed at 1 msec.
elecDt
	10e-6	Pretty close.
	50e-6	(reference)
	100e-6	Pretty close. But Ik has ugly oscillations at spikes.

In summary, one could probably run it at 100e-6, 5e-3, but the
trace shape is not great and the elec model is prone to oscillate. 
 Lower timesteps are fine.
Checkin 4605.

Started off on the neuroMesh version. 
==========================================================================
10 June
Half done with the neuroMesh version in testSigNeur.py.

Need to fix GslStoich/neuroMesh handling of BufPools. For now I've changed
it to a regular pool.

The setup seems to get most of the way, but it dies when running, in
StoichCore::updateRates. Various Nans and infs in there.

==========================================================================
12 June

Some additional documentation about behaviour of k1 in CplxEnzBase.

==========================================================================
13 June
There is a failure of one of the Hsolve unit tests, oops.
Tracked down, easily fixed.
==========================================================================
