/**********************************************************************
** This program is part of 'MOOSE', the
** Messaging Object Oriented Simulation Environment,
** also known as GENESIS 3 base code.
**           copyright (C) 2003-2006 Upinder S. Bhalla. and NCBS
** It is made available under the terms of the
** GNU Lesser General Public License version 2.1
** See the file COPYING.LIB for the full notice.
**********************************************************************/

#define DATA_TAG 0
#define ASYNC_TAG 1

#define ASYNC_BUF_SIZE 1000

#include <mpi.h>
#include "ParallelMsgSrc.h"
#include "ParallelFinfo.h"

class PostMaster
{
	public:
		PostMaster()
			: needsReinit_( 0 ), 
			numTicks_(0),
			comm_( MPI::COMM_WORLD )
		{
			remoteNode_ = 0;
			myNode_ = MPI::COMM_WORLD.Get_rank();
			pollFlag_ = 0;
			asyncBuf_ = new char[ASYNC_BUF_SIZE];
		}
		// Stuff here will be copied over to .h unchanged.
		void addSender( const Finfo* sender );
	author: 
		Upinder S. Bhalla, November 2006, NCBS
	description:
		Object for relaying messages between nodes. 

	field:
		readonly int myNode;

	eval_field:
		int getPollFlag() {
			return pollFlag_;
		}
		void setPollFlag( int value ) {
			pollFlag_ = value;
			while ( pollFlag_ )
				checkPendingRequests();
		}

		int getRemoteNode() {
			return remoteNode_;
		}
		void setRemoteNode( int value ) {
			remoteNode_ = value;
			asyncRequest_ = comm_.Irecv( 
				asyncBuf_, ASYNC_BUF_SIZE, MPI_CHAR,
				remoteNode_, ASYNC_TAG );
			cout << "Making initial asyncReq on node " << myNode_ << endl;
		}

	shared:
		single process( processIn, reinitIn );
		single remoteCommand( remoteCommandIn, remoteCommandOut, addOutgoingIn, addIncomingIn );
		multi parProcess( asyncIn, postIrecvIn, postSendIn, pollRecvIn, pollRecvOut );
	src:
		multi src(); // The data type is generic for any size data
		// Here we need the help of the target Ftype to 
		// generate the correct call for the recvFunc. It looks like
		// char* rfuncAdapter( char* data ) {
		// 	rfunc<T>( *( static_cast< T* >(data ) );
		//  return data + sizeof( T );
		// }
		// When we get the target finfo from respondToAdd we grab
		// this function from the Ftype.

		// This is used for asynchrnous calls between shells on 
		// different nodes. Especially needed for message setup.
		single remoteCommand( string );

		// Notifies originating parTick that recv is complete.
		multi pollRecv();
		
	dest:
		solve dest( );	// The data type again. Here we also need
			// the index info that the solve messages give so that we
			// can look up the location of the data in memory.
			// The recvfunc looks like
			// void PostRecvFunc<T>( Conn* c, T v ) {
			//////////////////////////////////////////////////////////
			//		long i = static_cast< SolverConn* >( c )->index(); 
			//		T* ptr = static_cast< T* >(
			//		static_cast< PostMasterWrapper* >( c->parent() )->
			//       	destPtr( i ) );
			// // This section won't work unless all Finfos know about
			// // PostMasters. So instead we put in a global function:
			//////////////////////////////////////////////////////////
			//		T* ptr = static_cast< T* >( getPostPtr( c ) );
			//		*ptr = v;
			// And more as needed if there are more arguments.
			// }
			// The getPostPtr( c ) does the typecasting of c->parent
			// to postmaster, and then calls the local getPostPtr( c )
			// function as listed below.
			// There is an issue here: Not all recvfuncs need to
			// be executed each timestep. So the PostRecvFunc should
			// also be setting a flag to indicate which to update.
			// This flag is only needed in those rare cases where
			// a scheduled trigger cannot be relied on.
			// To keep it invisible, this flag is handled by the 
			// getPostPtr( c ) call itself.
			//
			// Need a special respondToAdd which allocates the
			// space needed by the dest, and preferably also
			// figures out how to sched it. Uses the sender finfos
			// Ftype::size() (yet to be implemented).

		multi async( int tick ) {
			checkPendingRequests();
		}

			// This handles substage 0: Posting irecv. 
		multi postIrecv( int tick ) {
			request_ = comm_.Irecv(
				&(incoming_.front()),
				incomingEntireSize_[ tick ], 
				MPI_CHAR, remoteNode_, DATA_TAG );
		}

			// This handles substage 2: Posting send.
		multi postSend( int tick ) {
			unsigned long i = static_cast< unsigned long >( tick );
			if ( outgoingEntireSize_.size() > i && 
				outgoingEntireSize_[ tick ] > 0 ) { 
				comm_.Send(
					&(outgoing_.front()),
					outgoingEntireSize_[ tick ], 
					MPI_CHAR,
					remoteNode_,
					DATA_TAG
				);
			}
		}

			// This handles substage 4: Poll for irecv return.
			// It probes for arrival of a previously requested irecv.
			// If it has come, completes the recv, disperses the data,
			// and notifies the parent postmaster.
			// Otherwise yield.
		multi pollRecv( int tick ) {
			// This is a non-blocking call
			if ( request_ == 0 || incomingEntireSize_[ tick ] == 0 ) {
				pollRecvSrc_.sendTo( tick );
				return;
			}
			if ( request_.Test() ) {	// Data has arrived
				srcSrc_.send( &( incoming_.front() ), incomingSchedule_, tick );
				// Tell sending tick to increment # of completed recvs.
				pollRecvSrc_.sendTo( tick );
				request_ = 0;
			}
		}

		single process( ProcInfo info ) {
			// This deals with variable size messages like action 
			// potentials and shell commands.
			checkPendingRequests();
		}

		single reinit() {
			checkPendingRequests();
			if ( needsReinit_ ) {
				vector< unsigned long > segments( 1, 0 );
				unsigned long nMsgs = outgoingSize_.size();
				unsigned int offset = 0;
				segments[0] = nMsgs;
				outgoingOffset_.resize( nMsgs );
				for (unsigned long i = 0; i < nMsgs; i++ ) {
					outgoingOffset_[i] = offset;
					offset += outgoingSize_[i];
				}
				outgoing_.resize( offset );
				//destInConn_.resize( segments );
				// processInConn_.resize( segments );
				// informTargetNode(); // Sends out schedule info.
				// assignIncomingSchedule();
				needsReinit_ = 0;
			}
		}

		single remoteCommand( string data ) {
			checkPendingRequests();
			comm_.Send(
				data.c_str(),
				data.length(),
				MPI_CHAR,
				remoteNode_,
				ASYNC_TAG
			);
		}

		single addOutgoing( Field src, int tick, int size ) {
			unsigned int currSize = outgoingEntireSize_[tick];
			outgoingOffset_.push_back( currSize );
			outgoingSchedule_.push_back( tick );
			outgoingSize_.push_back( size );
		
			currSize += size;
			outgoingEntireSize_[tick] = currSize;
			if ( outgoing_.size() < currSize )
				outgoing_.resize( currSize );
		
			Field me( this, "destIn" );
			src.add( me );
		}
		
		single addIncoming( Field dest, int tick, int size) {
			unsigned int currSize = incomingEntireSize_[tick];
			incomingSchedule_.push_back( tick );
			incomingSize_.push_back( size );
		
			currSize += size;
			incomingEntireSize_[tick] = currSize;
			if ( incoming_.size() < currSize )
				incoming_.resize( currSize );
		
			Field me( this, "srcOut" );
			me.add( dest );
		}

	private:

		int remoteNode_;
		bool needsReinit_;
		unsigned long numTicks_;
		int pollFlag_;
		MPI::Comm& comm_;
		MPI::Request request_;
		MPI::Request asyncRequest_;
		MPI::Status status_;
	
		// Looks up the offset of the outgoing_ array to fill when the
		// local object wants to dump a value. 
		// This is filled after all the messages have been set up,
		// so that we can order the incoming data into the correct
		// locations for a single big isend.
		vector< unsigned int > outgoingOffset_;

		// Each outgoing message is sent on a specific clock tick.
		vector< unsigned int > outgoingSchedule_;

		// To build the target in memory, we also need a vector of sizes
		// In principle we could do this by having the next destOffset
		// stored rather than the current one. But it is messy if the
		// offsets are not sequential.
		vector< unsigned int > outgoingSize_;

		// Have a single big vector of chars so that the index
		// goes straight to the correct place. Each clock tick
		// fills in stuff starting at the beginning of this vector.
		vector< char > outgoing_;
		
		// This final vector is so we know how much data to send on
		// each clock tick.
		vector< unsigned int > outgoingEntireSize_;

		// When does each incoming message get called
		vector< unsigned int > incomingSchedule_;
		// Whater are the sizes of each message.
		vector< unsigned int > incomingSize_;
		// How big are the entire buffers arriving on each tick
		vector< unsigned int > incomingEntireSize_;
		// A buffer for the biggest of these arriving buffers.
		vector< char > incoming_;
		// A buffer for asynchronous incoming messages
		char* asyncBuf_;

	private_wrapper:
		char* getPostPtr( unsigned long index );
		// void informTargetNode();
		void connectTick( Element* tick );
		bool callsMe( Element* tickElm );
		bool connect( const string& target );
		void checkPendingRequests();
		// void assignIncomingSizes();
		// void assignIncomingSchedule();
};

////////////////////////////////////////////////////////////////////
// Stuff below here goes verbatim into PostMasterWrapper.cpp
////////////////////////////////////////////////////////////////////

wrapper_cpp:

///////////////////////////////////////////////////
// Constructor. As a hack, at this time I will allocate
// space for messages in destInConn_ and processInConn_
// Later we need a way to do this without preallocation.
///////////////////////////////////////////////////

char* PostMasterWrapper::getPostPtr( unsigned long index )
{
	static char dummy[1024]; 
	if ( index < outgoingOffset_.size() )
		return &outgoing_[ outgoingOffset_[index] ];
	cerr << "Error: bad index " << index << 
	" for postmaster " << name() << "\n";
	return dummy;
}

char* getPostPtr( Conn* c )
{
	static char dummy[1024]; 
	PostMasterWrapper* pm = 
		dynamic_cast< PostMasterWrapper* >( c->parent() );
	if ( pm ) {
		SolverConn* sc = dynamic_cast< SolverConn* >( c );
		if ( sc )
			return pm->getPostPtr( sc->index() );
	}
	return dummy;
}

///////////////////////////////////////////////////

void PostMaster::addSender( const Finfo* sender )
{
	outgoingSize_.push_back( sender->ftype()->size() );
	needsReinit_ = 1;
}

bool PostMasterWrapper::callsMe( Element* tickElm )
{
	Field me( this, "post" );
	Field tick( tickElm, "process" );
	vector< Field > list;
	me.dest( list );
	if ( list.size() == 0 )
		return 0;
	return ( std::find( list.begin(), list.end(), tick ) != list.end() );
}


// Check if any request is pending. Service it, then issue a new irecv.
// Idea is that we should have an irecv waiting at all times to
// avoid deadlock.
// Issue is that the size of an async request is not known. Have to
// set aside a big enough buffer and to guarantee that we never send
// anything bigger than this buffer.
// This should ideally use a single buffer per node rather than a
// buffer on each for each other node. Later.
void PostMasterWrapper::checkPendingRequests()
{
	// cout << "in PostMasterWrapper::checkPendingRequests on node " << 
			// myNode_ << ", for node " << remoteNode_ << endl;

	if ( asyncRequest_.Test( status_ ) ) {
		int size = status_.Get_count( MPI_CHAR );
		cout << "irecv'ed size " << size << " on node " <<
				myNode_ << " from node " << remoteNode_ << endl;
		if ( size < ASYNC_BUF_SIZE )
			asyncBuf_[size] = '\0';
		string temp( asyncBuf_ );
		// Post the next irecv before anything else, because we 
		// might end up issuing a send as part of the command.
		asyncRequest_ = comm_.Irecv( 
			asyncBuf_, ASYNC_BUF_SIZE, MPI_CHAR, remoteNode_, ASYNC_TAG );

		// Issue the command to the shell
		remoteCommandSrc_.send( temp );
	}

	//cout << "done PostMasterWrapper::checkPendingRequests on node " <<
	//		myNode_ << ", for node " << remoteNode_ << endl;
}

